Feed forward neural network with a   hidden layer can approximate continuous functions
This article may be too technical for most readers to understand  Please help improve it to make it understandable to non experts  without removing the technical details    July        Learn how and when to remove this message 
In the mathematical theory of artificial neural networks  universal approximation theorems are theorems                       of the following form  Given a family of neural networks  for each function 
  
    
      
        f
      
    
      displaystyle f 
  
 from a certain function space  there exists a sequence of neural networks 
  
    
      
        
            x d  
          
             
          
        
         
        
            x d  
          
             
          
        
         
          x     
      
    
      displaystyle  phi       phi       dots  
  
 from the family  such that 
  
    
      
        
            x d  
          
            n
          
        
          x     
        f
      
    
      displaystyle  phi   n  to f 
  
 according to some criterion  That is  the family of neural networks is dense in the function space 
The most popular version states that feedforward networks with non polynomial activation functions are dense in the space of continuous functions between two Euclidean spaces  with respect to the compact convergence topology 
Universal approximation theorems are existence theorems  They simply state that there exists such a sequence 
  
    
      
        
            x d  
          
             
          
        
         
        
            x d  
          
             
          
        
         
          x  ef 
          x     
        f
      
    
      displaystyle  phi       phi       dots  to f 
  
  and do not provide any way to actually find such a sequence  They also do not guarantee any method  such as backpropagation  might actually find such a sequence  Any method for searching the space of neural networks  including backpropagation  might find a converging sequence  or not  i e  the backpropagation might get stuck in a local optimum   
Universal approximation theorems are limit theorems  They simply state that for any 
  
    
      
        f
      
    
      displaystyle f 
  
 and a criterion of closeness 
  
    
      
          x f  
         gt 
         
      
    
      displaystyle  epsilon  gt   
  
  if there are enough neurons in a neural network  then there exists a neural network with that many neurons that does approximate 
  
    
      
        f
      
    
      displaystyle f 
  
 to within 
  
    
      
          x f  
      
    
      displaystyle  epsilon  
  
  There is no guarantee that any finite size  say        neurons  is enough 


Setup edit 
Artificial neural networks are combinations of multiple simple mathematical functions that implement more complicated functions from  typically  real valued vectors to real valued vectors   The spaces of multivariate functions that can be implemented by a network are determined by the structure of the network  the set of simple functions  and its multiplicative parameters  A great deal of theoretical work has gone into characterizing these function spaces 
Most universal approximation theorems are in one of two classes  The first quantifies the approximation capabilities of neural networks with an arbitrary number of artificial neurons   arbitrary width  case  and the second focuses on the case with an arbitrary number of hidden layers  each containing a limited number of artificial neurons   arbitrary depth  case   In addition to these two classes  there are also universal approximation theorems for neural networks with bounded number of hidden layers and a limited number of neurons in each layer   bounded depth and bounded width  case  

History edit 
Arbitrary width edit 
The first examples were the arbitrary width case  George Cybenko in      proved it for sigmoid activation functions             Kurt Hornik           de       Maxwell Stinchcombe  and Halbert White showed in      that multilayer feed forward networks with as few as one hidden layer are universal approximators             Hornik also showed in                 that it is not the specific choice of the activation function but rather the multilayer feed forward architecture itself that gives neural networks the potential of being universal approximators  Moshe Leshno et al in                 and later Allan Pinkus in                 showed that the universal approximation property is equivalent to having a nonpolynomial activation function 

Arbitrary depth edit 
The arbitrary depth case was also studied by a number of authors such as Gustaf Gripenberg in                  Dmitry Yarotsky             Zhou Lu et al in                  Boris Hanin and Mark Sellke in                  who focused on neural networks with ReLU activation function   In       Patrick Kidger and Terry Lyons             extended those results to neural networks with general activation functions such  e g  tanh or GeLU 
One special case of arbitrary depth is that each composition component comes from a finite set of mappings  In       Cai              constructed a finite set of mappings  named a vocabulary  such that any continuous function can be approximated by compositing a sequence from the vocabulary  This is similar to the concept of compositionality in linguistics  which is the idea that a finite vocabulary of basic elements can be combined via grammar to express an infinite range of meanings 

Bounded depth and bounded width edit 
The bounded depth and bounded width case was first studied by Maiorov and Pinkus in                   They showed that there exists an analytic sigmoidal activation function such that two hidden layer neural networks with bounded number of units in hidden layers are universal approximators 
In       Guliyev and Ismailov             constructed a smooth sigmoidal activation function providing universal approximation property for two hidden layer feedforward neural networks with less units in hidden layers  In       they also constructed             single hidden layer networks with bounded width that are still universal approximators for univariate functions  However  this does not apply for multivariable functions 
In       Shen et al              obtained precise quantitative information on the depth and width required to approximate a target function by deep and wide ReLU neural networks 

Quantitative bounds edit 
The question of minimal possible width for universality was first studied in       Park et al obtained the minimum width required for the universal approximation of Lp functions using feed forward neural networks with ReLU as activation functions              Similar results that can be directly applied to residual neural networks were also obtained in the same year by Paulo Tabuada and Bahman Gharesifard using control theoretic arguments                          In       Cai obtained the optimal minimum width bound for the universal approximation             
For the arbitrary depth case  Leonie Papon and Anastasis Kratsios derived explicit depth estimates depending on the regularity of the target function and of the activation function             

Kolmogorov network edit 
The Kolmogorov Arnold representation theorem is similar in spirit  Indeed  certain neural network families can directly apply the Kolmogorov Arnold theorem to yield a universal approximation theorem  Robert Hecht Nielsen showed that a three layer neural network can approximate any continuous multivariate function              This was extended to the discontinuous case by Vugar Ismailov              In       Ziming Liu and co authors showed a practical application             

Reservoir computing and quantum reservoir computing edit 
In reservoir computing a sparse recurrent neural network with fixed weights equipped of fading memory and echo state property is followed by a trainable output layer  Its universality has been demonstrated separately for what concerns networks of rate neurons              and spiking neurons  respectively               In       the framework has been generalized and extended to quantum reservoirs where the reservoir is based on qubits defined over Hilbert spaces              

Variants edit 
Discontinuous activation functions             noncompact domains                          certifiable networks              
random neural networks              and alternative network architectures and topologies                         
The universal approximation property of width bounded networks has been studied as a dual of classical universal approximation results on depth bounded networks  For input dimension dx and output dimension dy the minimum width required for the universal approximation of the Lp functions is exactly max dx      dy   for a ReLU network    More generally this also holds if both ReLU and a threshold activation function are used             
Universal function approximation on graphs  or rather on graph isomorphism classes  by popular graph convolutional neural networks  GCNs or GNNs  can be made as discriminative as the Weisfeiler Leman graph isomorphism test              In                   a universal approximation theorem result was established by Br el Gabrielsson  showing that graph representation with certain injective properties is sufficient for universal function approximation on bounded graphs and restricted universal function approximation on unbounded graphs  with an accompanying 
  
    
      
        
          
            O
          
        
         
        
           
          V
           
        
          x  c  
        
           
          E
           
        
         
      
    
      displaystyle   mathcal  O    left V right  cdot  left E right   
  
 runtime method that performed at state of the art on a collection of benchmarks  where 
  
    
      
        V
      
    
      displaystyle V 
  
 and 
  
    
      
        E
      
    
      displaystyle E 
  
 are the sets of nodes and edges of the graph respectively  
There are also a variety of results between non Euclidean spaces             and other commonly used architectures and  more generally  algorithmically generated sets of functions  such as the convolutional neural network  CNN  architecture                          radial basis functions              or neural networks with specific properties                         

Arbitrary width case edit 
A spate of papers in the     s     s  from George Cybenko and Kurt Hornik           de      etc  established several universal approximation theorems for arbitrary width and bounded depth                                                See                                    for reviews  The following is the most often quoted 
Universal approximation theorem Let 
  
    
      
        C
         
        X
         
        
          
            R
          
          
            m
          
        
         
      
    
      displaystyle C X  mathbb  R    m   
  
 denote the set of continuous functions from a subset 
  
    
      
        X
      
    
      displaystyle X 
  
 of a Euclidean 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
      displaystyle  mathbb  R    n  
  
 space to a Euclidean space 
  
    
      
        
          
            R
          
          
            m
          
        
      
    
      displaystyle  mathbb  R    m  
  
  Let 
  
    
      
          x c  
          x     
        C
         
        
          R
        
         
        
          R
        
         
      
    
      displaystyle  sigma  in C  mathbb  R    mathbb  R    
  
  Note that 
  
    
      
         
          x c  
          x     
        x
        
           
          
            i
          
        
         
          x c  
         
        
          x
          
            i
          
        
         
      
    
      displaystyle   sigma  circ x   i   sigma  x  i   
  
  so 
  
    
      
          x c  
          x     
        x
      
    
      displaystyle  sigma  circ x 
  
 denotes 
  
    
      
          x c  
      
    
      displaystyle  sigma  
  
 applied to each component of 
  
    
      
        x
      
    
      displaystyle x 
  
 
Then 
  
    
      
          x c  
      
    
      displaystyle  sigma  
  
 is not polynomial if and only if for every 
  
    
      
        n
          x     
        
          N
        
      
    
      displaystyle n in  mathbb  N   
  
  
  
    
      
        m
          x     
        
          N
        
      
    
      displaystyle m in  mathbb  N   
  
  compact 
  
    
      
        K
          x     
        
          
            R
          
          
            n
          
        
      
    
      displaystyle K subseteq  mathbb  R    n  
  
  
  
    
      
        f
          x     
        C
         
        K
         
        
          
            R
          
          
            m
          
        
         
         
          x b  
         gt 
         
      
    
      displaystyle f in C K  mathbb  R    m    varepsilon  gt   
  
 there exist 
  
    
      
        k
          x     
        
          N
        
      
    
      displaystyle k in  mathbb  N   
  
  
  
    
      
        A
          x     
        
          
            R
          
          
            k
              xd  
            n
          
        
      
    
      displaystyle A in  mathbb  R    k times n  
  
  
  
    
      
        b
          x     
        
          
            R
          
          
            k
          
        
      
    
      displaystyle b in  mathbb  R    k  
  
  
  
    
      
        C
          x     
        
          
            R
          
          
            m
              xd  
            k
          
        
      
    
      displaystyle C in  mathbb  R    m times k  
  
 such that

  
    
      
        
          sup
          
            x
              x     
            K
          
        
          x     
        f
         
        x
         
          x     
        g
         
        x
         
          x     
         lt 
          x b  
      
    
      displaystyle  sup   x in K   f x  g x    lt  varepsilon  
  

where

  
    
      
        g
         
        x
         
         
        C
          x  c  
         
          x c  
          x     
         
        A
          x  c  
        x
         
        b
         
         
      
    
      displaystyle g x  C cdot   sigma  circ  A cdot x b   
  



Also  certain non continuous activation functions can be used to approximate a sigmoid function  which then allows the above theorem to apply to those functions  For example  the step function works  In particular  this shows that a perceptron network with a single infinitely wide hidden layer can approximate arbitrary functions 
Such an 
  
    
      
        f
      
    
      displaystyle f 
  
 can also be approximated by a network of greater depth by using the same construction for the first layer and approximating the identity function with later layers 

Proof sketch
It suffices to prove the case where 
  
    
      
        m
         
         
      
    
      displaystyle m   
  
  since uniform convergence in 
  
    
      
        
          
            R
          
          
            m
          
        
      
    
      displaystyle  mathbb  R    m  
  
 is just uniform convergence in each coordinate 
Let 
  
    
      
        
          F
          
              x c  
          
        
      
    
      displaystyle F   sigma   
  
 be the set of all one hidden layer neural networks constructed with 
  
    
      
          x c  
      
    
      displaystyle  sigma  
  
  Let 
  
    
      
        
          C
          
             
          
        
         
        
          
            R
          
          
            d
          
        
         
        
          R
        
         
      
    
      displaystyle C      mathbb  R    d   mathbb  R    
  
 be the set of all 
  
    
      
        C
         
        
          
            R
          
          
            d
          
        
         
        
          R
        
         
      
    
      displaystyle C  mathbb  R    d   mathbb  R    
  
 with compact support 
If the function is a polynomial of degree 
  
    
      
        d
      
    
      displaystyle d 
  
  then 
  
    
      
        
          F
          
              x c  
          
        
      
    
      displaystyle F   sigma   
  
 is contained in the closed subspace of all polynomials of degree 
  
    
      
        d
      
    
      displaystyle d 
  
  so its closure is also contained in it  which is not all of 
  
    
      
        
          C
          
             
          
        
         
        
          
            R
          
          
            d
          
        
         
        
          R
        
         
      
    
      displaystyle C      mathbb  R    d   mathbb  R    
  
 
Otherwise  we show that 
  
    
      
        
          F
          
              x c  
          
        
      
    
      displaystyle F   sigma   
  
 s closure is all of 
  
    
      
        
          C
          
             
          
        
         
        
          
            R
          
          
            d
          
        
         
        
          R
        
         
      
    
      displaystyle C      mathbb  R    d   mathbb  R    
  
  Suppose we can construct arbitrarily good approximations of the ramp function 

  
    
      
        r
         
        x
         
         
        
          
             
            
              
                
                    x     
                   
                
                
                  
                    if  xa  
                  
                  x
                   lt 
                    x     
                   
                
              
              
                
                  
                    
                      
                         
                      
                    
                  
                  x
                
                
                  
                    if  xa  
                  
                  
                     
                  
                  x
                  
                     
                  
                    x     
                   
                
              
              
                
                  
                    
                      
                         
                      
                    
                  
                   
                
                
                  
                    if  xa  
                  
                  x
                   gt 
                   
                
              
            
            
          
        
      
    
      displaystyle r x    begin cases    amp   text if   x lt       phantom     x amp   text if    x  leq      phantom       amp   text if   x gt     end cases   
  

then it can be combined to construct arbitrary compactly supported continuous function to arbitrary precision  It remains to approximate the ramp function 
Any of the commonly used activation functions used in machine learning can obviously be used to approximate the ramp function  or first approximate the ReLU  then the ramp function 
if 
  
    
      
          x c  
      
    
      displaystyle  sigma  
  
 is  squashing   that is  it has limits 
  
    
      
          x c  
         
          x     
          x   e 
         
         lt 
          x c  
         
         
          x   e 
         
      
    
      displaystyle  sigma    infty   lt  sigma    infty   
  
  then one can first affinely scale down its x axis so that its graph looks like a step function with two sharp  overshoots   then make a linear sum of enough of them to make a  staircase  approximation of the ramp function  With more steps of the staircase  the overshoots smooth out and we get arbitrarily good approximation of the ramp function 
The case where 
  
    
      
          x c  
      
    
      displaystyle  sigma  
  
 is a generic non polynomial function is harder  and the reader is directed to            


The above proof has not specified how one might use a ramp function to approximate arbitrary functions in 
  
    
      
        
          C
          
             
          
        
         
        
          
            R
          
          
            n
          
        
         
        
          R
        
         
      
    
      displaystyle C      mathbb  R    n   mathbb  R    
  
  A sketch of the proof is that one can first construct flat bump functions  intersect them to obtain spherical bump functions that approximate the Dirac delta function  then use those to approximate arbitrary functions in 
  
    
      
        
          C
          
             
          
        
         
        
          
            R
          
          
            n
          
        
         
        
          R
        
         
      
    
      displaystyle C      mathbb  R    n   mathbb  R    
  
              The original proofs  such as the one by Cybenko  use methods from functional analysis  including the Hahn Banach and Riesz Markov Kakutani representation theorems  Cybenko first published the theorem in a technical report in                   then as a paper in                  
Notice also that the neural network is only required to approximate within a compact set 
  
    
      
        K
      
    
      displaystyle K 
  
  The proof does not describe how the function would be extrapolated outside of the region  
The problem with polynomials may be removed by allowing the outputs of the hidden layers to be multiplied together  the  pi sigma networks    yielding the generalization             


Universal approximation theorem for pi sigma networks With any nonconstant activation function  a one hidden layer pi sigma network is a universal approximator 


Arbitrary depth case edit 
The  dual  versions of the theorem consider networks of bounded width and arbitrary depth  A variant of the universal approximation theorem was proved for the arbitrary depth case by Zhou Lu et al  in                   They showed that networks of width n               with ReLU activation functions can approximate any Lebesgue integrable function on n dimensional input space with respect to 
  
    
      
        
          L
          
             
          
        
      
    
      displaystyle L     
  
 distance if network depth is allowed to grow  It was also shown that if the width was less than or equal to n  this general expressive power to approximate any Lebesgue integrable function was lost  In the same paper            it was shown that ReLU networks with width n               were sufficient to approximate any continuous function of n dimensional input variables               The following refinement  specifies the optimal minimum width for which such an approximation is possible and is due to             


Universal approximation theorem  L  distance  ReLU activation  arbitrary depth  minimal width  For any Bochner Lebesgue p integrable function 
  
    
      
        f
         
        
          
            R
          
          
            n
          
        
          x     
        
          
            R
          
          
            m
          
        
      
    
      displaystyle f  mathbb  R    n  to  mathbb  R    m  
  
 and any 
  
    
      
          x b  
         gt 
         
      
    
      displaystyle  varepsilon  gt   
  
  there exists a fully connected ReLU network 
  
    
      
        F
      
    
      displaystyle F 
  
 of width exactly 
  
    
      
        
          d
          
            m
          
        
         
        max
         
        n
         
         
         
        m
         
      
    
      displaystyle d  m   max  n   m   
  
  satisfying

  
    
      
        
            x   b 
          
            
              
                R
              
              
                n
              
            
          
        
          x     
        f
         
        x
         
          x     
        F
         
        x
         
        
            x     
          
            p
          
        
        
        
          d
        
        x
         lt 
          x b  
         
      
    
      displaystyle  int    mathbb  R    n    f x  F x     p    mathrm  d  x lt  varepsilon   
  

Moreover  there exists a function 
  
    
      
        f
          x     
        
          L
          
            p
          
        
         
        
          
            R
          
          
            n
          
        
         
        
          
            R
          
          
            m
          
        
         
      
    
      displaystyle f in L  p   mathbb  R    n   mathbb  R    m   
  
 and some 
  
    
      
          x b  
         gt 
         
      
    
      displaystyle  varepsilon  gt   
  
  for which there is no fully connected ReLU network of width less than 
  
    
      
        
          d
          
            m
          
        
         
        max
         
        n
         
         
         
        m
         
      
    
      displaystyle d  m   max  n   m   
  
 satisfying the above approximation bound 
Remark  If the activation is replaced by leaky ReLU  and the input is restricted in a compact domain  then the exact minimum width is             
  
    
      
        
          d
          
            m
          
        
         
        max
         
        n
         
        m
         
         
         
      
    
      displaystyle d  m   max  n m     
  
 
Quantitative refinement  In the case where 
  
    
      
        f
         
         
         
         
         
        
           
          
            n
          
        
          x     
        
          R
        
      
    
      displaystyle f        n  rightarrow  mathbb  R   
  
   i e  
  
    
      
        m
         
         
      
    
      displaystyle m   
  
  and 
  
    
      
          x c  
      
    
      displaystyle  sigma  
  
 is the ReLU activation function  the exact depth and width for a ReLU network to achieve 
  
    
      
          x b  
      
    
      displaystyle  varepsilon  
  
 error is also known               If  moreover  the target function 
  
    
      
        f
      
    
      displaystyle f 
  
 is smooth  then the required number of layer and their width can be exponentially smaller               Even if 
  
    
      
        f
      
    
      displaystyle f 
  
 is not smooth  the curse of dimensionality can be broken if 
  
    
      
        f
      
    
      displaystyle f 
  
 admits additional  compositional structure                          


Together  the central result of             yields the following universal approximation theorem for networks with bounded width  see also            for the first result of this kind  


Universal approximation theorem  Uniform non affine activation  arbitrary depth  constrained width   Let 
  
    
      
        
          
            X
          
        
      
    
      displaystyle   mathcal  X   
  
 be a compact subset of 
  
    
      
        
          
            R
          
          
            d
          
        
      
    
      displaystyle  mathbb  R    d  
  
   Let 
  
    
      
          x c  
         
        
          R
        
          x     
        
          R
        
      
    
      displaystyle  sigma   mathbb  R   to  mathbb  R   
  
 be any non affine continuous function which is continuously differentiable at at least one point  with nonzero derivative at that point  Let 
  
    
      
        
          
            
              N
            
          
          
            d
             
            D
             
            d
             
            D
             
             
          
          
              x c  
          
        
      
    
      displaystyle   mathcal  N    d D d D      sigma   
  
 denote the space of feed forward neural networks with 
  
    
      
        d
      
    
      displaystyle d 
  
 input neurons  
  
    
      
        D
      
    
      displaystyle D 
  
 output neurons  and an arbitrary number of hidden layers each with 
  
    
      
        d
         
        D
         
         
      
    
      displaystyle d D   
  
 neurons  such that every hidden neuron has activation function 
  
    
      
          x c  
      
    
      displaystyle  sigma  
  
 and every output neuron has the identity as its activation function  with input layer 
  
    
      
          x d  
      
    
      displaystyle  phi  
  
 and output layer 
  
    
      
          x c  
      
    
      displaystyle  rho  
  
  Then given any 
  
    
      
          x b  
         gt 
         
      
    
      displaystyle  varepsilon  gt   
  
 and any 
  
    
      
        f
          x     
        C
         
        
          
            X
          
        
         
        
          
            R
          
          
            D
          
        
         
      
    
      displaystyle f in C   mathcal  X    mathbb  R    D   
  
  there exists 
  
    
      
        
          
            
              f
                x e 
            
          
        
          x     
        
          
            
              N
            
          
          
            d
             
            D
             
            d
             
            D
             
             
          
          
              x c  
          
        
      
    
      displaystyle   hat  f   in   mathcal  N    d D d D      sigma   
  
 such that

  
    
      
        
          sup
          
            x
              x     
            
              
                X
              
            
          
        
        
            x     
          
            
              
                
                  f
                    x e 
                
              
            
             
            x
             
              x     
            f
             
            x
             
          
            x     
        
         lt 
          x b  
         
      
    
      displaystyle  sup   x in   mathcal  X    left    hat  f   x  f x  right   lt  varepsilon   
  

In other words  
  
    
      
        
          
            N
          
        
      
    
      displaystyle   mathcal  N   
  
 is dense in 
  
    
      
        C
         
        
          
            X
          
        
         
        
          
            R
          
          
            D
          
        
         
      
    
      displaystyle C   mathcal  X    mathbb  R    D   
  
 with respect to the topology of uniform convergence 
Quantitative refinement  The number of layers and the width of each layer required to approximate 
  
    
      
        f
      
    
      displaystyle f 
  
 to 
  
    
      
          x b  
      
    
      displaystyle  varepsilon  
  
 precision known              moreover  the result hold true when 
  
    
      
        
          
            X
          
        
      
    
      displaystyle   mathcal  X   
  
 and 
  
    
      
        
          
            R
          
          
            D
          
        
      
    
      displaystyle  mathbb  R    D  
  
 are replaced with any non positively curved Riemannian manifold 


Certain necessary conditions for the bounded width  arbitrary depth case have been established  but there is still a gap between the known sufficient and necessary conditions                                    

Bounded depth and bounded width case edit 
The first result on approximation capabilities of neural networks with bounded number of layers  each containing a limited number of artificial neurons was obtained by Maiorov and Pinkus              Their remarkable result revealed that such networks can be universal approximators and for achieving this property two hidden layers are enough 


Universal approximation theorem              There exists an activation function 
  
    
      
          x c  
      
    
      displaystyle  sigma  
  
 which is analytic  strictly increasing and sigmoidal and has the following property  For any 
  
    
      
        f
          x     
        C
         
         
         
         
        
           
          
            d
          
        
      
    
      displaystyle f in C       d  
  
  and 
  
    
      
          x b  
         gt 
         
      
    
      displaystyle  varepsilon  gt   
  
 there exist constants 
  
    
      
        
          d
          
            i
          
        
         
        
          c
          
            i
            j
          
        
         
        
            x b  
          
            i
            j
          
        
         
        
            x b  
          
            i
          
        
      
    
      displaystyle d  i  c  ij   theta   ij   gamma   i  
  
  and vectors 
  
    
      
        
          
            w
          
          
            i
            j
          
        
          x     
        
          
            R
          
          
            d
          
        
      
    
      displaystyle  mathbf  w    ij  in  mathbb  R    d  
  
  for which

  
    
      
        
           
          
            f
             
            
              x
            
             
              x     
            
                x     
              
                i
                 
                 
              
              
                 
                d
                 
                 
              
            
            
              d
              
                i
              
            
              x c  
            
               
              
                
                    x     
                  
                    j
                     
                     
                  
                  
                     
                    d
                  
                
                
                  c
                  
                    i
                    j
                  
                
                  x c  
                 
                
                  
                    w
                  
                  
                    i
                    j
                  
                
                  x  c  
                
                  x
                    x     
                
                
                    x b  
                  
                    i
                    j
                  
                
                 
                  x     
                
                    x b  
                  
                    i
                  
                
              
               
            
          
           
        
         lt 
          x b  
      
    
      displaystyle  left vert f  mathbf  x     sum   i      d   d  i  sigma  left  sum   j      d c  ij  sigma   mathbf  w    ij  cdot  mathbf  x    theta   ij    gamma   i  right  right vert  lt  varepsilon  
  

for all 
  
    
      
        
          x
        
         
         
        
          x
          
             
          
        
         
         
         
         
         
        
          x
          
            d
          
        
         
          x     
         
         
         
         
        
           
          
            d
          
        
      
    
      displaystyle  mathbf  x    x         x  d   in        d  
  
 


This is an existence result  It says that activation functions providing universal approximation property for bounded depth bounded width networks exist  Using certain algorithmic and computer programming techniques  Guliyev and Ismailov efficiently constructed such activation functions depending on a numerical parameter   The developed algorithm allows one to compute the activation functions at any point of the real axis instantly  For the algorithm and the corresponding computer code see              The theoretical result can be formulated as follows 


Universal approximation theorem                          Let  
  
    
      
         
        a
         
        b
         
      
    
      displaystyle  a b  
  
  be a finite segment of the real line  
  
    
      
        s
         
        b
          x     
        a
      
    
      displaystyle s b a 
  
 and 
  
    
      
          x bb 
      
    
      displaystyle  lambda  
  
  be any positive number  Then one can algorithmically construct a computable sigmoidal activation function 
  
    
      
          x c  
          x a 
        
          R
        
          x     
        
          R
        
      
    
      displaystyle  sigma  colon  mathbb  R   to  mathbb  R   
  
  which is infinitely differentiable  strictly increasing on 
  
    
      
         
          x     
          x   e 
         
        s
         
      
    
      displaystyle    infty  s  
  
  
  
    
      
          x bb 
      
    
      displaystyle  lambda  
  
  strictly increasing on 
  
    
      
         
        s
         
         
          x   e 
         
      
    
      displaystyle  s   infty   
  
  and satisfies the following properties 

For any 
  
    
      
        f
          x     
        C
         
        a
         
        b
         
      
    
      displaystyle f in C a b  
  
  and 
  
    
      
          x b  
         gt 
         
      
    
      displaystyle  varepsilon  gt   
  
  there exist numbers 
  
    
      
        
          c
          
             
          
        
         
        
          c
          
             
          
        
         
        
            x b  
          
             
          
        
      
    
      displaystyle c     c      theta      
  
  and 
  
    
      
        
            x b  
          
             
          
        
      
    
      displaystyle  theta      
  
  such that for all 
  
    
      
        x
          x     
         
        a
         
        b
         
      
    
      displaystyle x in  a b  
  
 
  
    
      
        
           
        
        f
         
        x
         
          x     
        
          c
          
             
          
        
          x c  
         
        x
          x     
        
            x b  
          
             
          
        
         
          x     
        
          c
          
             
          
        
          x c  
         
        x
          x     
        
            x b  
          
             
          
        
         
        
           
        
         lt 
          x b  
      
    
      displaystyle  f x  c     sigma  x  theta       c     sigma  x  theta        lt  varepsilon  
  

For any continuous function 
  
    
      
        F
      
    
      displaystyle F 
  
 on the 
  
    
      
        d
      
    
      displaystyle d 
  
 dimensional box 
  
    
      
         
        a
         
        b
        
           
          
            d
          
        
      
    
      displaystyle  a b   d  
  
 and 
  
    
      
          x b  
         gt 
         
      
    
      displaystyle  varepsilon  gt   
  
  there exist constants 
  
    
      
        
          e
          
            p
          
        
      
    
      displaystyle e  p  
  
  
  
    
      
        
          c
          
            p
            q
          
        
      
    
      displaystyle c  pq  
  
  
  
    
      
        
            x b  
          
            p
            q
          
        
      
    
      displaystyle  theta   pq  
  
 and 
  
    
      
        
            x b  
          
            p
          
        
      
    
      displaystyle  zeta   p  
  
 such that the inequality 
  
    
      
        
           
          
            F
             
            
              x
            
             
              x     
            
                x     
              
                p
                 
                 
              
              
                 
                d
                 
                 
              
            
            
              e
              
                p
              
            
              x c  
            
               
              
                
                    x     
                  
                    q
                     
                     
                  
                  
                    d
                  
                
                
                  c
                  
                    p
                    q
                  
                
                  x c  
                 
                
                  
                    w
                  
                  
                    q
                  
                
                  x  c  
                
                  x
                
                  x     
                
                    x b  
                  
                    p
                    q
                  
                
                 
                  x     
                
                    x b  
                  
                    p
                  
                
              
               
            
          
           
        
         lt 
          x b  
      
    
      displaystyle  left F  mathbf  x     sum   p      d   e  p  sigma  left  sum   q     d c  pq  sigma   mathbf  w    q  cdot  mathbf  x    theta   pq    zeta   p  right  right  lt  varepsilon  
  
 holds for all 
  
    
      
        
          x
        
         
         
        
          x
          
             
          
        
         
          x     
         
        
          x
          
            d
          
        
         
          x     
         
        a
         
        b
        
           
          
            d
          
        
      
    
      displaystyle  mathbf  x    x      ldots  x  d   in  a b   d  
  
  Here the weights 
  
    
      
        
          
            w
          
          
            q
          
        
      
    
      displaystyle  mathbf  w    q  
  
  
  
    
      
        q
         
         
         
          x     
         
        d
      
    
      displaystyle q    ldots  d 
  
  are fixed as follows  
  
    
      
        
          
            w
          
          
             
          
        
         
         
         
         
         
         
          x     
         
         
         
         
        
        
          
            w
          
          
             
          
        
         
         
         
         
         
         
          x     
         
         
         
         
        
          x     
         
        
        
          
            w
          
          
            d
          
        
         
         
         
         
         
         
          x     
         
         
         
         
      
    
      displaystyle  mathbf  w             ldots      quad  mathbf  w             ldots      quad  ldots   quad  mathbf  w    d        ldots      
  
 In addition  all the coefficients 
  
    
      
        
          e
          
            p
          
        
      
    
      displaystyle e  p  
  
  except one  are equal 

Here  
  
    
      
          x c  
          x a 
        
          R
        
          x     
        
          R
        
      
    
      displaystyle  sigma  colon  mathbb  R   to  mathbb  R   
  
 is 
  
    
      
          x bb 
      
    
      displaystyle  lambda  
  
 strictly increasing on some set 
  
    
      
        X
      
    
      displaystyle X 
  
  means that there exists a strictly increasing function 
  
    
      
        u
          x a 
        X
          x     
        
          R
        
      
    
      displaystyle u colon X to  mathbb  R   
  
 such that 
  
    
      
        
           
        
          x c  
         
        x
         
          x     
        u
         
        x
         
        
           
        
          x     
          x bb 
      
    
      displaystyle   sigma  x  u x   leq  lambda  
  
 for all 
  
    
      
        x
          x     
        X
      
    
      displaystyle x in X 
  
  Clearly  a 
  
    
      
          x bb 
      
    
      displaystyle  lambda  
  
 increasing function behaves like a usual increasing function as 
  
    
      
          x bb 
      
    
      displaystyle  lambda  
  
 gets small 
In the  depth width  terminology  the above theorem says that for certain activation functions depth 
  
    
      
         
      
    
      displaystyle   
  
 width 
  
    
      
         
      
    
      displaystyle   
  
 networks are universal approximators for univariate functions and depth 
  
    
      
         
      
    
      displaystyle   
  
 width 
  
    
      
         
         
        d
         
         
         
      
    
      displaystyle   d    
  
 networks are universal approximators for 
  
    
      
        d
      
    
      displaystyle d 
  
 variable functions  
  
    
      
        d
         gt 
         
      
    
      displaystyle d gt   
  
  

See also edit 
Kolmogorov Arnold representation theorem
Representer theorem
No free lunch theorem
Stone Weierstrass theorem
Fourier series
References edit 


  a b Hornik  Kurt  Stinchcombe  Maxwell  White  Halbert  January         Multilayer feedforward networks are universal approximators   Neural Networks                  doi                              

  Bal zs Csan d Cs ji        Approximation with Artificial Neural Networks  Faculty of Sciences  E tv s Lor nd University  Hungary

  a b c Cybenko  G           Approximation by superpositions of a sigmoidal function   Mathematics of Control  Signals  and Systems                  Bibcode     MCSS          C  CiteSeerX                       doi         BF          S CID              

  a b Hornik  Kurt          Approximation capabilities of multilayer feedforward networks   Neural Networks                  doi                            T  S CID              

  a b Leshno  Moshe  Lin  Vladimir Ya   Pinkus  Allan  Schocken  Shimon  January         Multilayer feedforward networks with a nonpolynomial activation function can approximate any function   Neural Networks                  doi         S                      S CID                

  a b c Pinkus  Allan  January         Approximation theory of the MLP model in neural networks   Acta Numerica              Bibcode     AcNum         P  doi         S                  S CID               

  a b Gripenberg  Gustaf  June         Approximation by neural networks with a bounded number of nodes at each level   Journal of Approximation Theory                    doi         S                     

  Yarotsky  Dmitry  October         Error bounds for approximations with deep ReLU networks   Neural Networks               arXiv             doi         j neunet              PMID                S CID             

  a b c d Lu  Zhou  Pu  Hongming  Wang  Feicheng  Hu  Zhiqiang  Wang  Liwei          The Expressive Power of Neural Networks  A View from the Width   Advances in Neural Information Processing Systems      Curran Associates             arXiv            

  a b Hanin  Boris  Sellke  Mark          Approximating Continuous Functions by ReLU Nets of Minimal Width   arXiv             stat ML  

  a b c d Kidger  Patrick  Lyons  Terry  July        Universal Approximation with Deep Narrow Networks  Conference on Learning Theory  arXiv            

  Yongqiang  Cai          Vocabulary for Universal Approximation  A Linguistic Perspective of Mapping Compositions   ICML             arXiv            

  a b c Maiorov  Vitaly  Pinkus  Allan  April         Lower bounds for approximation by MLP neural networks   Neurocomputing                   doi         S                     

  a b c Guliyev  Namig  Ismailov  Vugar  November         Approximation capability of two hidden layer feedforward neural networks with fixed weights   Neurocomputing                arXiv             doi         j neucom              S CID               

  a b Guliyev  Namig  Ismailov  Vugar  February         On the approximation by single hidden layer feedforward neural networks with fixed weights   Neural Networks               arXiv             doi         j neunet              PMID                S CID              

  Shen  Zuowei  Yang  Haizhao  Zhang  Shijun  January         Optimal approximation rate of ReLU networks in terms of width and depth   Journal de Math matiques Pures et Appliqu es                arXiv             doi         j matpur              S CID                

  a b Park  Sejun  Yun  Chulhee  Lee  Jaeho  Shin  Jinwoo         Minimum Width for Universal Approximation  International Conference on Learning Representations  arXiv            

  Tabuada  Paulo  Gharesifard  Bahman         Universal approximation power of deep residual neural networks via nonlinear control theory  International Conference on Learning Representations  arXiv            

  Tabuada  Paulo  Gharesifard  Bahman  May         Universal Approximation Power of Deep Residual Neural Networks Through the Lens of Control   IEEE Transactions on Automatic Control                     doi         TAC               S CID                  Erratum       doi         TAC              

  a b Cai  Yongqiang                Achieve the Minimum Width of Neural Networks for Universal Approximation   ICLR  arXiv            

  a b Kratsios  Anastasis  Papon  L onie          Universal Approximation Theorems for Differentiable Geometric Deep Learning   Journal of Machine Learning Research                  arXiv            

  Hecht Nielsen  Robert          Kolmogorov s mapping neural network existence theorem   Proceedings of International Conference on Neural Networks                 

  Ismailov  Vugar E   July         A three layer neural network can represent any multivariate function   Journal of Mathematical Analysis and Applications                   arXiv             doi         j jmaa              S CID                

  Liu  Ziming  Wang  Yixuan  Vaidya  Sachin  Ruehle  Fabian  Halverson  James  Solja i   Marin  Hou  Thomas Y   Tegmark  Max                KAN  Kolmogorov Arnold Networks   arXiv             cs LG  

  Grigoryeva  L   Ortega  J  P           Echo state networks are universal   Neural Networks                    arXiv             doi         j neunet              PMID               

  Maass  Wolfgang  Markram  Henry          On the computational power of circuits of spiking neurons   PDF   Journal of Computer and System Sciences                   doi         j jcss             

  Monzani  Francesco  Prati  Enrico          Universality conditions of unified classical and quantum reservoir computing   arXiv             quant ph  

  van Nuland  Teun          Noncompact uniform universal approximation   Neural Networks       arXiv             doi         j neunet              PMID               

  Baader  Maximilian  Mirman  Matthew  Vechev  Martin         Universal Approximation with Certified Networks  ICLR 

  Gelenbe  Erol  Mao  Zhi Hong  Li  Yan D           Function approximation with spiked random networks   IEEE Transactions on Neural Networks               doi                    PMID               

  Lin  Hongzhou  Jegelka  Stefanie         ResNet with one neuron hidden layers is a Universal Approximator  Advances in Neural Information Processing Systems  Vol           Curran Associates  pp                 

  Xu  Keyulu  Hu  Weihua  Leskovec  Jure  Jegelka  Stefanie         How Powerful are Graph Neural Networks   International Conference on Learning Representations 

  Br el Gabrielsson  Rickard         Universal Function Approximation on Graphs  Advances in Neural Information Processing Systems  Vol           Curran Associates 

  Kratsios  Anastasis  Bilokopytov  Eugene         Non Euclidean Universal Approximation  PDF   Advances in Neural Information Processing Systems  Vol           Curran Associates 

  Zhou  Ding Xuan          Universality of deep convolutional neural networks   Applied and Computational Harmonic Analysis                   arXiv             doi         j acha              S CID               

  Heinecke  Andreas  Ho  Jinn  Hwang  Wen Liang          Refinement and Universal Approximation via Sparsely Connected ReLU Convolution Nets   IEEE Signal Processing Letters                 Bibcode     ISPL          H  doi         LSP               S CID                

  Park  J   Sandberg  I  W           Universal Approximation Using Radial Basis Function Networks   Neural Computation                  doi         neco               PMID                S CID               

  Yarotsky  Dmitry          Universal Approximations of Invariant Maps by Neural Networks   Constructive Approximation               arXiv             doi         s                   S CID               

  Zakwan  Muhammad  d Angelo  Massimiliano  Ferrari Trecate  Giancarlo          Universal Approximation Property of Hamiltonian Deep Neural Networks   IEEE Control Systems Letters     arXiv             doi         LCSYS               S CID                

  Funahashi  Ken Ichi  January         On the approximate realization of continuous mappings by neural networks   Neural Networks                  doi                              

  a b Hornik  Kurt  Stinchcombe  Maxwell  White  Halbert  January         Multilayer feedforward networks are universal approximators   Neural Networks                  doi                              

  Haykin  Simon         Neural Networks  A Comprehensive Foundation  Volume    Prentice Hall  ISBN                    

  Hassoun  M         Fundamentals of Artificial Neural Networks MIT Press  p         

  Nielsen  Michael A          Neural Networks and Deep Learning 

  G  Cybenko   Continuous Valued Neural Networks with Two Hidden Layers are Sufficient   Technical Report  Department of Computer Science  Tufts University       

  Hanin  B          Approximating Continuous Functions by ReLU Nets of Minimal Width  arXiv preprint arXiv            

  Park  Yun  Lee  Shin  Sejun  Chulhee  Jaeho  Jinwoo                Minimum Width for Universal Approximation   ICLR  arXiv              cite journal     CS  maint  multiple names  authors list  link 

  Shen  Zuowei  Yang  Haizhao  Zhang  Shijun  January         Optimal approximation rate of ReLU networks in terms of width and depth   Journal de Math matiques Pures et Appliqu es                arXiv             doi         j matpur              S CID                

  Lu  Jianfeng  Shen  Zuowei  Yang  Haizhao  Zhang  Shijun  January         Deep Network Approximation for Smooth Functions   SIAM Journal on Mathematical Analysis                     arXiv             doi           M      X  S CID                

  Juditsky  Anatoli B   Lepski  Oleg V   Tsybakov  Alexandre B                 Nonparametric estimation of composite functions   The Annals of Statistics          doi            aos     ISSN                 S CID              

  Poggio  Tomaso  Mhaskar  Hrushikesh  Rosasco  Lorenzo  Miranda  Brando  Liao  Qianli                Why and when can deep but not shallow networks avoid the curse of dimensionality  A review   International Journal of Automation and Computing                   arXiv             doi         s                  ISSN                 S CID               

  Johnson  Jesse         Deep  Skinny Neural Networks are not Universal Approximators  International Conference on Learning Representations 


vteDifferentiable computingGeneral
Differentiable programming
Information geometry
Statistical manifold
Automatic differentiation
Neuromorphic computing
Pattern recognition
Ricci calculus
Computational learning theory
Inductive bias
Hardware
IPU
TPU
VPU
Memristor
SpiNNaker
Software libraries
TensorFlow
PyTorch
Keras
scikit learn
Theano
JAX
Flux jl
MindSpore

 Portals
Computer programming
Technology






Retrieved from  https   en wikipedia org w index php title Universal approximation theorem amp oldid