Field of machine learning
For reinforcement learning in psychology  see Reinforcement and Operant conditioning 
 The typical framing of a reinforcement learning  RL  scenario  an agent takes actions in an environment  which is interpreted into a reward and a state representation  which are fed back to the agent 
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
Reinforcement learning  RL  is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal  Reinforcement learning is one of the three basic machine learning paradigms  alongside supervised learning and unsupervised learning 
Reinforcement learning differs from supervised learning in not needing labelled input output pairs to be presented  and in not needing sub optimal actions to be explicitly corrected  Instead  the focus is on finding a balance between exploration  of uncharted territory  and exploitation  of current knowledge  with the goal of maximizing the cumulative reward  the feedback of which might be incomplete or delayed              The search for this balance is known as the exploration exploitation dilemma 

The environment is typically stated in the form of a Markov decision process  MDP   as many reinforcement learning algorithms use dynamic programming techniques             The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process  and they target large MDPs where exact methods become infeasible             
Principles edit 
Due to its generality  reinforcement learning is studied in many disciplines  such as game theory  control theory  operations research  information theory  simulation based optimization  multi agent systems  swarm intelligence  and statistics  In the operations research and control literature  RL is called approximate dynamic programming  or neuro dynamic programming  The problems of interest in RL have also been studied in the theory of optimal control  which is concerned mostly with the existence and characterization of optimal solutions  and algorithms for their exact computation  and less with learning or approximation  particularly in the absence of a mathematical model of the environment  
Basic reinforcement learning is modeled as a Markov decision process 

A set of environment and agent states  the state space   
  
    
      
        
          
            S
          
        
      
    
      displaystyle   mathcal  S   
  
 
A set of actions  the action space   
  
    
      
        
          
            A
          
        
      
    
      displaystyle   mathcal  A   
  
  of the agent 

  
    
      
        
          P
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
         
        Pr
         
        
          S
          
            t
             
             
          
        
         
        
          s
            x     
        
          x     
        
          S
          
            t
          
        
         
        s
         
        
          A
          
            t
          
        
         
        a
         
      
    
      displaystyle P  a  s s    Pr S  t    s  mid S  t  s A  t  a  
  
  the transition probability  at time 
  
    
      
        t
      
    
      displaystyle t 
  
  from state 
  
    
      
        s
      
    
      displaystyle s 
  
 to state 
  
    
      
        
          s
            x     
        
      
    
      displaystyle s  
  
 under action 
  
    
      
        a
      
    
      displaystyle a 
  
 

  
    
      
        
          R
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
      
    
      displaystyle R  a  s s   
  
  the immediate reward after transition from 
  
    
      
        s
      
    
      displaystyle s 
  
 to 
  
    
      
        
          s
            x     
        
      
    
      displaystyle s  
  
 under action 
  
    
      
        a
      
    
      displaystyle a 
  
 
The purpose of reinforcement learning is for the agent to learn an optimal  or near optimal  policy that maximizes the reward function or other user provided reinforcement signal that accumulates from immediate rewards  This is similar to processes that appear to occur in animal psychology  For example  biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements  and interpret pleasure and food intake as positive reinforcements  In some circumstances  animals learn to adopt behaviors that optimize these rewards  This suggests that animals are capable of reinforcement learning                       
A basic reinforcement learning agent interacts with its environment in discrete time steps  At each time step t  the agent receives the current state 
  
    
      
        
          S
          
            t
          
        
      
    
      displaystyle S  t  
  
 and reward 
  
    
      
        
          R
          
            t
          
        
      
    
      displaystyle R  t  
  
  It then chooses an action 
  
    
      
        
          A
          
            t
          
        
      
    
      displaystyle A  t  
  
 from the set of available actions  which is subsequently sent to the environment  The environment moves to a new state 
  
    
      
        
          S
          
            t
             
             
          
        
      
    
      displaystyle S  t    
  
 and the reward 
  
    
      
        
          R
          
            t
             
             
          
        
      
    
      displaystyle R  t    
  
 associated with the transition 
  
    
      
         
        
          S
          
            t
          
        
         
        
          A
          
            t
          
        
         
        
          S
          
            t
             
             
          
        
         
      
    
      displaystyle  S  t  A  t  S  t     
  
 is determined  The goal of a reinforcement learning agent is to learn a policy 

  
    
      
          x c  
         
        
          
            S
          
        
          xd  
        
          
            A
          
        
          x     
         
         
         
         
         
      
    
      displaystyle  pi    mathcal  S   times   mathcal  A   rightarrow       
  
  
  
    
      
          x c  
         
        s
         
        a
         
         
        Pr
         
        
          A
          
            t
          
        
         
        a
          x     
        
          S
          
            t
          
        
         
        s
         
      
    
      displaystyle  pi  s a   Pr A  t  a mid S  t  s  
  

that maximizes the expected cumulative reward 
Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state  in this case  the problem is said to have full observability  If the agent only has access to a subset of states  or if the observed states are corrupted by noise  the agent is said to have partial observability  and formally the problem must be formulated as a partially observable Markov decision process  In both cases  the set of actions available to the agent can be restricted  For example  the state of an account balance could be restricted to be positive  if the current value of the state is   and the state transition attempts to reduce the value by    the transition will not be allowed 
When the agent s performance is compared to that of an agent that acts optimally  the difference in performance yields the notion of regret  In order to act near optimally  the agent must reason about long term consequences of its actions  i e   maximize future rewards   although the immediate reward associated with this might be negative 
Thus  reinforcement learning is particularly well suited to problems that include a long term versus short term reward trade off  It has been applied successfully to various problems  including energy storage             robot control             photovoltaic generators             backgammon  checkers             Go  AlphaGo   and autonomous driving systems             
Two elements make reinforcement learning powerful  the use of samples to optimize performance  and the use of function approximation to deal with large environments  Thanks to these two key components  RL can be used in large environments in the following situations 

A model of the environment is known  but an analytic solution is not available 
Only a simulation model of the environment is given  the subject of simulation based optimization              
The only way to collect information about the environment is to interact with it 
The first two of these problems could be considered planning problems  since some form of model is available   while the last one could be considered to be a genuine learning problem  However  reinforcement learning converts both planning problems to machine learning problems 

Exploration edit 
The exploration vs  exploitation trade off has been most thoroughly studied through the multi armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis                    
Reinforcement learning requires clever exploration mechanisms  randomly selecting actions  without reference to an estimated probability distribution  shows poor performance  The case of  small  finite Markov decision processes is relatively well understood  However  due to the lack of algorithms that scale well with the number of states  or scale to problems with infinite state spaces   simple exploration methods are the most practical 
One such method is 
  
    
      
          x b  
      
    
      displaystyle  varepsilon  
  
 greedy  where 
  
    
      
         
         lt 
          x b  
         lt 
         
      
    
      displaystyle   lt  varepsilon  lt   
  
 is a parameter controlling the amount of exploration vs  exploitation  With probability 
  
    
      
         
          x     
          x b  
      
    
      displaystyle    varepsilon  
  
  exploitation is chosen  and the agent chooses the action that it believes has the best long term effect  ties between actions are broken uniformly at random   Alternatively  with probability 
  
    
      
          x b  
      
    
      displaystyle  varepsilon  
  
  exploration is chosen  and the action is chosen uniformly at random  
  
    
      
          x b  
      
    
      displaystyle  varepsilon  
  
 is usually a fixed parameter but can be adjusted either according to a schedule  making the agent explore progressively less   or adaptively based on heuristics             

Algorithms for control learning edit 
Even if the issue of exploration is disregarded and even if the state was observable  assumed hereafter   the problem remains to use past experience to find out which actions lead to higher cumulative rewards 

Criterion of optimality edit 
Policy edit 
The agent s action selection is modeled as a map called policy 


  
    
      
          x c  
         
        
          
            A
          
        
          xd  
        
          
            S
          
        
          x     
         
         
         
         
         
      
    
      displaystyle  pi    mathcal  A   times   mathcal  S   rightarrow       
  


  
    
      
          x c  
         
        a
         
        s
         
         
        Pr
         
        
          A
          
            t
          
        
         
        a
          x     
        
          S
          
            t
          
        
         
        s
         
      
    
      displaystyle  pi  a s   Pr A  t  a mid S  t  s  
  

The policy map gives the probability of taking action 
  
    
      
        a
      
    
      displaystyle a 
  
 when in state 
  
    
      
        s
      
    
      displaystyle s 
  
                                   There are also deterministic policies  
  
    
      
          x c  
      
    
      displaystyle  pi  
  
 for which 
  
    
      
          x c  
         
        s
         
      
    
      displaystyle  pi  s  
  
 denotes the action that should be played at state 
  
    
      
        s
      
    
      displaystyle s 
  
 

State value function edit 
The state value function 
  
    
      
        
          V
          
              x c  
          
        
         
        s
         
      
    
      displaystyle V   pi   s  
  
 is defined as  expected discounted return starting with state 
  
    
      
        s
      
    
      displaystyle s 
  
  i e  
  
    
      
        
          S
          
             
          
        
         
        s
      
    
      displaystyle S     s 
  
  and successively following policy 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
  Hence  roughly speaking  the value function estimates  how good  it is to be in a given state                                  


  
    
      
        
          V
          
              x c  
          
        
         
        s
         
         
        
          
            E
          
        
          x     
         
        G
          x     
        
          S
          
             
          
        
         
        s
         
         
        
          
            E
          
        
          x     
        
           
          
            
                x     
              
                t
                 
                 
              
              
                  x   e 
              
            
            
                x b  
              
                t
              
            
            
              R
              
                t
                 
                 
              
            
              x     
            
              S
              
                 
              
            
             
            s
          
           
        
         
      
    
      displaystyle V   pi   s   operatorname   mathbb  E     G mid S     s   operatorname   mathbb  E     left  sum   t      infty   gamma   t R  t    mid S     s right   
  

where the random variable 
  
    
      
        G
      
    
      displaystyle G 
  
 denotes the discounted return  and is defined as the sum of future discounted rewards 


  
    
      
        G
         
        
            x     
          
            t
             
             
          
          
              x   e 
          
        
        
            x b  
          
            t
          
        
        
          R
          
            t
             
             
          
        
         
        
          R
          
             
          
        
         
          x b  
        
          R
          
             
          
        
         
        
            x b  
          
             
          
        
        
          R
          
             
          
        
         
          x     
         
      
    
      displaystyle G  sum   t      infty   gamma   t R  t    R      gamma R      gamma     R      dots   
  

where 
  
    
      
        
          R
          
            t
             
             
          
        
      
    
      displaystyle R  t    
  
 is the reward for transitioning from state 
  
    
      
        
          S
          
            t
          
        
      
    
      displaystyle S  t  
  
 to 
  
    
      
        
          S
          
            t
             
             
          
        
      
    
      displaystyle S  t    
  
  
  
    
      
         
          x     
          x b  
         lt 
         
      
    
      displaystyle   leq  gamma  lt   
  
 is the discount rate  
  
    
      
          x b  
      
    
      displaystyle  gamma  
  
 is less than    so rewards in the distant future are weighted less than rewards in the immediate future 
The algorithm must find a policy with maximum expected discounted return  From the theory of Markov decision processes it is known that  without loss of generality  the search can be restricted to the set of so called stationary policies  A policy is stationary if the action distribution returned by it depends only on the last state visited  from the observation agent s history   The search can be further restricted to deterministic stationary policies  A deterministic stationary policy deterministically selects actions based on the current state  Since any such policy can be identified with a mapping from the set of states to the set of actions  these policies can be identified with such mappings with no loss of generality 

Brute force edit 
The brute force approach entails two steps 

For each possible policy  sample returns while following it
Choose the policy with the largest expected discounted return
One problem with this is that the number of policies can be large  or even infinite  Another is that the variance of the returns may be large  which requires many samples to accurately estimate the discounted return of each policy 
These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others  The two main approaches for achieving this are value function estimation and direct policy search 

Value function edit 
See also  Value function
Value function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returns 
  
    
      
        
          
            E
          
        
          x     
         
        G
         
      
    
      displaystyle  operatorname   mathbb  E     G  
  
 for some policy  usually either the  current   on policy  or the optimal  off policy  one  
These methods rely on the theory of Markov decision processes  where optimality is defined in a sense stronger than the one above  A policy is optimal if it achieves the best expected discounted return from any initial state  i e   initial distributions play no role in this definition   Again  an optimal policy can always be found among stationary policies 
To define optimality in a formal manner  define the state value of a policy 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
 by


  
    
      
        
          V
          
              x c  
          
        
         
        s
         
         
        
          
            E
          
        
          x     
         
        G
          x     
        s
         
          x c  
         
         
      
    
      displaystyle V   pi   s   operatorname   mathbb  E     G mid s  pi    
  

where 
  
    
      
        G
      
    
      displaystyle G 
  
 stands for the discounted return associated with following 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
 from the initial state 
  
    
      
        s
      
    
      displaystyle s 
  
  Defining 
  
    
      
        
          V
          
              x     
          
        
         
        s
         
      
    
      displaystyle V     s  
  
 as the maximum possible state value of 
  
    
      
        
          V
          
              x c  
          
        
         
        s
         
      
    
      displaystyle V   pi   s  
  
  where 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
 is allowed to change 


  
    
      
        
          V
          
              x     
          
        
         
        s
         
         
        
          max
          
              x c  
          
        
        
          V
          
              x c  
          
        
         
        s
         
         
      
    
      displaystyle V     s   max    pi  V   pi   s   
  

A policy that achieves these optimal state values in each state is called optimal  Clearly  a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return  since 
  
    
      
        
          V
          
              x     
          
        
         
        s
         
         
        
          max
          
              x c  
          
        
        
          E
        
         
        G
          x     
        s
         
          x c  
         
      
    
      displaystyle V     s   max    pi   mathbb  E   G mid s  pi   
  
  where 
  
    
      
        s
      
    
      displaystyle s 
  
 is a state randomly sampled from the distribution 
  
    
      
          x bc 
      
    
      displaystyle  mu  
  
 of initial states  so 
  
    
      
          x bc 
         
        s
         
         
        Pr
         
        
          S
          
             
          
        
         
        s
         
      
    
      displaystyle  mu  s   Pr S     s  
  
  
Although state values suffice to define optimality  it is useful to define action values  Given a state 
  
    
      
        s
      
    
      displaystyle s 
  
  an action 
  
    
      
        a
      
    
      displaystyle a 
  
 and a policy 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
  the action value of the pair 
  
    
      
         
        s
         
        a
         
      
    
      displaystyle  s a  
  
 under 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
 is defined by


  
    
      
        
          Q
          
              x c  
          
        
         
        s
         
        a
         
         
        
          
            E
          
        
          x     
         
        G
          x     
        s
         
        a
         
          x c  
         
         
        
      
    
      displaystyle Q   pi   s a   operatorname   mathbb  E     G mid s a  pi      
  

where 
  
    
      
        G
      
    
      displaystyle G 
  
 now stands for the random discounted return associated with first taking action 
  
    
      
        a
      
    
      displaystyle a 
  
 in state 
  
    
      
        s
      
    
      displaystyle s 
  
 and following 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
  thereafter 
The theory of Markov decision processes states that if 
  
    
      
        
            x c  
          
              x     
          
        
      
    
      displaystyle  pi      
  
 is an optimal policy  we act optimally  take the optimal action  by choosing the action from 
  
    
      
        
          Q
          
            
                x c  
              
                  x     
              
            
          
        
         
        s
         
          x  c  
         
      
    
      displaystyle Q   pi       s  cdot   
  
 with the highest action value at each state  
  
    
      
        s
      
    
      displaystyle s 
  
  The action value function of such an optimal policy  
  
    
      
        
          Q
          
            
                x c  
              
                  x     
              
            
          
        
      
    
      displaystyle Q   pi       
  
  is called the optimal action value function and is commonly denoted by 
  
    
      
        
          Q
          
              x     
          
        
      
    
      displaystyle Q     
  
  In summary  the knowledge of the optimal action value function alone suffices to know how to act optimally 
Assuming full knowledge of the Markov decision process  the two basic approaches to compute the optimal action value function are value iteration and policy iteration  Both algorithms compute a sequence of functions 
  
    
      
        
          Q
          
            k
          
        
      
    
      displaystyle Q  k  
  
  
  
    
      
        k
         
         
         
         
         
         
         
          x     
      
    
      displaystyle k        ldots  
  
  that converge to 
  
    
      
        
          Q
          
              x     
          
        
      
    
      displaystyle Q     
  
  Computing these functions involves computing expectations over the whole state space  which is impractical for all but the smallest  finite  Markov decision processes  In reinforcement learning methods  expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state action spaces 

Monte Carlo methods edit 
Monte Carlo methods             are used to solve reinforcement learning problems by averaging sample returns  Unlike methods that require full knowledge of the environment s dynamics  Monte Carlo methods rely solely on actual or simulated experience sequences of states  actions  and rewards obtained from interaction with an environment  This makes them applicable in situations where the complete dynamics are unknown  Learning from actual experience does not require prior knowledge of the environment and can still lead to optimal behavior  When using simulated experience  only a model capable of generating sample transitions is required  rather than a full specification of transition probabilities  which is necessary for dynamic programming methods 
Monte Carlo methods apply to episodic tasks  where experience is divided into episodes that eventually terminate  Policy and value function updates occur only after the completion of an episode  making these methods incremental on an episode by episode basis  though not on a step by step  online  basis  The term  Monte Carlo  generally refers to any method involving random sampling  however  in this context  it specifically refers to methods that compute averages from complete returns  rather than partial returns 
These methods function similarly to the bandit algorithms  in which returns are averaged for each state action pair  The key difference is that actions taken in one state affect the returns of subsequent states within the same episode  making the problem non stationary  To address this non stationarity  Monte Carlo methods use the framework of general policy iteration  GPI   While dynamic programming computes value functions using full knowledge of the Markov decision process  MDP   Monte Carlo methods learn these functions through sample returns  The value functions and policies interact similarly to dynamic programming to achieve optimality  first addressing the prediction problem and then extending to policy improvement and control  all based on sampled experience             

Temporal difference methods edit 
Main article  Temporal difference learning
The first problem is corrected by allowing the procedure to change the policy  at some or all states  before the values settle  This too may be problematic as it might prevent convergence  Most current algorithms do this  giving rise to the class of generalized policy iteration algorithms  Many actor critic methods belong to this category 
The second issue can be corrected by allowing trajectories to contribute to any state action pair in them  This may also help to some extent with the third problem  although a better solution when returns have high variance is Sutton s temporal difference  TD  methods that are based on the recursive Bellman equation                          The computation in TD methods can be incremental  when after each transition the memory is changed and the transition is thrown away   or batch  when the transitions are batched and the estimates are computed once based on the batch   Batch methods  such as the least squares temporal difference method              may use the information in the samples better  while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity  Some methods try to combine the two approaches  Methods based on temporal differences also overcome the fourth issue 
Another problem specific to TD comes from their reliance on the recursive Bellman equation  Most TD methods have a so called 
  
    
      
          x bb 
      
    
      displaystyle  lambda  
  
 parameter 
  
    
      
         
         
          x     
          x bb 
          x     
         
         
      
    
      displaystyle    leq  lambda  leq    
  
 that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations  This can be effective in palliating this issue 

Function approximation methods edit 
In order to address the fifth issue  function approximation methods are used  Linear function approximation starts with a mapping 
  
    
      
          x d  
      
    
      displaystyle  phi  
  
 that assigns a finite dimensional vector to each state action pair  Then  the action values of a state action pair 
  
    
      
         
        s
         
        a
         
      
    
      displaystyle  s a  
  
 are obtained by linearly combining the components of 
  
    
      
          x d  
         
        s
         
        a
         
      
    
      displaystyle  phi  s a  
  
 with some weights 
  
    
      
          x b  
      
    
      displaystyle  theta  
  
 


  
    
      
        Q
         
        s
         
        a
         
         
        
            x     
          
            i
             
             
          
          
            d
          
        
        
            x b  
          
            i
          
        
        
            x d  
          
            i
          
        
         
        s
         
        a
         
         
      
    
      displaystyle Q s a   sum   i     d  theta   i  phi   i  s a   
  

The algorithms then adjust the weights  instead of adjusting the values associated with the individual state action pairs  Methods based on ideas from nonparametric statistics  which can be seen to construct their own features  have been explored 
Value iteration can also be used as a starting point  giving rise to the Q learning algorithm and its many variants              Including Deep Q learning methods when a neural network is used to represent Q  with various applications in stochastic search problems             
The problem with using action values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy  though this problem is mitigated to some extent by temporal difference methods  Using the so called compatible function approximation method compromises generality and efficiency 

Direct policy search edit 
An alternative method is to search directly in  some subset of  the policy space  in which case the problem becomes a case of stochastic optimization  The two approaches available are gradient based and gradient free methods 
Gradient based methods  policy gradient methods  start with a mapping from a finite dimensional  parameter  space to the space of policies  given the parameter vector 
  
    
      
          x b  
      
    
      displaystyle  theta  
  
  let 
  
    
      
        
            x c  
          
              x b  
          
        
      
    
      displaystyle  pi    theta   
  
 denote the policy associated to 
  
    
      
          x b  
      
    
      displaystyle  theta  
  
  Defining the performance function by 
  
    
      
          x c  
         
          x b  
         
         
        
            x c  
          
            
                x c  
              
                  x b  
              
            
          
        
      
    
      displaystyle  rho   theta    rho    pi    theta    
  
 under mild conditions this function will be differentiable as a function of the parameter vector 
  
    
      
          x b  
      
    
      displaystyle  theta  
  
  If the gradient of 
  
    
      
          x c  
      
    
      displaystyle  rho  
  
 was known  one could use gradient ascent  Since an analytic expression for the gradient is not available  only a noisy estimate is available  Such an estimate can be constructed in many ways  giving rise to algorithms such as Williams s REINFORCE method              which is known as the likelihood ratio method in the simulation based optimization literature              
A large class of methods avoids relying on gradient information  These include simulated annealing  cross entropy search or methods of evolutionary computation  Many gradient free methods can achieve  in theory and in the limit  a global optimum 
Policy search methods may converge slowly given noisy data  For example  this happens in episodic problems when the trajectories are long and the variance of the returns is large  Value function based methods that rely on temporal differences might help in this case  In recent years  actor critic methods have been proposed and performed well on various problems             
Policy search methods have been used in the robotics context              Many policy search methods may get stuck in local optima  as they are based on local search  

Model based algorithms edit 
Finally  all of the above methods can be combined with algorithms that first learn a model of the Markov decision process  the probability of each next state given an action taken from an existing state  For instance  the Dyna algorithm learns a model from experience  and uses that to provide more modelled transitions for a value function  in addition to the real transitions              Such methods can sometimes be extended to use of non parametric models  such as when the transitions are simply stored and  replayed  to the learning algorithm             
Model based methods can be more computationally intensive than model free approaches  and their utility can be limited by the extent to which the Markov decision process can be learnt             
There are other ways to use models than to update a value function              For instance  in model predictive control the model is used to update the behavior directly 

Theory edit 
Both the asymptotic and finite sample behaviors of most algorithms are well understood  Algorithms with provably good online performance  addressing the exploration issue  are known 
Efficient exploration of Markov decision processes is given in Burnetas and Katehakis                     Finite time performance bounds have also appeared for many algorithms  but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations 
For incremental algorithms  asymptotic convergence issues have been settled      clarification needed      Temporal difference based algorithms converge under a wider set of conditions than was previously possible  for example  when used with arbitrary  smooth function approximation  

Research edit 
This section needs additional citations for verification  Please help improve this article by adding citations to reliable sources     in this section  Unsourced material may be challenged and removed    October        Learn how and when to remove this message 
Research topics include 

actor critic architecture            
actor critic scenery architecture           
adaptive methods that work with fewer  or no  parameters under a large number of conditions
bug detection in software projects            
continuous learning
combinations with logic based frameworks            
exploration in large Markov decision processes
entity based reinforcement learning                                    
human feedback            
interaction between implicit and explicit learning in skill acquisition
intrinsic motivation which differentiates information seeking  curiosity type behaviours from task dependent goal directed behaviours large scale empirical evaluations
large  or continuous  action spaces
modular and hierarchical reinforcement learning            
multiagent distributed reinforcement learning is a topic of interest  Applications are expanding             
occupant centric control
optimization of computing resources                                    
partial information  e g   using predictive state representation 
reward function based on maximising novel information                                    
sample based planning  e g   based on Monte Carlo tree search  
securities trading            
transfer learning            
TD learning modeling dopamine based learning in the brain  Dopaminergic projections from the substantia nigra to the basal ganglia function are the prediction error 
value function and policy search methods
Comparison of key algorithms edit 
The following table lists the key algorithms for learning a policy depending on several criteria 

The algorithm can be on policy  it performs policy updates using trajectories sampled via the current policy              or off policy 
The action space may be discrete  e g  the action space could be  going up    going left    going right    going down    stay   or continuous  e g  moving the arm with a given angle  
The state space may be discrete  e g  the agent could be in a cell in a grid  or continuous  e g  the agent could be located at a given position in the plane  



Algorithm
Description
Policy
Action space
State space
Operator


Monte Carlo
Every visit to Monte Carlo
Either
Discrete
Discrete
Sample means of state values or action values


TD learning
State action reward state
Off policy
Discrete
Discrete
State value


Q learning
State action reward state
Off policy
Discrete
Discrete
Action value


SARSA
State action reward state action
On policy
Discrete
Discrete
Action value


DQN
Deep Q Network
Off policy
Discrete
Continuous
Action value


DDPG
Deep Deterministic Policy Gradient
Off policy
Continuous
Continuous
Action value


A C
Asynchronous Advantage Actor Critic Algorithm
On policy
Discrete
Continuous
Advantage   action value   state value 


TRPO
Trust Region Policy Optimization
On policy
Continuous or Discrete
Continuous
Advantage


PPO
Proximal Policy Optimization
On policy
Continuous or Discrete
Continuous
Advantage


TD 

Twin Delayed Deep Deterministic Policy Gradient

Off policy

Continuous

Continuous

Action value


SAC

Soft Actor Critic

Off policy

Continuous

Continuous

Advantage


DSAC                                    
Distributional Soft Actor Critic
Off policy
Continuous
Continuous
Action value distribution

Associative reinforcement learning edit 
Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks  In associative reinforcement learning tasks  the learning system interacts in a closed loop with its environment             

Deep reinforcement learning edit 
This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space              The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end to end reinforcement learning             

Adversarial deep reinforcement learning edit 
Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies  In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations                                      While some methods have been proposed to overcome these susceptibilities  in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies             

Fuzzy reinforcement learning edit 
By introducing fuzzy inference in reinforcement learning              approximating the state action value function with fuzzy rules in continuous space becomes possible  The IF   THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language  Extending FRL with Fuzzy Rule Interpolation             allows the use of reduced size sparse fuzzy rule bases to emphasize cardinal rules  most important state action values  

Inverse reinforcement learning edit 
In inverse reinforcement learning  IRL   no reward function is given  Instead  the reward function is inferred given an observed behavior from an expert  The idea is to mimic observed behavior  which is often optimal or close to optimal              One popular IRL paradigm is named maximum entropy inverse reinforcement learning  MaxEnt IRL               MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject to constraints related to matching expected feature counts  Recently it has been shown that MaxEnt IRL is a particular case of a more general framework named random utility inverse reinforcement learning  RU IRL               RU IRL is based on random utility theory and Markov decision processes  While prior IRL approaches assume that the apparent random behavior of an observed agent is due to it following a random policy  RU IRL assumes that the observed agent follows a deterministic policy but randomness in observed behavior is due to the fact that an observer only has partial access to the features the observed agent uses in decision making  The utility function is modeled as a random variable to account for the ignorance of the observer regarding the features the observed agent actually considers in its utility function 

Multi objective reinforcement learning edit 
Multi objective reinforcement learning  MORL  is a form of reinforcement learning concerned with conflicting alternatives  It is distinct from multi objective optimization in that it is concerned with agents acting in environments                         

Safe reinforcement learning edit 
Safe reinforcement learning  SRL  can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and or respect safety constraints during the learning and or deployment processes              An alternative approach is risk averse reinforcement learning  where instead of the expected return  a risk measure of the return is optimized  such as the conditional value at risk  CVaR               In addition to mitigating risk  the CVaR objective increases robustness to model uncertainties                          However  CVaR optimization in risk averse RL requires special care  to prevent gradient bias             and blindness to success             

Self reinforcement learning edit 
Self reinforcement learning  or self learning   is a learning paradigm which does not use the concept of immediate reward 
  
    
      
        
          R
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
      
    
      displaystyle R  a  s s   
  
 after transition from 
  
    
      
        s
      
    
      displaystyle s 
  
 to 
  
    
      
        
          s
            x     
        
      
    
      displaystyle s  
  
 with action 
  
    
      
        a
      
    
      displaystyle a 
  
  It does not use an external reinforcement  it only uses the agent internal self reinforcement  The internal self reinforcement is provided by mechanism of feelings and emotions  In the learning process emotions are backpropagated by a mechanism of secondary reinforcement  The learning equation does not include the immediate reward  it only includes the state evaluation 
The self reinforcement algorithm updates a memory matrix 
  
    
      
        W
         
        
           
        
        
           
        
        w
         
        a
         
        s
         
        
           
        
        
           
        
      
    
      displaystyle W   w a s    
  
 such that in each iteration executes the following machine learning routine 

In situation 
  
    
      
        s
      
    
      displaystyle s 
  
 perform action 
  
    
      
        a
      
    
      displaystyle a 
  
 
Receive a consequence situation 
  
    
      
        
          s
            x     
        
      
    
      displaystyle s  
  
 
Compute state evaluation 
  
    
      
        v
         
        
          s
            x     
        
         
      
    
      displaystyle v s   
  
 of how good is to be in the consequence situation 
  
    
      
        
          s
            x     
        
      
    
      displaystyle s  
  
 
Update crossbar memory 
  
    
      
        
          w
            x     
        
         
        a
         
        s
         
         
        w
         
        a
         
        s
         
         
        v
         
        
          s
            x     
        
         
      
    
      displaystyle w  a s  w a s  v s   
  
 
Initial conditions of the memory are received as input from the genetic environment  It is a system with only one input  situation   and only one output  action  or behavior  
Self reinforcement  self learning  was introduced in      along with a neural network capable of self reinforcement learning  named Crossbar Adaptive Array  CAA                           The CAA computes  in a crossbar fashion  both decisions about actions and emotions  feelings  about consequence states  The system is driven by the interaction between cognition and emotion             

Reinforcement Learning in Natural Language Processing edit 
In recent years  Reinforcement learning has become a significant concept in Natural Language Processing  NLP   where tasks are often sequential decision making rather than static classification  Reinforcement learning is where an agent take actions in an environment to maximize the accumulation of rewards  This framework is best fit for many NLP tasks  including dialogue generation  text summarization  and machine translation  where the quality of the output depends on optimizing long term or human centered goals rather than the prediction of single correct label 
Early application of RL in NLP emerged in dialogue systems  where conversation was determined as a series of actions optimized for fluency and coherence  These early attempts  including policy gradient and sequence level training techniques  laid a foundation for the broader application of reinforcement learning to other areas of NLP 
A major breakthrough happened with the introduction of Reinforcement Learning from Human Feedback  RLHF   a method in which human feedbacks are used to train a reward model that guides the RL agent  Unlike traditional rule based or supervised systems  RLHF allows models to align their behavior with human judgments on complex and subjective tasks  This technique was initially used in the development of InstructGPT  an effective language model trained to follow human instructions and later in ChatGPT which incorporates RLHF for improving output responses and ensuring safety 
More recently  researchers have explored the use of offline RL in NLP to improve dialogue systems without the need of live human interaction  These methods optimize for user engagement  coherence  and diversity based on past conversation logs and pre trained reward models 

Statistical comparison of reinforcement learning algorithms edit 
Efficient comparison of RL algorithms is essential for research  deployment and monitoring of RL systems  To compare different algorithms on a given environment  an agent can be trained for each algorithm  Since the performance is sensitive to implementation details  all algorithms should be implemented as closely as possible to each other              After the training is finished  the agents can be run on a sample of test episodes  and their scores  returns  can be compared  Since episodes are typically assumed to be i i d  standard statistical tools can be used for hypothesis testing  such as T test and permutation test              This requires to accumulate all the rewards within an episode into a single number the episodic return  However  this causes a loss of information  as different time steps are averaged together  possibly with different levels of noise  Whenever the noise level varies across the episode  the statistical power can be improved significantly  by weighting the rewards according to their estimated noise             

Challenges and Limitations edit 
Despite significant advancements  reinforcement learning  RL  continues to face several challenges and limitations that hinder its widespread application in real world scenarios 

Sample Inefficiency edit 
RL algorithms often require a large number of interactions with the environment to learn effective policies  leading to high computational costs and time intensive to train the agent  For instance  OpenAI s Dota playing bot utilized thousands of years of simulated gameplay to achieve human level performance  Techniques like experience replay and curriculum learning have been proposed to deprive sample inefficiency  but these techniques add more complexity and are not always sufficient for real world applications 

Stability and Convergence Issues edit 
Training RL models  particularly for deep neural network based models  can be unstable and prone to divergence  A small change in the policy or environment can lead to extreme fluctuations in performance  making it difficult to achieve consistent results  This instability is further enhanced in the case of the continuous or high dimensional action space  where the learning step becomes more complex and less predictable 

Generalization and Transferability edit 
The RL agents trained in specific environments often struggle to generalize their learned policies to new  unseen scenarios  This is the major setback preventing the application of RL to dynamic real world environments where adaptability is crucial  The challenge is to develop such algorithms that can transfer knowledge across tasks and environments without extensive retraining 

Bias and Reward Function Issues edit 
Designing appropriate reward functions is critical in RL because poorly designed reward functions can lead to unintended behaviors  In addition  RL systems trained on biased data may perpetuate existing biases and lead to discriminatory or unfair outcomes  Both of these issues requires careful consideration of reward structures and data sources to ensure fairness and desired behaviors 

See also edit 

Active learning  machine learning 
Apprenticeship learning
Error driven learning
Model free  reinforcement learning 
Multi agent reinforcement learning
Optimal control
Q learning
Reinforcement learning from human feedback
State action reward state action  SARSA 
Temporal difference learning

References edit 


  Kaelbling  Leslie P   Littman  Michael L   Moore  Andrew W           Reinforcement Learning  A Survey   Journal of Artificial Intelligence Research              arXiv cs          doi         jair      S CID               Archived from the original on            

  van Otterlo  M   Wiering  M           Reinforcement Learning and Markov Decision Processes   Reinforcement Learning  Adaptation  Learning  and Optimization  Vol           pp             doi                              ISBN                        

  a b Li  Shengbo         Reinforcement Learning for Sequential Decision and Optimal Control  First      ed    Springer Verlag  Singapore  pp              doi                            ISBN                         S CID                  cite book     CS  maint  location missing publisher  link 

  Russell  Stuart J   Norvig  Peter         Artificial intelligence        a modern approach  Third      ed    Upper Saddle River  New Jersey  Prentice Hall  pp                 ISBN                        

  Lee  Daeyeol  Seo  Hyojung  Jung  Min Whan     July         Neural Basis of Reinforcement Learning and Decision Making   Annual Review of Neuroscience                   doi         annurev neuro                PMC               PMID               

  Salazar Duque  Edgar Mauricio  Giraldo  Juan S   Vergara  Pedro P   Nguyen  Phuong  Van Der Molen  Anne  Slootweg  Han          Community energy storage operation via reinforcement learning with eligibility traces   Electric Power Systems Research       Bibcode     EPSR          S  doi         j epsr              S CID                

  Xie  Zhaoming  Hung Yu Ling  Nam Hee Kim  Michiel van de Panne          ALLSTEPS  Curriculum driven Learning of Stepping Stone Skills   arXiv             cs GR  

  Vergara  Pedro P   Salazar  Mauricio  Giraldo  Juan S   Palensky  Peter          Optimal dispatch of PV inverters in unbalanced distribution systems using Reinforcement Learning   International Journal of Electrical Power  amp  Energy Systems       Bibcode     IJEPE         V  doi         j ijepes              S CID                

  Sutton  amp  Barto       Chapter    

  Ren  Yangang  Jiang  Jianhua  Zhan  Guojian  Li  Shengbo Eben  Chen  Chen  Li  Keqiang  Duan  Jingliang          Self Learned Intelligence for Integrated Decision and Control of Automated Vehicles at Signalized Intersections   IEEE Transactions on Intelligent Transportation Systems                        arXiv             doi         TITS              

  Gosavi  Abhijit         Simulation based Optimization  Parametric Optimization Techniques and Reinforcement  Operations Research Computer Science Interfaces Series  Springer  ISBN                        

  a b Burnetas  Apostolos N   Katehakis  Michael N           Optimal adaptive policies for Markov Decision Processes   Mathematics of Operations Research                   doi         moor           JSTOR             

  Tokic  Michel  Palm  G nther          Value Difference Based Exploration  Adaptive Control Between Epsilon Greedy and Softmax   PDF   KI       Advances in Artificial Intelligence  Lecture Notes in Computer Science  vol             Springer  pp                ISBN                       

  a b c  Reinforcement learning  An introduction   PDF   Archived from the original  PDF  on             Retrieved            

  Singh  Satinder P   Sutton  Richard S                 Reinforcement learning with replacing eligibility traces   Machine Learning                   doi         BF          ISSN                

  Sutton  Richard S          Temporal Credit Assignment in Reinforcement Learning  PhD thesis   University of Massachusetts  Amherst  MA  Archived from the original on             Retrieved            

  Sutton  amp  Barto           Temporal Difference Learning 

  Bradtke  Steven J   Barto  Andrew G           Learning to predict by the method of temporal differences   Machine Learning             CiteSeerX                      doi         A                S CID               

  Watkins  Christopher J C H          Learning from Delayed Rewards  PDF   PhD thesis   King s College  Cambridge  UK 

  Matzliach  Barouch  Ben Gal  Irad  Kagan  Evgeny          Detection of Static and Mobile Targets by an Autonomous Agent with Deep Q Learning Abilities   Entropy                Bibcode     Entrp         M  doi         e          PMC               PMID               

  Williams  Ronald J           A class of gradient estimating algorithms for reinforcement learning in neural networks   Proceedings of the IEEE First International Conference on Neural Networks  CiteSeerX                      

  Peters  Jan  Vijayakumar  Sethu  Schaal  Stefan         Reinforcement Learning for Humanoid Robotics  PDF   IEEE RAS International Conference on Humanoid Robots  Archived from the original  PDF  on            

  Juliani  Arthur                Simple Reinforcement Learning with Tensorflow Part    Asynchronous Actor Critic Agents  A C    Medium  Retrieved            

  Deisenroth  Marc Peter  Neumann  Gerhard  Peters  Jan         A Survey on Policy Search for Robotics  PDF   Foundations and Trends in Robotics  Vol          NOW Publishers  pp              doi                     hdl               

  Sutton  Richard          Integrated Architectures for Learning  Planning and Reacting based on Dynamic Programming   Machine Learning  Proceedings of the Seventh International Workshop 

  Lin  Long Ji          Self improving reactive agents based on reinforcement learning  planning and teaching   PDF   Machine Learning volume    doi         BF         

  Zou  Lan               Zou  Lan  ed     Chapter     Meta reinforcement learning   Meta Learning  Academic Press  pp                doi         b                           ISBN                         retrieved           

  van Hasselt  Hado  Hessel  Matteo  Aslanides  John          When to use parametric models in reinforcement learning    PDF   Advances in Neural Information Processing Systems    

  Grondman  Ivo  Vaandrager  Maarten  Busoniu  Lucian  Babuska  Robert  Schuitema  Erik                Efficient Model Learning Methods for Actor Critic Control   IEEE Transactions on Systems  Man  and Cybernetics  Part B  Cybernetics                    doi         TSMCB               ISSN                 PMID               

   On the Use of Reinforcement Learning for Testing Game Mechanics        ACM   Computers in Entertainment   cie acm org  Retrieved            

  Riveret  Regis  Gao  Yang          A probabilistic argumentation framework for reinforcement learning agents   Autonomous Agents and Multi Agent Systems                     doi         s                   S CID               

  Haramati  Dan  Daniel  Tal  Tamar  Aviv          Entity Centric Reinforcement Learning for Object Manipulation from Pixels   arXiv            

  Thompson  Isaac Symes  Caron  Alberto  Hicks  Chris  Mavroudis  Vasilios                Entity based Reinforcement Learning for Autonomous Cyber Defence   Proceedings of the Workshop on Autonomous Cybersecurity  AutonomousCyber       ACM  pp              doi                         

  Winter  Clemens                Entity Based Reinforcement Learning   Clemens Winter s Blog 

  Yamagata  Taku  McConville  Ryan  Santos Rodriguez  Raul                Reinforcement Learning with Feedback from Multiple Humans with Diverse Skills   arXiv             cs LG  

  Kulkarni  Tejas D   Narasimhan  Karthik R   Saeedi  Ardavan  Tenenbaum  Joshua B           Hierarchical Deep Reinforcement Learning  Integrating Temporal Abstraction and Intrinsic Motivation   Proceedings of the   th International Conference on Neural Information Processing Systems  NIPS     USA  Curran Associates Inc              arXiv             Bibcode     arXiv         K  ISBN                        

   Reinforcement Learning   Successes of Reinforcement Learning   umichrl pbworks com  Retrieved            

  Dey  Somdip  Singh  Amit Kumar  Wang  Xiaohang  McDonald Maier  Klaus  March         User Interaction Aware Reinforcement Learning for Power and Thermal Efficiency of CPU GPU Mobile MPSoCs        Design  Automation  amp  Test in Europe Conference  amp  Exhibition  DATE   PDF   pp                  doi          DATE                    ISBN                         S CID                

  Quested  Tony   Smartphones get smarter with Essex innovation   Business Weekly  Retrieved            

  Williams  Rhiannon                Future smartphones  will prolong their own battery life by monitoring owners  behaviour    i  Retrieved            

  Kaplan  F   Oudeyer  P           Maximizing Learning Progress  An Internal Reward System for Development   In Iida  F   Pfeifer  R   Steels  L   Kuniyoshi  Y   eds    Embodied Artificial Intelligence  Lecture Notes in Computer Science  Vol             Berlin  Heidelberg  Springer  pp                doi                               ISBN                         S CID              

  Klyubin  A   Polani  D   Nehaniv  C           Keep your options open  an information based driving principle for sensorimotor systems   PLOS ONE          e      Bibcode     PLoSO         K  doi         journal pone          PMC               PMID               

  Barto  A  G           Intrinsic motivation and reinforcement learning   Intrinsically Motivated Learning in Natural and Artificial Systems  PDF   Berlin  Heidelberg  Springer  pp             

  Dab rius  Kevin  Granat  Elvin  Karlsson  Patrik          Deep Execution   Value and Policy Based Reinforcement Learning for Trading and Beating Market Benchmarks   The Journal of Machine Learning in Finance     SSRN              

  George Karimpanal  Thommen  Bouffanais  Roland          Self organizing maps for storage and transfer of knowledge in reinforcement learning   Adaptive Behavior                   arXiv             doi                           ISSN                 S CID               

  cf  Sutton  amp  Barto       Section      p     

  J Duan  Y Guan  S Li          Distributional Soft Actor Critic  Off policy reinforcement learning for addressing value estimation errors   IEEE Transactions on Neural Networks and Learning Systems                      arXiv             doi         TNNLS               PMID                S CID                

  Y Ren  J Duan  S Li          Improving Generalization of Reinforcement Learning with Minimax Distributional Soft Actor Critic        IEEE   rd International Conference on Intelligent Transportation Systems  ITSC   pp            arXiv             doi         ITSC                    ISBN                         S CID                

  Duan  J  Wang  W  Xiao  L          Distributional Soft Actor Critic with Three Refinements   IEEE Transactions on Pattern Analysis and Machine Intelligence  PP                 arXiv             doi         TPAMI               PMID               

  Soucek  Branko    May        Dynamic  Genetic and Chaotic Programming  The Sixth Generation Computer Technology Series  John Wiley  amp  Sons  Inc  p           ISBN                  X 

  Francois Lavet  Vincent  et      al           An Introduction to Deep Reinforcement Learning   Foundations and Trends in Machine Learning                     arXiv             Bibcode     arXiv         F  doi                     S CID               

  Mnih  Volodymyr  et      al           Human level control through deep reinforcement learning   Nature                       Bibcode     Natur         M  doi         nature       PMID                S CID                

  Goodfellow  Ian  Shlens  Jonathan  Szegedy  Christian          Explaining and Harnessing Adversarial Examples   International Conference on Learning Representations  arXiv           

  Behzadan  Vahid  Munir  Arslan          Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks   Machine Learning and Data Mining in Pattern Recognition  Lecture Notes in Computer Science  Vol              pp                arXiv             doi                               ISBN                         S CID              

  Huang  Sandy  Papernot  Nicolas  Goodfellow  Ian  Duan  Yan  Abbeel  Pieter               Adversarial Attacks on Neural Network Policies  OCLC                 

  Korkmaz  Ezgi          Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs   Thirty Sixth AAAI Conference on Artificial Intelligence  AAAI                         arXiv             doi         aaai v  i         S CID                

  Berenji  H R           Fuzzy Q learning  A new approach for fuzzy dynamic programming   Proceedings of      IEEE  rd International Fuzzy Systems Conference  Orlando  FL  USA  IEEE  pp                doi         FUZZY              ISBN                  X  S CID               

  Vincze  David          Fuzzy rule interpolation and reinforcement learning   PDF        IEEE   th International Symposium on Applied Machine Intelligence and Informatics  SAMI   IEEE  pp                doi         SAMI               ISBN                         S CID               

  Ng  A  Y   Russell  S  J           Algorithms for Inverse Reinforcement Learning   PDF   Proceeding ICML     Proceedings of the Seventeenth International Conference on Machine Learning  Morgan Kaufmann Publishers  pp                ISBN                    

  Ziebart  Brian D   Maas  Andrew  Bagnell  J  Andrew  Dey  Anind K                 Maximum entropy inverse reinforcement learning   Proceedings of the   rd National Conference on Artificial Intelligence   Volume    AAAI     Chicago  Illinois  AAAI Press             ISBN                         S CID             

  Pitombeira Neto  Anselmo R   Santos  Helano P   Coelho da Silva  Ticiana L   de Macedo  Jos  Antonio F   March         Trajectory modeling via random utility inverse reinforcement learning   Information Sciences               arXiv             doi         j ins              ISSN                 S CID                

  Hayes C  Radulescu R  Bargiacchi E  et      al           A practical guide to multi objective reinforcement learning and planning   Autonomous Agents and Multi Agent Systems      arXiv             doi         s                y  S CID                 

  Tzeng  Gwo Hshiung  Huang  Jih Jeng         Multiple Attribute Decision Making  Methods and Applications   st      ed    CRC Press  ISBN                    

  Garc a  Javier  Fern ndez  Fernando    January         A comprehensive survey on safe reinforcement learning   PDF   The Journal of Machine Learning Research                    

  Dabney  Will  Ostrovski  Georg  Silver  David  Munos  Remi                Implicit Quantile Networks for Distributional Reinforcement Learning   Proceedings of the   th International Conference on Machine Learning  PMLR             arXiv            

  Chow  Yinlam  Tamar  Aviv  Mannor  Shie  Pavone  Marco          Risk Sensitive and Robust Decision Making  a CVaR Optimization Approach   Advances in Neural Information Processing Systems      Curran Associates  Inc  arXiv            

   Train Hard  Fight Easy  Robust Meta Reinforcement Learning   scholar google com  Retrieved            

  Tamar  Aviv  Glassner  Yonatan  Mannor  Shie                Optimizing the CVaR via Sampling   Proceedings of the AAAI Conference on Artificial Intelligence          arXiv            doi         aaai v  i        ISSN                

  Greenberg  Ido  Chow  Yinlam  Ghavamzadeh  Mohammad  Mannor  Shie                Efficient Risk Averse Reinforcement Learning   Advances in Neural Information Processing Systems                   arXiv            

  Bozinovski  S           A self learning system using secondary reinforcement   In Trappl  Robert  ed    Cybernetics and Systems Research  Proceedings of the Sixth European Meeting on Cybernetics and Systems Research  North Holland  pp           ISBN                  

  Bozinovski S          Neuro genetic agents and structural theory of self reinforcement learning systems   CMPSCI Technical Report         University of Massachusetts at Amherst    

  Bozinovski  S          Modeling mechanisms of cognition emotion interaction in artificial neural networks  since        Procedia Computer Science p         

  Engstrom  Logan  Ilyas  Andrew  Santurkar  Shibani  Tsipras  Dimitris  Janoos  Firdaus  Rudolph  Larry  Madry  Aleksander                Implementation Matters in Deep RL  A Case Study on PPO and TRPO   ICLR 

  Colas  C dric                A Hitchhiker s Guide to Statistical Comparisons of Reinforcement Learning Algorithms   International Conference on Learning Representations  arXiv            

  Greenberg  Ido  Mannor  Shie                Detecting Rewards Deterioration in Episodic Reinforcement Learning   Proceedings of the   th International Conference on Machine Learning  PMLR             arXiv            


Further reading edit 
Annaswamy  Anuradha M     May         Adaptive Control and Intersections with Reinforcement Learning   Annual Review of Control  Robotics  and Autonomous Systems                doi         annurev control                ISSN                 S CID                
Auer  Peter  Jaksch  Thomas  Ortner  Ronald          Near optimal regret bounds for reinforcement learning   Journal of Machine Learning Research                
Bertsekas  Dimitri P                 REINFORCEMENT LEARNING AND OPTIMAL CONTROL   st      ed    Athena Scientific  ISBN                        
Busoniu  Lucian  Babuska  Robert  De Schutter  Bart  Ernst  Damien         Reinforcement Learning and Dynamic Programming using Function Approximators  Taylor  amp  Francis CRC Press  ISBN                        
Fran ois Lavet  Vincent  Henderson  Peter  Islam  Riashat  Bellemare  Marc G   Pineau  Joelle          An Introduction to Deep Reinforcement Learning   Foundations and Trends in Machine Learning                     arXiv             Bibcode     arXiv         F  doi                     S CID               
Li  Shengbo Eben         Reinforcement Learning for Sequential Decision and Optimal Control   st      ed    Springer Verlag  Singapore  doi                            ISBN                        
Powell  Warren         Approximate dynamic programming  solving the curses of dimensionality  Wiley Interscience  Archived from the original on             Retrieved            
Sutton  Richard S           Learning to predict by the method of temporal differences   Machine Learning           doi         BF         
Sutton  Richard S   Barto  Andrew G                 Reinforcement Learning  An Introduction   nd      ed    MIT Press  ISBN                        
Szita  Istvan  Szepesvari  Csaba          Model based Reinforcement Learning with Nearly Tight Exploration Complexity Bounds   PDF   ICML       Omnipress  pp                  Archived from the original  PDF  on            
External links edit 
Dissecting Reinforcement Learning Series of blog post on reinforcement learning with Python code
A  Long  Peek into Reinforcement Learning
vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects

vteComputer scienceNote  This template roughly follows the      ACM Computing Classification System Hardware
Printed circuit board
Peripheral
Integrated circuit
Very Large Scale Integration
Systems on Chip  SoCs 
Energy consumption  Green computing 
Electronic design automation
Hardware acceleration
Processor
Size   Form
Computer systems organization
Computer architecture
Computational complexity
Dependability
Embedded system
Real time computing
Networks
Network architecture
Network protocol
Network components
Network scheduler
Network performance evaluation
Network service
Software organization
Interpreter
Middleware
Virtual machine
Operating system
Software quality
Software notations and tools
Programming paradigm
Programming language
Compiler
Domain specific language
Modeling language
Software framework
Integrated development environment
Software configuration management
Software library
Software repository
Software development
Control variable
Software development process
Requirements analysis
Software design
Software construction
Software deployment
Software engineering
Software maintenance
Programming team
Open source model
Theory of computation
Model of computation
Stochastic
Formal language
Automata theory
Computability theory
Computational complexity theory
Logic
Semantics
Algorithms
Algorithm design
Analysis of algorithms
Algorithmic efficiency
Randomized algorithm
Computational geometry
Mathematics of computing
Discrete mathematics
Probability
Statistics
Mathematical software
Information theory
Mathematical analysis
Numerical analysis
Theoretical computer science
Information systems
Database management system
Information storage systems
Enterprise information system
Social information systems
Geographic information system
Decision support system
Process control system
Multimedia information system
Data mining
Digital library
Computing platform
Digital marketing
World Wide Web
Information retrieval
Security
Cryptography
Formal methods
Security hacker
Security services
Intrusion detection system
Hardware security
Network security
Information security
Application security
Human computer interaction
Interaction design
Augmented reality
Virtual reality
Social computing
Ubiquitous computing
Visualization
Accessibility
Concurrency
Concurrent computing
Parallel computing
Distributed computing
Multithreading
Multiprocessing
Artificial intelligence
Natural language processing
Knowledge representation and reasoning
Computer vision
Automated planning and scheduling
Search methodology
Control method
Philosophy of artificial intelligence
Distributed artificial intelligence
Machine learning
Supervised learning
Unsupervised learning
Reinforcement learning
Multi task learning
Cross validation
Graphics
Animation
Rendering
Photograph manipulation
Graphics processing unit
Image compression
Solid modeling
Applied computing
Quantum Computing
E commerce
Enterprise software
Computational mathematics
Computational physics
Computational chemistry
Computational biology
Computational social science
Computational engineering
Differentiable computing
Computational healthcare
Digital art
Electronic publishing
Cyberwarfare
Electronic voting
Video games
Word processing
Operations research
Educational technology
Document management

 Category
 Outline
 Glossaries






Retrieved from  https   en wikipedia org w index php title Reinforcement learning amp oldid