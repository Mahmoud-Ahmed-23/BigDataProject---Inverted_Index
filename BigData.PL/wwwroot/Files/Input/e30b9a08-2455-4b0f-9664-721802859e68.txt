Type of recurrent neural network architecture
 LSTM  redirects here  For other uses  see LSTM  disambiguation  
This article may be too technical for most readers to understand  Please help improve it to make it understandable to non experts  without removing the technical details    March        Learn how and when to remove this message 
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
The Long Short Term Memory  LSTM  cell can process data sequentially and keep its hidden state through time 
Long short term memory  LSTM             is a type of recurrent neural network  RNN  aimed at mitigating the vanishing gradient problem            commonly encountered by traditional RNNs  Its relative insensitivity to gap length is its advantage over other RNNs  hidden Markov models  and other sequence learning methods  It aims to provide a short term memory for RNN that can last thousands of timesteps  thus  long short term memory               The name is made in analogy with long term memory and short term memory and their relationship  studied by cognitive psychologists since the early   th century 
An LSTM unit is typically composed of a cell and three gates  an input gate  an output gate             and a forget gate             The cell remembers values over arbitrary time intervals  and the gates regulate the flow of information into and out of the cell  Forget gates decide what information to discard from the previous state  by mapping the previous state and the current input to a value between   and    A  rounded  value of   signifies retention of the information  and a value of   represents discarding  Input gates decide which pieces of new information to store in the current cell state  using the same system as forget gates  Output gates control which pieces of information in the current cell state to output  by assigning a value from   to   to the information  considering the previous and current states  Selectively outputting relevant information from the current state allows the LSTM network to maintain useful  long term dependencies to make predictions  both in current and future time steps 
LSTM has wide applications in classification                        data processing  time series analysis tasks             speech recognition                         machine translation                          speech activity detection              robot control                          video games                          healthcare             


Motivation edit 
In theory  classic RNNs can keep track of arbitrary long term dependencies in the input sequences  The problem with classic RNNs is computational  or practical  in nature  when training a classic RNN using back propagation  the long term gradients which are back propagated can  vanish   meaning they can tend to zero due to very small numbers creeping into the computations  causing the model to effectively stop learning  RNNs using LSTM units partially solve the vanishing gradient problem  because LSTM units allow gradients to also flow with little to no attenuation  However  LSTM networks can still suffer from the exploding gradient problem             
The intuition behind the LSTM architecture is to create an additional module in a neural network that learns when to remember and when to forget pertinent information             In other words  the network effectively learns which information might be needed later on in a sequence and when that information is no longer needed  For instance  in the context of natural language processing  the network can learn grammatical dependencies              An LSTM might process the sentence  Dave  as a result of his controversial claims  is now a pariah  by remembering the  statistically likely  grammatical gender and number of the subject Dave  note that this information is pertinent for the pronoun his and note that this information is no longer important after the verb is 

Variants edit 
In the equations below  the lowercase variables represent vectors  Matrices 
  
    
      
        
          W
          
            q
          
        
      
    
      displaystyle W  q  
  
 and 
  
    
      
        
          U
          
            q
          
        
      
    
      displaystyle U  q  
  
 contain  respectively  the weights of the input and recurrent connections  where the subscript 
  
    
      
        
          
          
            q
          
        
      
    
      displaystyle   q  
  
 can either be the input gate 
  
    
      
        i
      
    
      displaystyle i 
  
  output gate 
  
    
      
        o
      
    
      displaystyle o 
  
  the forget gate 
  
    
      
        f
      
    
      displaystyle f 
  
 or the memory cell 
  
    
      
        c
      
    
      displaystyle c 
  
  depending on the activation being calculated  In this section  we are thus using a  vector notation   So  for example  
  
    
      
        
          c
          
            t
          
        
          x     
        
          
            R
          
          
            h
          
        
      
    
      displaystyle c  t  in  mathbb  R    h  
  
 is not just one unit of one LSTM cell  but contains 
  
    
      
        h
      
    
      displaystyle h 
  
 LSTM cell s units 
See              for an empirical study of   architectural variants of LSTM 

LSTM with a forget gate edit 
The compact forms of the equations for the forward pass of an LSTM cell with a forget gate are                       


  
    
      
        
          
            
              
                
                  f
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    g
                  
                
                 
                
                  W
                  
                    f
                  
                
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    f
                  
                
                
                  h
                  
                    t
                      x     
                     
                  
                
                 
                
                  b
                  
                    f
                  
                
                 
              
            
            
              
                
                  i
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    g
                  
                
                 
                
                  W
                  
                    i
                  
                
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    i
                  
                
                
                  h
                  
                    t
                      x     
                     
                  
                
                 
                
                  b
                  
                    i
                  
                
                 
              
            
            
              
                
                  o
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    g
                  
                
                 
                
                  W
                  
                    o
                  
                
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    o
                  
                
                
                  h
                  
                    t
                      x     
                     
                  
                
                 
                
                  b
                  
                    o
                  
                
                 
              
            
            
              
                
                  
                    
                      
                        c
                          x e 
                      
                    
                  
                  
                    t
                  
                
              
              
                
                 
                
                  tanh
                  
                    c
                  
                
                  x     
                 
                
                  W
                  
                    c
                  
                
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    c
                  
                
                
                  h
                  
                    t
                      x     
                     
                  
                
                 
                
                  b
                  
                    c
                  
                
                 
              
            
            
              
                
                  c
                  
                    t
                  
                
              
              
                
                 
                
                  f
                  
                    t
                  
                
                  x     
                
                  c
                  
                    t
                      x     
                     
                  
                
                 
                
                  i
                  
                    t
                  
                
                  x     
                
                  
                    
                      
                        c
                          x e 
                      
                    
                  
                  
                    t
                  
                
              
            
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                 
                
                  o
                  
                    t
                  
                
                  x     
                
                    x c  
                  
                    h
                  
                
                 
                
                  c
                  
                    t
                  
                
                 
              
            
          
        
      
    
      displaystyle   begin aligned f  t  amp   sigma   g  W  f x  t  U  f h  t    b  f    i  t  amp   sigma   g  W  i x  t  U  i h  t    b  i    o  t  amp   sigma   g  W  o x  t  U  o h  t    b  o      tilde  c    t  amp   tanh   c  W  c x  t  U  c h  t    b  c    c  t  amp  f  t  odot c  t    i  t  odot   tilde  c    t   h  t  amp  o  t  odot  sigma   h  c  t   end aligned   
  

where the initial values are 
  
    
      
        
          c
          
             
          
        
         
         
      
    
      displaystyle c       
  
 and 
  
    
      
        
          h
          
             
          
        
         
         
      
    
      displaystyle h       
  
 and the operator 
  
    
      
          x     
      
    
      displaystyle  odot  
  
 denotes the Hadamard product  element wise product   The subscript 
  
    
      
        t
      
    
      displaystyle t 
  
 indexes the time step 

Variables edit 
Letting the superscripts 
  
    
      
        d
      
    
      displaystyle d 
  
 and 
  
    
      
        h
      
    
      displaystyle h 
  
 refer to the number of input features and number of hidden units  respectively 


  
    
      
        
          x
          
            t
          
        
          x     
        
          
            R
          
          
            d
          
        
      
    
      displaystyle x  t  in  mathbb  R    d  
  
  input vector to the LSTM unit

  
    
      
        
          f
          
            t
          
        
          x     
        
          
             
             
             
             
             
          
          
            h
          
        
      
    
      displaystyle f  t  in          h  
  
  forget gate s activation vector

  
    
      
        
          i
          
            t
          
        
          x     
        
          
             
             
             
             
             
          
          
            h
          
        
      
    
      displaystyle i  t  in          h  
  
  input update gate s activation vector

  
    
      
        
          o
          
            t
          
        
          x     
        
          
             
             
             
             
             
          
          
            h
          
        
      
    
      displaystyle o  t  in          h  
  
  output gate s activation vector

  
    
      
        
          h
          
            t
          
        
          x     
        
          
             
              x     
             
             
             
             
          
          
            h
          
        
      
    
      displaystyle h  t  in           h  
  
  hidden state vector also known as output vector of the LSTM unit

  
    
      
        
          
            
              
                c
                  x e 
              
            
          
          
            t
          
        
          x     
        
          
             
              x     
             
             
             
             
          
          
            h
          
        
      
    
      displaystyle   tilde  c    t  in           h  
  
  cell input activation vector

  
    
      
        
          c
          
            t
          
        
          x     
        
          
            R
          
          
            h
          
        
      
    
      displaystyle c  t  in  mathbb  R    h  
  
  cell state vector

  
    
      
        W
          x     
        
          
            R
          
          
            h
              xd  
            d
          
        
      
    
      displaystyle W in  mathbb  R    h times d  
  
  
  
    
      
        U
          x     
        
          
            R
          
          
            h
              xd  
            h
          
        
      
    
      displaystyle U in  mathbb  R    h times h  
  
 and 
  
    
      
        b
          x     
        
          
            R
          
          
            h
          
        
      
    
      displaystyle b in  mathbb  R    h  
  
  weight matrices and bias vector parameters which need to be learned during training
Activation functions edit 

  
    
      
        
            x c  
          
            g
          
        
      
    
      displaystyle  sigma   g  
  
  sigmoid function 

  
    
      
        
            x c  
          
            c
          
        
      
    
      displaystyle  sigma   c  
  
  hyperbolic tangent function 

  
    
      
        
            x c  
          
            h
          
        
      
    
      displaystyle  sigma   h  
  
  hyperbolic tangent function or  as the peephole LSTM paper                         suggests  
  
    
      
        
            x c  
          
            h
          
        
         
        x
         
         
        x
      
    
      displaystyle  sigma   h  x  x 
  
 
Peephole LSTM edit 
A peephole LSTM unit with input  i e  
  
    
      
        i
      
    
      displaystyle i 
  
   output  i e  
  
    
      
        o
      
    
      displaystyle o 
  
   and forget  i e  
  
    
      
        f
      
    
      displaystyle f 
  
  gates
The figure on the right is a graphical representation of an LSTM unit with peephole connections  i e  a peephole LSTM                           Peephole connections allow the gates to access the constant error carousel  CEC   whose activation is the cell state              
  
    
      
        
          h
          
            t
              x     
             
          
        
      
    
      displaystyle h  t    
  
 is not used  
  
    
      
        
          c
          
            t
              x     
             
          
        
      
    
      displaystyle c  t    
  
 is used instead in most places 


  
    
      
        
          
            
              
                
                  f
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    g
                  
                
                 
                
                  W
                  
                    f
                  
                
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    f
                  
                
                
                  c
                  
                    t
                      x     
                     
                  
                
                 
                
                  b
                  
                    f
                  
                
                 
              
            
            
              
                
                  i
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    g
                  
                
                 
                
                  W
                  
                    i
                  
                
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    i
                  
                
                
                  c
                  
                    t
                      x     
                     
                  
                
                 
                
                  b
                  
                    i
                  
                
                 
              
            
            
              
                
                  o
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    g
                  
                
                 
                
                  W
                  
                    o
                  
                
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    o
                  
                
                
                  c
                  
                    t
                      x     
                     
                  
                
                 
                
                  b
                  
                    o
                  
                
                 
              
            
            
              
                
                  c
                  
                    t
                  
                
              
              
                
                 
                
                  f
                  
                    t
                  
                
                  x     
                
                  c
                  
                    t
                      x     
                     
                  
                
                 
                
                  i
                  
                    t
                  
                
                  x     
                
                    x c  
                  
                    c
                  
                
                 
                
                  W
                  
                    c
                  
                
                
                  x
                  
                    t
                  
                
                 
                
                  b
                  
                    c
                  
                
                 
              
            
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                 
                
                  o
                  
                    t
                  
                
                  x     
                
                    x c  
                  
                    h
                  
                
                 
                
                  c
                  
                    t
                  
                
                 
              
            
          
        
      
    
      displaystyle   begin aligned f  t  amp   sigma   g  W  f x  t  U  f c  t    b  f    i  t  amp   sigma   g  W  i x  t  U  i c  t    b  i    o  t  amp   sigma   g  W  o x  t  U  o c  t    b  o    c  t  amp  f  t  odot c  t    i  t  odot  sigma   c  W  c x  t  b  c    h  t  amp  o  t  odot  sigma   h  c  t   end aligned   
  

Each of the gates can be thought as a  standard  neuron in a feed forward  or multi layer  neural network  that is  they compute an activation  using an activation function  of a weighted sum  
  
    
      
        
          i
          
            t
          
        
         
        
          o
          
            t
          
        
      
    
      displaystyle i  t  o  t  
  
 and 
  
    
      
        
          f
          
            t
          
        
      
    
      displaystyle f  t  
  
 represent the activations of respectively the input  output and forget gates  at time step 
  
    
      
        t
      
    
      displaystyle t 
  
 
The   exit arrows from the memory cell 
  
    
      
        c
      
    
      displaystyle c 
  
 to the   gates 
  
    
      
        i
         
        o
      
    
      displaystyle i o 
  
 and 
  
    
      
        f
      
    
      displaystyle f 
  
 represent the peephole connections  These peephole connections actually denote the contributions of the activation of the memory cell 
  
    
      
        c
      
    
      displaystyle c 
  
 at time step 
  
    
      
        t
          x     
         
      
    
      displaystyle t   
  
  i e  the contribution of 
  
    
      
        
          c
          
            t
              x     
             
          
        
      
    
      displaystyle c  t    
  
  and not 
  
    
      
        
          c
          
            t
          
        
      
    
      displaystyle c  t  
  
  as the picture may suggest   In other words  the gates 
  
    
      
        i
         
        o
      
    
      displaystyle i o 
  
 and 
  
    
      
        f
      
    
      displaystyle f 
  
 calculate their activations at time step 
  
    
      
        t
      
    
      displaystyle t 
  
  i e   respectively  
  
    
      
        
          i
          
            t
          
        
         
        
          o
          
            t
          
        
      
    
      displaystyle i  t  o  t  
  
 and 
  
    
      
        
          f
          
            t
          
        
      
    
      displaystyle f  t  
  
  also considering the activation of the memory cell 
  
    
      
        c
      
    
      displaystyle c 
  
 at time step 
  
    
      
        t
          x     
         
      
    
      displaystyle t   
  
  i e  
  
    
      
        
          c
          
            t
              x     
             
          
        
      
    
      displaystyle c  t    
  
 
The single left to right arrow exiting the memory cell is not a peephole connection and denotes 
  
    
      
        
          c
          
            t
          
        
      
    
      displaystyle c  t  
  
 
The little circles containing a 
  
    
      
          xd  
      
    
      displaystyle  times  
  
 symbol represent an element wise multiplication between its inputs  The big circles containing an S like curve represent the application of a differentiable function  like the sigmoid function  to a weighted sum 

Peephole convolutional LSTM edit 
Peephole convolutional LSTM              The 
  
    
      
          x     
      
    
      displaystyle   
  
 denotes the convolution operator 


  
    
      
        
          
            
              
                
                  f
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    g
                  
                
                 
                
                  W
                  
                    f
                  
                
                  x     
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    f
                  
                
                  x     
                
                  h
                  
                    t
                      x     
                     
                  
                
                 
                
                  V
                  
                    f
                  
                
                  x     
                
                  c
                  
                    t
                      x     
                     
                  
                
                 
                
                  b
                  
                    f
                  
                
                 
              
            
            
              
                
                  i
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    g
                  
                
                 
                
                  W
                  
                    i
                  
                
                  x     
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    i
                  
                
                  x     
                
                  h
                  
                    t
                      x     
                     
                  
                
                 
                
                  V
                  
                    i
                  
                
                  x     
                
                  c
                  
                    t
                      x     
                     
                  
                
                 
                
                  b
                  
                    i
                  
                
                 
              
            
            
              
                
                  c
                  
                    t
                  
                
              
              
                
                 
                
                  f
                  
                    t
                  
                
                  x     
                
                  c
                  
                    t
                      x     
                     
                  
                
                 
                
                  i
                  
                    t
                  
                
                  x     
                
                    x c  
                  
                    c
                  
                
                 
                
                  W
                  
                    c
                  
                
                  x     
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    c
                  
                
                  x     
                
                  h
                  
                    t
                      x     
                     
                  
                
                 
                
                  b
                  
                    c
                  
                
                 
              
            
            
              
                
                  o
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    g
                  
                
                 
                
                  W
                  
                    o
                  
                
                  x     
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    o
                  
                
                  x     
                
                  h
                  
                    t
                      x     
                     
                  
                
                 
                
                  V
                  
                    o
                  
                
                  x     
                
                  c
                  
                    t
                  
                
                 
                
                  b
                  
                    o
                  
                
                 
              
            
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                 
                
                  o
                  
                    t
                  
                
                  x     
                
                    x c  
                  
                    h
                  
                
                 
                
                  c
                  
                    t
                  
                
                 
              
            
          
        
      
    
      displaystyle   begin aligned f  t  amp   sigma   g  W  f  x  t  U  f  h  t    V  f  odot c  t    b  f    i  t  amp   sigma   g  W  i  x  t  U  i  h  t    V  i  odot c  t    b  i    c  t  amp  f  t  odot c  t    i  t  odot  sigma   c  W  c  x  t  U  c  h  t    b  c    o  t  amp   sigma   g  W  o  x  t  U  o  h  t    V  o  odot c  t  b  o    h  t  amp  o  t  odot  sigma   h  c  t   end aligned   
  

Training edit 
An RNN using LSTM units can be trained in a supervised fashion on a set of training sequences  using an optimization algorithm like gradient descent combined with backpropagation through time to compute the gradients needed during the optimization process  in order to change each weight of the LSTM network in proportion to the derivative of the error  at the output layer of the LSTM network  with respect to corresponding weight 
A problem with using gradient descent for standard RNNs is that error gradients vanish exponentially quickly with the size of the time lag between important events  This is due to 
  
    
      
        
          lim
          
            n
              x     
              x   e 
          
        
        
          W
          
            n
          
        
         
         
      
    
      displaystyle  lim   n to  infty  W  n    
  
 if the spectral radius of 
  
    
      
        W
      
    
      displaystyle W 
  
 is smaller than                          
However  with LSTM units  when error values are back propagated from the output layer  the error remains in the LSTM unit s cell  This  error carousel  continuously feeds error back to each of the LSTM unit s gates  until they learn to cut off the value 

CTC score function edit 
Many applications use stacks of LSTM RNNs             and train them by connectionist temporal classification  CTC             to find an RNN weight matrix that maximizes the probability of the label sequences in a training set  given the corresponding input sequences  CTC achieves both alignment and recognition 

Alternatives edit 
Sometimes  it can be advantageous to train  parts of  an LSTM by neuroevolution            or by policy gradient methods  especially when there is no  teacher   that is  training labels  

Applications edit 
Applications of LSTM include 


Robot control            
Time series prediction           
Speech recognition                                    
Rhythm learning            
Hydrological rainfall runoff modeling            
Music composition            
Grammar learning                                    
Handwriting recognition                        
Human action recognition            
Sign language translation            
Protein homology detection            
Predicting subcellular localization of proteins            
Time series anomaly detection            
Several prediction tasks in the area of business process management            
Prediction in medical care pathways            
Semantic parsing            
Object co segmentation                        
Airport passenger management            
Short term traffic forecast            
Drug design            
Market Prediction            
Activity Classification in Video            
      Google started using an LSTM trained by CTC for speech recognition on Google Voice                          According to the official blog post  the new model cut transcription errors by                 
      Google started using an LSTM to suggest messages in the Allo conversation app              In the same year  Google released the Google Neural Machine Translation system for Google Translate which used LSTMs to reduce translation errors by                                         
Apple announced in its Worldwide Developers Conference that it would start using the LSTM for quicktype                                     in the iPhone and for Siri                         
Amazon released Polly  which generates the voices behind Alexa  using a bidirectional LSTM for the text to speech technology             
       Facebook performed some     billion automatic translations every day using long short term memory networks             
Microsoft reported reaching       recognition accuracy on the Switchboard corpus  incorporating a vocabulary of         words  The approach used  dialog session based long short term memory              
      OpenAI used LSTM trained by policy gradients to beat humans in the complex video game of Dota                and to control a human like robot hand that manipulates physical objects with unprecedented dexterity                         
      DeepMind used LSTM trained by policy gradients to excel at the complex video game of Starcraft II                         

History edit 
Development edit 
Aspects of LSTM were anticipated by  focused back propagation   Mozer                     cited by the LSTM paper            
Sepp Hochreiter s      German diploma thesis analyzed the vanishing gradient problem and developed principles of the method             His supervisor  J rgen Schmidhuber  considered the thesis highly significant             
An early version of LSTM was published in      in a technical report by Sepp Hochreiter and J rgen Schmidhuber              then published in the NIPS      conference            
The most commonly used reference point for LSTM was published in      in the journal Neural Computation             By introducing Constant Error Carousel  CEC  units  LSTM deals with the vanishing gradient problem  The initial version of LSTM block included cells  input and output gates             
 Felix Gers  J rgen Schmidhuber  and Fred Cummins                    introduced the forget gate  also called  keep gate   into the LSTM architecture in       enabling the LSTM to reset its own state              This is the most commonly used version of LSTM nowadays 
 Gers  Schmidhuber  and Cummins        added peephole connections                          Additionally  the output activation function was omitted             

Development of variants edit 
 Graves  Fernandez  Gomez  and Schmidhuber                   introduce a new error function for LSTM  Connectionist Temporal Classification  CTC  for simultaneous alignment and recognition of sequences 
 Graves  Schmidhuber                    published LSTM with full backpropagation through time and bidirectional LSTM 
 Kyunghyun Cho et al                     published a simplified variant of the forget gate LSTM             called Gated recurrent unit  GRU  
 Rupesh Kumar Srivastava  Klaus Greff  and Schmidhuber        used LSTM principles             to create the Highway network  a feedforward neural network with hundreds of layers  much deeper than previous networks                                      Concurrently  the ResNet architecture was developed  It is equivalent to an open gated or gateless highway network             
A modern upgrade of LSTM called xLSTM is published by a team led by Sepp Hochreiter  Maximilian et al                                 One of the   blocks  mLSTM  of the architecture are parallelizable like the Transformer architecture  the other ones  sLSTM  allow state tracking 

Applications edit 
      First successful application of LSTM to speech Alex Graves et al                         
      Gers and Schmidhuber trained LSTM to learn languages unlearnable by traditional models such as Hidden Markov Models                         
Hochreiter et al  used LSTM for meta learning  i e  learning a learning algorithm              
      Daan Wierstra  Faustino Gomez  and Schmidhuber trained LSTM by neuroevolution without a teacher            
Mayer et al  trained LSTM to control robots             
      Wierstra  Foerster  Peters  and Schmidhuber trained LSTM by policy gradients for reinforcement learning without a teacher             
Hochreiter  Heuesel  and Obermayr applied LSTM to protein homology detection the field of biology             
      Justin Bayer et al  introduced neural architecture search for LSTM                         
      An LSTM trained by CTC won the ICDAR connected handwriting recognition competition  Three such models were submitted by a team led by Alex Graves              One was the most accurate model in the competition and another was the fastest              This was the first time an RNN won international competitions             
      Alex Graves  Abdel rahman Mohamed  and Geoffrey Hinton used LSTM networks as a major component of a network that achieved a record       phoneme error rate on the classic TIMIT natural speech dataset             
Researchers from Michigan State University  IBM Research  and Cornell University published a study in the Knowledge Discovery and Data Mining  KDD  conference                                      Their Time Aware LSTM  T LSTM  performs better on certain data sets than standard LSTM 

See also edit 

Attention  machine learning 
Deep learning
Differentiable neural computer
Gated recurrent unit
Highway network
Long term potentiation
Prefrontal cortex basal ganglia working memory
Recurrent neural network
Seq seq
Time aware long short term memory
Transformer  machine learning model 
Time series

References edit 


  a b c d e Sepp Hochreiter  J rgen Schmidhuber          Long short term memory   Neural Computation                    doi         neco                PMID               S CID              

  a b c Hochreiter  Sepp         Untersuchungen zu dynamischen neuronalen Netzen  PDF   diploma thesis   Technical University Munich  Institute of Computer Science 

  a b Hochreiter  Sepp  Schmidhuber  J rgen                LSTM can solve hard long time lag problems   Proceedings of the  th International Conference on Neural Information Processing Systems  NIPS     Cambridge  MA  USA  MIT Press          

  a b c Felix A  Gers  J rgen Schmidhuber  Fred Cummins          Learning to Forget  Continual Prediction with LSTM   Neural Computation                      CiteSeerX                      doi                             PMID                S CID               

  a b c Graves  Alex  Fern ndez  Santiago  Gomez  Faustino  Schmidhuber  J rgen          Connectionist temporal classification  Labelling unsegmented sequence data with recurrent neural networks   In Proceedings of the International Conference on Machine Learning  ICML                CiteSeerX                     

  Karim  Fazle  Majumdar  Somshubra  Darabi  Houshang  Chen  Shun          LSTM Fully Convolutional Networks for Time Series Classification   IEEE Access                arXiv             Bibcode     IEEEA         K  doi         ACCESS               ISSN                

  a b c d Wierstra  Daan  Schmidhuber  J   Gomez  F  J           Evolino  Hybrid Neuroevolution Optimal Linear Search for Sequence Learning   Proceedings of the   th International Joint Conference on Artificial Intelligence  IJCAI   Edinburgh          

  Sak  Hasim  Senior  Andrew  Beaufays  Francoise          Long Short Term Memory recurrent neural network architectures for large scale acoustic modeling   PDF   Archived from the original  PDF  on            

  Li  Xiangang  Wu  Xihong                Constructing Long Short Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition   arXiv            cs CL  

  a b Wu  Yonghui  Schuster  Mike  Chen  Zhifeng  Le  Quoc V   Norouzi  Mohammad  Macherey  Wolfgang  Krikun  Maxim  Cao  Yuan  Gao  Qin                Google s Neural Machine Translation System  Bridging the Gap between Human and Machine Translation   arXiv             cs CL  

  a b Ong  Thuy    August         Facebook s translations are now powered completely by AI   www allthingsdistributed com  Retrieved            

  Sahidullah  Md  Patino  Jose  Cornell  Samuele  Yin  Ruiking  Sivasankaran  Sunit  Bredin  Herve  Korshunov  Pavel  Brutti  Alessio  Serizel  Romain  Vincent  Emmanuel  Evans  Nicholas  Marcel  Sebastien  Squartini  Stefano  Barras  Claude                The Speed Submission to DIHARD II  Contributions  amp  Lessons Learned   arXiv             eess AS  

  a b c Mayer  H   Gomez  F   Wierstra  D   Nagy  I   Knoll  A   Schmidhuber  J   October         A System for Robotic Heart Surgery that Learns to Tie Knots Using Recurrent Neural Networks        IEEE RSJ International Conference on Intelligent Robots and Systems  pp                CiteSeerX                       doi         IROS              ISBN                         S CID               

  a b  Learning Dexterity   OpenAI  July           Retrieved            

  a b Rodriguez  Jesus  July            The Science Behind OpenAI Five that just Produced One of the Greatest Breakthrough in the History of AI   Towards Data Science  Archived from the original on             Retrieved            

  a b Stanford  Stacy  January             DeepMind s AI  AlphaStar Showcases Significant Progress Towards AGI   Medium ML Memoirs  Retrieved            

  Schmidhuber  J rgen          The     s  Our Decade of Deep Learning   Outlook on the     s   AI Blog  IDSIA  Switzerland  Retrieved            

  Calin  Ovidiu     February        Deep Learning Architectures  Cham  Switzerland  Springer Nature  p            ISBN                        

  Lakretz  Yair  Kruszewski  German  Desbordes  Theo  Hupkes  Dieuwke  Dehaene  Stanislas  Baroni  Marco          The emergence of number and syntax units in   The emergence of number and syntax units  PDF   Association for Computational Linguistics  pp              doi          v  N         hdl           cb     e  d       e b fed  ca ebb   S CID              

  a b c d Klaus Greff  Rupesh Kumar Srivastava  Jan Koutn k  Bas R  Steunebrink  J rgen Schmidhuber          LSTM  A Search Space Odyssey   IEEE Transactions on Neural Networks and Learning Systems                      arXiv             Bibcode     arXiv         G  doi         TNNLS               PMID                S CID              

  a b c d e f Gers  F  A   Schmidhuber  J           LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages   PDF   IEEE Transactions on Neural Networks                     doi                    PMID                S CID               

  a b c d Gers  F   Schraudolph  N   Schmidhuber  J           Learning precise timing with LSTM recurrent networks   PDF   Journal of Machine Learning Research             

  Xingjian Shi  Zhourong Chen  Hao Wang  Dit Yan Yeung  Wai kin Wong  Wang chun Woo          Convolutional LSTM Network  A Machine Learning Approach for Precipitation Nowcasting   Proceedings of the   th International Conference on Neural Information Processing Systems           arXiv             Bibcode     arXiv         S 

  Hochreiter  S   Bengio  Y   Frasconi  P   Schmidhuber  J           Gradient Flow in Recurrent Nets  the Difficulty of Learning Long Term Dependencies  PDF Download Available    In Kremer and  S  C   Kolen  J  F   eds    A Field Guide to Dynamical Recurrent Neural Networks  IEEE Press 

  Fern ndez  Santiago  Graves  Alex  Schmidhuber  J rgen          Sequence labelling in structured domains with hierarchical recurrent neural networks   Proc    th Int  Joint Conf  On Artificial Intelligence  Ijcai                CiteSeerX                     

  a b Graves  A   Schmidhuber  J           Framewise phoneme classification with bidirectional LSTM and other neural network architectures   Neural Networks                     CiteSeerX                       doi         j neunet              PMID                S CID              

  Fern ndez  S   Graves  A   Schmidhuber  J     September         An Application of Recurrent Neural Networks to Discriminative Keyword Spotting   Proceedings of the   th International Conference on Artificial Neural Networks  ICANN     Berlin  Heidelberg  Springer Verlag           ISBN                      Retrieved    December      

  a b Graves  Alex  Mohamed  Abdel rahman  Hinton  Geoffrey          Speech recognition with deep recurrent neural networks        IEEE International Conference on Acoustics  Speech and Signal Processing  pp                  arXiv            doi         ICASSP               ISBN                         S CID                

  Kratzert  Frederik  Klotz  Daniel  Shalev  Guy  Klambauer  G nter  Hochreiter  Sepp  Nearing  Grey                Towards learning universal  regional  and local hydrological behaviors via machine learning applied to large sample datasets   Hydrology and Earth System Sciences                      arXiv             Bibcode     HESS          K  doi         hess               ISSN                

  Eck  Douglas  Schmidhuber  J rgen                Learning the Long Term Structure of the Blues   Artificial Neural Networks   ICANN       Lecture Notes in Computer Science  Vol             Springer  Berlin  Heidelberg  pp                CiteSeerX                       doi                           ISBN                     

  Schmidhuber  J   Gers  F   Eck  D   Schmidhuber  J   Gers  F           Learning nonregular languages  A comparison of simple recurrent networks and LSTM   Neural Computation                     CiteSeerX                      doi                             PMID                S CID               

  Perez Ortiz  J  A   Gers  F  A   Eck  D   Schmidhuber  J           Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets   Neural Networks                   CiteSeerX                       doi         s                      PMID               

  A  Graves  J  Schmidhuber  Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks  Advances in Neural Information Processing Systems     NIPS     pp          Vancouver  MIT Press       

  Graves  A   Fern ndez  S   Liwicki  M   Bunke  H   Schmidhuber  J     December         Unconstrained Online Handwriting Recognition with Recurrent Neural Networks   Proceedings of the   th International Conference on Neural Information Processing Systems  NIPS     USA  Curran Associates Inc            ISBN                     Retrieved    December      

  Baccouche  M   Mamalet  F   Wolf  C   Garcia  C   Baskurt  A           Sequential Deep Learning for Human Action Recognition   In Salah  A  A   Lepri  B   eds     nd International Workshop on Human Behavior Understanding  HBU   Lecture Notes in Computer Science  Vol             Amsterdam  Netherlands  Springer  pp              doi                              ISBN                        

  Huang  Jie  Zhou  Wengang  Zhang  Qilin  Li  Houqiang  Li  Weiping                Video based Sign Language Recognition without Temporal Segmentation   arXiv             cs CV  

  a b Hochreiter  S   Heusel  M   Obermayer  K           Fast model based protein homology detection without alignment   Bioinformatics                      doi         bioinformatics btm     PMID               

  Thireou  T   Reczko  M           Bidirectional Long Short Term Memory Networks for predicting the subcellular localization of eukaryotic proteins   IEEE ACM Transactions on Computational Biology and Bioinformatics                  doi         tcbb            PMID                S CID               

  Malhotra  Pankaj  Vig  Lovekesh  Shroff  Gautam  Agarwal  Puneet  April         Long Short Term Memory Networks for Anomaly Detection in Time Series   PDF   European Symposium on Artificial Neural Networks  Computational Intelligence and Machine Learning   ESANN       Archived from the original  PDF  on             Retrieved            

  Tax  N   Verenich  I   La Rosa  M   Dumas  M           Predictive Business Process Monitoring with LSTM Neural Networks   Advanced Information Systems Engineering  Lecture Notes in Computer Science  Vol              pp                arXiv             doi                               ISBN                         S CID              

  Choi  E   Bahadori  M T   Schuetz  E   Stewart  W   Sun  J           Doctor AI  Predicting Clinical Events via Recurrent Neural Networks   JMLR Workshop and Conference Proceedings               arXiv             Bibcode     arXiv         C  PMC               PMID               

  Jia  Robin  Liang  Percy          Data Recombination for Neural Semantic Parsing   arXiv             cs CL  

  Wang  Le  Duan  Xuhuan  Zhang  Qilin  Niu  Zhenxing  Hua  Gang  Zheng  Nanning                Segment Tube  Spatio Temporal Action Localization in Untrimmed Videos with Per Frame Segmentation   PDF   Sensors                Bibcode     Senso         W  doi         s          ISSN                 PMC               PMID               

  Duan  Xuhuan  Wang  Le  Zhai  Changbo  Zheng  Nanning  Zhang  Qilin  Niu  Zhenxing  Hua  Gang          Joint Spatio Temporal Action Localization in Untrimmed Videos with Per Frame Segmentation          th IEEE International Conference on Image Processing  ICIP     th IEEE International Conference on Image Processing  ICIP   pp                doi         icip               ISBN                        

  Orsini  F   Gastaldi  M   Mantecchini  L   Rossi  R          Neural networks trained with WiFi traces to predict airport passenger behavior   th International Conference on Models and Technologies for Intelligent Transportation Systems  Krakow  IEEE  arXiv             doi         MTITS                       

  Zhao  Z   Chen  W   Wu  X   Chen  P C Y   Liu  J           LSTM network  A deep learning approach for Short term traffic forecast   IET Intelligent Transport Systems                 doi         iet its            S CID                

  Gupta A  M ller AT  Huisman BJH  Fuchs JA  Schneider P  Schneider G          Generative Recurrent Networks for De Novo Drug Design   Mol Inform            doi         minf            PMC               PMID                 cite journal     CS  maint  multiple names  authors list  link 

  Saiful Islam  Md   Hossain  Emam                Foreign Exchange Currency Rate Prediction using a GRU LSTM Hybrid Network   Soft Computing Letters             doi         j socl              ISSN                

    Cite Abbey Martin  Andrew J  Hill  Konstantin M  Seiler  amp  Mehala Balamurali        Automatic excavator action recognition and localisation for untrimmed video using hybrid LSTM Transformer networks  International Journal of Mining  Reclamation and Environment  DOI                                 

  Beaufays  Fran oise  August             The neural networks behind Google Voice transcription   Research Blog  Retrieved            

  Sak  Ha im  Senior  Andrew  Rao  Kanishka  Beaufays  Fran oise  Schalkwyk  Johan  September             Google voice search  faster and more accurate   Research Blog  Retrieved            

   Neon prescription    or rather  New transcription for Google Voice   Official Google Blog     July       Retrieved            

  Khaitan  Pranav  May             Chat Smarter with Allo   Research Blog  Retrieved            

  Metz  Cade  September             An Infusion of AI Makes Google Translate More Powerful Than Ever   WIRED   Wired  Retrieved            

   A Neural Network for Machine Translation  at Production Scale   Google AI Blog     September       Retrieved            

  Efrati  Amir  June             Apple s Machines Can Learn Too   The Information  Retrieved            

  Ranger  Steve  June             iPhone  AI and big data  Here s how Apple plans to protect your privacy   ZDNet  Retrieved            

   Can Global Semantic Context Improve Neural Language Models    Apple   Apple Machine Learning Journal  Retrieved            

  Smith  Chris                iOS     Siri now works in third party apps  comes with extra AI features   BGR  Retrieved            

  Capes  Tim  Coles  Paul  Conkie  Alistair  Golipour  Ladan  Hadjitarkhani  Abie  Hu  Qiong  Huddleston  Nancy  Hunt  Melvyn  Li  Jiangchuan  Neeracher  Matthias  Prahallad  Kishore                Siri On Device Deep Learning Guided Unit Selection Text to Speech System   Interspeech       ISCA             doi          Interspeech           

  Vogels  Werner     November         Bringing the Magic of Amazon AI and Alexa to Apps on AWS    All Things Distributed   www allthingsdistributed com  Retrieved            

  Xiong  W   Wu  L   Alleva  F   Droppo  J   Huang  X   Stolcke  A   April         The Microsoft      Conversational Speech Recognition System        IEEE International Conference on Acoustics  Speech and Signal Processing  ICASSP   IEEE  pp                  arXiv             doi         ICASSP               ISBN                        

  a b c d e f Schmidhuber  Juergen     May         Deep Learning  Our Miraculous Year             arXiv             cs NE  

  Mozer  Mike          A Focused Backpropagation Algorithm for Temporal Pattern Recognition   Complex Systems 

  Schmidhuber  Juergen          Annotated History of Modern AI and Deep Learning   arXiv             cs NE  

  Sepp Hochreiter  J rgen Schmidhuber     August        Long Short Term Memory  Wikidata      Q        

  a b c Gers  Felix  Schmidhuber  J rgen  Cummins  Fred          Learning to forget  Continual prediction with LSTM    th International Conference on Artificial Neural Networks  ICANN      Vol             pp                doi         cp           ISBN                    

  Cho  Kyunghyun  van Merrienboer  Bart  Gulcehre  Caglar  Bahdanau  Dzmitry  Bougares  Fethi  Schwenk  Holger  Bengio  Yoshua          Learning Phrase Representations using RNN Encoder Decoder for Statistical Machine Translation   arXiv            cs CL  

  Srivastava  Rupesh Kumar  Greff  Klaus  Schmidhuber  J rgen    May         Highway Networks   arXiv             cs LG  

  Srivastava  Rupesh K  Greff  Klaus  Schmidhuber  Juergen          Training Very Deep Networks   Advances in Neural Information Processing Systems      Curran Associates  Inc             

  Schmidhuber  J rgen          The most cited neural networks all build on work done in my labs   AI Blog  IDSIA  Switzerland  Retrieved            

  He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  Sun  Jian         Deep Residual Learning for Image Recognition       IEEE Conference on Computer Vision and Pattern Recognition  CVPR   Las Vegas  NV  USA  IEEE  pp                arXiv             doi         CVPR          ISBN                        

  Beck  Maximilian  P ppel  Korbinian  Spanring  Markus  Auer  Andreas  Prudnikova  Oleksandra  Kopp  Michael  Klambauer  G nter  Brandstetter  Johannes  Hochreiter  Sepp                xLSTM  Extended Long Short Term Memory   arXiv             cs LG  

  NX AI xlstm  NXAI              retrieved           

  Graves  Alex  Beringer  Nicole  Eck  Douglas  Schmidhuber  Juergen         Biologically Plausible Speech Recognition with LSTM Neural Nets  Workshop on Biologically Inspired Approaches to Advanced Information Technology  Bio ADIT       Lausanne  Switzerland  pp               

  Hochreiter  S   Younger  A  S   Conwell  P  R           Learning to Learn Using Gradient Descent   Artificial Neural Networks   ICANN       PDF   Lecture Notes in Computer Science  Vol             pp              CiteSeerX                    doi                           ISBN                         ISSN                 S CID               

  Wierstra  Daan  Foerster  Alexander  Peters  Jan  Schmidhuber  Juergen          Solving Deep Memory POMDPs with Recurrent Policy Gradients   International Conference on Artificial Neural Networks ICANN    

  Bayer  Justin  Wierstra  Daan  Togelius  Julian  Schmidhuber  Juergen          Evolving memory cell structures for sequence learning   International Conference on Artificial Neural Networks ICANN     Cyprus 

  Graves  A   Liwicki  M   Fern ndez  S   Bertolami  R   Bunke  H   Schmidhuber  J   May         A Novel Connectionist System for Unconstrained Handwriting Recognition   IEEE Transactions on Pattern Analysis and Machine Intelligence                   CiteSeerX                       doi         tpami           ISSN                 PMID                S CID               

  M rgner  Volker  Abed  Haikal El  July         ICDAR      Arabic Handwriting Recognition Competition          th International Conference on Document Analysis and Recognition  pp                  doi         ICDAR           ISBN                         S CID               

   Patient Subtyping via Time Aware LSTM Networks   PDF   msu edu  Retrieved    Nov      

   Patient Subtyping via Time Aware LSTM Networks   Kdd org  Retrieved    May      

   SIGKDD   Kdd org  Retrieved    May      


Further reading edit 
Monner  Derek D   Reggia  James A           A generalized LSTM like training algorithm for second order recurrent neural networks   PDF   Neural Networks                 doi         j neunet              PMC               PMID                High performing extension of LSTM that has been simplified to a single node type and can train arbitrary architectures
Gers  Felix A   Schraudolph  Nicol N   Schmidhuber  J rgen  Aug         Learning precise timing with LSTM recurrent networks   PDF   Journal of Machine Learning Research             
Gers  Felix          Long Short Term Memory in Recurrent Neural Networks   PDF   PhD thesis 
Abidogun  Olusola Adeniyi         Data Mining  Fraud Detection and Mobile Telecommunications  Call Pattern Analysis with Unsupervised Neural Networks  Master s Thesis  Thesis   University of the Western Cape  hdl            Archived  PDF  from the original on May          
original with two chapters devoted to explaining recurrent neural networks  especially LSTM 
External links edit 
Recurrent Neural Networks with over    LSTM papers by J rgen Schmidhuber s group at IDSIA
Zhang  Aston  Lipton  Zachary  Li  Mu  Smola  Alexander J                 Long Short Term Memory  LSTM    Dive into deep learning  Cambridge New York Port Melbourne New Delhi Singapore  Cambridge University Press  ISBN                        
vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects






Retrieved from  https   en wikipedia org w index php title Long short term memory amp oldid