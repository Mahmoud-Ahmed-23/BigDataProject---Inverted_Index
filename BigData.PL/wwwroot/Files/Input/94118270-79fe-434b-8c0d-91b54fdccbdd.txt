Probabilistic classification algorithm
Example of a naive Bayes classifier depicted as a Bayesian Network
In statistics  naive  sometimes simple or idiot s  Bayes classifiers are a family of  probabilistic classifiers  which assumes that the features are conditionally independent  given the target class             In other words  a naive Bayes model assumes the information about the class provided by each variable is unrelated to the information from the others  with no information shared between the predictors  The highly unrealistic nature of this assumption  called the naive independence assumption  is what gives the classifier its name  These classifiers are some of the simplest Bayesian network models            
Naive Bayes classifiers generally perform worse than more advanced models like logistic regressions  especially at quantifying uncertainty  with naive Bayes models often producing wildly overconfident probabilities   However  they are highly scalable  requiring only one parameter for each feature or predictor in a learning problem  Maximum likelihood training can be done by evaluating a closed form expression  simply by counting observations in each group                                    rather than the expensive iterative approximation algorithms required by most other models 
Despite the use of Bayes  theorem in the classifier s decision rule  naive Bayes is not  necessarily  a Bayesian method  and naive Bayes models can be fit to data using either Bayesian or frequentist methods                       


Introduction edit 
Naive Bayes is a simple technique for constructing classifiers  models that assign class labels to problem instances  represented as vectors of feature values  where the class labels are drawn from some finite set  There is not a single algorithm for training such classifiers  but a family of algorithms based on a common principle  all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature  given the class variable  For example  a fruit may be considered to be an apple if it is red  round  and about         cm in diameter   A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple  regardless of any possible correlations between the color  roundness  and diameter features 
In many practical applications  parameter estimation for naive Bayes models uses the method of maximum likelihood  in other words  one can work with the naive Bayes model without accepting Bayesian probability or using any Bayesian methods 
Despite their naive design and apparently oversimplified assumptions  naive Bayes classifiers have worked quite well in many complex real world situations  In       an analysis of the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of naive Bayes classifiers             Still  a comprehensive comparison with other classification algorithms in      showed that Bayes classification is outperformed by other approaches  such as boosted trees or random forests            
An advantage of naive Bayes is that it only requires a small amount of training data to estimate the parameters necessary for classification            

Probabilistic model edit 
Abstractly  naive Bayes is a conditional probability model  it assigns probabilities 
  
    
      
        p
         
        
          C
          
            k
          
        
          x     
        
          x
          
             
          
        
         
          x     
         
        
          x
          
            n
          
        
         
      
    
      displaystyle p C  k  mid x      ldots  x  n   
  
 for each of the K possible outcomes or classes 
  
    
      
        
          C
          
            k
          
        
      
    
      displaystyle C  k  
  
 given a problem instance to be classified  represented by a vector 
  
    
      
        
          x
        
         
         
        
          x
          
             
          
        
         
          x     
         
        
          x
          
            n
          
        
         
      
    
      displaystyle  mathbf  x    x      ldots  x  n   
  
 encoding some n features  independent variables             
The problem with the above formulation is that if the number of features n is large or if a feature can take on a large number of values  then basing such a model on probability tables is infeasible  The model must therefore be reformulated to make it more tractable  Using Bayes  theorem  the conditional probability can be decomposed as 

  
    
      
        p
         
        
          C
          
            k
          
        
          x     
        
          x
        
         
         
        
          
            
              p
               
              
                C
                
                  k
                
              
               
                xa  
              p
               
              
                x
              
                x     
              
                C
                
                  k
                
              
               
            
            
              p
               
              
                x
              
               
            
          
        
        
      
    
      displaystyle p C  k  mid  mathbf  x      frac  p C  k    p  mathbf  x   mid C  k    p  mathbf  x        
  

In plain English  using Bayesian probability terminology  the above equation can be written as

  
    
      
        
          posterior
        
         
        
          
            
              
                prior
              
                xd  
              
                likelihood
              
            
            evidence
          
        
        
      
    
      displaystyle   text posterior     frac    text prior   times   text likelihood     text evidence      
  

In practice  there is interest only in the numerator of that fraction  because the denominator does not depend on 
  
    
      
        C
      
    
      displaystyle C 
  
 and the values of the features 
  
    
      
        
          x
          
            i
          
        
      
    
      displaystyle x  i  
  
 are given  so that the denominator is effectively constant 
The numerator is equivalent to the joint probability model

  
    
      
        p
         
        
          C
          
            k
          
        
         
        
          x
          
             
          
        
         
          x     
         
        
          x
          
            n
          
        
         
        
      
    
      displaystyle p C  k  x      ldots  x  n     
  

which can be rewritten as follows  using the chain rule for repeated applications of the definition of conditional probability 

  
    
      
        
          
            
              
                p
                 
                
                  C
                  
                    k
                  
                
                 
                
                  x
                  
                     
                  
                
                 
                  x     
                 
                
                  x
                  
                    n
                  
                
                 
              
              
                
                 
                p
                 
                
                  x
                  
                     
                  
                
                 
                  x     
                 
                
                  x
                  
                    n
                  
                
                 
                
                  C
                  
                    k
                  
                
                 
              
            
            
              
              
                
                 
                p
                 
                
                  x
                  
                     
                  
                
                  x     
                
                  x
                  
                     
                  
                
                 
                  x     
                 
                
                  x
                  
                    n
                  
                
                 
                
                  C
                  
                    k
                  
                
                 
                  xa  
                p
                 
                
                  x
                  
                     
                  
                
                 
                  x     
                 
                
                  x
                  
                    n
                  
                
                 
                
                  C
                  
                    k
                  
                
                 
              
            
            
              
              
                
                 
                p
                 
                
                  x
                  
                     
                  
                
                  x     
                
                  x
                  
                     
                  
                
                 
                  x     
                 
                
                  x
                  
                    n
                  
                
                 
                
                  C
                  
                    k
                  
                
                 
                  xa  
                p
                 
                
                  x
                  
                     
                  
                
                  x     
                
                  x
                  
                     
                  
                
                 
                  x     
                 
                
                  x
                  
                    n
                  
                
                 
                
                  C
                  
                    k
                  
                
                 
                  xa  
                p
                 
                
                  x
                  
                     
                  
                
                 
                  x     
                 
                
                  x
                  
                    n
                  
                
                 
                
                  C
                  
                    k
                  
                
                 
              
            
            
              
              
                
                 
                  x  ef 
              
            
            
              
              
                
                 
                p
                 
                
                  x
                  
                     
                  
                
                  x     
                
                  x
                  
                     
                  
                
                 
                  x     
                 
                
                  x
                  
                    n
                  
                
                 
                
                  C
                  
                    k
                  
                
                 
                  xa  
                p
                 
                
                  x
                  
                     
                  
                
                  x     
                
                  x
                  
                     
                  
                
                 
                  x     
                 
                
                  x
                  
                    n
                  
                
                 
                
                  C
                  
                    k
                  
                
                 
                  x  ef 
                p
                 
                
                  x
                  
                    n
                      x     
                     
                  
                
                  x     
                
                  x
                  
                    n
                  
                
                 
                
                  C
                  
                    k
                  
                
                 
                  xa  
                p
                 
                
                  x
                  
                    n
                  
                
                  x     
                
                  C
                  
                    k
                  
                
                 
                  xa  
                p
                 
                
                  C
                  
                    k
                  
                
                 
              
            
          
        
      
    
      displaystyle   begin aligned p C  k  x      ldots  x  n   amp  p x      ldots  x  n  C  k     amp  p x     mid x      ldots  x  n  C  k    p x      ldots  x  n  C  k     amp  p x     mid x      ldots  x  n  C  k    p x     mid x      ldots  x  n  C  k    p x      ldots  x  n  C  k     amp   cdots    amp  p x     mid x      ldots  x  n  C  k    p x     mid x      ldots  x  n  C  k   cdots p x  n    mid x  n  C  k    p x  n  mid C  k    p C  k     end aligned   
  

Now the  naive  conditional independence assumptions come into play  assume that all features in 
  
    
      
        
          x
        
      
    
      displaystyle  mathbf  x   
  
 are mutually independent  conditional on the category 
  
    
      
        
          C
          
            k
          
        
      
    
      displaystyle C  k  
  
  Under this assumption 

  
    
      
        p
         
        
          x
          
            i
          
        
          x     
        
          x
          
            i
             
             
          
        
         
          x     
         
        
          x
          
            n
          
        
         
        
          C
          
            k
          
        
         
         
        p
         
        
          x
          
            i
          
        
          x     
        
          C
          
            k
          
        
         
        
         
      
    
      displaystyle p x  i  mid x  i     ldots  x  n  C  k   p x  i  mid C  k      
  

Thus  the joint model can be expressed as

  
    
      
        
          
            
              
                p
                 
                
                  C
                  
                    k
                  
                
                  x     
                
                  x
                  
                     
                  
                
                 
                  x     
                 
                
                  x
                  
                    n
                  
                
                 
                  x   d 
                  xa  
              
              
                p
                 
                
                  C
                  
                    k
                  
                
                 
                
                  x
                  
                     
                  
                
                 
                  x     
                 
                
                  x
                  
                    n
                  
                
                 
              
            
            
              
              
                
                 
                p
                 
                
                  C
                  
                    k
                  
                
                 
                  xa  
                p
                 
                
                  x
                  
                     
                  
                
                  x     
                
                  C
                  
                    k
                  
                
                 
                  xa  
                p
                 
                
                  x
                  
                     
                  
                
                  x     
                
                  C
                  
                    k
                  
                
                 
                  xa  
                p
                 
                
                  x
                  
                     
                  
                
                  x     
                
                  C
                  
                    k
                  
                
                 
                  xa  
                  x  ef 
              
            
            
              
              
                
                 
                p
                 
                
                  C
                  
                    k
                  
                
                 
                
                    x   f 
                  
                    i
                     
                     
                  
                  
                    n
                  
                
                p
                 
                
                  x
                  
                    i
                  
                
                  x     
                
                  C
                  
                    k
                  
                
                 
                
                 
              
            
          
        
      
    
      displaystyle   begin aligned p C  k  mid x      ldots  x  n   varpropto    amp p C  k  x      ldots  x  n     amp  p C  k    p x     mid C  k    p x     mid C  k    p x     mid C  k     cdots    amp  p C  k   prod   i     n p x  i  mid C  k      end aligned   
  

where 
  
    
      
          x   d 
      
    
      displaystyle  varpropto  
  
 denotes proportionality since the denominator 
  
    
      
        p
         
        
          x
        
         
      
    
      displaystyle p  mathbf  x    
  
 is omitted 
This means that under the above independence assumptions  the conditional distribution over the class variable 
  
    
      
        C
      
    
      displaystyle C 
  
 is 

  
    
      
        p
         
        
          C
          
            k
          
        
          x     
        
          x
          
             
          
        
         
          x     
         
        
          x
          
            n
          
        
         
         
        
          
             
            Z
          
        
          xa  
        p
         
        
          C
          
            k
          
        
         
        
            x   f 
          
            i
             
             
          
          
            n
          
        
        p
         
        
          x
          
            i
          
        
          x     
        
          C
          
            k
          
        
         
      
    
      displaystyle p C  k  mid x      ldots  x  n     frac     Z    p C  k   prod   i     n p x  i  mid C  k   
  

where the evidence 
  
    
      
        Z
         
        p
         
        
          x
        
         
         
        
            x     
          
            k
          
        
        p
         
        
          C
          
            k
          
        
         
          xa  
        p
         
        
          x
        
          x     
        
          C
          
            k
          
        
         
      
    
      displaystyle Z p  mathbf  x     sum   k p C  k    p  mathbf  x   mid C  k   
  
 is a scaling factor dependent only on 
  
    
      
        
          x
          
             
          
        
         
          x     
         
        
          x
          
            n
          
        
      
    
      displaystyle x      ldots  x  n  
  
  that is  a constant if the values of the feature variables are known 

Constructing a classifier from the probability model edit 
The discussion so far has derived the independent feature model  that is  the naive Bayes probability model  The naive Bayes classifier combines this model with a decision rule  One common rule is to pick the hypothesis that is most probable so as to minimize the probability of misclassification  this is known as the maximum a posteriori or MAP decision rule  The corresponding classifier  a Bayes classifier  is the function that assigns a class label 
  
    
      
        
          
            
              y
                x e 
            
          
        
         
        
          C
          
            k
          
        
      
    
      displaystyle   hat  y   C  k  
  
 for some k as follows 

  
    
      
        
          
            
              y
                x e 
            
          
        
         
        
          
            argmax
            
              k
                x     
               
               
               
                x     
               
              K
               
            
          
        
          xa  
        p
         
        
          C
          
            k
          
        
         
        
          
              x   f 
            
              i
               
               
            
            
              n
            
          
          p
           
          
            x
            
              i
            
          
            x     
          
            C
            
              k
            
          
           
           
        
      
    
      displaystyle   hat  y     underset  k in      ldots  K     operatorname  argmax      p C  k   displaystyle  prod   i     n p x  i  mid C  k    
  


Likelihood functions 
  
    
      
        p
         
        
          x
        
          x     
        Y
         
      
    
      displaystyle p  mathbf  x   mid Y  
  
  Confusion matrix and ROC curve  For the naive Bayes classifier and given that the a priori probabilities 
  
    
      
        p
         
        Y
         
      
    
      displaystyle p Y  
  
 are the same for all classes  then the decision boundary  green line  would be placed on the point where the two probability densities intersect  due to 
  
    
      
        p
         
        Y
          x     
        
          x
        
         
         
        
          
            
              p
               
              Y
               
                xa  
              p
               
              
                x
              
                x     
              Y
               
            
            
              p
               
              
                x
              
               
            
          
        
          x   d 
        p
         
        
          x
        
          x     
        Y
         
      
    
      displaystyle p Y mid  mathbf  x      frac  p Y   p  mathbf  x   mid Y   p  mathbf  x      propto p  mathbf  x   mid Y  
  
 
Parameter estimation and event models edit 
A class s prior may be calculated by assuming equiprobable classes  i e   
  
    
      
        p
         
        
          C
          
            k
          
        
         
         
        
          
             
            K
          
        
      
    
      displaystyle p C  k     frac     K   
  
  or by calculating an estimate for the class probability from the training set 

  
    
      
        
          prior for a given class
        
         
        
          
            no  of samples in that class
            total no  of samples
          
        
        
      
    
      displaystyle   text prior for a given class     frac   text no  of samples in that class    text total no  of samples      
  

To estimate the parameters for a feature s distribution  one must assume a distribution or generate nonparametric models for the features from the training set            
The assumptions on distributions of features are called the  event model  of the naive Bayes classifier  For discrete features like the ones encountered in document classification  include spam filtering   multinomial and Bernoulli distributions are popular  These assumptions lead to two distinct models  which are often confused                        

Gaussian naive Bayes edit 
When dealing with continuous data  a typical assumption is that the continuous values associated with each class are distributed according to a normal  or Gaussian  distribution  For example  suppose the training data contains a continuous attribute  
  
    
      
        x
      
    
      displaystyle x 
  
  The data is first segmented by the class  and then the mean and variance of 
  
    
      
        x
      
    
      displaystyle x 
  
 is computed in each class  Let 
  
    
      
        
            x bc 
          
            k
          
        
      
    
      displaystyle  mu   k  
  
 be the mean of the values in 
  
    
      
        x
      
    
      displaystyle x 
  
 associated with class 
  
    
      
        
          C
          
            k
          
        
      
    
      displaystyle C  k  
  
  and let 
  
    
      
        
            x c  
          
            k
          
          
             
          
        
      
    
      displaystyle  sigma   k      
  
 be the Bessel corrected variance of the values in 
  
    
      
        x
      
    
      displaystyle x 
  
 associated with class 
  
    
      
        
          C
          
            k
          
        
      
    
      displaystyle C  k  
  
  Suppose one has collected some observation value 
  
    
      
        v
      
    
      displaystyle v 
  
  Then  the probability density of 
  
    
      
        v
      
    
      displaystyle v 
  
 given a class 
  
    
      
        
          C
          
            k
          
        
      
    
      displaystyle C  k  
  
  i e   
  
    
      
        p
         
        x
         
        v
          x     
        
          C
          
            k
          
        
         
      
    
      displaystyle p x v mid C  k   
  
  can be computed by plugging 
  
    
      
        v
      
    
      displaystyle v 
  
 into the equation for a normal distribution parameterized by 
  
    
      
        
            x bc 
          
            k
          
        
      
    
      displaystyle  mu   k  
  
 and 
  
    
      
        
            x c  
          
            k
          
          
             
          
        
      
    
      displaystyle  sigma   k      
  
  Formally 

  
    
      
        p
         
        x
         
        v
          x     
        
          C
          
            k
          
        
         
         
        
          
             
            
               
                x c  
              
                  x c  
                
                  k
                
                
                   
                
              
            
          
        
        
        
          e
          
              x     
            
              
                
                   
                  v
                    x     
                  
                      x bc 
                    
                      k
                    
                  
                  
                     
                    
                       
                    
                  
                
                
                   
                  
                      x c  
                    
                      k
                    
                    
                       
                    
                  
                
              
            
          
        
      
    
      displaystyle p x v mid C  k     frac      sqrt    pi  sigma   k          e     frac   v  mu   k          sigma   k         
  

Another common technique for handling continuous values is to use binning to discretize the feature values and obtain a new set of Bernoulli distributed features  Some literature suggests that this is required in order to use naive Bayes  but it is not true  as the discretization may throw away discriminative information            
Sometimes the distribution of class conditional marginal densities is far from normal  In these cases  kernel density estimation can be used for a more realistic estimate of the marginal densities of each class  This method  which was introduced by John and Langley             can boost the accuracy of the classifier considerably                         

Multinomial naive Bayes edit 
With a multinomial event model  samples  feature vectors  represent the frequencies with which certain events have been generated by a multinomial 
  
    
      
         
        
          p
          
             
          
        
         
          x     
         
        
          p
          
            n
          
        
         
      
    
      displaystyle  p      dots  p  n   
  
 where 
  
    
      
        
          p
          
            i
          
        
      
    
      displaystyle p  i  
  
 is the probability that event i occurs  or K such multinomials in the multiclass case   A feature vector 
  
    
      
        
          x
        
         
         
        
          x
          
             
          
        
         
          x     
         
        
          x
          
            n
          
        
         
      
    
      displaystyle  mathbf  x    x      dots  x  n   
  
 is then a histogram  with 
  
    
      
        
          x
          
            i
          
        
      
    
      displaystyle x  i  
  
 counting the number of times event i was observed in a particular instance  This is the event model typically used for document classification  with events representing the occurrence of a word in a single document  see bag of words assumption               The likelihood of observing a histogram x is given by 

  
    
      
        p
         
        
          x
        
          x     
        
          C
          
            k
          
        
         
         
        
          
            
               
              
                  x     
                
                  i
                   
                   
                
                
                  n
                
              
              
                x
                
                  i
                
              
               
               
            
            
              
                  x   f 
                
                  i
                   
                   
                
                
                  n
                
              
              
                x
                
                  i
                
              
               
            
          
        
        
            x   f 
          
            i
             
             
          
          
            n
          
        
        
          
            
              p
              
                k
                i
              
            
          
          
            
              x
              
                i
              
            
          
        
      
    
      displaystyle p  mathbf  x   mid C  k     frac    sum   i     n x  i      prod   i     n x  i     prod   i     n  p  ki    x  i   
  

where 
  
    
      
        
          p
          
            k
            i
          
        
          
        p
         
        i
          x     
        
          C
          
            k
          
        
         
      
    
      displaystyle p  ki   p i mid C  k   
  
  
The multinomial naive Bayes classifier becomes a linear classifier when expressed in log space             

  
    
      
        
          
            
              
                log
                  x     
                p
                 
                
                  C
                  
                    k
                  
                
                  x     
                
                  x
                
                 
              
              
                
                  x   d 
                log
                  x     
                
                   
                  
                    p
                     
                    
                      C
                      
                        k
                      
                    
                     
                    
                        x   f 
                      
                        i
                         
                         
                      
                      
                        n
                      
                    
                    
                      
                        
                          p
                          
                            k
                            i
                          
                        
                      
                      
                        
                          x
                          
                            i
                          
                        
                      
                    
                  
                   
                
              
            
            
              
              
                
                 
                log
                  x     
                p
                 
                
                  C
                  
                    k
                  
                
                 
                 
                
                    x     
                  
                    i
                     
                     
                  
                  
                    n
                  
                
                
                  x
                  
                    i
                  
                
                  x  c  
                log
                  x     
                
                  p
                  
                    k
                    i
                  
                
              
            
            
              
              
                
                 
                b
                 
                
                  
                    w
                  
                  
                    k
                  
                  
                      x  a  
                  
                
                
                  x
                
              
            
          
        
      
    
      displaystyle   begin aligned  log p C  k  mid  mathbf  x    amp  varpropto  log  left p C  k   prod   i     n  p  ki    x  i   right    amp   log p C  k    sum   i     n x  i  cdot  log p  ki    amp  b  mathbf  w    k    top   mathbf  x   end aligned   
  

where 
  
    
      
        b
         
        log
          x     
        p
         
        
          C
          
            k
          
        
         
      
    
      displaystyle b  log p C  k   
  
 and 
  
    
      
        
          w
          
            k
            i
          
        
         
        log
          x     
        
          p
          
            k
            i
          
        
      
    
      displaystyle w  ki   log p  ki  
  
  Estimating the parameters in log space is advantageous since multiplying a large number of small values can lead to significant rounding error  Applying a log transform reduces the effect of this rounding error 
If a given class and feature value never occur together in the training data  then the frequency based probability estimate will be zero  because the probability estimate is directly proportional to the number of occurrences of a feature s value  This is problematic because it will wipe out all information in the other probabilities when they are multiplied  Therefore  it is often desirable to incorporate a small sample correction  called pseudocount  in all probability estimates such that no probability is ever set to be exactly zero  This way of regularizing naive Bayes is called Laplace smoothing when the pseudocount is one  and Lidstone smoothing in the general case 
Rennie et al  discuss problems with the multinomial assumption in the context of document classification and possible ways to alleviate those problems  including the use of tf idf weights instead of raw term frequencies and document length normalization  to produce a naive Bayes classifier that is competitive with support vector machines             

Bernoulli naive Bayes edit 
In the multivariate Bernoulli event model  features are independent Boolean variables  binary variables  describing inputs  Like the multinomial model  this model is popular for document classification tasks             where binary term occurrence features are used rather than term frequencies  If 
  
    
      
        
          x
          
            i
          
        
      
    
      displaystyle x  i  
  
 is a Boolean expressing the occurrence or absence of the i th term from the vocabulary  then the likelihood of a document given a class 
  
    
      
        
          C
          
            k
          
        
      
    
      displaystyle C  k  
  
 is given by            

  
    
      
        p
         
        
          x
        
          x     
        
          C
          
            k
          
        
         
         
        
            x   f 
          
            i
             
             
          
          
            n
          
        
        
          p
          
            k
            i
          
          
            
              x
              
                i
              
            
          
        
         
         
          x     
        
          p
          
            k
            i
          
        
        
           
          
             
             
              x     
            
              x
              
                i
              
            
             
          
        
      
    
      displaystyle p  mathbf  x   mid C  k    prod   i     n p  ki   x  i     p  ki       x  i    
  

where 
  
    
      
        
          p
          
            k
            i
          
        
      
    
      displaystyle p  ki  
  
 is the probability of class 
  
    
      
        
          C
          
            k
          
        
      
    
      displaystyle C  k  
  
 generating the term 
  
    
      
        
          x
          
            i
          
        
      
    
      displaystyle x  i  
  
  This event model is especially popular for classifying short texts  It has the benefit of explicitly modelling the absence of terms  Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one 

Semi supervised parameter estimation edit 
Given a way to train a naive Bayes classifier from labeled data  it s possible to construct a semi supervised training algorithm that can learn from a combination of labeled and unlabeled data by running the supervised learning algorithm in a loop             

Given a collection 
  
    
      
        D
         
        L
          x   e 
        U
      
    
      displaystyle D L uplus U 
  
 of labeled samples L and unlabeled samples U  start by training a naive Bayes classifier on L 
Until convergence  do 
Predict class probabilities 
  
    
      
        P
         
        C
          x     
        x
         
      
    
      displaystyle P C mid x  
  
 for all examples x in 
  
    
      
        D
      
    
      displaystyle D 
  
 
Re train the model based on the probabilities  not the labels  predicted in the previous step 
Convergence is determined based on improvement to the model likelihood 
  
    
      
        P
         
        D
          x     
          x b  
         
      
    
      displaystyle P D mid  theta   
  
  where 
  
    
      
          x b  
      
    
      displaystyle  theta  
  
 denotes the parameters of the naive Bayes model 
This training algorithm is an instance of the more general expectation maximization algorithm  EM   the prediction step inside the loop is the E step of EM  while the re training of naive Bayes is the M step  The algorithm is formally justified by the assumption that the data are generated by a mixture model  and the components of this mixture model are exactly the classes of the classification problem             

Discussion edit 
Despite the fact that the far reaching independence assumptions are often inaccurate  the naive Bayes classifier has several properties that make it surprisingly useful in practice  In particular  the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution  This helps alleviate problems stemming from the curse of dimensionality  such as the need for data sets that scale exponentially with the number of features  While naive Bayes often fails to produce a good estimate for the correct class probabilities              this may not be a requirement for many applications  For example  the naive Bayes classifier will make the correct MAP decision rule classification so long as the correct class is predicted as more probable than any other class  This is true regardless of whether the probability estimate is slightly  or even grossly inaccurate  In this manner  the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model              Other reasons for the observed success of the naive Bayes classifier are discussed in the literature cited below 

Relation to logistic regression edit 
In the case of discrete inputs  indicator or frequency features for discrete events   naive Bayes classifiers form a generative discriminative pair with multinomial logistic regression classifiers  each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood 
  
    
      
        p
         
        C
         
        
          x
        
         
      
    
      displaystyle p C  mathbf  x    
  
  while logistic regression fits the same probability model to optimize the conditional 
  
    
      
        p
         
        C
          x     
        
          x
        
         
      
    
      displaystyle p C mid  mathbf  x    
  
             
More formally  we have the following 


Theorem Naive Bayes classifiers on binary features are subsumed by logistic regression classifiers 


Proof
Consider a generic multiclass classification problem  with possible classes 
  
    
      
        Y
          x     
         
         
         
         
         
         
         
        n
         
      
    
      displaystyle Y in         n   
  
  then the  non naive  Bayes classifier gives  by Bayes theorem 

  
    
      
        p
         
        Y
          x     
        X
         
        x
         
         
        
          softmax
        
         
         
        ln
          x     
        p
         
        Y
         
        k
         
         
        ln
          x     
        p
         
        X
         
        x
          x     
        Y
         
        k
         
        
           
          
            k
          
        
         
      
    
      displaystyle p Y mid X x    text softmax      ln p Y k   ln p X x mid Y k     k   
  

The naive Bayes classifier gives  

  
    
      
        
          softmax
        
        
           
          
            
               
              
                ln
                  x     
                p
                 
                Y
                 
                k
                 
                 
                
                  
                     
                     
                  
                
                
                    x     
                  
                    i
                  
                
                 
                
                  a
                  
                    i
                     
                    k
                  
                  
                     
                  
                
                  x     
                
                  a
                  
                    i
                     
                    k
                  
                  
                      x     
                  
                
                 
                
                  x
                  
                    i
                  
                
                 
                 
                
                  a
                  
                    i
                     
                    k
                  
                  
                     
                  
                
                 
                
                  a
                  
                    i
                     
                    k
                  
                  
                      x     
                  
                
                 
              
               
            
            
              k
            
          
           
        
      
    
      displaystyle   text softmax   left  left   ln p Y k    frac         sum   i  a  i k      a  i k      x  i   a  i k      a  i k       right    k  right  
  

where   

  
    
      
        
          a
          
            i
             
            s
          
          
             
          
        
         
        ln
          x     
        p
         
        
          X
          
            i
          
        
         
         
         
          x     
        Y
         
        s
         
         
        
        
          a
          
            i
             
            s
          
          
              x     
          
        
         
        ln
          x     
        p
         
        
          X
          
            i
          
        
         
          x     
         
          x     
        Y
         
        s
         
      
    
      displaystyle a  i s       ln p X  i     mid Y s   quad a  i s       ln p X  i     mid Y s  
  

This is exactly a logistic regression classifier 


The link between the two can be seen by observing that the decision function for naive Bayes  in the binary case  can be rewritten as  predict class 
  
    
      
        
          C
          
             
          
        
      
    
      displaystyle C     
  
 if the odds of 
  
    
      
        p
         
        
          C
          
             
          
        
          x     
        
          x
        
         
      
    
      displaystyle p C     mid  mathbf  x    
  
 exceed those of 
  
    
      
        p
         
        
          C
          
             
          
        
          x     
        
          x
        
         
      
    
      displaystyle p C     mid  mathbf  x    
  
   Expressing this in log space gives 

  
    
      
        log
          x     
        
          
            
              p
               
              
                C
                
                   
                
              
                x     
              
                x
              
               
            
            
              p
               
              
                C
                
                   
                
              
                x     
              
                x
              
               
            
          
        
         
        log
          x     
        p
         
        
          C
          
             
          
        
          x     
        
          x
        
         
          x     
        log
          x     
        p
         
        
          C
          
             
          
        
          x     
        
          x
        
         
         gt 
         
      
    
      displaystyle  log   frac  p C     mid  mathbf  x     p C     mid  mathbf  x       log p C     mid  mathbf  x     log p C     mid  mathbf  x    gt   
  

The left hand side of this equation is the log odds  or logit  the quantity predicted by the linear model that underlies logistic regression  Since naive Bayes is also a linear model for the two  discrete  event models  it can be reparametrised as a linear function 
  
    
      
        b
         
        
          
            w
          
          
              x  a  
          
        
        x
         gt 
         
      
    
      displaystyle b  mathbf  w     top  x gt   
  
  Obtaining the probabilities is then a matter of applying the logistic function to 
  
    
      
        b
         
        
          
            w
          
          
              x  a  
          
        
        x
      
    
      displaystyle b  mathbf  w     top  x 
  
  or in the multiclass case  the softmax function 
Discriminative classifiers have lower asymptotic error than generative ones  however  research by Ng and Jordan has shown that in some practical cases naive Bayes can outperform logistic regression because it reaches its asymptotic error faster             

Examples edit 
Person classification edit 
Problem  classify whether a given person is a male or a female based on the measured features 
The features include height  weight  and foot size  Although with NB classifier we treat them as independent  they are not in reality 

Training edit 
Example training set below 




Person
height  feet 
weight  lbs 
foot size  inches 


male
 
   
  


male
            
   
  


male
           
   
  


male
            
   
  


female
 
   
 


female
          
   
 


female
           
   
 


female
           
   
 


The classifier created from the training set using a Gaussian distribution assumption would be  given variances are unbiased sample variances  




Person
mean  height 
variance  height 
mean  weight 
variance  weight 
mean  foot size 
variance  foot size 


male
     
             
      
            
     
             


female
      
             
     
            
   
      

The following example assumes equiprobable classes so that P male   P female         This prior probability distribution might be based on prior knowledge of frequencies in the larger population or in the training set 

Testing edit 
Below is a sample to be classified as male or female 




Person
height  feet 
weight  lbs 
foot size  inches 


sample
 
   
 

In order to classify the sample  one has to determine which posterior is greater  male or female  For the classification as male the posterior is given by

  
    
      
        
          posterior  male 
        
         
        
          
            
              P
               
              
                male
              
               
              
              p
               
              
                height
              
                x     
              
                male
              
               
              
              p
               
              
                weight
              
                x     
              
                male
              
               
              
              p
               
              
                foot size
              
                x     
              
                male
              
               
            
            evidence
          
        
      
    
      displaystyle   text posterior  male      frac  P   text male     p   text height   mid   text male     p   text weight   mid   text male     p   text foot size   mid   text male      text evidence    
  

For the classification as female the posterior is given by

  
    
      
        
          posterior  female 
        
         
        
          
            
              P
               
              
                female
              
               
              
              p
               
              
                height
              
                x     
              
                female
              
               
              
              p
               
              
                weight
              
                x     
              
                female
              
               
              
              p
               
              
                foot size
              
                x     
              
                female
              
               
            
            evidence
          
        
      
    
      displaystyle   text posterior  female      frac  P   text female     p   text height   mid   text female     p   text weight   mid   text female     p   text foot size   mid   text female      text evidence    
  

The evidence  also termed normalizing constant  may be calculated 

  
    
      
        
          
            
              
                
                  evidence
                
                 
                P
                 
                
                  male
                
                 
                
                p
                 
                
                  height
                
                  x     
                
                  male
                
                 
                
                p
                 
                
                  weight
                
                  x     
                
                  male
                
                 
                
                p
                 
                
                  foot size
                
                  x     
                
                  male
                
                 
              
            
            
              
                 
                P
                 
                
                  female
                
                 
                
                p
                 
                
                  height
                
                  x     
                
                  female
                
                 
                
                p
                 
                
                  weight
                
                  x     
                
                  female
                
                 
                
                p
                 
                
                  foot size
                
                  x     
                
                  female
                
                 
              
            
          
        
      
    
      displaystyle   begin aligned   text evidence   P   text male     p   text height   mid   text male     p   text weight   mid   text male     p   text foot size   mid   text male      P   text female     p   text height   mid   text female     p   text weight   mid   text female     p   text foot size   mid   text female    end aligned   
  

However  given the sample  the evidence is a constant and thus scales both posteriors equally  It therefore does not affect classification and can be ignored   The probability distribution for the sex of the sample can now be determined 

  
    
      
        P
         
        
          male
        
         
         
           
      
    
      displaystyle P   text male        
  


  
    
      
        p
         
        
          height
        
          x     
        
          male
        
         
         
        
          
             
            
               
                x c  
              
                  x c  
                
                   
                
              
            
          
        
        exp
          x     
        
           
          
            
              
                  x     
                 
                 
                  x     
                  x bc 
                
                   
                  
                     
                  
                
              
              
                 
                
                    x c  
                  
                     
                  
                
              
            
          
           
        
          x     
              
         
      
    
      displaystyle p   text height   mid   text male      frac      sqrt    pi  sigma         exp  left   frac       mu          sigma        right  approx         
  

where 
  
    
      
          x bc 
         
             
      
    
      displaystyle  mu        
  
 and 
  
    
      
        
            x c  
          
             
          
        
         
              
          x  c  
        
            
          
              x     
             
          
        
      
    
      displaystyle  sigma             cdot         
  
 are the parameters of normal distribution which have been previously determined from the training set  Note that a value greater than   is OK here   it is a probability density rather than a probability  because height is a continuous variable 

  
    
      
        p
         
        
          weight
        
          x     
        
          male
        
         
         
        
          
             
            
               
                x c  
              
                  x c  
                
                   
                
              
            
          
        
        exp
          x     
        
           
          
            
              
                  x     
                 
                   
                  x     
                  x bc 
                
                   
                  
                     
                  
                
              
              
                 
                
                    x c  
                  
                     
                  
                
              
            
          
           
        
         
              
          x  c  
        
            
          
              x     
             
          
        
      
    
      displaystyle p   text weight   mid   text male      frac      sqrt    pi  sigma         exp  left   frac         mu          sigma        right         cdot         
  


  
    
      
        p
         
        
          foot size
        
          x     
        
          male
        
         
         
        
          
             
            
               
                x c  
              
                  x c  
                
                   
                
              
            
          
        
        exp
          x     
        
           
          
            
              
                  x     
                 
                 
                  x     
                  x bc 
                
                   
                  
                     
                  
                
              
              
                 
                
                    x c  
                  
                     
                  
                
              
            
          
           
        
         
              
          x  c  
        
            
          
              x     
             
          
        
      
    
      displaystyle p   text foot size   mid   text male      frac      sqrt    pi  sigma         exp  left   frac       mu          sigma        right         cdot         
  


  
    
      
        
          posterior numerator  male 
        
         
        
          their product
        
         
              
          x  c  
        
            
          
              x     
             
          
        
      
    
      displaystyle   text posterior numerator  male      text their product          cdot         
  


  
    
      
        P
         
        
          female
        
         
         
           
      
    
      displaystyle P   text female        
  


  
    
      
        p
         
        
          height
        
          x     
        
          female
        
         
         
            
          x  c  
        
            
          
              x     
             
          
        
      
    
      displaystyle p   text height   mid   text female         cdot         
  


  
    
      
        p
         
        
          weight
        
          x     
        
          female
        
         
         
              
          x  c  
        
            
          
              x     
             
          
        
      
    
      displaystyle p   text weight   mid   text female           cdot         
  


  
    
      
        p
         
        
          foot size
        
          x     
        
          female
        
         
         
              
          x  c  
        
            
          
              x     
             
          
        
      
    
      displaystyle p   text foot size   mid   text female           cdot         
  


  
    
      
        
          posterior numerator  female 
        
         
        
          their product
        
         
              
          x  c  
        
            
          
              x     
             
          
        
      
    
      displaystyle   text posterior numerator  female      text their product          cdot         
  

Since posterior numerator is greater in the female case  the prediction is that the sample is female 

Document classification edit 
Here is a worked example of naive Bayesian classification to the document classification problem 
Consider the problem of classifying documents by their content  for example into spam and non spam e mails  Imagine that documents are drawn from a number of classes of documents which can be modeled as sets of words where the  independent  probability that the i th word of a given document occurs in a document from class C can be written as

  
    
      
        p
         
        
          w
          
            i
          
        
          x     
        C
         
        
      
    
      displaystyle p w  i  mid C    
  

 For this treatment  things are further simplified by assuming that words are randomly distributed in the document   that is  words are not dependent on the length of the document  position within the document with relation to other words  or other document context  
Then the probability that a given document D contains all of the words 
  
    
      
        
          w
          
            i
          
        
      
    
      displaystyle w  i  
  
  given a class C  is

  
    
      
        p
         
        D
          x     
        C
         
         
        
            x   f 
          
            i
          
        
        p
         
        
          w
          
            i
          
        
          x     
        C
         
        
      
    
      displaystyle p D mid C   prod   i p w  i  mid C    
  

The question that has to be answered is   what is the probability that a given document D belongs to a given class C   In other words  what is 
  
    
      
        p
         
        C
          x     
        D
         
        
      
    
      displaystyle p C mid D    
  
 
Now by definition

  
    
      
        p
         
        D
          x     
        C
         
         
        
          
            
              p
               
              D
                x     
              C
               
            
            
              p
               
              C
               
            
          
        
      
    
      displaystyle p D mid C   p D cap C   over p C   
  

and

  
    
      
        p
         
        C
          x     
        D
         
         
        
          
            
              p
               
              D
                x     
              C
               
            
            
              p
               
              D
               
            
          
        
      
    
      displaystyle p C mid D   p D cap C   over p D   
  

Bayes  theorem manipulates these into a statement of probability in terms of likelihood 

  
    
      
        p
         
        C
          x     
        D
         
         
        
          
            
              p
               
              C
               
              
              p
               
              D
                x     
              C
               
            
            
              p
               
              D
               
            
          
        
      
    
      displaystyle p C mid D    frac  p C   p D mid C   p D    
  

Assume for the moment that there are only two mutually exclusive classes  S and  S  e g  spam and not spam   such that every element  email  is in either one or the other 

  
    
      
        p
         
        D
          x     
        S
         
         
        
            x   f 
          
            i
          
        
        p
         
        
          w
          
            i
          
        
          x     
        S
         
        
      
    
      displaystyle p D mid S   prod   i p w  i  mid S    
  

and

  
    
      
        p
         
        D
          x     
          xac 
        S
         
         
        
            x   f 
          
            i
          
        
        p
         
        
          w
          
            i
          
        
          x     
          xac 
        S
         
        
      
    
      displaystyle p D mid  neg S   prod   i p w  i  mid  neg S    
  

Using the Bayesian result above  one can write 

  
    
      
        p
         
        S
          x     
        D
         
         
        
          
            
              p
               
              S
               
            
            
              p
               
              D
               
            
          
        
        
        
            x   f 
          
            i
          
        
        p
         
        
          w
          
            i
          
        
          x     
        S
         
      
    
      displaystyle p S mid D   p S   over p D     prod   i p w  i  mid S  
  


  
    
      
        p
         
          xac 
        S
          x     
        D
         
         
        
          
            
              p
               
                xac 
              S
               
            
            
              p
               
              D
               
            
          
        
        
        
            x   f 
          
            i
          
        
        p
         
        
          w
          
            i
          
        
          x     
          xac 
        S
         
      
    
      displaystyle p  neg S mid D   p  neg S   over p D     prod   i p w  i  mid  neg S  
  

Dividing one by the other gives 

  
    
      
        
          
            
              p
               
              S
                x     
              D
               
            
            
              p
               
                xac 
              S
                x     
              D
               
            
          
        
         
        
          
            
              p
               
              S
               
              
              
                  x   f 
                
                  i
                
              
              p
               
              
                w
                
                  i
                
              
                x     
              S
               
            
            
              p
               
                xac 
              S
               
              
              
                  x   f 
                
                  i
                
              
              p
               
              
                w
                
                  i
                
              
                x     
                xac 
              S
               
            
          
        
      
    
      displaystyle  p S mid D   over p  neg S mid D    p S    prod   i p w  i  mid S   over p  neg S    prod   i p w  i  mid  neg S   
  

Which can be re factored as 

  
    
      
        
          
            
              p
               
              S
                x     
              D
               
            
            
              p
               
                xac 
              S
                x     
              D
               
            
          
        
         
        
          
            
              p
               
              S
               
            
            
              p
               
                xac 
              S
               
            
          
        
        
        
            x   f 
          
            i
          
        
        
          
            
              p
               
              
                w
                
                  i
                
              
                x     
              S
               
            
            
              p
               
              
                w
                
                  i
                
              
                x     
                xac 
              S
               
            
          
        
      
    
      displaystyle  p S mid D   over p  neg S mid D    p S   over p  neg S     prod   i  p w  i  mid S   over p w  i  mid  neg S   
  

Thus  the probability ratio p S   D    p  S   D  can be expressed in terms of a series of likelihood ratios 
The actual probability p S   D  can be easily computed from log  p S   D    p  S   D   based on the observation that p S   D    p  S   D      
Taking the logarithm of all these ratios  one obtains 

  
    
      
        ln
          x     
        
          
            
              p
               
              S
                x     
              D
               
            
            
              p
               
                xac 
              S
                x     
              D
               
            
          
        
         
        ln
          x     
        
          
            
              p
               
              S
               
            
            
              p
               
                xac 
              S
               
            
          
        
         
        
            x     
          
            i
          
        
        ln
          x     
        
          
            
              p
               
              
                w
                
                  i
                
              
                x     
              S
               
            
            
              p
               
              
                w
                
                  i
                
              
                x     
                xac 
              S
               
            
          
        
      
    
      displaystyle  ln  p S mid D   over p  neg S mid D    ln  p S   over p  neg S    sum   i  ln  p w  i  mid S   over p w  i  mid  neg S   
  

 This technique of  log likelihood ratios  is a common technique in statistics 
In the case of two mutually exclusive alternatives  such as this example   the conversion of a log likelihood ratio to a probability takes the form of a sigmoid curve  see logit for details  
Finally  the document can be classified as follows   It is spam if 
  
    
      
        p
         
        S
          x     
        D
         
         gt 
        p
         
          xac 
        S
          x     
        D
         
      
    
      displaystyle p S mid D  gt p  neg S mid D  
  
  i  e   
  
    
      
        ln
          x     
        
          
            
              p
               
              S
                x     
              D
               
            
            
              p
               
                xac 
              S
                x     
              D
               
            
          
        
         gt 
         
      
    
      displaystyle  ln  p S mid D   over p  neg S mid D   gt   
  
   otherwise it is not spam 

Spam filtering edit 
Naive Bayes classifiers are a popular statistical technique of e mail filtering  They typically use bag of words features to identify email spam  an approach commonly used in text classification  Naive Bayes classifiers work by correlating the use of tokens  typically words  or sometimes other things   with spam and non spam e mails and then using Bayes  theorem to calculate a probability that an email is or is not spam 
Naive Bayes spam filtering is a baseline technique for dealing with spam that can tailor itself to the email needs of individual users and give low false positive spam detection rates that are generally acceptable to users  Bayesian algorithms were used for email filtering as early as        Although naive Bayesian filters did not become popular until later  multiple programs were released in      to address the growing problem of unwanted email               The first scholarly publication on Bayesian spam filtering was by Sahami et al  in                  
Variants of the basic technique have been implemented in a number of research works and commercial software products              Many modern mail clients implement Bayesian spam filtering  Users can also install separate email filtering programs  Server side email filters  such as DSPAM  SpamAssassin              SpamBayes              Bogofilter  and ASSP  make use of Bayesian spam filtering techniques  and the functionality is sometimes embedded within mail server software itself  CRM     oft cited as a Bayesian filter  is not intended to use a Bayes filter in production  but includes the  unigram  feature for reference             

Dealing with rare words edit 
In the case a word has never been met during the learning phase  both the numerator and the denominator are equal to zero  both in the general formula and in the spamicity formula  The software can decide to discard such words for which there is no information available 
More generally  the words that were encountered only a few times during the learning phase cause a problem  because it would be an error to trust blindly the information they provide  A simple solution is to simply avoid taking such unreliable words into account as well 
Applying again Bayes  theorem  and assuming the classification between spam and ham of the emails containing a given word   replica   is a random variable with beta distribution  some programs decide to use a corrected probability 


  
    
      
        
          Pr
            x     
        
         
        S
        
           
        
        W
         
         
        
          
            
              s
                x  c  
              Pr
               
              S
               
               
              n
                x  c  
              Pr
               
              S
              
                 
              
              W
               
            
            
              s
               
              n
            
          
        
      
    
      displaystyle  Pr   S W    frac  s cdot  Pr S  n cdot  Pr S W   s n   
  

where 


  
    
      
        
          Pr
            x     
        
         
        S
        
           
        
        W
         
      
    
      displaystyle  Pr   S W  
  
 is the corrected probability for the message to be spam  knowing that it contains a given word       

  
    
      
        s
      
    
      displaystyle s 
  
 is the strength we give to background information about incoming spam       

  
    
      
        Pr
         
        S
         
      
    
      displaystyle  Pr S  
  
 is the probability of any incoming message to be spam       

  
    
      
        n
      
    
      displaystyle n 
  
 is the number of occurrences of this word during the learning phase       

  
    
      
        Pr
         
        S
        
           
        
        W
         
      
    
      displaystyle  Pr S W  
  
 is the spamicity of this word 
 Demonstration              
This corrected probability is used instead of the spamicity in the combining formula 
This formula can be extended to the case where n is equal to zero  and where the spamicity is not defined   and evaluates in this case to 
  
    
      
        P
        r
         
        S
         
      
    
      displaystyle Pr S  
  
 

Other heuristics edit 
 Neutral  words like  the    a    some   or  is   in English   or their equivalents in other languages  can be ignored  These are also known as Stop words  More generally  some bayesian filtering filters simply ignore all the words which have a spamicity next to      as they contribute little to a good decision  The words taken into consideration are those whose spamicity is next to      distinctive signs of legitimate messages   or next to      distinctive signs of spam   A method can be for example to keep only those ten words  in the examined message  which have the greatest absolute value                       pI  
Some software products take into account the fact that a given word appears several times in the examined message              others don t 
Some software products use patterns  sequences of words  instead of isolated natural languages words              For example  with a  context window  of four words  they compute the spamicity of  Viagra is good for   instead of computing the spamicities of  Viagra    is    good   and  for   This method gives more sensitivity to context and eliminates the Bayesian noise better  at the expense of a bigger database 

Disadvantages edit 
Depending on the implementation  Bayesian spam filtering may be susceptible to Bayesian poisoning  a technique used by spammers in an attempt to degrade the effectiveness of spam filters that rely on Bayesian filtering  A spammer practicing Bayesian poisoning will send out emails with large amounts of legitimate text  gathered from legitimate news or literary sources   Spammer tactics include insertion of random innocuous words that are not normally associated with spam  thereby decreasing the email s spam score  making it more likely to slip past a Bayesian spam filter  However  with  for example  Paul Graham s scheme only the most significant probabilities are used  so that padding the text out with non spam related words does not affect the detection probability significantly 
Words that normally appear in large quantities in spam may also be transformed by spammers  For example   Viagra  would be replaced with  Viaagra  or  V agra  in the spam message  The recipient of the message can still read the changed words  but each of these words is met more rarely by the Bayesian filter  which hinders its learning process  As a general rule  this spamming technique does not work very well  because the derived words end up recognized by the filter just like the normal ones             
Another technique used to try to defeat Bayesian spam filters is to replace text with pictures  either directly included or linked  The whole text of the message  or some part of it  is replaced with a picture where the same text is  drawn   The spam filter is usually unable to analyze this picture  which would contain the sensitive words like  Viagra   However  since many mail clients disable the display of linked pictures for security reasons  the spammer sending links to distant pictures might reach fewer targets  Also  a picture s size in bytes is bigger than the equivalent text s size  so the spammer needs more bandwidth to send messages directly including pictures  Some filters are more inclined to decide that a message is spam if it has mostly graphical contents  A solution used by Google in its Gmail email system is to perform an OCR  Optical Character Recognition  on every mid to large size image  analyzing the text inside                         

See also edit 
AODE
Anti spam techniques
Bayes classifier
Bayesian network
Bayesian poisoning
Email filtering
Linear classifier
Logistic regression
Markovian discrimination
Mozilla Thunderbird mail client with native implementation of Bayes filters                        
Perceptron
Random naive Bayes
Take the best heuristic
References edit 


  a b c Hand  D  J   Yu  K           Idiot s Bayes   not so stupid after all    International Statistical Review                   doi                  ISSN                 JSTOR              

  McCallum  Andrew   Graphical Models  Lecture   Bayesian Network Representation   PDF   Archived  PDF  from the original on             Retrieved    October      

  a b Russell  Stuart  Norvig  Peter                Artificial Intelligence  A Modern Approach   nd      ed    Prentice Hall  ISBN                     

  Zhang  Harry  The Optimality of Naive Bayes  PDF   FLAIRS     conference 

  Caruana  R   Niculescu Mizil  A          An empirical comparison of supervised learning algorithms  Proc    rd International Conference on Machine Learning  CiteSeerX                      

   Why does Naive Bayes work better when the number of features  gt  gt  sample size compared to more sophisticated ML algorithms    Cross Validated Stack Exchange  Retrieved    January      

  Narasimha Murty  M   Susheela Devi  V          Pattern Recognition  An Algorithmic Approach  ISBN                     

  a b John  George H   Langley  Pat         Estimating Continuous Distributions in Bayesian Classifiers  Proc  Eleventh Conf  on Uncertainty in Artificial Intelligence  Morgan Kaufmann  pp                arXiv           

  a b c McCallum  Andrew  Nigam  Kamal         A comparison of event models for Naive Bayes text classification  PDF   AAAI    workshop on learning for text categorization  Vol            Archived  PDF  from the original on            

  Metsis  Vangelis  Androutsopoulos  Ion  Paliouras  Georgios         Spam filtering with Naive Bayes which Naive Bayes   Third conference on email and anti spam  CEAS   Vol          

  Piryonesi  S  Madeh  El Diraby  Tamer E                 Role of Data Analytics in Infrastructure Asset Management  Overcoming Data Size and Quality Problems   Journal of Transportation Engineering  Part B  Pavements                     doi         JPEODX          S CID                

  Hastie  Trevor          The elements of statistical learning        data mining  inference  and prediction        with     full color illustrations  Tibshirani  Robert   Friedman  J  H   Jerome H    New York  Springer  ISBN                     OCLC               

  James  Gareth  Witten  Daniela  Hastie  Trevor  Tibshirani  Robert         An introduction to statistical learning  with applications in R  Second      ed    New York  NY  Springer  p            doi                            ISBN                         Retrieved    November      

  a b Rennie  J   Shih  L   Teevan  J   Karger  D          Tackling the poor assumptions of naive Bayes classifiers  PDF   ICML  Archived  PDF  from the original on            

  a b Nigam  Kamal  McCallum  Andrew  Thrun  Sebastian  Mitchell  Tom          Learning to classify text from labeled and unlabeled documents using EM   PDF   Machine Learning                     doi         A                S CID              Archived  PDF  from the original on            

  Niculescu Mizil  Alexandru  Caruana  Rich         Predicting good probabilities with supervised learning  PDF   ICML  doi                          Archived from the original  PDF  on             Retrieved            

  Rish  Irina         An empirical study of the naive Bayes classifier  PDF   IJCAI Workshop on Empirical Methods in AI  Archived  PDF  from the original on            

  a b Ng  Andrew Y   Jordan  Michael I          On discriminative vs  generative classifiers  A comparison of logistic regression and naive Bayes  NIPS  Vol          

  Brunton  Finn         Spam  A Shadow History of the Internet  MIT Press  p            ISBN                     Archived from the original on             Retrieved            

  M  Sahami  S  Dumais  D  Heckerman  E  Horvitz          A Bayesian approach to filtering junk e mail   PDF   AAAI    Workshop on Learning for Text Categorization  Archived  PDF  from the original on             Retrieved            

   Junk Mail Controls   MozillaZine  November       Archived from the original on             Retrieved            

   Installation   Ubuntu manuals              Archived from the original on    September       Retrieved             Gary Robinson s f x  and combining algorithms  as used in SpamAssassin

   Background Reading   SpamBayes project              Archived from the original on   September       Retrieved             Sharpen your pencils  this is the mathematical background  such as it is    The paper that started the ball rolling  Paul Graham s A Plan for Spam   Gary Robinson has an interesting essay suggesting some improvements to Graham s original approach   Gary Robinson s Linux Journal article discussed using the chi squared distribution 

   Archived copy   Archived from the original on             Retrieved              cite web     CS  maint  archived copy as title  link 

  Gary Robinson          A statistical approach to the spam problem   Linux Journal  Archived from the original on             Retrieved            

  Brian Burton          SpamProbe   Bayesian Spam Filtering Tweaks   Archived from the original on             Retrieved            

  Jonathan A  Zdziarski          Bayesian Noise Reduction  Contextual Symmetry Logic Utilizing Pattern Consistency Analysis       permanent dead link     

  Paul Graham         A Plan for Spam Archived            at the Wayback Machine

   Gmail uses Google s innovative technology to keep spam out of your inbox   Archived from the original on             Retrieved            

  Zhu  Z   Jia  Z  Xiao  H  Zhang  G  Liang  H   Wang  P          Li  S  Jin  Q  Jiang  X  Park  J  eds     A Modified Minimum Risk Bayes and It s  sic  Application in Spam   Lecture Notes in Electrical Engineering       Dordrecht  Springer             doi                               

  Hristea  Florentina T          The Na ve Bayes Model for Unsupervised Word Sense Disambiguation  London  Berlin  Springer  Verlag Heidelberg Berlin  p           ISBN                        

  Zheng  J   Tang  Yongchuan          One Generalization of the Naive Bayes to Fuzzy Sets and the Design of the Fuzzy Naive Bayes Classifier   In Mira  Jose   lvarez  Jose R  eds    Artificial Intelligence and Knowledge Engineering Applications  A Bioinspired Approach  Lecture Notes in Computer Science  Vol             Berlin  Springer  Berlin  Heidelberg  p            doi                      ISBN                         ISSN                


Further reading edit 
Domingos  Pedro  Pazzani  Michael          On the optimality of the simple Bayesian classifier under zero one loss   Machine Learning                     doi         A               
Webb  G  I   Boughton  J   Wang  Z           Not So Naive Bayes  Aggregating One Dependence Estimators   Machine Learning                doi         s                 
Mozina  M   Demsar  J   Kattan  M   Zupan  B          Nomograms for Visualization of Naive Bayesian Classifier  PDF   Proc  PKDD       pp               
Maron  M  E           Automatic Indexing  An Experimental Inquiry   Journal of the ACM                  doi                        hdl      uva x           S CID              
Minsky  M          Steps toward Artificial Intelligence  Proc  IRE  Vol           pp            
External links edit 
Book Chapter  Naive Bayes text classification  Introduction to Information Retrieval
Naive Bayes for Text Classification with Unbalanced Classes
vteUnsolicited digital communicationProtocolsEmail spam
Address munging
Bulk email software
Directory harvest attack
DNSBL
DNSWL
Email spoofing
Image spam
Joe job
Pink contract
Spambot
Other
Auto dialer
Cold calling
Flyposting
Junk fax
Messaging
Mobile phone
Newsgroup
Robocall
Spambot
Telemarketing
VoIP
Anti spam
Client side
Challenge response spam filtering
Context filtering
Disposable email address
Distributed Checksum Clearinghouse
Email authentication
Greylisting
List poisoning
MyWOT
Naive Bayes spam filtering
SORBS
SpamCop
Spamhaus
Spamdexing
Blog spam
Cloaking
Doorway page
Forum spam
Google bombing
Keyword stuffing
Link farm
Referrer spam
Scraper site
Social spam
Spam blogs
Sping
URL redirection
Internet fraud
Advance fee scam
Lottery scam
Make Money Fast
Phishing
Voice






Retrieved from  https   en wikipedia org w index php title Naive Bayes classifier amp oldid