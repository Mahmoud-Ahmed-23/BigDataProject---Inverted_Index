Mathematical function conceived as a crude model
Artificial neuron structure
An artificial neuron is a mathematical function conceived as a model of a biological neuron in a neural network  The artificial neuron is the elementary unit of an artificial neural network            
The design of the artificial neuron was inspired by biological neural circuitry  Its inputs are analogous to excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites  or activation  Its weights are analogous to synaptic weights  and its output is analogous to a neuron s action potential which is transmitted along its axon 
Usually  each input is separately weighted  and the sum is often added to a term known as a bias  loosely corresponding to the threshold potential   before being passed through a nonlinear function known as an activation function  Depending on the task  these functions could have a sigmoid shape  e g  for binary classification   but they may also take the form of other nonlinear functions  piecewise linear functions  or step functions  They are also often monotonically increasing  continuous  differentiable  and bounded  Non monotonic  unbounded  and oscillating activation functions with multiple zeros that outperform sigmoidal and ReLU like activation functions on many tasks have also been recently explored  The threshold function has inspired building logic gates referred to as threshold logic  applicable to building logic circuits resembling brain processing  For example  new devices such as memristors have been extensively used to develop such logic            
The artificial neuron activation function should not be confused with a linear system s transfer function 
An artificial neuron may be referred to as a semi linear unit  Nv neuron  binary neuron  linear threshold function  or McCulloch Pitts  MCP  neuron  depending on the structure used 
Simple artificial neurons  such as the McCulloch Pitts model  are sometimes described as  caricature models   since they are intended to reflect one or more neurophysiological observations  but without regard to realism             Artificial neurons can also refer to artificial cells in neuromorphic engineering that are similar to natural physical neurons 


Basic structure edit 
For a given artificial neuron 
  
    
      
        k
      
    
      displaystyle k 
  
  let there be 
  
    
      
        m
         
         
      
    
      displaystyle m   
  
 inputs with signals 
  
    
      
        
          x
          
             
          
        
      
    
      displaystyle x     
  
 through 
  
    
      
        
          x
          
            m
          
        
      
    
      displaystyle x  m  
  
 and weights 
  
    
      
        
          w
          
            k
             
          
        
      
    
      displaystyle w  k   
  
 through 
  
    
      
        
          w
          
            k
            m
          
        
      
    
      displaystyle w  km  
  
  Usually  the input 
  
    
      
        
          x
          
             
          
        
      
    
      displaystyle x     
  
 is assigned the value     which makes it a bias input with 
  
    
      
        
          w
          
            k
             
          
        
         
        
          b
          
            k
          
        
      
    
      displaystyle w  k   b  k  
  
  This leaves only 
  
    
      
        m
      
    
      displaystyle m 
  
 actual inputs to the neuron  
  
    
      
        
          x
          
             
          
        
      
    
      displaystyle x     
  
 to 
  
    
      
        
          x
          
            m
          
        
      
    
      displaystyle x  m  
  
 
The output of the 
  
    
      
        k
      
    
      displaystyle k 
  
 th neuron is 


  
    
      
        
          y
          
            k
          
        
         
          x c  
        
           
          
            
                x     
              
                j
                 
                 
              
              
                m
              
            
            
              w
              
                k
                j
              
            
            
              x
              
                j
              
            
          
           
        
      
    
      displaystyle y  k   varphi  left  sum   j     m w  kj x  j  right  
  
 
where 
  
    
      
          x c  
      
    
      displaystyle  varphi  
  
  phi  is the  activation function 

The output is analogous to the axon of a biological neuron  and its value propagates to the input of the next layer  through a synapse  It may also exit the system  possibly as part of an output vector 
It has no learning process as such  Its activation function weights are calculated  and its threshold value is predetermined 

McCulloch Pitts  MCP  neuron edit 
Main article  Perceptron
An MCP neuron is a kind of restricted artificial neuron which operates in discrete time steps  Each has zero or more inputs  and are written as 
  
    
      
        
          x
          
             
          
        
         
         
         
         
         
        
          x
          
            n
          
        
      
    
      displaystyle x         x  n  
  
  It has one output  written as 
  
    
      
        y
      
    
      displaystyle y 
  
  Each input can be either excitatory or inhibitory  The output can either be quiet or firing  An MCP neuron also has a threshold 
  
    
      
        b
          x     
         
         
         
         
         
         
         
         
         
         
         
      
    
      displaystyle b in               
  
 
In an MCP neural network  all the neurons operate in synchronous discrete time steps of 
  
    
      
        t
         
         
         
         
         
         
         
         
         
         
         
         
      
    
      displaystyle t             
  
  At time 
  
    
      
        t
         
         
      
    
      displaystyle t   
  
  the output of the neuron is 
  
    
      
        y
         
        t
         
         
         
         
         
      
    
      displaystyle y t      
  
 if the number of firing excitatory inputs is at least equal to the threshold  and no inhibitory inputs are firing  
  
    
      
        y
         
        t
         
         
         
         
         
      
    
      displaystyle y t      
  
 otherwise 
Each output can be the input to an arbitrary number of neurons  including itself  i e   self loops are possible   However  an output cannot connect more than once with a single neuron  Self loops do not cause contradictions  since the network operates in synchronous discrete time steps 
As a simple example  consider a single neuron with threshold    and a single inhibitory self loop  Its output would oscillate between   and   at every step  acting as a  clock  
Any finite state machine can be simulated by a MCP neural network             Furnished with an infinite tape  MCP neural networks can simulate any Turing machine            

Biological models edit 
Main article  Biological neuron model
Neuron and myelinated axon  with signal flow from inputs at dendrites to outputs at axon terminals
Artificial neurons are designed to mimic aspects of their biological counterparts  However a significant performance gap exists between biological and artificial neural networks  In particular single biological neurons in the human brain with oscillating activation function capable of learning the XOR function have been discovered            

Dendrites   in biological neurons  dendrites act as the input vector  These dendrites allow the cell to receive signals from a large   gt       number of neighboring neurons  As in the above mathematical treatment  each dendrite is able to perform  multiplication  by that dendrite s  weight value   The multiplication is accomplished by increasing or decreasing the ratio of synaptic neurotransmitters to signal chemicals introduced into the dendrite in response to the synaptic neurotransmitter  A negative multiplication effect can be achieved by transmitting signal inhibitors  i e  oppositely charged ions  along the dendrite in response to the reception of synaptic neurotransmitters 
Soma   in biological neurons  the soma acts as the summation function  seen in the above mathematical description  As positive and negative signals  exciting and inhibiting  respectively  arrive in the soma from the dendrites  the positive and negative ions are effectively added in summation  by simple virtue of being mixed together in the solution inside the cell s body 
Axon   the axon gets its signal from the summation behavior which occurs inside the soma  The opening to the axon essentially samples the electrical potential of the solution inside the soma  Once the soma reaches a certain potential  the axon will transmit an all in signal pulse down its length  In this regard  the axon behaves as the ability for us to connect our artificial neuron to other artificial neurons 
Unlike most artificial neurons  however  biological neurons fire in discrete pulses  Each time the electrical potential inside the soma reaches a certain threshold  a pulse is transmitted down the axon  This pulsing can be translated into continuous values  The rate  activations per second  etc   at which an axon fires converts directly into the rate at which neighboring cells get signal ions introduced into them  The faster a biological neuron fires  the faster nearby neurons accumulate electrical potential  or lose electrical potential  depending on the  weighting  of the dendrite that connects to the neuron that fired   It is this conversion that allows computer scientists and mathematicians to simulate biological neural networks using artificial neurons which can output distinct values  often from    to    

Encoding edit 
Research has shown that unary coding is used in the neural circuits responsible for birdsong production                        The use of unary in biological networks is presumably due to the inherent simplicity of the coding  Another contributing factor could be that unary coding provides a certain degree of error correction            

Physical artificial cells edit 
There is research and development into physical artificial neurons   organic and inorganic 
For example  some artificial neurons can receive                         and release dopamine  chemical signals rather than electrical signals  and communicate with natural rat muscle and brain cells  with potential for use in BCIs prosthetics                         
Low power biocompatible memristors may enable construction of artificial neurons which function at voltages of biological action potentials and could be used to directly process biosensing signals  for neuromorphic computing and or direct communication with biological neurons                                     
Organic neuromorphic circuits made out of polymers  coated with an ion rich gel to enable a material to carry an electric charge like real neurons  have been built into a robot  enabling it to learn sensorimotorically within the real world  rather than via simulations or virtually                          Moreover  artificial spiking neurons made of soft matter  polymers  can operate in biologically relevant environments and enable the synergetic communication between the artificial and biological domains                         

History edit 
The first artificial neuron was the Threshold Logic Unit  TLU   or Linear Threshold Unit              first proposed by Warren McCulloch and Walter Pitts in      in A logical calculus of the ideas immanent in nervous activity  The model was specifically targeted as a computational model of the  nerve net  in the brain              As an activation function  it employed a threshold  equivalent to using the Heaviside step function  Initially  only a simple model was considered  with binary inputs and outputs  some restrictions on the possible weights  and a more flexible threshold value  Since the beginning it was already noticed that any Boolean function could be implemented by networks of such devices  what is easily seen from the fact that one can implement the AND and OR functions  and use them in the disjunctive or the conjunctive normal form 
Researchers also soon realized that cyclic networks  with feedbacks through neurons  could define dynamical systems with memory  but most of the research concentrated  and still does  on strictly feed forward networks because of the smaller difficulty they present 
One important and pioneering artificial neural network that used the linear threshold function was the perceptron  developed by Frank Rosenblatt  This model already considered more flexible weight values in the neurons  and was used in machines with adaptive capabilities  The representation of the threshold values as a bias term was introduced by Bernard Widrow in        see ADALINE 
In the late     s  when research on neural networks regained strength  neurons with more continuous shapes started to be considered  The possibility of differentiating the activation function allows the direct use of the gradient descent and other optimization algorithms for the adjustment of the weights  Neural networks also started to be used as a general function approximation model  The best known training algorithm called backpropagation has been rediscovered several times but its first development goes back to the work of Paul Werbos                         

Types of activation function edit 
Main article  Activation function
The activation function of a neuron is chosen to have a number of properties which either enhance or simplify the network containing the neuron  Crucially  for instance  any multilayer perceptron using a linear activation function has an equivalent single layer network  a non linear function is therefore necessary to gain the advantages of a multi layer network      citation needed     
Below  
  
    
      
        u
      
    
      displaystyle u 
  
 refers in all cases to the weighted sum of all the inputs to the neuron  i e  for 
  
    
      
        n
      
    
      displaystyle n 
  
 inputs 


  
    
      
        u
         
        
            x     
          
            i
             
             
          
          
            n
          
        
        
          w
          
            i
          
        
        
          x
          
            i
          
        
      
    
      displaystyle u  sum   i     n w  i x  i  
  

where 
  
    
      
        w
      
    
      displaystyle w 
  
 is a vector of synaptic weights and 
  
    
      
        x
      
    
      displaystyle x 
  
 is a vector of inputs 

Step function edit 
Main article  Step function
The output 
  
    
      
        y
      
    
      displaystyle y 
  
 of this activation function is binary  depending on whether the input meets a specified threshold  
  
    
      
          x b  
      
    
      displaystyle  theta  
  
  theta   The  signal  is sent  i e  the output is set to    if the activation meets or exceeds the threshold 


  
    
      
        y
         
        
          
             
            
              
                
                   
                
                
                  
                    if  xa  
                  
                  u
                    x     
                    x b  
                
              
              
                
                   
                
                
                  
                    if  xa  
                  
                  u
                   lt 
                    x b  
                
              
            
            
          
        
      
    
      displaystyle y   begin cases   amp   text if   u geq  theta     amp   text if   u lt  theta  end cases   
  

This function is used in perceptrons  and appears in many other models  It performs a division of the space of inputs by a hyperplane  It is specially useful in the last layer of a network  intended for example to perform binary classification of the inputs 

Linear combination edit 
Main article  Linear combination
In this case  the output unit is simply the weighted sum of its inputs  plus a bias term  A number of such linear neurons perform a linear transformation of the input vector  This is usually more useful in the early layers of a network  A number of analysis tools exist based on linear models  such as harmonic analysis  and they can all be used in neural networks with this linear neuron  The bias term allows us to make affine transformations to the data 

Sigmoid edit 
Main article  Sigmoid function
A fairly simple nonlinear function  the sigmoid function such as the logistic function also has an easily calculated derivative  which can be important when calculating the weight updates in the network  It thus makes the network more easily manipulable mathematically  and was attractive to early computer scientists who needed to minimize the computational load of their simulations  It was previously commonly seen in multilayer perceptrons  However  recent work has shown sigmoid neurons to be less effective than rectified linear neurons  The reason is that the gradients computed by the backpropagation algorithm tend to diminish towards zero as activations propagate through layers of sigmoidal neurons  making it difficult to optimize neural networks using multiple layers of sigmoidal neurons 

Rectifier edit 
Main article  Rectifier  neural networks 
In the context of artificial neural networks  the rectifier or ReLU  Rectified Linear Unit  is an activation function defined as the positive part of its argument 


  
    
      
        f
         
        x
         
         
        
          x
          
             
          
        
         
        max
         
         
         
        x
         
         
      
    
      displaystyle f x  x      max   x   
  

where 
  
    
      
        x
      
    
      displaystyle x 
  
 is the input to a neuron  This is also known as a ramp function and is analogous to half wave rectification in electrical engineering  This activation function was first introduced to a dynamical network by Hahnloser et al  in a      paper in Nature             with strong biological motivations and mathematical justifications              It has been demonstrated for the first time in      to enable better training of deeper networks              compared to the widely used activation functions prior to       i e   the logistic sigmoid  which is inspired by probability theory  see logistic regression  and its more practical             counterpart  the hyperbolic tangent 
A commonly used variant of the ReLU activation function is the Leaky ReLU which allows a small  positive gradient when the unit is not active 

  
    
      
        f
         
        x
         
         
        
          
             
            
              
                
                  x
                
                
                  
                    if  xa  
                  
                  x
                   gt 
                   
                   
                
              
              
                
                  a
                  x
                
                
                  
                    otherwise
                  
                   
                
              
            
            
          
        
      
    
      displaystyle f x    begin cases x amp   text if   x gt     ax amp   text otherwise    end cases   
  

where 
  
    
      
        x
      
    
      displaystyle x 
  
 is the input to the neuron and 
  
    
      
        a
      
    
      displaystyle a 
  
 is a small positive constant  set to      in the original paper              

Pseudocode algorithm edit 
The following is a simple pseudocode implementation     citation needed      of a single Threshold Logic Unit  TLU  which takes Boolean inputs  true or false   and returns a single Boolean output when activated  An object oriented model is used  No method of training is defined  since several exist  If a purely functional model were used  the class TLU below would be replaced with a function TLU with input parameters threshold  weights  and inputs that returned a Boolean value 

class TLU defined as 
    data member threshold   number
    data member weights   list of numbers of size X

    function member fire inputs   list of booleans of size X    boolean defined as 
        variable T   number
        T    
        for each i in   to X do
            if inputs i  is true then
                T   T   weights i 
            end if
        end for each
        if T  gt  threshold then
            return true
        else 
            return false
        end if
    end function
end class

See also edit 
Binding neuron
Connectionism
References edit 


  Rami A  Alzahrani  Alice C  Parker   Neuromorphic Circuits With Neural Modulation Enhancing the Information Content of Neural Signaling   Proceedings of International Conference on Neuromorphic Systems       Art           New York  Association for Computing Machinery  doi                          ISBN                         S CID                

  Maan  A  K   Jayadevi  D  A   James  A  P     January         A Survey of Memristive Threshold Logic Circuits   IEEE Transactions on Neural Networks and Learning Systems  PP                  arXiv             Bibcode     arXiv         M  doi         TNNLS               ISSN              X  PMID                S CID              

  
F  C  Hoppensteadt and E  M  Izhikevich         Weakly connected neural networks  Springer  p          ISBN                        

  Minsky  Marvin Lee               Computation  Finite and Infinite Machines  Prentice Hall  ISBN                        

  McCulloch  Warren S   Pitts  Walter                A logical calculus of the ideas immanent in nervous activity   The Bulletin of Mathematical Biophysics                  doi         BF          ISSN                

  Gidon  Albert  Zolnik  Timothy Adam  Fidzinski  Pawel  Bolduan  Felix  Papoutsi  Athanasia  Poirazi  Panayiota  Holtkamp  Martin  Vida  Imre  Larkum  Matthew Evan                Dendritic action potentials and computation in human layer     cortical neurons   Science                     Bibcode     Sci           G  doi         science aax      PMID                S CID                

  Squire  L   Albright  T   Bloom  F   Gage  F   Spitzer  N   eds   October        Neural network models of birdsong production  learning  and coding  PDF   New Encyclopedia of Neuroscience  Elservier  Archived from the original  PDF  on             Retrieved    April      

  Moore  J M   et      al           Motor pathway convergence predicts syllable repertoire size in oscine birds   Proc  Natl  Acad  Sci  USA                         Bibcode     PNAS          M  doi         pnas             PMC               PMID               

  Potluri  Pushpa Sree     November         Error Correction Capacity of Unary Coding   arXiv            cs IT  

  Kleiner  Kurt     August         Making computer chips act more like brain cells   Knowable Magazine  doi         knowable           Retrieved    September      

  Keene  Scott T   Lubrano  Claudia  Kazemzadeh  Setareh  Melianas  Armantas  Tuchman  Yaakov  Polino  Giuseppina  Scognamiglio  Paola  Cin   Lucio  Salleo  Alberto  van de Burgt  Yoeri  Santoro  Francesca  September         A biohybrid synapse with neurotransmitter mediated plasticity   Nature Materials                   Bibcode     NatMa         K  doi         s               y  ISSN                 PMID                S CID                
University press release   Researchers develop artificial synapse that works with living cells   Stanford University via medicalxpress com  Retrieved    September      

   Artificial neuron swaps dopamine with rat brain cells like a real one   New Scientist  Retrieved    September      

  Wang  Ting  Wang  Ming  Wang  Jianwu  Yang  Le  Ren  Xueyang  Song  Gang  Chen  Shisheng  Yuan  Yuehui  Liu  Ruiqing  Pan  Liang  Li  Zheng  Leow  Wan Ru  Luo  Yifei  Ji  Shaobo  Cui  Zequn  He  Ke  Zhang  Feilong  Lv  Fengting  Tian  Yuanyuan  Cai  Kaiyu  Yang  Bowen  Niu  Jingyi  Zou  Haochen  Liu  Songrui  Xu  Guoliang  Fan  Xing  Hu  Benhui  Loh  Xian Jun  Wang  Lianhui  Chen  Xiaodong    August         A chemically mediated artificial neuron   Nature Electronics                  doi         s                   hdl               ISSN                 S CID                

   Scientists create tiny devices that work like the human brain   The Independent  April           Archived from the original on April           Retrieved May          

   Researchers unveil electronics that mimic the human brain in efficient learning   phys org  Archived from the original on May           Retrieved May          

  Fu  Tianda  Liu  Xiaomeng  Gao  Hongyan  Ward  Joy E   Liu  Xiaorong  Yin  Bing  Wang  Zhongrui  Zhuo  Ye  Walker  David J  F   Joshua Yang  J   Chen  Jianhan  Lovley  Derek R   Yao  Jun  April             Bioinspired bio voltage memristors   Nature Communications                Bibcode     NatCo         F  doi         s                y  PMC               PMID               

  Bolakhe  Saugat   Lego Robot with an Organic  Brain  Learns to Navigate a Maze   Scientific American  Retrieved   February      

  Krauhausen  Imke  Koutsouras  Dimitrios A   Melianas  Armantas  Keene  Scott T   Lieberth  Katharina  Ledanseur  Hadrien  Sheelamanthula  Rajendar  Giovannitti  Alexander  Torricelli  Fabrizio  Mcculloch  Iain  Blom  Paul W  M   Salleo  Alberto  Burgt  Yoeri van de  Gkoupidenis  Paschalis  December         Organic neuromorphic electronics for sensorimotor integration and learning in robotics   Science Advances          eabl      Bibcode     SciA          K  doi         sciadv abl      hdl               PMC               PMID                S CID                

  Sarkar  Tanmoy  Lieberth  Katharina  Pavlou  Aristea  Frank  Thomas  Mailaender  Volker  McCulloch  Iain  Blom  Paul W  M   Torriccelli  Fabrizio  Gkoupidenis  Paschalis    November         An organic artificial spiking neuron for in situ neuromorphic sensing and biointerfacing   Nature Electronics                   doi         s                y  hdl               ISSN                 S CID                

   Artificial neurons emulate biological counterparts to enable synergetic operation   Nature Electronics                      November       doi         s                   ISSN                 S CID                

  Martin Anthony  January        Discrete Mathematics of Neural Networks  Selected Topics  SIAM  pp           ISBN                        

  Charu C  Aggarwal     July        Data Classification  Algorithms and Applications  CRC Press  pp             ISBN                        

  Paul Werbos  Beyond Regression  New Tools for Prediction and Analysis in the Behavioral Sciences  PhD thesis  Harvard University      

  Werbos  P J           Backpropagation through time  what it does and how to do it   Proceedings of the IEEE                      doi                  ISSN                 S CID               

  Hahnloser  Richard H  R   Sarpeshkar  Rahul  Mahowald  Misha A   Douglas  Rodney J   Seung  H  Sebastian          Digital selection and analogue amplification coexist in a cortex inspired silicon circuit   Nature                       Bibcode     Natur         H  doi                   ISSN                 PMID                S CID              

  R Hahnloser  H S  Seung         Permitted and Forbidden Sets in Symmetric Threshold Linear Networks  NIPS      

  Xavier Glorot  Antoine Bordes  Yoshua Bengio         Deep sparse rectifier neural networks  PDF   AISTATS 

  Yann LeCun  Leon Bottou  Genevieve B  Orr  Klaus Robert M ller          Efficient BackProp   PDF   In G  Orr  K  M ller  eds    Neural Networks  Tricks of the Trade  Springer 

  Andrew L  Maas  Awni Y  Hannun  Andrew Y  Ng         Rectifier Nonlinearities Improve Neural Network Acoustic Models 


Further reading edit 

McCulloch  Warren S   Pitts  Walter          A logical calculus of the ideas immanent in nervous activity   Bulletin of Mathematical Biophysics                  doi         bf         
Samardak  A   Nogaret  A   Janson  N  B   Balanov  A  G   Farrer  I   Ritchie  D  A                 Noise Controlled Signal Transmission in a Multithread Semiconductor Neuron   Physical Review Letters                    Bibcode     PhRvL    v    S  doi         physrevlett             PMID                S CID               

External links edit 
Artifical          sic      neuron mimicks function of human cells
McCulloch Pitts Neurons  Overview 





Retrieved from  https   en wikipedia org w index php title Artificial neuron amp oldid