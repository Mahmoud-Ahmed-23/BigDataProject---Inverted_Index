Machine learning technique
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
Attention mechanism  overview
Attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence  In natural language processing  importance is represented by  soft  weights assigned to each word in a sentence  More generally  attention encodes vectors called token embeddings across a fixed width sequence that can range from tens to millions of tokens in size 
Unlike  hard  weights  which are computed during the backwards training pass   soft  weights exist only in the forward pass and therefore change with every step of the input  Earlier designs implemented the attention mechanism in a serial recurrent neural network  RNN  language translation system  but a more recent design  namely the transformer  removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme 
Inspired by ideas about attention in humans  the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks  Recurrent neural networks favor more recent information contained in words at the end of a sentence  while information earlier in the sentence tends to be attenuated  Attention allows a token equal access to any part of a sentence directly  rather than only through the previous state 


History edit 
See also  Timeline of machine learning
Academic reviews of the history of the attention mechanism are provided in Niu et al             and Soydaner            



    s      s

Psychology biology of attention   cocktail party effect              focusing on content by filtering out background noise   filter model of attention             partial report paradigm  and saccade control            
       Group Method of Data Handling                        Kolmogorov Gabor polynomials implement multiplicative units or  gates             



    s

sigma pi units             higher order neural networks            
Neocognitron and its variants                         



    s

fast weight controller                                                   Neuron weights  generate fast  dynamic links  similar to keys  amp  values             


    


RNN   Attention               Attention network was grafted onto RNN encoder decoder to improve language translation of long sentences   See Overview section 



    

Attention applied to images                                     


    

Transformers                Attention   position encoding   MLP   skip connections   This design improved accuracy and removed the sequential disadvantages of the RNN 
See also  Transformer  deep learning architecture         History

Overview edit 
The modern era of machine attention was revitalized by grafting an attention mechanism  Fig     orange  to an Encoder Decoder 





Animated sequence of  language translation




Fig     Encoder decoder with attention              Numerical subscripts                  k    k  indicate vector sizes while lettered subscripts i and i     indicate time steps   Pinkish regions in H matrix and w vector are zero values   See Legend for details  



Legend


Label

Description


   

Max  sentence length


   

Embedding size  word dimension 


   

Length of hidden vector


 k    k

Dictionary size of input  amp  output languages respectively 


x  Y

 k and   k   hot dictionary vectors   x   x implemented as a lookup table rather than vector multiplication   Y is the   hot maximizer of the linear Decoder layer D  that is  it takes the argmax of D s linear layer output 


x

    long word embedding vector   The vectors are usually pre calculated from other projects such as GloVe or Word Vec 


h

    long encoder hidden vector   At each point in time  this vector summarizes all the preceding words before it   The final h can be viewed as a  sentence  vector  or a thought vector as Hinton calls it 


s

    long decoder hidden state vector 


E

    neuron recurrent neural network encoder       outputs   Input count is         from source embedding       from recurrent connections   The encoder feeds directly into the decoder only to initialize it  but not thereafter  hence  that direct connection is shown very faintly 


D

  layer decoder   The recurrent layer has     neurons and the fully connected linear layer has   k neurons  the size of the target vocabulary                 The linear layer alone has   million          k  weights       times more weights than the recurrent layer 


score

    long alignment score


w

    long vector attention weight   These are  soft  weights which changes during the forward pass  in contrast to  hard  neuronal weights that change during the learning phase 


A

Attention module   this can be a dot product of recurrent states  or the query key value fully connected layers   The output is a     long vector w 


H

             hidden vectors h concatenated into a matrix


c

    long context vector   H   w   c is a linear combination of h vectors weighted by w 


Figure   shows the internal step by step operation of the attention block  A  in Fig   


Figure     The diagram shows the attention forward pass calculating correlations of the word  that  with other words in  See that girl run    Given the right weights from training  the network should be able to identify  girl  as a highly correlated word   Some things to note 
This example focuses on the attention of a single word  that    In practice  the attention of each word is calculated in parallel to speed up calculations   Simply changing the lowercase  x  vector to the uppercase  X  matrix will yield the formula for this 
Softmax scaling qWkT              prevents a high variance in qWkT that would allow a single word to excessively dominate the softmax resulting in attention to only one word  as a discrete hard max would do 
Notation  the commonly written row wise softmax formula above assumes that vectors are rows  which runs contrary to the standard math notation of column vectors  More correctly  we should take the transpose of the context vector and use the column wise softmax  resulting in the more correct form

  
    
      
        
          
            
              
                 
                X
                
                  W
                  
                    v
                  
                
                
                   
                  
                    T
                  
                
                  x     
                
                   
                   
                  
                    W
                    
                      k
                    
                  
                  
                    X
                    
                      T
                    
                  
                   
                    x     
                  
                     
                    
                      
                        x
                          x f 
                      
                    
                    
                      W
                      
                        q
                      
                    
                    
                       
                      
                        T
                      
                    
                  
                  
                     
                    
                      s
                      m
                    
                  
                
              
            
          
        
      
    
      displaystyle   begin aligned  XW  v    T     W  k X  T       underline  x  W  q    T     sm   end aligned   
  
 
This attention scheme has been compared to the Query Key analogy of relational databases   That comparison suggests an asymmetric role for the Query and Key vectors  where one item of interest  the Query vector  that   is matched against all possible items  the Key vectors of each word in the sentence     However  both Self and Cross Attentions  parallel calculations matches all tokens of the K matrix with all tokens of the Q matrix  therefore the roles of these vectors are symmetric   Possibly because the simplistic database analogy is flawed  much effort has gone into understanding attention mechanisms further by studying their roles in focused settings  such as in context learning              masked language tasks              stripped down transformers              bigram statistics              N gram statistics              pairwise convolutions              and arithmetic factoring             

Interpreting attention weights edit 
In translating between languages  alignment is the process of matching words from the source sentence to words of the translated sentence  Networks that perform verbatim translation without regard to word order would show the highest scores along the  dominant  diagonal of the matrix  The off diagonal dominance shows that the attention mechanism is more nuanced  
Consider an example of translating I love you to French  On the first pass through the decoder      of the attention weight is on the first English word I  so the network offers the word je  On the second pass of the decoder      of the attention weight is on the third English word you  so it offers t   On the last pass      of the attention weight is on the second English word love  so it offers aime 
In the I love you example  the second word love is aligned with the third word aime  Stacking soft row vectors together for je  t   and aime yields an alignment matrix 





I

love

you


je

    

    

    


t 

    

    

    


aime

    

    

    

Sometimes  alignment can be multiple to multiple  For example  the English phrase look it up corresponds to cherchez le  Thus   soft  attention weights work better than  hard  attention weights  setting one attention weight to    and the others to     as we would like the model to make a context vector consisting of a weighted sum of the hidden vectors  rather than  the best one   as there may not be a best hidden vector 

Variants edit 
Comparison of the data flow in CNN  RNN  and self attention
Many variants of attention implement soft weights  such as

fast weight programmers  or fast weight controllers                     A  slow  neural network outputs the  fast  weights of another neural network through outer products  The slow network learns by gradient descent  It was later renamed as  linearized self attention              
Bahdanau style attention              also referred to as additive attention 
Luong style attention              which is known as multiplicative attention 
highly parallelizable self attention introduced in      as decomposable attention             and successfully used in transformers a year later 
positional attention and factorized positional attention             
For convolutional neural networks  attention mechanisms can be distinguished by the dimension on which they operate  namely  spatial attention              channel attention              or combinations                         
These variants recombine the encoder side inputs to redistribute those effects to each target output  Often  a correlation style matrix of dot products provides the re weighting coefficients   In the figures below  W is the matrix of context attention weights  similar to the formula in Core Calculations section above 




   encoder decoder dot product

   encoder decoder QKV

   encoder only dot product

   encoder only QKV

   Pytorch tutorial


Both encoder  amp  decoder are needed to calculate attention             

Both encoder  amp  decoder are needed to calculate attention             

Decoder is not used to calculate attention  With only   input into corr  W is an auto correlation of dot products  wij   xi xj             

Decoder is not used to calculate attention             

A fully connected layer is used to calculate attention instead of dot product correlation             


Legend


Label
Description


Variables X  H  S  T
Upper case variables represent the entire sentence  and not just the current word  For example  H is a matrix of the encoder hidden state one word per column 


S  T
S  decoder hidden state  T  target word embedding  In the Pytorch Tutorial variant training phase  T alternates between   sources depending on the level of teacher forcing used  T could be the embedding of the network s output word  i e  embedding argmax FC output    Alternatively with teacher forcing  T could be the embedding of the known correct word which can occur with a constant forcing probability  say     


X  H
H  encoder hidden state  X  input word embeddings 


W
Attention coefficients


Qw  Kw  Vw  FC
Weight matrices for query  key  value respectively  FC is a fully connected weight matrix 


    
   vector concatenation     matrix multiplication 


corr
Column wise softmax matrix of all combinations of dot products   The dot products are xi   xj in variant     hi  sj in variant    and column      i   Kw   H     column      j   Qw   S   in variant    and column      i   Kw   X     column      j   Qw   X   in variant    Variant   uses a fully connected layer to determine the coefficients  If the variant is QKV  then the dot products are normalized by the        d where d is the height of the QKV matrices 

Optimizations edit 
Flash attention edit 
The size of the attention matrix is proportional to the square of the number of input tokens  Therefore  when the input is long  calculating the attention matrix requires a lot of GPU memory  Flash attention is an implementation that reduces the memory needs and increases efficiency without sacrificing accuracy  It achieves this by partitioning the attention computation into smaller blocks that fit into the GPU s faster on chip memory  reducing the need to store large intermediate matrices and thus lowering memory usage while increasing computational efficiency             

Flex Attention edit 
Flex Attention              is an attention kernel developed by Meta that allows users to modify attention scores prior to softmax and dynamically chooses the optimal attention algorithm 

Self Attention and Transformers edit 
The major breakthrough came with self attention  where each element in the input sequence attends to all others  enabling the model to capture global dependencies  This idea was central to the Transformer architecture  which replaced recurrence entirely with attention mechanisms  As a result  Transformers became the foundation for models like BERT  GPT  and T   Vaswani et al         

Applications edit 
Attention is widely used in natural language processing  computer vision  and speech recognition  In NLP  it improves context understanding in tasks like question answering and summarization  In vision  visual attention helps models focus on relevant image regions  enhancing object detection and image captioning 

Mathematical representation edit 
Standard Scaled Dot Product Attention edit 
For matrices  
  
    
      
        
          Q
        
          x     
        
          
            R
          
          
            m
              xd  
            
              d
              
                k
              
            
          
        
         
        
          K
        
          x     
        
          
            R
          
          
            n
              xd  
            
              d
              
                k
              
            
          
        
      
    
      displaystyle  mathbf  Q   in  mathbb  R    m times d  k    mathbf  K   in  mathbb  R    n times d  k   
  
 and 
  
    
      
        
          V
        
          x     
        
          
            R
          
          
            n
              xd  
            
              d
              
                v
              
            
          
        
      
    
      displaystyle  mathbf  V   in  mathbb  R    n times d  v   
  
  the scaled dot product  or QKV attention is defined as 

  
    
      
        
          Attention
        
         
        
          Q
        
         
        
          K
        
         
        
          V
        
         
         
        
          softmax
        
        
           
          
            
              
                
                  Q
                
                
                  
                    K
                  
                  
                    T
                  
                
              
              
                
                  d
                  
                    k
                  
                
              
            
          
           
        
        
          V
        
          x     
        
          
            R
          
          
            m
              xd  
            
              d
              
                v
              
            
          
        
      
    
      displaystyle   text Attention    mathbf  Q    mathbf  K    mathbf  V      text softmax   left   frac   mathbf  Q   mathbf  K    T    sqrt  d  k     right  mathbf  V   in  mathbb  R    m times d  v   
  

where 
  
    
      
        
          

          
          
            T
          
        
      
    
      displaystyle     T  
  
 denotes transpose and the softmax function is applied independently to every row of its argument  The matrix 
  
    
      
        
          Q
        
      
    
      displaystyle  mathbf  Q   
  
 contains 
  
    
      
        m
      
    
      displaystyle m 
  
 queries  while matrices 
  
    
      
        
          K
        
         
        
          V
        
      
    
      displaystyle  mathbf  K    mathbf  V   
  
 jointly contain an unordered set of 
  
    
      
        n
      
    
      displaystyle n 
  
 key value pairs  Value vectors in matrix 
  
    
      
        
          V
        
      
    
      displaystyle  mathbf  V   
  
 are weighted using the weights resulting from the softmax operation  so that the rows of the 
  
    
      
        m
      
    
      displaystyle m 
  
 by 
  
    
      
        
          d
          
            v
          
        
      
    
      displaystyle d  v  
  
 output matrix are confined to the convex hull of the points in 
  
    
      
        
          
            R
          
          
            
              d
              
                v
              
            
          
        
      
    
      displaystyle  mathbb  R    d  v   
  
 given by the rows of 
  
    
      
        
          V
        
      
    
      displaystyle  mathbf  V   
  
 
To understand the permutation invariance and permutation equivariance properties of QKV attention              let 
  
    
      
        
          A
        
          x     
        
          
            R
          
          
            m
              xd  
            m
          
        
      
    
      displaystyle  mathbf  A   in  mathbb  R    m times m  
  
 and 
  
    
      
        
          B
        
          x     
        
          
            R
          
          
            n
              xd  
            n
          
        
      
    
      displaystyle  mathbf  B   in  mathbb  R    n times n  
  
 be permutation matrices  and 
  
    
      
        
          D
        
          x     
        
          
            R
          
          
            m
              xd  
            n
          
        
      
    
      displaystyle  mathbf  D   in  mathbb  R    m times n  
  
 an arbitrary matrix  The softmax function is permutation equivariant in the sense that 


  
    
      
        
          softmax
        
         
        
          A
        
        
          D
        
        
          B
        
         
         
        
          A
        
        
        
          softmax
        
         
        
          D
        
         
        
          B
        
      
    
      displaystyle   text softmax    mathbf  A   mathbf  D   mathbf  B     mathbf  A      text softmax    mathbf  D    mathbf  B   
  

By noting that the transpose of a permutation matrix is also its inverse  it follows that 


  
    
      
        
          Attention
        
         
        
          A
        
        
          Q
        
         
        
          B
        
        
          K
        
         
        
          B
        
        
          V
        
         
         
        
          A
        
        
        
          Attention
        
         
        
          Q
        
         
        
          K
        
         
        
          V
        
         
      
    
      displaystyle   text Attention    mathbf  A   mathbf  Q    mathbf  B   mathbf  K    mathbf  B   mathbf  V     mathbf  A      text Attention    mathbf  Q    mathbf  K    mathbf  V    
  

which shows that QKV attention is equivariant with respect to re ordering the queries  rows of 
  
    
      
        
          Q
        
      
    
      displaystyle  mathbf  Q   
  
   and invariant to re ordering of the key value pairs in 
  
    
      
        
          K
        
         
        
          V
        
      
    
      displaystyle  mathbf  K    mathbf  V   
  
  These properties are inherited when applying linear transforms to the inputs and outputs of QKV attention blocks  For example  a simple self attention function defined as 


  
    
      
        
          X
        
          x  a  
        
          Attention
        
         
        
          X
        
        
          
            T
          
          
            q
          
        
         
        
          X
        
        
          
            T
          
          
            k
          
        
         
        
          X
        
        
          
            T
          
          
            v
          
        
         
      
    
      displaystyle  mathbf  X   mapsto   text Attention    mathbf  X   mathbf  T    q   mathbf  X   mathbf  T    k   mathbf  X   mathbf  T    v   
  

is permutation equivariant with respect to re ordering the rows of the input matrix 
  
    
      
        X
      
    
      displaystyle X 
  
 in a non trivial way  because every row of the output is a function of all the rows of the input  Similar properties hold for multi head attention  which is defined below 

Masked Attention edit 
When QKV attention is used as a building block for an autoregressive decoder  and when at training time all input and output matrices have 
  
    
      
        n
      
    
      displaystyle n 
  
 rows  a masked attention variant is used 

  
    
      
        
          Attention
        
         
        
          Q
        
         
        
          K
        
         
        
          V
        
         
         
        
          softmax
        
        
           
          
            
              
                
                  
                    Q
                  
                  
                    
                      K
                    
                    
                      T
                    
                  
                
                
                  
                    d
                    
                      k
                    
                  
                
              
            
             
            
              M
            
          
           
        
        
          V
        
      
    
      displaystyle   text Attention    mathbf  Q    mathbf  K    mathbf  V      text softmax   left   frac   mathbf  Q   mathbf  K    T    sqrt  d  k      mathbf  M   right  mathbf  V   
  

where the mask  
  
    
      
        
          M
        
          x     
        
          
            R
          
          
            n
              xd  
            n
          
        
      
    
      displaystyle  mathbf  M   in  mathbb  R    n times n  
  
 is a strictly upper triangular matrix  with zeros on and below the diagonal and 
  
    
      
          x     
          x   e 
      
    
      displaystyle   infty  
  
 in every element above the diagonal  The softmax output  also in 
  
    
      
        
          
            R
          
          
            n
              xd  
            n
          
        
      
    
      displaystyle  mathbb  R    n times n  
  
 is then lower triangular  with zeros in all elements above the diagonal  The masking ensures that for all 
  
    
      
         
          x     
        i
         lt 
        j
          x     
        n
      
    
      displaystyle   leq i lt j leq n 
  
  row 
  
    
      
        i
      
    
      displaystyle i 
  
 of the attention output is independent of row 
  
    
      
        j
      
    
      displaystyle j 
  
 of any of the three input matrices  The permutation invariance and equivariance properties of standard QKV attention do not hold for the masked variant 

Multi Head Attention edit 
Decoder multiheaded cross attention
Multi head attention

  
    
      
        
          MultiHead
        
         
        
          Q
        
         
        
          K
        
         
        
          V
        
         
         
        
          Concat
        
         
        
          
            head
          
          
             
          
        
         
         
         
         
         
        
          
            head
          
          
            h
          
        
         
        
          
            W
          
          
            O
          
        
      
    
      displaystyle   text MultiHead    mathbf  Q    mathbf  K    mathbf  V      text Concat     text head             text head    h   mathbf  W    O  
  

where each head is computed with QKV attention as 

  
    
      
        
          
            head
          
          
            i
          
        
         
        
          Attention
        
         
        
          Q
        
        
          
            W
          
          
            i
          
          
            Q
          
        
         
        
          K
        
        
          
            W
          
          
            i
          
          
            K
          
        
         
        
          V
        
        
          
            W
          
          
            i
          
          
            V
          
        
         
      
    
      displaystyle   text head    i    text Attention    mathbf  Q   mathbf  W    i   Q   mathbf  K   mathbf  W    i   K   mathbf  V   mathbf  W    i   V   
  

and 
  
    
      
        
          
            W
          
          
            i
          
          
            Q
          
        
         
        
          
            W
          
          
            i
          
          
            K
          
        
         
        
          
            W
          
          
            i
          
          
            V
          
        
      
    
      displaystyle  mathbf  W    i   Q   mathbf  W    i   K   mathbf  W    i   V  
  
  and 
  
    
      
        
          
            W
          
          
            O
          
        
      
    
      displaystyle  mathbf  W    O  
  
 are parameter matrices 
The permutation properties of  standard  unmasked  QKV attention apply here also  For permutation matrices  
  
    
      
        
          A
        
         
        
          B
        
      
    
      displaystyle  mathbf  A    mathbf  B   
  
 


  
    
      
        
          MultiHead
        
         
        
          A
        
        
          Q
        
         
        
          B
        
        
          K
        
         
        
          B
        
        
          V
        
         
         
        
          A
        
        
        
          MultiHead
        
         
        
          Q
        
         
        
          K
        
         
        
          V
        
         
      
    
      displaystyle   text MultiHead    mathbf  A   mathbf  Q    mathbf  B   mathbf  K    mathbf  B   mathbf  V     mathbf  A      text MultiHead    mathbf  Q    mathbf  K    mathbf  V    
  

from which we also see that multi head self attention 


  
    
      
        
          X
        
          x  a  
        
          MultiHead
        
         
        
          X
        
        
          
            T
          
          
            q
          
        
         
        
          X
        
        
          
            T
          
          
            k
          
        
         
        
          X
        
        
          
            T
          
          
            v
          
        
         
      
    
      displaystyle  mathbf  X   mapsto   text MultiHead    mathbf  X   mathbf  T    q   mathbf  X   mathbf  T    k   mathbf  X   mathbf  T    v   
  

is equivariant with respect to re ordering of the rows of input matrix 
  
    
      
        X
      
    
      displaystyle X 
  
 

Bahdanau  Additive  Attention edit 

  
    
      
        
          Attention
        
         
        
          Q
        
         
        
          K
        
         
        
          V
        
         
         
        
          softmax
        
         
        tanh
          x     
         
        
          
            W
          
          
            Q
          
        
        
          Q
        
         
        
          
            W
          
          
            K
          
        
        
          K
        
         
        
          V
        
         
      
    
      displaystyle   text Attention    mathbf  Q    mathbf  K    mathbf  V      text softmax    tanh  mathbf  W    Q  mathbf  Q    mathbf  W    K  mathbf  K    mathbf  V    
  

where 
  
    
      
        
          
            W
          
          
            Q
          
        
      
    
      displaystyle  mathbf  W    Q  
  
 and 
  
    
      
        
          
            W
          
          
            K
          
        
      
    
      displaystyle  mathbf  W    K  
  
 are learnable weight matrices             

Luong Attention  General  edit 

  
    
      
        
          Attention
        
         
        
          Q
        
         
        
          K
        
         
        
          V
        
         
         
        
          softmax
        
         
        
          Q
        
        
          W
        
        
          
            K
          
          
            T
          
        
         
        
          V
        
      
    
      displaystyle   text Attention    mathbf  Q    mathbf  K    mathbf  V      text softmax    mathbf  Q   mathbf  W   mathbf  K    T   mathbf  V   
  

where 
  
    
      
        
          W
        
      
    
      displaystyle  mathbf  W   
  
 is a learnable weight matrix             

Self Attention edit 
Self attention is essentially the same as cross attention  except that query  key  and value vectors all come from the same model  Both encoder and decoder can use self attention  but with subtle differences 
For encoder self attention  we can start with a simple encoder without self attention  such as an  embedding layer   which simply converts each input word into a vector by a fixed lookup table  This gives a sequence of hidden vectors 
  
    
      
        
          h
          
             
          
        
         
        
          h
          
             
          
        
         
          x     
      
    
      displaystyle h     h      dots  
  
  These can then be applied to a dot product attention mechanism  to obtain
  
    
      
        
          
            
              
                
                  h
                  
                     
                  
                    x     
                
              
              
                
                 
                
                  A
                  t
                  t
                  e
                  n
                  t
                  i
                  o
                  n
                
                 
                
                  h
                  
                     
                  
                
                
                  W
                  
                    Q
                  
                
                 
                H
                
                  W
                  
                    K
                  
                
                 
                H
                
                  W
                  
                    V
                  
                
                 
              
            
            
              
                
                  h
                  
                     
                  
                    x     
                
              
              
                
                 
                
                  A
                  t
                  t
                  e
                  n
                  t
                  i
                  o
                  n
                
                 
                
                  h
                  
                     
                  
                
                
                  W
                  
                    Q
                  
                
                 
                H
                
                  W
                  
                    K
                  
                
                 
                H
                
                  W
                  
                    V
                  
                
                 
              
            
            
              
              
                
                  x  ef 
              
            
          
        
      
    
      displaystyle   begin aligned h      amp   mathrm  Attention   h    W  Q  HW  K  HW  V    h      amp   mathrm  Attention   h    W  Q  HW  K  HW  V     amp  cdots  end aligned   
  
or more succinctly  
  
    
      
        
          H
            x     
        
         
        
          A
          t
          t
          e
          n
          t
          i
          o
          n
        
         
        H
        
          W
          
            Q
          
        
         
        H
        
          W
          
            K
          
        
         
        H
        
          W
          
            V
          
        
         
      
    
      displaystyle H   mathrm  Attention   HW  Q  HW  K  HW  V   
  
  This can be applied repeatedly  to obtain a multilayered encoder  This is the  encoder self attention   sometimes called the  all to all attention   as the vector at every position can attend to every other 

Masking edit 
Decoder self attention with causal masking  detailed diagramFor decoder self attention  all to all attention is inappropriate  because during the autoregressive decoding process  the decoder cannot attend to future outputs that has yet to be decoded  This can be solved by forcing the attention weights 
  
    
      
        
          w
          
            i
            j
          
        
         
         
      
    
      displaystyle w  ij    
  
 for all 
  
    
      
        i
         lt 
        j
      
    
      displaystyle i lt j 
  
  called  causal masking   This attention mechanism is the  causally masked self attention  
See also edit 
Recurrent neural network
seq seq
Transformer  deep learning architecture 
Attention
Dynamic neural network
References edit 


  Niu  Zhaoyang  Zhong  Guoqiang  Yu  Hui                A review on the attention mechanism of deep learning   Neurocomputing              doi         j neucom              ISSN                

  Soydaner  Derya  August         Attention mechanism in neural networks  where it comes and where it goes   Neural Computing and Applications                        arXiv             doi         s                   ISSN                

  Cherry EC          Some Experiments on the Recognition of Speech  with One and with Two Ears   PDF   The Journal of the Acoustical Society of America                  Bibcode     ASAJ          C  doi                    hdl             M         A F       ISSN                

  Broadbent  D         Perception and Communication  London  Pergamon Press 

  Kowler  Eileen  Anderson  Eric  Dosher  Barbara  Blaser  Erik                The role of attention in the programming of saccades   Vision Research                      doi                            U  ISSN                 PMID              

  Ivakhnenko  A  G          Cybernetic Predicting Devices  CCM Information Corporation 

  Ivakhnenko  A  G   Grigor evich Lapa  Valentin         Cybernetics and forecasting techniques  American Elsevier Pub  Co 

  Schmidhuber  J rgen          Annotated History of Modern AI and Deep Learning   arXiv             cs NE  

  Rumelhart  David E   Hinton  G  E   Mcclelland  James L                 A General Framework for Parallel Distributed Processing   PDF   In Rumelhart  David E   Hinton  G  E   PDP Research Group  eds    Parallel Distributed Processing  Volume    Explorations in the Microstructure of Cognition  Foundations  Cambridge  Massachusetts  MIT Press  ISBN                        

  Giles  C  Lee  Maxwell  Tom                Learning  invariance  and generalization in high order neural networks   Applied Optics                      doi         AO            ISSN                 PMID               

  Fukushima  Kunihiko                Neural network model for selective attention in visual pattern recognition and associative recall   Applied Optics                      Bibcode     ApOpt         F  doi         AO            ISSN                 PMID               

  Ba  Jimmy  Mnih  Volodymyr  Kavukcuoglu  Koray                Multiple Object Recognition with Visual Attention   arXiv            cs LG  

  a b Schmidhuber  J rgen          Learning to control fast weight memories  an alternative to recurrent nets   Neural Computation                  doi         neco               S CID               

  Christoph von der Malsburg  The correlation theory of brain function  Internal Report       MPI Biophysical Chemistry        http   cogprints org        vdM correlation pdf See Reprint in Models of Neural Networks II  chapter    pages         Springer  Berlin       

  Jerome A  Feldman   Dynamic connections in neural networks   Biological Cybernetics  vol      no     pp         Dec       

  Hinton  Geoffrey E   Plaut  David C           Using Fast Weights to Deblur Old Memories   Proceedings of the Annual Meeting of the Cognitive Science Society    

  a b 
Schlag  Imanol  Irie  Kazuki  Schmidhuber  J rgen          Linear Transformers Are Secretly Fast Weight Programmers   ICML       Springer  pp                 

  a b c 
Bahdanau  Dzmitry  Cho  Kyunghyun  Bengio  Yoshua          Neural Machine Translation by Jointly Learning to Align and Translate   arXiv            cs CL  

  Vinyals  Oriol  Toshev  Alexander  Bengio  Samy  Erhan  Dumitru          Show and Tell  A Neural Image Caption Generator   pp                 

  Xu  Kelvin  Ba  Jimmy  Kiros  Ryan  Cho  Kyunghyun  Courville  Aaron  Salakhudinov  Ruslan  Zemel  Rich  Bengio  Yoshua                Show  Attend and Tell  Neural Image Caption Generation with Visual Attention   Proceedings of the   nd International Conference on Machine Learning  PMLR            

  Bahdanau  Dzmitry  Cho  Kyunghyun  Bengio  Yoshua     May         Neural Machine Translation by Jointly Learning to Align and Translate   arXiv            cs CL    orig date   Sep      

  Vaswani  Ashish  Shazeer  Noam  Parmar  Niki  Uszkoreit  Jakob  Jones  Llion  Gomez  Aidan N  Kaiser   ukasz  Polosukhin  Illia          Attention is All you Need   PDF   Advances in Neural Information Processing Systems      Curran Associates  Inc 

  
Britz  Denny  Goldie  Anna  Luong  Minh Thanh  Le  Quoc                Massive Exploration of Neural Machine Translation Architectures   arXiv             cs CV  

   Pytorch org seq seq tutorial   Retrieved December         

  Zhang  Ruiqi          Trained Transformers Learn Linear Models In Context   PDF   Journal of Machine Learning Research           arXiv            

  Rende  Riccardo          Mapping of attention mechanisms to a generalized Potts model   Physical Review Research                 arXiv             Bibcode     PhRvR    b    R  doi         PhysRevResearch          

  He  Bobby          Simplifying Transformers Blocks   arXiv             cs LG  

  
Nguyen  Timothy          Understanding Transformers via N gram Statistics   arXiv             cs CL  

   Transformer Circuits   transformer circuits pub 

  Transformer Neural Network Derived From Scratch         Event occurs at        Retrieved            

  Charton  Fran ois          Learning the Greatest Common Divisor  Explaining Transformer Predictions   arXiv             cs LG  

  a b c 
Luong  Minh Thang                Effective Approaches to Attention Based Neural Machine Translation   arXiv           v   cs CL  

  Cheng  Jianpeng  Dong  Li  Lapata  Mirella                Long Short Term Memory Networks for Machine Reading   arXiv             cs CL  

  
 Learning Positional Attention for Sequential Recommendation   catalyzex com 

  
Zhu  Xizhou  Cheng  Dazhi  Zhang  Zheng  Lin  Stephen  Dai  Jifeng          An Empirical Study of Spatial Attention Mechanisms in Deep Networks        IEEE CVF International Conference on Computer Vision  ICCV   pp                  arXiv             doi         ICCV             ISBN                         S CID                

  
Hu  Jie  Shen  Li  Sun  Gang          Squeeze and Excitation Networks        IEEE CVF Conference on Computer Vision and Pattern Recognition  pp                  arXiv             doi         CVPR             ISBN                         S CID                

  
Woo  Sanghyun  Park  Jongchan  Lee  Joon Young  Kweon  In So                CBAM  Convolutional Block Attention Module   arXiv             cs CV  

  
Georgescu  Mariana Iuliana  Ionescu  Radu Tudor  Miron  Andreea Iuliana  Savencu  Olivian  Ristea  Nicolae Catalin  Verga  Nicolae  Khan  Fahad Shahbaz                Multimodal Multi Head Convolutional Attention with Various Kernel Sizes for Medical Image Super Resolution   arXiv             eess IV  

  
Neil Rhodes         CS     NN     Attention  Keys  Queries   amp  Values   Event occurs at        Retrieved            

  
Alfredo Canziani  amp  Yann Lecun         NYU Deep Learning course  Spring        Event occurs at        Retrieved            

  
Alfredo Canziani  amp  Yann Lecun         NYU Deep Learning course  Spring        Event occurs at        Retrieved            

  
Robertson  Sean   NLP From Scratch  Translation With a Sequence To Sequence Network and Attention   pytorch org  Retrieved            

  Mittal  Aayush                Flash Attention  Revolutionizing Transformer Efficiency   Unite AI  Retrieved            

  https   pytorch org blog flexattention 

  Lee  Juho  Lee  Yoonho  Kim  Jungtaek  Kosiorek  Adam R  Choi  Seungjin  Teh  Yee Whye          Set Transformer  A Framework for Attention based Permutation Invariant Neural Networks   arXiv             cs LG  


External links edit 
Olah  Chris  Carter  Shan  September            Attention and Augmented Recurrent Neural Networks   Distill         Distill Working Group  doi          distill       
Dan Jurafsky and James H  Martin        Speech and Language Processing   rd ed  draft  January        ch       Attention and ch      Self Attention Networks  Transformers
Alex Graves    May        Attention and Memory in Deep Learning  video lecture   DeepMind   UCL  via YouTube
vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects






Retrieved from  https   en wikipedia org w index php title Attention  machine learning  amp oldid