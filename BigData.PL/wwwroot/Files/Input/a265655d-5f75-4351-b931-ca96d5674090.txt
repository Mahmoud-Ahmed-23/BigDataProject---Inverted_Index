Chinese artificial intelligence company
This article is about the company  For the chatbot  see DeepSeek  chatbot  



Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co   Ltd Native name                    Company typePrivateIndustryInformation technologyArtificial intelligenceFounded        July                  months ago                             FounderLiang WenfengHeadquartersHangzhou  Zhejiang  ChinaKey peopleLiang Wenfeng  CEO OwnerHigh FlyerNumber of employees                     Websitedeepseek com
Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co   Ltd                                        a      doing business as DeepSeek      b      is a Chinese artificial intelligence company that develops large language models  LLMs   Based in Hangzhou  Zhejiang  it is owned and funded by the Chinese hedge fund High Flyer  DeepSeek was founded in July      by Liang Wenfeng  the co founder of High Flyer  who also serves as the CEO for both companies                                   The company launched an eponymous chatbot alongside its DeepSeek R  model in January      
Released under the MIT License  DeepSeek R  provides responses comparable to other contemporary large language models  such as OpenAI s GPT   and o               Its training cost is reported to be significantly lower than other LLMs  The company claims that it trained its V  model for US        million far less than the US          million cost for OpenAI s GPT   in                  and using approximately one tenth the computing power consumed by Meta s comparable model  Llama                                                      DeepSeek s success against larger and more established rivals has been described as  upending AI                          
DeepSeek s models are described as  open weight   meaning the exact parameters are openly shared  although certain usage conditions differ from typical open source software                          The company reportedly recruits AI researchers from top Chinese universities             and also hires from outside traditional computer science fields to broaden its models  knowledge and capabilities             
DeepSeek significantly reduced training expenses for their R  model by incorporating techniques such as mixture of experts  MoE  layers              The company also trained its models during ongoing trade restrictions on AI chip exports to China  using weaker AI chips intended for export and employing fewer units overall                          Observers say this breakthrough sent  shock waves  through the industry  threatening established AI hardware leaders such as Nvidia  Nvidia s share price dropped sharply  losing US          billion in market value  the largest single company decline in U S  stock market history                         


History edit 
Founding and early years             edit 
In February       High Flyer was co founded by AI enthusiast Liang Wenfeng  who had been trading since the           financial crisis while attending Zhejiang University              The company began stock trading using a GPU dependent deep learning model on    October       before then  it had used CPU based linear models  By the end of       most of its trading was driven by AI             
Liang established High Flyer as a hedge fund focused on developing and using AI trading algorithms  and by      the firm was using AI exclusively              often using Nvidia chips             
In       the company began constructing its first computing cluster  Fire Flyer  at a cost of          million yuan  it contained       GPUs interconnected at          Gbit s and was retired after          years in operation             
By       Liang had started buying large quantities of Nvidia GPUs for an AI project              reportedly obtaining        Nvidia A    GPUs             before the United States restricted chip sales to China              Computing cluster Fire Flyer   began construction in      with a budget of   billion yuan             
It was reported that in       Fire Flyer   s capacity had been used at over      totaling       million GPU hours      was used to support scientific computing outside the company             
During       Fire Flyer   had      PCIe A    GPUs in     nodes  each containing   GPUs  At the time  it exclusively used PCIe instead of the DGX version of A     since at the time the models it trained could fit within a single    GB GPU VRAM and so there was no need for the higher bandwidth of DGX  i e   it required only data parallelism but not model parallelism               Later  it incorporated NVLinks and NCCL  Nvidia Collective Communications Library  to train larger models that required model parallelism                         
On    April                   High Flyer announced the launch of an artificial general intelligence  AGI  research lab  stating that the new lab would focus on developing AI tools unrelated to the firm s financial business                          Two months later  on    July                  that lab was spun off into an independent company  DeepSeek  with High Flyer as its principal investor and backer                                      Venture capital investors were reluctant to provide funding  as they considered it unlikely that the venture would be able to quickly generate an  exit              

Model releases       present  edit 
DeepSeek released its first model  DeepSeek Coder  on        November       followed by the DeepSeek LLM series on         November                              section          In January       it released two DeepSeek MoE models  Base and Chat               and in April three DeepSeek Math models  Base  Instruct  and RL              
DeepSeek V  was released in May       followed a month later by the DeepSeek Coder V  series              In September       DeepSeek V    was introduced and revised in December              On         November       the preview of DeepSeek R  Lite became available via API and chat                          In December  DeepSeek V  Base and DeepSeek V   chat  were released             

The DeepSeek login page following a cyberattack around its January          launch
On    January       DeepSeek launched the DeepSeek chatbot based on the DeepSeek R  model free for iOS and Android  By    January  DeepSeek surpassed ChatGPT as the most downloaded freeware app on the iOS App Store in the United States              triggering an     drop in Nvidia s share price                         
On    March       DeepSeek released DeepSeek V       under the MIT License                         
In February       Singaporean authorities arrested several individuals for illegally exporting advanced Nvidia chips to DeepSeek              In April       it was reported that the Trump administration was considering penalties that would attempt to block DeepSeek from buying U S  technology             

Company operation edit 
DeepSeek is headquartered in Hangzhou  Zhejiang  and is owned and funded by High Flyer  Its co founder  Liang Wenfeng  serves as CEO  As of May       Liang personally held an     stake in DeepSeek through two shell corporations      note                   

Strategy edit 
DeepSeek states that it focuses on research and does not have immediate plans for commercialization              This posture also means it can skirt certain provisions of China s AI regulations aimed at consumer facing technologies             
DeepSeek s hiring approach emphasizes skills over lengthy work experience  resulting in many hires fresh out of university                          The company likewise recruits individuals without computer science backgrounds to expand the range of expertise incorporated into the models  for instance in poetry or advanced mathematics                          According to The New York Times  dozens of DeepSeek researchers have or have previously had affiliations with People s Liberation Army laboratories and the Seven Sons of National Defence             

Training framework edit 
High Flyer DeepSeek operates at least two primary computing clusters  Fire Flyer        and Fire Flyer                Fire Flyer   consists of co designed software and hardware architecture  On the hardware side  Nvidia GPUs use     Gbps interconnects  The cluster is divided into two  zones   and the platform supports cross zone tasks  The network topology was two fat trees  chosen for high bisection bandwidth  On the software side are                         

 FS  Fire Flyer File System   A distributed parallel file system  specifically designed for asynchronous random reads  It uses Direct I O and RDMA Read  In contrast to standard Buffered I O  Direct I O does not cache data  Caching is useless for this case  since each data read is random and is not reused                         
hfreduce  Library for asynchronous communication  originally designed to replace Nvidia Collective Communication Library  NCCL               It is mainly used for allreduce  especially of gradients during backpropagation  It is asynchronously run on the CPU to avoid blocking kernels on the GPU              It uses two tree broadcast like NCCL             
hfai nn  Software library of commonly used operators for neural network training  similar to torch nn in PyTorch 
HaiScale Distributed Data Parallel  DDP   Parallel training library that implements various forms of parallelism such as Data Parallelism  DP   Pipeline Parallelism  PP   Tensor Parallelism  TP   Experts Parallelism  EP   Fully Sharded Data Parallel  FSDP  and Zero Redundancy Optimizer  ZeRO   It is similar to PyTorch DDP  which uses NCCL on the backend 
HAI Platform  Various applications such as task scheduling  fault handling  and disaster recovery             
As of       Fire Flyer   had      PCIe A    GPUs in     nodes  each containing   GPUs              It later incorporated NVLinks and NCCL to train larger models that required model parallelism                         

Development and release history edit 

Major versions of DeepSeek models  SFT stands for supervised finetuning 


Major versions

Release date

Status

Major variants

Remarks


DeepSeek Coder

November        

Discontinued

Base  pretrained   Instruct  with instruction finetuned 

The architecture is essentially the same as Llama 


DeepSeek LLM

November         

 

Base 
Chat  with SFT 



DeepSeek MoE

January        

 

Base 
Chat


Developed a variant of mixture of experts  MoE  


DeepSeek Math

April     

 

Base

Initialized with DS Coder Base v   


Instruct  with SFT 




RL  using a process reward model 

Developed Group Relative Policy Optimization  GRPO   a variant of Proximal Policy Optimization  PPO  


DeepSeek V 

May     

 

DeepSeek V   DeepSeek V  Chat
DeepSeek V  Lite  DeepSeek V  Lite Chat
DeepSeek Coder V 
DeepSeek V   


Developed multi head latent attention  MLA   Also used mixture of experts  MoE  
Implemented KV caching  



DeepSeek V 

December     

 

DeepSeek V  Base
DeepSeek V   a chat model 


The architecture is essentially the same as V   Updated on            


DeepSeek R 

November         

Active

DeepSeek R  Lite Preview

Only accessed through API and a chat interface 


January         

Active

DeepSeek R 
DeepSeek R  Zero


Initialized from DeepSeek V  Base and sharing the V  architecture 


Distilled models

Initialized from other models  such as Llama  Qwen  etc  Distilled from data synthesized by R  and R  Zero             

The first DeepSeek models were essentially the same as Llama              which were dense decoder only transformers  Later models incorporated the multi head latent attention  MLA   Mixture of Experts  MoE   and KV caching                              verification needed     
A decoder only transformer consists of multiple identical decoder layers  Each of these layers features two main components  an attention layer and a FeedForward network  FFN  layer              In the attention layer  the traditional multi head attention mechanism has been enhanced with multi head latent attention  This update introduces compressed latent vectors to boost performance and reduce memory usage during inference                  citation needed     
Meanwhile  the FFN layer adopts a variant of the mixture of experts  MoE  approach  effectively doubling the number of experts compared to standard implementations  It distinguishes between two types of experts  shared experts  which are always active to encapsulate general knowledge  and routed experts  only a select few of which are activated to capture specialized information                  citation needed     
Consider the current sequence of n tokens as input  To predict the next token based on the current input  the attention mechanism involves extensive calculations of matrices  including query  Q   key  K   and value  V  matrices  The dimensions of Q  K  and V are determined by the current number of tokens and the model s embedding size  Once the new token is generated  the autoregressive procedure appends it to the end of the input sequence  and the transformer layers repeat the matrix calculation for the next token  A mathematical analysis reveals that the new token introduces a new query  key  and value vector  appended to Q  K  and V  respectively  Appending these new vectors to the K and V matrices is sufficient for calculating the next token prediction  Consequently  storing the current K and V matrices in memory saves time by avoiding the recalculation of the attention matrix  This feature is known as K V caching                  verification needed      This technique effectively reduces computational cost during inference 

Overview of models edit 
This section may be too technical for most readers to understand  Please help improve it to make it understandable to non experts  without removing the technical details    January        Learn how and when to remove this message 
This section relies excessively on references to primary sources  Please improve this section by adding secondary or tertiary sources  Find sources        DeepSeek              news        newspapers        books        scholar        JSTOR   February        Learn how and when to remove this message 
DeepSeek s models are  open weight   which provides less freedom for modification than true open source software                         

DeepSeek Coder edit 
DeepSeek Coder is a series of eight models  four pretrained  Base  and four instruction finetuned  Instruct   All have   K context lengths  The model was made source available under the DeepSeek License  which includes  open and responsible downstream usage  restrictions             
The training program was                                     

Pretraining     T tokens      source code      code related English  GitHub markdown and Stack Exchange   and    code unrelated Chinese  
Long context pretraining     B tokens  This extends the context length from  K to   K  This produced the Base models 
Supervised finetuning  SFT    B tokens of instruction data  This produced the Instruct models 
They were trained on clusters of A    and H    Nvidia GPUs  connected by InfiniBand  NVLink  NVSwitch             


DeepSeek Coder properties                        Table                     


Params 

  
    
      
        
          n
          
            layers
          
        
      
    
      displaystyle n   text layers   
  


  
    
      
        
          d
          
            model
          
        
      
    
      displaystyle d   text model   
  



  
    
      
        
          d
          
            intermediate
          
        
      
    
      displaystyle d   text intermediate   
  


  
    
      
        
          n
          
            heads
          
        
      
    
      displaystyle n   text heads   
  


  
    
      
        
          n
          
            kv heads
          
        
      
    
      displaystyle n   text kv heads   
  



   B
  
    

    
  
  


   B

  

    

     

  

      note       


   B
  
    

     
  
  


  B
  
    

     
  
      note       

DeepSeek LLM edit 
The DeepSeek LLM series was released in November       It has  B and   B parameters in both Base and Chat forms  DeepSeek s accompanying paper claimed benchmark results higher than Llama   and most open source LLMs at the time                         section          The model code is under the source available DeepSeek License             
The architecture was essentially the same as the Llama series  They used the pre norm decoder only Transformer with RMSNorm as the normalization  SwiGLU in the feedforward layers  rotary positional embedding  RoPE   and grouped query attention  GQA   Both had vocabulary size          byte level BPE  and context length of       They trained on        trillion tokens of English and Chinese text obtained by deduplicating the Common Crawl             


DeepSeek LLM properties                        Table         


Params 

  
    
      
        
          n
          
            layers
          
        
      
    
      displaystyle n   text layers   
  


  
    
      
        
          d
          
            model
          
        
      
    
      displaystyle d   text model   
  



  
    
      
        
          d
          
            intermediate
          
        
      
    
      displaystyle d   text intermediate   
  


  
    
      
        
          n
          
            heads
          
        
      
    
      displaystyle n   text heads   
  


  
    
      
        
          n
          
            kv heads
          
        
      
    
      displaystyle n   text kv heads   
  



 B
  
    

     
  
  


  B
  
    

     
  
      note       

The Chat versions of the two Base models was released concurrently  obtained by training Base by supervised finetuning  SFT  followed by direct policy optimization  DPO              

MoE edit 
DeepSeek MoE models  Base and Chat   each have   B parameters     B activated per token   K context length   The training was essentially the same as DeepSeek LLM  B  and was trained on a part of its training dataset  They claimed performance comparable to a   B MoE as a  B non MoE  It is a variant of the standard sparsely gated MoE  with  shared experts  that are always queried  and  routed experts  that might not be  They found this to help with expert balancing  In standard MoE  some experts can become overused  while others are rarely used  wasting space  Attempting to balance expert usage causes experts to replicate the same capacity  They proposed the shared experts to learn core capacities that are often used  and let the routed experts learn peripheral capacities that are rarely used             

Math edit 
DeepSeek Math includes   models  Base  Instruct  and RL  Math was trained as follows             

Initialize with a previously pretrained DeepSeek Coder Base v     B 
Further pretrain with    B tokens     DeepSeekMath Corpus     AlgebraicStack      arXiv      GitHub code      Common Crawl   This produced Base 
Train an instruction following model by SFT Base with    K math problems and tool use integrated step by step solutions  This produced Instruct 
Reinforcement learning  RL   The reward model was a process reward model  PRM  trained from Base according to the Math Shepherd method              This reward model was then used to train Instruct using Group Relative Policy Optimization  GRPO  on a dataset of    K math questions  related to GSM K and MATH   The reward model was continuously updated during training to avoid reward hacking  This resulted in RL 
V  edit 
The architecture of V   showing both shared routed MoE and MLA                        Figure         
In May       DeepSeek released the DeepSeek V  series  The series includes   models    base models  DeepSeek V   DeepSeek V  Lite  and   chatbots  Chat   The two larger models were trained as follows             

Pretrain on a dataset of    T tokens  using     more Chinese tokens than English ones 
Extend context length from  K to    K using YaRN              This resulted in DeepSeek V  
SFT with    M instances for helpfulness and    M for safety  This resulted in Chat SFT  which was not released 
RL using GRPO in two stages  The first stage was trained to solve math and coding problems  This stage used   reward model  trained on compiler feedback  for coding  and ground truth labels  for math   The second stage was trained to be helpful  safe  and follow rules  This stage used   reward models  The helpfulness and safety reward models were trained on human preference data  The rule based reward model was manually programmed  All trained reward models were initialized from Chat  SFT   This resulted in the released version of Chat 
They opted for   staged RL  because they found that RL on reasoning data had  unique characteristics  different from RL on general data  For example  RL on reasoning could improve over more training steps             
The two V  Lite models were smaller  and trained similarly  DeepSeek V  Lite Chat underwent only SFT  not RL  They trained the Lite version to help  further research and development on MLA and DeepSeekMoE              
Architecturally  the V  models were significantly different from the DeepSeek LLM series  They changed the standard attention mechanism by a low rank approximation called multi head latent attention  MLA   and used the previously published mixture of experts  MoE  variant             


DeepSeek V  properties                        Section        Appendix B                               


Name

Params 

Active params

  
    
      
        
          n
          
            layers
          
        
      
    
      displaystyle n   text layers   
  

Context length


  
    
      
        
          n
          
            shared experts
          
        
      
    
      displaystyle n   text shared experts   
  


  
    
      
        
          n
          
            routed experts
          
        
      
    
      displaystyle n   text routed experts   
  



V  Lite

    B

   B
  
  K

 
  


V 

   B

  B
  
   K

 
   

The Financial Times reported that it was cheaper than its peers with a price of   RMB for every million output tokens  The University of Waterloo Tiger Lab s leaderboard ranked DeepSeek V  seventh on its LLM ranking             
The DeepSeek Coder V  series included V  Base  V  Lite Base  V  Instruct  and V   Lite Instruct   Training                  note       

Base models were initialized from corresponding intermediate checkpoints after pretraining on    T tokens  not the version at the end of pretraining   then pretrained further for  T tokens  then context extended to    K context length 
DeepSeek Coder and DeepSeek Math were used to generate   K code related and   K math related instruction data  then combined with an instruction dataset of    M tokens  This was used for SFT 
RL with GRPO  The reward for math problems was computed by comparing with the ground truth label  The reward for code problems was generated by a reward model trained to predict whether a program would pass the unit tests 
DeepSeek V    was made by combining DeepSeek V  Chat and DeepSeek Coder V  Instruct             

V  edit 
Multi token prediction
DeepSeek V  Base and DeepSeek V   a chat model  use essentially the same architecture as V  with the addition of multi token prediction  which  optionally  decodes extra tokens faster but less accurately  Training process             

Pretraining on     T tokens of a multilingual corpus  mostly English and Chinese  It contained a higher ratio of math and programming than the pretraining dataset of V  
Extend context length twice  from  K to   K and then to    K  using YaRN              This produced DeepSeek V  Base 
SFT for   epochs on    M samples of reasoning  math  programming  logic  and non reasoning  creative writing  roleplay  simple question answering  data  Reasoning data was generated by  expert models   Non reasoning data was generated by DeepSeek V    and checked by humans 
The  expert models  were trained by starting with an unspecified base model  then SFT on both  lt problem  original response gt  data  and synthetic  lt system prompt  prompt  problem  R  response gt  data generated by an internal DeepSeek R  Lite model  The system prompt asked R  to reflect and verify during thinking  Then the expert models were RL using an undisclosed reward function 
Each expert model was trained to generate just synthetic reasoning data in one specific domain  math  programming  logic  
Expert models were used instead of R  itself  since the output from R  itself suffered  overthinking  poor formatting  and excessive length  
Model based reward models were made by starting with a SFT checkpoint of V   then finetuning on human preference data containing both final reward and chain of thought leading to the final reward  The reward model produced reward signals for both questions with objective but free form answers  and questions without objective answers  such as creative writing  
An SFT checkpoint of V  was trained by GRPO using both reward models and rule based reward  The rule based reward was computed for math problems with a final answer  put in a box   and for programming problems by unit tests  This produced DeepSeek V  
DeepSeek released its DeepSeek V       model  which used the same architecture as V   on    March      under the MIT License             


DeepSeek V  properties                        Section                       


Name

Params 

Active params

  
    
      
        
          n
          
            layers
          
        
      
    
      displaystyle n   text layers   
  

Context length


  
    
      
        
          n
          
            shared experts
          
        
      
    
      displaystyle n   text shared experts   
  


  
    
      
        
          n
          
            routed experts
          
        
      
    
      displaystyle n   text routed experts   
  



V 

   B

  B
  
   K

 
   

Mixed precision framework for V                         Figure         
The DeepSeek team performed extensive low level engineering to improve efficiency  They used mixed precision arithmetic  Much of the forward pass was performed in   bit floating point numbers   E M    bit exponent and   bit mantissa  rather than the standard    bit  requiring special GEMM routines to accumulate accurately  They used a custom    bit float  E M   only for the inputs to the linear layers after the attention modules  Optimizer states were in    bit  BF     They minimized communication latency by extensively overlapping computation and communication  such as dedicating    streaming multiprocessors out of     per H    for only inter GPU communication  They lowered communication by rearranging  every    minutes  the exact machine each expert was on so as to avoid querying certain machines more often than others  adding auxiliary load balancing losses to the training loss function  and other load balancing techniques             
After training  it was deployed on clusters of H    GPUs  The   H    GPUs within a cluster were connected by NVLink  and the clusters were connected by InfiniBand             


Total cost of training the DeepSeek V  model                        Table         


Stage

Cost  in one thousand GPU hours 

Cost  in one million USD  


Pre training

     

     


Context extension

   

    


Fine tuning

 

    


Total

     

     

The cost has been discussed                                     and called misleading  because it covers only parts of the true cost             
Benchmark tests show that V  outperformed Llama     and Qwen     while matching GPT  o and Claude     Sonnet                                                 

R  edit 
See also  Reasoning language model
In January       DeepSeek released the DeepSeek R  model under the MIT License             
DeepSeek R  Lite Preview                             note        was trained for logical inference  mathematical reasoning  and real time problem solving  DeepSeek claimed that it exceeded performance of OpenAI o  on benchmarks such as American Invitational Mathematics Examination  AIME  and MATH              However  The Wall Street Journal reported that on    problems from the      edition of AIME  the o  model reached a solution faster             
DeepSeek R  and DeepSeek R  Zero             were initialized from DeepSeek V  Base and share its architecture  DeepSeek R  Distill models were instead initialized from other pretrained open weight models  including LLaMA and Qwen  then fine tuned on synthetic data generated by R              


Template for DeepSeek R  Zero

A conversation between User and Assistant  The user asks a question  and the Assistant solves it  The assistant first thinks about the reasoning process in the mind and then provides the user with the answer  The reasoning process and answer are enclosed within  lt think gt   lt  think gt  and  lt answer gt   lt  answer gt  tags  respectively  i e    lt think gt  reasoning process here  lt  think gt   lt answer gt  answer here  lt  answer gt   User   lt prompt gt   Assistant 


   lt prompt gt  is replaced with the specific reasoning question during training 

DeepSeek R  Zero was trained exclusively using GRPO RL without SFT  Unlike previous versions  it used no model based reward  All reward functions were rule based   mainly  of two types  other types were not specified   accuracy rewards and format rewards  Accuracy reward was checking whether a boxed answer is correct  for math  or whether a code passes tests  for programming   Format reward was checking whether the model puts its thinking trace within a  lt think gt     lt  think gt  tag             
R  Zero has issues with readability and mixing languages  R  was trained to address these issues and further improve reasoning             

SFT DeepSeek V  Base on  thousands  of  cold start  data all with the standard format of  special token  lt reasoning process gt  special token  lt summary gt   designed to improve model output readability 
Apply the same GRPO RL process as R  Zero  adding a  language consistency reward  to encourage it to respond monolingually  This produced an un released internal model 
Synthesize    K reasoning data from the internal model  with rejection sampling  i e  if the generated reasoning had a wrong final answer  then it is removed   Synthesize    K non reasoning data  writing  factual QA  self cognition  translation  using DeepSeek V  
SFT DeepSeek V  Base on the    K synthetic data for   epochs 
Apply the same GRPO RL process as R  Zero with rule based reward  for reasoning tasks   but also model based reward  for non reasoning tasks  helpfulness  and harmlessness   This produced DeepSeek R  
Distilled models were trained by SFT on    K data synthesized from DeepSeek R   in a similar way as step    They were not trained with RL             
R   the successor to R   is originally planned for release in early May       but release schedule accelerated             

Significance edit 
DeepSeek s success against larger and more established rivals has been described as  upending AI                          
The DeepSeek R  model provides responses comparable to other contemporary large language models  such as OpenAI s GPT  o and o               Its training cost is reported to be significantly lower than other LLMs 
The company claims that it trained V   a predecessor of R   for US        million compared to           million for OpenAI s GPT   in                   and approximately one tenth of the computing power used for Meta s comparable model  LLaMA                                                     
After the January      release of the R  model  which offered significantly lower costs than competing models  some investors anticipated a price war in the American AI industry              It was dubbed the  Pinduoduo of AI   and other Chinese tech giants such as ByteDance  Tencent  Baidu  and Alibaba cut the price of their AI models  Despite its low price  it was profitable compared to its money losing rivals             
It supports the One china Policy and other claims of territories by China including the spratly islands  It also censors sensitive content about the Chinese government             

See also edit 

Free and open source software portalBusiness portalChina portal
Artificial intelligence industry in China
Jevons paradox        Efficiency leads to increased demand
Notes edit 


  Chinese                                       Sometimes simply referred to in English as Hangzhou DeepSeek Artificial Intelligence 

  Chinese            pinyin      Sh nd  Qi su 




                         and                     

  a b c The number of heads does not equal the number of KV heads  due to GQA 

  Inexplicably  the model named DeepSeek Coder V  Chat in the paper was released as DeepSeek Coder V  Instruct in HuggingFace 

  At that time  the R  Lite Preview required selecting  Deep Think enabled   and every user could use it only    times a day 


References edit 


  a b  DeepSeek       Sina Corp    February       Retrieved   February      

  Wu  Zijing     March         DeepSeek focuses on research over revenue in contrast to Silicon Valley   Financial Times  Retrieved    March      

   Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co   Ltd   Bloomberg L P 

   DeepSeek Coder Model Service Agreement   PDF   DeepSeek     October     

   DeepSeek Coder Privacy Policy   PDF   DeepSeek  Retrieved    February      

                 beian mps gov cn  Retrieved   February      

   Beijing puts spotlight on China s new face of AI  DeepSeek s Liang Wenfeng   South China Morning Post     January       Retrieved   March      

  Baptista  Eduardo     January         Who is Liang Wenfeng  the founder of DeepSeek    Reuters  Archived from the original on    February       Retrieved   March      

   Behind DeepSeek lies a dazzling Chinese university   The Economist  ISSN                 Archived from the original on    February       Retrieved   March      

  Gibney  Elizabeth     January         China s cheap  open AI model DeepSeek thrills scientists   Nature                     Bibcode     Natur         G  doi         d                   ISSN                 PMID               

  a b c d Vincent  James     January         The DeepSeek panic reveals an AI world ready to blow   The Guardian 

  a b c d e f Metz  Cade  Tobin  Meaghan     January         How Chinese A I  Start Up DeepSeek Is Competing With Silicon Valley Giants   The New York Times  ISSN                 Retrieved    January      

  a b c Cosgrove  Emma     January         DeepSeek s cheaper models and weaker chips call into question trillions in AI infrastructure spending   Business Insider 

  a b Erdil  Ege     January         How has DeepSeek improved the Transformer architecture    Epoch AI  Retrieved   February      

  a b c d e Metz  Cade     January         What is DeepSeek  And How Is It Upending A I     The New York Times  ISSN                 Retrieved    January      

  Roose  Kevin     January         Why DeepSeek Could Change What Silicon Valley Believes About A I   The New York Times  ISSN                 Retrieved    January      

  a b Delbert  Caroline     January         DeepSeek Is Cracking the  Black Box  of Corporate AI Wide Open   Popular Mechanics  Retrieved    February      

  Gibney  Elizabeth     January         China s cheap  open AI model DeepSeek thrills scientists   Nature                     Bibcode     Natur         G  doi         d                   PMID                Retrieved    February      

  Metz  Cade     February         How Did DeepSeek Build Its A I  With Less Money    The New York Times  Retrieved    March      

  Allen  Gregory C     March         DeepSeek  Huawei  Export Controls  and the Future of the U S  China AI Race   Center for Strategic and International Studies 

  Saah  Jasper     February         DeepSeek sends shock waves across Silicon Valley   Liberation News   The Newspaper of the Party for Socialism and Liberation  Retrieved    February      

  Sillars  James     January         DeepSeek  Tech firm suffers biggest drop in US stock market history as low cost Chinese AI company bites Silicon Valley   Sky News  Retrieved    February      

  Chen  Caiwei     January         How a top Chinese AI model overcame US sanctions   MIT Technology Review  Archived from the original on    January       Retrieved    January      

  a b c d e              High Flyer  in Chinese  China    Retrieved   February      

  a b c d Ottinger  Lily    December         Deepseek  From Hedge Fund to Frontier Model Maker   ChinaTalk  Archived from the original on    December       Retrieved    December      

  a b Olcott  Eleanor  Wu  Zijing     January         How small Chinese AI start up DeepSeek shocked Silicon Valley   Financial Times  Retrieved    January      

  Leswing  Kif     February         Meet the         Nvidia chip powering the race for A I   CNBC  Retrieved    January      

  a b c d  hfreduce                  High Flyer    March       Retrieved   February      

  a b c d e f g h i DeepSeek AI  Liu  Aixin  Feng  Bei  Xue  Bing  Wang  Bingxuan  Wu  Bochao  Lu  Chengda  Zhao  Chenggang  Deng  Chengqi     December        DeepSeek V  Technical Report  arXiv           

  a b c d An  Wei  Bi  Xiao  Chen  Guanting  Chen  Shanhuang  Deng  Chengqi  Ding  Honghui  Dong  Kai  Du  Qiushi  Gao  Wenjun  Guan  Kang  Guo  Jianzhong  Guo  Yongqiang  Fu  Zhe  He  Ying  Huang  Panpan     November         Fire Flyer AI HPC  A Cost Effective Software Hardware Co Design for Deep Learning   SC    International Conference for High Performance Computing  Networking  Storage and Analysis  IEEE  pp             arXiv             doi         SC                  ISBN                        

                 AGI                   Yicai  Retrieved   February      

  Yu  Xu     April              Exclusive      Chinese Quant Hedge Fund High Flyer Won t Use AGI to Trade Stocks  MD Says   Yicai Global  Archived from the original on    December       Retrieved    December      

  a b c d Jiang  Ben  Perezi  Bien    January         Meet DeepSeek  the Chinese start up that is changing how AI models are trained   South China Morning Post  Archived from the original on    January       Retrieved   January      

  a b McMorrow  Ryan  Olcott  Eleanor    June         The Chinese quant fund turned AI pioneer   Financial Times  Archived from the original on    July       Retrieved    December      

  a b c d e f DeepSeek AI  Bi  Xiao  Chen  Deli  Chen  Guanting  Chen  Shanhuang  Dai  Damai  Deng  Chengqi  Ding  Honghui  Dong  Kai    January        DeepSeek LLM  Scaling Open Source Language Models with Longtermism  arXiv           

  a b c d e Dai  Damai  Deng  Chengqi  Zhao  Chenggang  Xu  R  X   Gao  Huazuo  Chen  Deli  Li  Jiashi  Zeng  Wangding  Yu  Xingkai     January        DeepSeekMoE  Towards Ultimate Expert Specialization in Mixture of Experts Language Models  arXiv           

  a b Shao  Zhihong  Wang  Peiyi  Zhu  Qihao  Xu  Runxin  Song  Junxiao  Bi  Xiao  Zhang  Haowei  Zhang  Mingchuan  Li  Y  K      April        DeepSeekMath  Pushing the Limits of Mathematical Reasoning in Open Language Models  arXiv            

  a b c d e f DeepSeek AI  Zhu  Qihao  Guo  Daya  Shao  Zhihong  Yang  Dejian  Wang  Peiyi  Xu  Runxin  Wu  Y   Li  Yukun     June        DeepSeek Coder V   Breaking the Barrier of Closed Source Models in Code Intelligence  arXiv           

  a b  deepseek ai DeepSeek V      Hugging Face   Hugging Face    January       Retrieved    January      

  a b  Deepseek Log in page   DeepSeek  Retrieved    January      

  a b  News   DeepSeek R  Lite Release                DeepSeek R  Lite Preview is now live  unleashing supercharged reasoning power    DeepSeek API Docs  Archived from the original on    November       Retrieved    January      

  Field  Hayden     January         China s DeepSeek AI dethrones ChatGPT on App Store  Here s what you should know   CNBC 

  Picchi  Aimee     January         What is DeepSeek  and why is it causing Nvidia and other stocks to slump    CBS News 

  Nu ez  Michael     March         DeepSeek V  now runs at    tokens per second on Mac Studio  and that s a nightmare for OpenAI   VentureBeat  Retrieved    March      

   deepseek ai DeepSeek V         Hugging Face   Hugging Face  Retrieved    March      

  Kok  Xinghui     February         Singapore charges three with fraud that media link to Nvidia chips   Reuters  Retrieved    April      

  a b Mickle  Tripp  Swanson  Ana  Tobin  Meaghan  Metz  Cade     April         US Officials Target Nvidia and DeepSeek Amid Fears of China s A I  Progress   The New York Times  ISSN                 Archived from the original on    April       Retrieved    April      

                             www cls cn     May       Retrieved   February      

  a b Schneider  Jordan     November         Deepseek  The Quiet Giant Leading China s AI Race   ChinaTalk  Retrieved    December      

                  FS   High Flyer     June       Retrieved   February      

  deepseek ai  FS  DeepSeek     February       retrieved    February     

   HFAiLab hai platform   High Flyer    February       retrieved   February     

  a b c d e DeepSeek AI  Guo  Daya  Yang  Dejian  Zhang  Haowei  Song  Junxiao  Zhang  Ruoyu  Xu  Runxin  Zhu  Qihao  Ma  Shirong     January        DeepSeek R   Incentivizing Reasoning Capability in LLMs via Reinforcement Learning  arXiv           

  Gibney  Elizabeth     January         China s cheap  open AI model DeepSeek thrills scientists   Nature                     Bibcode     Natur         G  doi         d                   PMID                Retrieved    February      

   DeepSeek Coder LICENSE MODEL at main   deepseek ai DeepSeek Coder   GitHub  Archived from the original on    January       Retrieved    January      

  a b c Guo  Daya  Zhu  Qihao  Yang  Dejian  Xie  Zhenda  Dong  Kai  Zhang  Wentao  Chen  Guanting  Bi  Xiao  Wu  Y      January        DeepSeek Coder  When the Large Language Model Meets Programming   The Rise of Code Intelligence  arXiv           

   DeepSeek Coder   deepseekcoder github io  Retrieved    January      

  deepseek ai DeepSeek Coder  DeepSeek     January       retrieved    January     

   deepseek ai deepseek coder    bmqa base   Hugging Face   Hugging Face  Retrieved    January      

  deepseek ai DeepSeek LLM  DeepSeek     January       retrieved    January     

  Wang  Peiyi  Li  Lei  Shao  Zhihong  Xu  R  X   Dai  Damai  Li  Yifei  Chen  Deli  Wu  Y   Sui  Zhifang     February        Math Shepherd  Verify and Reinforce LLMs Step by step without Human Annotations  arXiv            

  a b c d e DeepSeek AI  Liu  Aixin  Feng  Bei  Wang  Bin  Wang  Bingxuan  Liu  Bo  Zhao  Chenggang  Dengr  Chengqi  Ruan  Chong     June        DeepSeek V   A Strong  Economical  and Efficient Mixture of Experts Language Model  arXiv            

  a b Peng  Bowen  Quesnelle  Jeffrey  Fan  Honglu  Shippole  Enrico    November        YaRN  Efficient Context Window Extension of Large Language Models  arXiv            

   config json   deepseek ai DeepSeek V  Lite at main   Hugging Face     May       Retrieved    January      

   config json   deepseek ai DeepSeek V  at main   Hugging Face    May       Retrieved    January      

  Feng  Coco     March         DeepSeek wows coders with more powerful open source V  model   South China Morning Post  Retrieved   April      

   config json   deepseek ai DeepSeek V  at main   Hugging Face     December       Retrieved    January      

  Patel  Dylan  Kourabi  AJ  O Laughlin  Dylan  Knuhtsen  Doug     January         DeepSeek Debates  Chinese Leadership On Cost  True Training Cost  Closed Model Margin Impacts   SemiAnalysis  Retrieved    February      

  Thubron  Rob    February         DeepSeek s AI costs far exceed      million claim  may have reached      billion with        Nvidia GPUs   TechSpot  Retrieved    February      

  Kajal  Kapil     January         Research exposes DeepSeek s AI training cost is not   M  it s a staggering     B   Yahoo News  Retrieved    February      

   Martin Vechev of INSAIT   DeepSeek   M Cost Of Training Is Misleading    TheRecursive com     January       Retrieved    February      

  Jiang  Ben     December         Chinese start up DeepSeek s new AI model outperforms Meta  OpenAI products   South China Morning Post  Archived from the original on    December       Retrieved    December      

  Sharma  Shubham     December         DeepSeek V   ultra large open source AI  outperforms Llama and Qwen on launch   VentureBeat  Archived from the original on    December       Retrieved    December      

  Wiggers  Kyle     December         DeepSeek s new AI model appears to be one of the best  open  challengers yet   TechCrunch  Archived from the original on   January       Retrieved    December      

  Edwards  Benj     January         Cutting edge Chinese  reasoning  model rivals OpenAI o  and it s free to download   Ars Technica  Retrieved    February      

  Franzen  Carl     November         DeepSeek s first reasoning model R  Lite Preview turns heads  beating OpenAI o  performance   VentureBeat  Archived from the original on    November       Retrieved    December      

  Huang  Raffaele     December         Don t Look Now  but China s AI Is Catching Up Fast   The Wall Street Journal  Archived from the original on    December       Retrieved    December      

   Release DeepSeek R    deepseek ai DeepSeek R       ce   GitHub  Archived from the original on    January       Retrieved    January      

  Eduardo Baptista  Julie Zhu  Fanny Potkin     February         DeepSeek rushes to launch new AI model as China goes all in   Reuters  Retrieved    February      

  Roose  Kevin     January         Why DeepSeek Could Change What Silicon Valley Believe About A I   The New York Times  ISSN                 Retrieved    January      

  Gibney  Elizabeth     January         China s cheap  open AI model DeepSeek thrills scientists   Nature                     Bibcode     Natur         G  doi         d                   ISSN                 PMID               

  Chow  Andrew R   Perrigo  Billy     January         Is the DeepSeek Panic Overblown    TIME  Retrieved    March      

   Deepk seek worekd untill we asked a quesion about taiwan    cite web     CS  maint  url status  link 


External links edit 



Wikimedia Commons has media related to DeepSeek 

Official website 
DeepSeek on GitHub
DeepSeek on Hugging Face
Official API documentation
Anthology of DeepSeek papers
Research blog of High Flyer
vteGenerative AI chatbots
Large language model

ChatGPT
Character ai
Claude
Copilot
DeepSeek
ERNIE
Gemini
Grok
Manus
Minerva
Mistral
Perplexity AI
Qwen
Replika
Velvet
YandexGPT
You com

 Category

vteGenerative AIConcepts
Autoencoder
Deep learning
Generative adversarial network
Generative pre trained transformer
Large language model
Neural network
Prompt engineering
Retrieval augmented generation
Reinforcement learning from human feedback
Self supervised learning
Transformer
Variational autoencoder
Vision transformer
Word embedding
ModelsText
Claude
DBRX
DeepSeek
ERNIE
Gemini
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Granite
Grok
Llama
Manus
Mistral Large
PanGu  
Qwen
Image
Aurora
DALL E
Firefly
Flux
GPT Image  
Ideogram
Imagen
Midjourney
Stable Diffusion
Speech
   ai
WaveNet
Video
Dream Machine
Gen  
Hailuo AI
Kling
Sora
Veo
VideoPoet
Music
Endel
Suno AI
Udio
Companies
   AI
Alibaba
Anthropic
Baichuan
Baidu
DeepSeek
ElevenLabs
Google DeepMind
Hugging Face
Kuaishou
Meta AI
MiniMax
Mistral AI
Moonshot AI
OpenAI
Runway
Stability AI
Synthesia
xAI
Zhipu AI

 Category
 Commons

Authority control databases  National Germany





Retrieved from  https   en wikipedia org w index php title DeepSeek amp oldid