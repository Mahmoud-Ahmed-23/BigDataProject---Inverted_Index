AI to benefit humanity


Part of a series onArtificial intelligence  AI 
Major goals
Artificial general intelligence
Intelligent agent
Recursive self improvement
Planning
Computer vision
General game playing
Knowledge reasoning
Natural language processing
Robotics
AI safety

Approaches
Machine learning
Symbolic
Deep learning
Bayesian networks
Evolutionary algorithms
Hybrid intelligent systems
Systems integration

Applications
Bioinformatics
Deepfake
Earth sciences
 Finance 
Generative AI
Art
Audio
Music
Government
Healthcare
Mental health
Industry
Translation
 Military 
Physics
Projects

Philosophy
Artificial consciousness
Chinese room
Friendly AI
Control problem Takeover
Ethics
Existential risk
Turing test
Uncanny valley

History
Timeline
Progress
AI winter
AI boom

Glossary
Glossary
vte
Friendly artificial intelligence  friendly AI or FAI  is hypothetical artificial general intelligence  AGI  that would have a positive  benign  effect on humanity or at least align with human interests such as fostering the improvement of the human species  It is a part of the ethics of artificial intelligence and is closely related to machine ethics  While machine ethics is concerned with how an artificially intelligent agent should behave  friendly artificial intelligence research is focused on how to practically bring about this behavior and ensuring it is adequately constrained 


Etymology and usage edit 
Eliezer Yudkowsky  AI researcher and creator of the term
The term was coined by Eliezer Yudkowsky             who is best known for popularizing the idea                        to discuss superintelligent artificial agents that reliably implement human values  Stuart J  Russell and Peter Norvig s leading artificial intelligence textbook  Artificial Intelligence  A Modern Approach  describes the idea            

Yudkowsky        goes into more detail about how to design a Friendly AI  He asserts that friendliness  a desire not to harm humans  should be designed in from the start  but that the designers should recognize both that their own designs may be flawed  and that the robot will learn and evolve over time  Thus the challenge is one of mechanism design       to define a mechanism for evolving AI systems under a system of checks and balances  and to give the systems utility functions that will remain friendly in the face of such changes 
 Friendly  is used in this context as technical terminology  and picks out agents that are safe and useful  not necessarily ones that are  friendly  in the colloquial sense  The concept is primarily invoked in the context of discussions of recursively self improving artificial agents that rapidly explode in intelligence  on the grounds that this hypothetical technology would have a large  rapid  and difficult to control impact on human society            

Risks of unfriendly AI edit 
Main article  Existential risk from artificial general intelligence
The roots of concern about artificial intelligence are very old  Kevin LaGrandeur showed that the dangers specific to AI can be seen in ancient literature concerning artificial humanoid servants such as the golem  or the proto robots of Gerbert of Aurillac and Roger Bacon   In those stories  the extreme intelligence and power of these humanoid creations clash with their status as slaves  which by nature are seen as sub human   and cause disastrous conflict             By      these themes prompted Isaac Asimov to create the  Three Laws of Robotics  principles hard wired into all the robots in his fiction  intended to prevent them from turning on their creators  or allowing them to come to harm            
In modern times as the prospect of superintelligent AI looms nearer  philosopher Nick Bostrom has said that superintelligent AI systems with goals that are not aligned with human ethics are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity  He put it this way 

Basically we should assume that a  superintelligence  would be able to achieve whatever goals it has  Therefore  it is extremely important that the goals we endow it with  and its entire motivation system  is  human friendly  
In       Eliezer Yudkowsky called for the creation of  friendly AI  to mitigate existential risk from advanced artificial intelligence  He explains   The AI does not hate you  nor does it love you  but you are made out of atoms which it can use for something else             
Steve Omohundro says that a sufficiently advanced AI system will  unless explicitly counteracted  exhibit a number of basic  drives   such as resource acquisition  self preservation  and continuous self improvement  because of the intrinsic nature of any goal driven systems and that these drives will   without special precautions   cause the AI to exhibit undesired behavior                       
Alexander Wissner Gross says that AIs driven to maximize their future freedom of action  or causal path entropy  might be considered friendly if their planning horizon is longer than a certain threshold  and unfriendly if their planning horizon is shorter than that threshold                         
Luke Muehlhauser  writing for the Machine Intelligence Research Institute  recommends that machine ethics researchers adopt what Bruce Schneier has called the  security mindset   Rather than thinking about how a system will work  imagine how it could fail  For instance  he suggests even an AI that only makes accurate predictions and communicates via a text interface might cause unintended harm             
In       Luke Muehlhauser and Nick Bostrom underlined the need for  friendly AI               nonetheless  the difficulties in designing a  friendly  superintelligence  for instance via programming counterfactual moral thinking  are considerable                         

Coherent extrapolated volition edit 
Yudkowsky advances the Coherent Extrapolated Volition  CEV  model  According to him  our coherent extrapolated volition is  our wish if we knew more  thought faster  were more the people we wished we were  had grown up farther together  where the extrapolation converges rather than diverges  where our wishes cohere rather than interfere  extrapolated as we wish that extrapolated  interpreted as we wish that interpreted              
Rather than a Friendly AI being designed directly by human programmers  it is to be designed by a  seed AI  programmed to first study human nature and then produce the AI that humanity would want  given sufficient time and insight  to arrive at a satisfactory answer              The appeal to an objective through contingent human nature  perhaps expressed  for mathematical purposes  in the form of a utility function or other decision theoretic formalism   as providing the ultimate criterion of  Friendliness   is an answer to the meta ethical problem of defining an objective morality  extrapolated volition is intended to be what humanity objectively would want  all things considered  but it can only be defined relative to the psychological and cognitive qualities of present day  unextrapolated humanity 

Other approaches edit 
See also  AI control problem        Alignment  and AI safety
Steve Omohundro has proposed a  scaffolding  approach to AI safety  in which one provably safe AI generation helps build the next provably safe generation             
Seth Baum argues that the development of safe  socially beneficial artificial intelligence or artificial general intelligence is a function of the social psychology of AI research communities and so can be constrained by extrinsic measures and motivated by intrinsic measures  Intrinsic motivations can be strengthened when messages resonate with AI developers  Baum argues that  in contrast   existing messages about beneficial AI are not always framed well   Baum advocates for  cooperative relationships  and positive framing of AI researchers  and cautions against characterizing AI researchers as  not want ing  to pursue beneficial designs              
In his book Human Compatible  AI researcher Stuart J  Russell lists three principles to guide the development of beneficial machines   He emphasizes that these principles are not meant to be explicitly coded into the machines  rather  they are intended for the human developers   The principles are as follows                                   


The machine s only objective is to maximize the realization of human preferences 
The machine is initially uncertain about what those preferences are 
The ultimate source of information about human preferences is human behavior 
The  preferences  Russell refers to  are all encompassing  they cover everything you might care about  arbitrarily far into the future                                      Similarly   behavior  includes any choice between options                                    and the uncertainty is such that some probability  which may be quite small  must be assigned to every logically possible human preference                                   

Public policy edit 
James Barrat  author of Our Final Invention  suggested that  a public private partnership has to be created to bring A I  makers together to share ideas about security something like the International Atomic Energy Agency  but in partnership with corporations   He urges AI researchers to convene a meeting similar to the Asilomar Conference on Recombinant DNA  which discussed risks of biotechnology              
John McGinnis encourages governments to accelerate friendly AI research  Because the goalposts of friendly AI are not necessarily eminent  he suggests a model similar to the National Institutes of Health  where  Peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance AI and assure that such advances would be accompanied by appropriate safeguards   McGinnis feels that peer review is better  than regulation to address technical issues that are not possible to capture through bureaucratic mandates   McGinnis notes that his proposal stands in contrast to that of the Machine Intelligence Research Institute  which generally aims to avoid government involvement in friendly AI             

Criticism edit 
See also  Technological singularity        Criticisms
Some critics believe that both human level AI and superintelligence are unlikely and that  therefore  friendly AI is unlikely  Writing in The Guardian  Alan Winfield compares human level artificial intelligence with faster than light travel in terms of difficulty and states that while we need to be  cautious and prepared  given the stakes involved  we  don t need to be obsessing  about the risks of superintelligence              Boyles and Joaquin  on the other hand  argue that Luke Muehlhauser and Nick Bostrom s proposal to create friendly AIs appear to be bleak  This is because Muehlhauser and Bostrom seem to hold the idea that intelligent machines could be programmed to think counterfactually about the moral values that human beings would have had              In an article in AI  amp  Society  Boyles and Joaquin maintain that such AIs would not be that friendly considering the following  the infinite amount of antecedent counterfactual conditions that would have to be programmed into a machine  the difficulty of cashing out the set of moral values that is  those that are more ideal than the ones human beings possess at present  and the apparent disconnect between counterfactual antecedents and ideal value consequent             
Some philosophers claim that any truly  rational  agent  whether artificial or human  will naturally be benevolent  in this view  deliberate safeguards designed to produce a friendly AI could be unnecessary or even harmful              Other critics question whether artificial intelligence can be friendly  Adam Keiper and Ari N  Schulman  editors of the technology journal The New Atlantis  say that it will be impossible ever to guarantee  friendly  behavior in AIs because problems of ethical complexity will not yield to software advances or increases in computing power  They write that the criteria upon which friendly AI theories are based work  only when one has not only great powers of prediction about the likelihood of myriad possible outcomes but certainty and consensus on how one values the different outcomes             
The inner workings of advanced AI systems may be complex and difficult to interpret  leading to concerns about transparency and accountability             

See also edit 

Affective computing
AI alignment
AI effect
AI takeover
Ambient intelligence
Applications of artificial intelligence
Artificial intelligence arms race
Artificial intelligence systems integration
Autonomous agent
Embodied agent
Emotion recognition
Existential risk from artificial general intelligence
Hallucination  artificial intelligence 
Hybrid intelligent system
Intelligence explosion
Intelligent agent
Intelligent control
Machine ethics
Machine Intelligence Research Institute
OpenAI
Regulation of algorithms
Roko s basilisk
Sentiment analysis
Singularitarianism   a moral philosophy advocated by proponents of Friendly AI
Suffering risks
Technological singularity
Three Laws of Robotics

References edit 


  Tegmark  Max          Life  Our Universe and Everything   Our Mathematical Universe  My Quest for the Ultimate Nature of Reality  First      ed    Knopf Doubleday Publishing  ISBN                     Its owner may cede control to what Eliezer Yudkowsky terms a  Friendly AI     

  a b Russell  Stuart  Norvig  Peter         Artificial Intelligence  A Modern Approach  Prentice Hall  ISBN                        

  Leighton  Jonathan         The Battle for Compassion  Ethics in an Apathetic Universe  Algora  ISBN                        

  Wallach  Wendell  Allen  Colin         Moral Machines  Teaching Robots Right from Wrong  Oxford University Press  Inc  ISBN                        

  Kevin LaGrandeur          The Persistent Peril of the Artificial Slave   Science Fiction Studies               doi         sciefictstud            Archived from the original on January           Retrieved May         

  Isaac Asimov          Introduction   The Rest of the Robots  Doubleday  ISBN                       cite book    ISBN   Date incompatibility  help 

  Eliezer Yudkowsky          Artificial Intelligence as a Positive and Negative Factor in Global Risk   PDF   In Nick Bostrom  Milan M   irkovi   eds    Global Catastrophic Risks  pp                Archived  PDF  from the original on October           Retrieved October          

  Omohundro  S  M   February         The basic AI drives   Artificial General Intelligence                CiteSeerX                      

  Bostrom  Nick          Chapter    The Superintelligent Will   Superintelligence  Paths  Dangers  Strategies  Oxford  Oxford University Press  ISBN                    

  Dvorsky  George  April             How Skynet Might Emerge From Simple Physics   Gizmodo  Archived from the original on October          Retrieved December          

  Wissner Gross  A  D   Freer  C  E           Causal entropic forces   Physical Review Letters                    Bibcode     PhRvL    p    W  doi         PhysRevLett             hdl               PMID               

  Muehlhauser  Luke  July             AI Risk and the Security Mindset   Machine Intelligence Research Institute  Archived from the original on July           Retrieved July          

  a b Muehlhauser  Luke  Bostrom  Nick  December             Why We Need Friendly AI   Think                  doi         s                  ISSN                 S CID                

  a b Boyles  Robert James M   Joaquin  Jeremiah Joven  July             Why friendly AIs won t be that friendly  a friendly reply to Muehlhauser and Bostrom   AI  amp  Society                   doi         s                   ISSN                 S CID                

  Chan  Berman  March            The rise of artificial intelligence and the crisis of moral passivity   AI  amp  Society                   doi         s                   ISSN                 S CID                 Archived from the original on February           Retrieved January          

  a b Eliezer Yudkowsky          Coherent Extrapolated Volition   PDF   Singularity Institute for Artificial Intelligence  Archived  PDF  from the original on September           Retrieved September          

  a b Hendry  Erica R   January             What Happens When Artificial Intelligence Turns On Us    Smithsonian Magazine  Archived from the original on July           Retrieved July          

  Baum  Seth D   September             On the promotion of safe and socially beneficial artificial intelligence   AI  amp  Society                   doi         s                  ISSN                 S CID               

  a b c d Russell  Stuart  October           Human Compatible  Artificial Intelligence and the Problem of Control  United States  Viking  ISBN                         OCLC                 

  McGinnis  John O   Summer         Accelerating AI   Northwestern University Law Review                      Archived from the original on December          Retrieved July          

  Winfield  Alan  August            Artificial intelligence will not turn into a Frankenstein s monster   The Guardian  Archived from the original on September           Retrieved September          

  Kornai  Andr s  May             Bounding the impact of AGI   Journal of Experimental  amp  Theoretical Artificial Intelligence          Informa UK Limited           doi                x              ISSN              X  S CID                  the essence of AGIs is their reasoning facilities  and it is the very logic of their being that will compel them to behave in a moral fashion    The real nightmare scenario  is one where  humans find it advantageous to strongly couple themselves to AGIs  with no guarantees against self deception 

  Keiper  Adam  Schulman  Ari N   Summer         The Problem with  Friendly  Artificial Intelligence   The New Atlantis  No           pp              Archived from the original on January           Retrieved January          

  Norvig  Peter  Russell  Stuart         Artificial Intelligence  A Modern Approach   rd      ed    Pearson  ISBN                     


Further reading edit 
Yudkowsky  E          Artificial Intelligence as a Positive and Negative Factor in Global Risk  In Global Catastrophic Risks  Oxford University Press Discusses Artificial Intelligence from the perspective of Existential risk   In particular  Sections     give background to the definition of Friendly AI in Section     Section   gives two classes of mistakes  technical and philosophical  which would both lead to the accidental creation of non Friendly AIs   Sections      discuss further related issues 
Omohundro  S          The Basic AI Drives Appeared in AGI      Proceedings of the First Conference on Artificial General Intelligence 
Mason  C          Human Level AI Requires Compassionate Intelligence Archived            at the Wayback Machine Appears in AAAI      Workshop on Meta Reasoning  Thinking About Thinking 
Froding  B  and Peterson  M          Friendly AI Ethics and Information Technology  Vol      pp          
External links edit 
Ethical Issues in Advanced Artificial Intelligence by Nick Bostrom
What is Friendly AI          A brief description of Friendly AI by the Machine Intelligence Research Institute 
Creating Friendly AI      The Analysis and Design of Benevolent Goal Architectures         A near book length description from the MIRI
Critique of the MIRI Guidelines on Friendly AI         by Bill Hibbard
Commentary on MIRI s Guidelines on Friendly AI         by Peter Voss 
The Problem with  Friendly  Artificial Intelligence         On the motives for and impossibility of FAI  by Adam Keiper and Ari N  Schulman 
vteExistential risk from artificial intelligenceConcepts
AGI
AI alignment
AI capability control
AI safety
AI takeover
Consequentialism
Effective accelerationism
Ethics of artificial intelligence
Existential risk from artificial intelligence
Friendly artificial intelligence
Instrumental convergence
Vulnerable world hypothesis
Intelligence explosion
Longtermism
Machine ethics
Suffering risks
Superintelligence
Technological singularity
Organizations
Alignment Research Center
Center for AI Safety
Center for Applied Rationality
Center for Human Compatible Artificial Intelligence
Centre for the Study of Existential Risk
EleutherAI
Future of Humanity Institute
Future of Life Institute
Google DeepMind
Humanity 
Institute for Ethics and Emerging Technologies
Leverhulme Centre for the Future of Intelligence
Machine Intelligence Research Institute
OpenAI
People
Scott Alexander
Sam Altman
Yoshua Bengio
Nick Bostrom
Paul Christiano
Eric Drexler
Sam Harris
Stephen Hawking
Dan Hendrycks
Geoffrey Hinton
Bill Joy
Shane Legg
Elon Musk
Steve Omohundro
Huw Price
Martin Rees
Stuart J  Russell
Jaan Tallinn
Max Tegmark
Frank Wilczek
Roman Yampolskiy
Eliezer Yudkowsky
Other
Statement on AI risk of extinction
Human Compatible
Open letter on artificial intelligence       
Our Final Invention
The Precipice
Superintelligence  Paths  Dangers  Strategies
Do You Trust This Computer 
Artificial Intelligence Act
 Category





Retrieved from  https   en wikipedia org w index php title Friendly artificial intelligence amp oldid