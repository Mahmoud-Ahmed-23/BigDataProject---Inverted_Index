Algorithm for supervised learning of binary classifiers
 Perceptrons  redirects here  For the      book  see Perceptrons  book  
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
In machine learning  the perceptron is an algorithm for supervised learning of binary classifiers   A binary classifier is a function that can decide whether or not an input  represented by a vector of numbers  belongs to some specific class              It is a type of linear classifier  i e  a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector 


History edit 
Mark I Perceptron machine  the first implementation of the perceptron algorithm  It was connected to a camera with       cadmium sulfide photocells to make a     pixel image  The main visible feature is the sensory to association plugboard  which sets different combinations of input features  To the right are arrays of potentiometers that implemented the adaptive weights                                  
See also  History of artificial intelligence        Perceptrons
The Mark   Perceptron  being adjusted by Charles Wightman  Mark I Perceptron project engineer              Sensory units at left  association units in center  and control panel and response units at far right  The sensory to association plugboard is behind the closed panel to the right of the operator  The letter  C  on the front panel is a display of the current state of the sensory input            
The artificial neuron network was invented in      by Warren McCulloch and Walter Pitts in A logical calculus of the ideas immanent in nervous activity             
In       Frank Rosenblatt was at the Cornell Aeronautical Laboratory  He simulated the perceptron on an IBM                            Later  he obtained funding by the Information Systems Branch of the United States Office of Naval Research and the Rome Air Development Center  to build a custom made computer  the Mark I Perceptron  It was first publicly demonstrated on    June                  The machine was  part of a previously secret four year NPIC  the US  National Photographic Interpretation Center  effort from      through      to develop this algorithm into a useful tool for photo interpreters             
Rosenblatt described the details of the perceptron in a      paper              His organization of a perceptron is constructed of three kinds of cells   units    AI  AII  R  which stand for  projection    association  and  response   He presented at the first international symposium on AI  Mechanisation of Thought Processes  which took place in      November             
Rosenblatt s project was funded under Contract Nonr          Cognitive Systems Research Program   which lasted from      to                   and Contract Nonr           Project PARA    PARA  means  Perceiving and Recognition Automata    which lasted from                 to                  
In       the Institute for Defense Analysis awarded his group a         contract  By September       the ONR awarded further          worth of contracts  with          committed for                  
The ONR research manager  Marvin Denicoff  stated that ONR  instead of ARPA  funded the Perceptron project  because the project was unlikely to produce technological results in the near or medium term  Funding from ARPA go up to the order of millions dollars  while from ONR are on the order of        dollars  Meanwhile  the head of IPTO at ARPA  J C R  Licklider  was interested in  self organizing    adaptive  and other biologically inspired methods in the     s  but by the mid     s he was openly critical of these  including the perceptron  Instead he strongly favored the logical AI approach of Simon and Newell             

Mark I Perceptron machine edit 
Main article  Mark I Perceptron
Organization of a biological brain and a perceptron 
The perceptron was intended to be a machine  rather than a program  and while its first implementation was in software for the IBM      it was subsequently implemented in custom built hardware as the Mark I Perceptron with the project name  Project PARA               designed for image recognition  The machine is currently in Smithsonian National Museum of American History             
The Mark I Perceptron had three layers  One version was implemented as follows 

An array of     photocells arranged in a   x   grid  named  sensory units   S units   or  input retina   Each S unit can connect to up to    A units 
A hidden layer of     perceptrons  named  association units   A units  
An output layer of eight perceptrons  named  response units   R units  
Rosenblatt called this three layered perceptron network the alpha perceptron  to distinguish it from other perceptron models he experimented with            
The S units are connected to the A units randomly  according to a table of random numbers  via a plugboard  see photo   to  eliminate any particular intentional bias in the perceptron   The connection weights are fixed  not learned  Rosenblatt was adamant about the random connections  as he believed the retina was randomly connected to the visual cortex  and he wanted his perceptron machine to resemble human visual perception             
The A units are connected to the R units  with adjustable weights encoded in potentiometers  and weight updates during learning were performed by electric motors                                  The hardware details are in an operators  manual             

Components of the Mark I Perceptron  From the operator s manual             
In a      press conference organized by the US Navy  Rosenblatt made statements about the perceptron that caused a heated controversy among the fledgling AI community  based on Rosenblatt s statements  The New York Times reported the perceptron to be  the embryo of an electronic computer that  the Navy  expects will be able to walk  talk  see  write  reproduce itself and be conscious of its existence              
The Photo Division of Central Intelligence Agency  from      to       studied the use of Mark I Perceptron machine for recognizing militarily interesting silhouetted targets  such as planes and ships  in aerial photos                         

Principles of Neurodynamics        edit 
Rosenblatt described his experiments with many variants of the Perceptron machine in a book Principles of Neurodynamics         The book is a published version of the      report             
Among the variants are 

 cross coupling   connections between units within the same layer  with possibly closed loops 
 back coupling   connections from units in a later layer to units in a previous layer  
four layer perceptrons where the last two layers have adjustible weights  and thus a proper multilayer perceptron  
incorporating time delays to perceptron units  to allow for processing sequential data 
analyzing audio  instead of images  
The machine was shipped from Cornell to Smithsonian in       under a government transfer administered by the Office of Naval Research            

Perceptrons        edit 
Main article  Perceptrons  book 
Although the perceptron initially seemed promising  it was quickly proved that perceptrons could not be trained to recognise many classes of patterns  This caused the field of neural network research to stagnate for many years  before it was recognised that a feedforward neural network with two or more layers  also called a multilayer perceptron  had greater processing power than perceptrons with one layer  also called a single layer perceptron  
Single layer perceptrons are only capable of learning linearly separable patterns              For a classification task with some step activation function  a single node will have a single line dividing the data points forming the patterns  More nodes can create more dividing lines  but those lines must somehow be combined to form more complex classifications  A second layer of perceptrons  or even linear nodes  are sufficient to solve many otherwise non separable problems 
In       a famous book entitled Perceptrons by Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function  It is often incorrectly believed that they also conjectured that a similar result would hold for a multi layer perceptron network  However  this is not true  as both Minsky and Papert already knew that multi layer perceptrons were capable of producing an XOR function   See the page on Perceptrons  book  for more information   Nevertheless  the often miscited Minsky and Papert text caused a significant decline in interest and funding of neural network research  It took ten more years until neural network research experienced a resurgence in the     s                  verification needed      This text was reprinted in      as  Perceptrons   Expanded Edition  where some errors in the original text are shown and corrected 

Subsequent work edit 
Rosenblatt continued working on perceptrons despite diminishing funding  The last attempt was Tobermory  built between      and       built for speech recognition              It occupied an entire room              It had   layers with        weights implemented by toroidal magnetic cores  By the time of its completion  simulation on digital computers had become faster than purpose built perceptron machines              He died in a boating accident in      

Isometric view of Tobermory Phase I             
The kernel perceptron algorithm was already introduced in      by Aizerman et al              Margin bounds guarantees were given for the Perceptron algorithm in the general non separable case first by Freund and Schapire                    and more recently by Mohri and Rostamizadeh        who extend previous results and give new and more favorable L  bounds                         
The perceptron is a simplified model of a biological neuron  While the complexity of biological neuron models is often required to fully understand neural behavior  research suggests a perceptron like linear model can produce some behavior seen in real neurons             
The solution spaces of decision boundaries for all binary functions and learning behaviors are studied in             

Definition edit 
The appropriate weights are applied to the inputs  and the resulting weighted sum passed to a function that produces the output o In the modern sense  the perceptron is an algorithm for learning a binary classifier called a threshold function  a function that maps its input 
  
    
      
        
          x
        
      
    
      displaystyle  mathbf  x   
  
  a real valued vector  to an output value 
  
    
      
        f
         
        
          x
        
         
      
    
      displaystyle f  mathbf  x    
  
  a single binary value  

  
    
      
        f
         
        
          x
        
         
         
        h
         
        
          w
        
          x  c  
        
          x
        
         
        b
         
      
    
      displaystyle f  mathbf  x    h  mathbf  w   cdot  mathbf  x   b  
  

where 
  
    
      
        h
      
    
      displaystyle h 
  
 is the Heaviside step function  where an input of 
  
    
      
         gt 
         
      
    
      textstyle  gt   
  
 outputs    otherwise   is the output    
  
    
      
        
          w
        
      
    
      displaystyle  mathbf  w   
  
 is a vector of real valued weights  
  
    
      
        
          w
        
          x  c  
        
          x
        
      
    
      displaystyle  mathbf  w   cdot  mathbf  x   
  
 is the dot product 
  
    
      
        
            x     
          
            i
             
             
          
          
            m
          
        
        
          w
          
            i
          
        
        
          x
          
            i
          
        
      
    
      textstyle  sum   i     m w  i x  i  
  
  where m is the number of inputs to the perceptron  and b is the bias  The bias shifts the decision boundary away from the origin and does not depend on any input value 
Equivalently  since 
  
    
      
        
          w
        
          x  c  
        
          x
        
         
        b
         
         
        
          w
        
         
        b
         
          x  c  
         
        
          x
        
         
         
         
      
    
      displaystyle  mathbf  w   cdot  mathbf  x   b   mathbf  w   b  cdot   mathbf  x      
  
  we can add the bias term 
  
    
      
        b
      
    
      displaystyle b 
  
 as another weight 
  
    
      
        
          
            w
          
          
            m
             
             
          
        
      
    
      displaystyle  mathbf  w    m    
  
 and add a coordinate 
  
    
      
         
      
    
      displaystyle   
  
 to each input 
  
    
      
        
          x
        
      
    
      displaystyle  mathbf  x   
  
  and then write it as a linear classifier that passes the origin 
  
    
      
        f
         
        
          x
        
         
         
        h
         
        
          w
        
          x  c  
        
          x
        
         
      
    
      displaystyle f  mathbf  x    h  mathbf  w   cdot  mathbf  x    
  

The binary value of 
  
    
      
        f
         
        
          x
        
         
      
    
      displaystyle f  mathbf  x    
  
    or    is used to perform binary classification on 
  
    
      
        
          x
        
      
    
      displaystyle  mathbf  x   
  
 as either a positive or a negative instance  Spatially  the bias shifts the position  though not the orientation  of the planar decision boundary 
In the context of neural networks  a perceptron is an artificial neuron using the Heaviside step function as the activation function  The perceptron algorithm is also termed the single layer perceptron  to distinguish it from a multilayer perceptron  which is a misnomer for a more complicated neural network   As a linear classifier  the single layer perceptron is the simplest feedforward neural network 

Power of representation edit 
Information theory edit 
From an information theory point of view  a single perceptron with K inputs has a capacity of  K bits of information              This result is due to Thomas Cover             
Specifically let 
  
    
      
        T
         
        N
         
        K
         
      
    
      displaystyle T N K  
  
 be the number of ways to linearly separate N points in K dimensions  then
  
    
      
        T
         
        N
         
        K
         
         
        
           
          
            
              
                
                  
                     
                    
                      N
                    
                  
                
                
                  K
                    x     
                  N
                
              
              
                
                   
                  
                      x     
                    
                      k
                       
                       
                    
                    
                      K
                        x     
                       
                    
                  
                  
                     
                    
                      
                        
                          
                            N
                              x     
                             
                          
                        
                        
                          
                            k
                          
                        
                      
                    
                     
                  
                
                
                  K
                   lt 
                  N
                
              
            
          
          
        
      
    
      displaystyle T N K   left    begin array  cc    N  amp K geq N    sum   k     K    left   begin array  c N    k end array   right  amp K lt N end array   right  
  
When K is large  
  
    
      
        T
         
        N
         
        K
         
        
           
        
        
           
          
            N
          
        
      
    
      displaystyle T N K     N  
  
 is very close to one when 
  
    
      
        N
          x     
         
        K
      
    
      displaystyle N leq  K 
  
  but very close to zero when 
  
    
      
        N
         gt 
         
        K
      
    
      displaystyle N gt  K 
  
  In words  one perceptron unit can almost certainly memorize a random assignment of binary labels on N points when 
  
    
      
        N
          x     
         
        K
      
    
      displaystyle N leq  K 
  
  but almost certainly not when 
  
    
      
        N
         gt 
         
        K
      
    
      displaystyle N gt  K 
  
 

Boolean function edit 
When operating on only binary inputs  a perceptron is called a linearly separable Boolean function  or threshold Boolean function  The sequence of numbers of threshold Boolean functions on n inputs is OEIS A        The value is only known exactly up to 
  
    
      
        n
         
         
      
    
      displaystyle n   
  
 case  but the order of magnitude is known quite exactly  it has upper bound 
  
    
      
        
           
          
            
              n
              
                 
              
            
              x     
            n
            
              log
              
                 
              
            
              x     
            n
             
            O
             
            n
             
          
        
      
    
      displaystyle    n     n log     n O n   
  
 and lower bound 
  
    
      
        
           
          
            
              n
              
                 
              
            
              x     
            n
            
              log
              
                 
              
            
              x     
            n
              x     
            O
             
            n
             
          
        
      
    
      displaystyle    n     n log     n O n   
  
             
Any Boolean linear threshold function can be implemented with only integer weights  Furthermore  the number of bits necessary and sufficient for representing a single integer weight parameter is 
  
    
      
          x    
         
        n
        ln
          x     
        n
         
      
    
      displaystyle  Theta  n ln n  
  
             

Universal approximation theorem edit 
Main article  Universal approximation theorem
A single perceptron can learn to classify any half space  It cannot solve any linearly nonseparable vectors  such as the Boolean exclusive or problem  the famous  XOR problem   
A perceptron network with one hidden layer can learn to classify any compact subset arbitrarily closely  Similarly  it can also approximate any compactly supported continuous function arbitrarily closely  This is essentially a special case of the theorems by George Cybenko and Kurt Hornik 

Conjunctively local perceptron edit 
Main article  Perceptrons  book 
Perceptrons  Minsky and Papert        studied the kind of perceptron networks necessary to learn various Boolean functions 
Consider a perceptron network with 
  
    
      
        n
      
    
      displaystyle n 
  
 input units  one hidden layer  and one output  similar to the Mark I Perceptron machine  It computes a Boolean function of type 
  
    
      
        f
         
        
           
          
            n
          
        
          x     
         
      
    
      displaystyle f    n  to   
  
  They call a function conjunctively local of order 
  
    
      
        k
      
    
      displaystyle k 
  
  iff there exists a perceptron network such that each unit in the hidden layer connects to at most 
  
    
      
        k
      
    
      displaystyle k 
  
 input units 
Theorem   Theorem         The parity function is conjunctively local of order 
  
    
      
        n
      
    
      displaystyle n 
  
 
Theorem   Section       The connectedness function is conjunctively local of order 
  
    
      
          x a  
         
        
          n
          
             
            
               
            
             
          
        
         
      
    
      displaystyle  Omega  n        
  
 

Learning algorithm for a single layer perceptron edit 
A diagram showing a perceptron updating its linear boundary as more training examples are added
Below is an example of a learning algorithm for a single layer perceptron with a single output unit  For a single layer perceptron with multiple output units  since the weights of one output unit are completely separate from all the others   the same algorithm can be run for each output unit 
For multilayer perceptrons  where a hidden layer exists  more sophisticated algorithms such as backpropagation must be used  If the activation function or the underlying process being modeled by the perceptron is nonlinear  alternative learning algorithms such as the delta rule can be used as long as the activation function is differentiable  Nonetheless  the learning algorithm described in the steps below will often work  even for multilayer perceptrons with nonlinear activation functions 
When multiple perceptrons are combined in an artificial neural network  each output neuron operates independently of all the others  thus  learning each output can be considered in isolation 

Definitions edit 
We first define some variables 


  
    
      
        r
      
    
      displaystyle r 
  
 is the learning rate of the perceptron  Learning rate is a positive number usually chosen to be less than    The larger the value  the greater the chance for volatility in the weight changes 

  
    
      
        y
         
        f
         
        
          z
        
         
      
    
      displaystyle y f  mathbf  z    
  
 denotes the output from the perceptron for an input vector 
  
    
      
        
          z
        
      
    
      displaystyle  mathbf  z   
  
 

  
    
      
        D
         
         
         
        
          
            x
          
          
             
          
        
         
        
          d
          
             
          
        
         
         
          x     
         
         
        
          
            x
          
          
            s
          
        
         
        
          d
          
            s
          
        
         
         
      
    
      displaystyle D     mathbf  x       d       dots    mathbf  x    s  d  s     
  
 is the training set of 
  
    
      
        s
      
    
      displaystyle s 
  
 samples  where 

  
    
      
        
          
            x
          
          
            j
          
        
      
    
      displaystyle  mathbf  x    j  
  
 is the 
  
    
      
        n
      
    
      displaystyle n 
  
 dimensional input vector 

  
    
      
        
          d
          
            j
          
        
      
    
      displaystyle d  j  
  
 is the desired output value of the perceptron for that input 
We show the values of the features as follows 


  
    
      
        
          x
          
            j
             
            i
          
        
      
    
      displaystyle x  j i  
  
 is the value of the 
  
    
      
        i
      
    
      displaystyle i 
  
th feature of the 
  
    
      
        j
      
    
      displaystyle j 
  
th training input vector 

  
    
      
        
          x
          
            j
             
             
          
        
         
         
      
    
      displaystyle x  j      
  
 
To represent the weights  


  
    
      
        
          w
          
            i
          
        
      
    
      displaystyle w  i  
  
 is the 
  
    
      
        i
      
    
      displaystyle i 
  
th value in the weight vector  to be multiplied by the value of the 
  
    
      
        i
      
    
      displaystyle i 
  
th input feature 
Because 
  
    
      
        
          x
          
            j
             
             
          
        
         
         
      
    
      displaystyle x  j      
  
  the 
  
    
      
        
          w
          
             
          
        
      
    
      displaystyle w     
  
 is effectively a bias that we use instead of the bias constant 
  
    
      
        b
      
    
      displaystyle b 
  
 
To show the time dependence of 
  
    
      
        
          w
        
      
    
      displaystyle  mathbf  w   
  
  we use 


  
    
      
        
          w
          
            i
          
        
         
        t
         
      
    
      displaystyle w  i  t  
  
 is the weight 
  
    
      
        i
      
    
      displaystyle i 
  
 at time 
  
    
      
        t
      
    
      displaystyle t 
  
 
Steps edit 
Initialize the weights  Weights may be initialized to   or to a small random value  In the example below  we use   For each example j in our training set D  perform the following steps over the input 
  
    
      
        
          
            x
          
          
            j
          
        
      
    
      displaystyle  mathbf  x    j  
  
 and desired output 
  
    
      
        
          d
          
            j
          
        
      
    
      displaystyle d  j  
  
 
Calculate the actual output 

  
    
      
        
          
            
              
                
                  y
                  
                    j
                  
                
                 
                t
                 
              
              
                
                 
                f
                 
                
                  w
                
                 
                t
                 
                  x  c  
                
                  
                    x
                  
                  
                    j
                  
                
                 
              
            
            
              
              
                
                 
                f
                 
                
                  w
                  
                     
                  
                
                 
                t
                 
                
                  x
                  
                    j
                     
                     
                  
                
                 
                
                  w
                  
                     
                  
                
                 
                t
                 
                
                  x
                  
                    j
                     
                     
                  
                
                 
                
                  w
                  
                     
                  
                
                 
                t
                 
                
                  x
                  
                    j
                     
                     
                  
                
                 
                  x  ef 
                 
                
                  w
                  
                    n
                  
                
                 
                t
                 
                
                  x
                  
                    j
                     
                    n
                  
                
                 
              
            
          
        
      
    
      displaystyle   begin aligned y  j  t  amp  f  mathbf  w   t  cdot  mathbf  x    j     amp  f w     t x  j    w     t x  j    w     t x  j     dotsb  w  n  t x  j n   end aligned   
  
Update the weights 

  
    
      
        
          w
          
            i
          
        
         
        t
         
         
         
         
        
          w
          
            i
          
        
         
        t
         
        
        
           
        
        
        r
          x  c  
         
        
          d
          
            j
          
        
          x     
        
          y
          
            j
          
        
         
        t
         
         
        
          x
          
            j
             
            i
          
        
      
    
      displaystyle w  i  t    w  i  t     boldsymbol       r cdot  d  j  y  j  t  x  j i  
  
  for all features 
  
    
      
         
          x     
        i
          x     
        n
      
    
      displaystyle   leq i leq n 
  
  
  
    
      
        r
      
    
      displaystyle r 
  
 is the learning rate For offline learning  the second step may be repeated until the iteration error 
  
    
      
        
          
             
            s
          
        
        
            x     
          
            j
             
             
          
          
            s
          
        
        
           
        
        
          d
          
            j
          
        
          x     
        
          y
          
            j
          
        
         
        t
         
        
           
        
      
    
      displaystyle   frac     s   sum   j     s  d  j  y  j  t   
  
 is less than a user specified error threshold 
  
    
      
          x b  
      
    
      displaystyle  gamma  
  
  or a predetermined number of iterations have been completed  where s is again the size of the sample set 
The algorithm updates the weights after every training sample in step  b 

Convergence of one perceptron on a linearly separable dataset edit 
Illustration of the perceptron convergence  In the picture  
  
    
      
          x b  
         
            
         
        R
         
         
         
        r
         
         
      
    
      displaystyle  gamma       R   r   
  
  All data points have 
  
    
      
        y
         
         
         
      
    
      displaystyle y    
  
  since the negative samples are equivalent to 
  
    
      
        y
         
         
         
      
    
      displaystyle y    
  
 after reflection through the origin  As the learning proceeds  the weight vector performs a somewhat random walk in the space of weights  Each step is at least    degrees away from its current direction  thus increasing its norm square by at most 
  
    
      
        R
      
    
      displaystyle R 
  
  Each step adds to 
  
    
      
        w
      
    
      displaystyle w 
  
 by a point in the samples  and since all the samples have 
  
    
      
        
          x
          
             
          
        
          x     
            
      
    
      displaystyle x     geq      
  
  the weight vector must move along 
  
    
      
        
          x
          
             
          
        
      
    
      displaystyle x     
  
 by at least 
  
    
      
            
      
    
      displaystyle      
  
  Since the norm grows like 
  
    
      
        
          
            t
          
        
      
    
      displaystyle   sqrt  t   
  
 but the 
  
    
      
        
          x
          
             
          
        
      
    
      displaystyle x     
  
 component grows like 
  
    
      
        t
      
    
      displaystyle t 
  
  this would eventually force the weight vector to point almost entirely in the 
  
    
      
        
          x
          
             
          
        
      
    
      displaystyle x     
  
 direction  and thus achieve convergence 
A single perceptron is a linear classifier  It can only reach a stable state if all input vectors are classified correctly  In case the training set D is not linearly separable  i e  if the positive examples cannot be separated from the negative examples by a hyperplane  then the algorithm would not converge since there is no solution  Hence  if linear separability of the training set is not known a priori  one of the training variants below should be used  Detailed analysis and extensions to the convergence theorem are in Chapter    of Perceptrons        
Linear separability is testable in time 
  
    
      
        min
         
        O
         
        
          n
          
            d
            
               
            
             
          
        
         
         
        O
         
        
          d
          
             
            n
          
        
         
         
        O
         
        
          n
          
            d
              x     
             
          
        
        ln
          x     
        n
         
         
      
    
      displaystyle  min O n  d     O d   n   O n  d    ln n   
  
  where 
  
    
      
        n
      
    
      displaystyle n 
  
 is the number of data points  and 
  
    
      
        d
      
    
      displaystyle d 
  
 is the dimension of each point             
If the training set is linearly separable  then the perceptron is guaranteed to converge after making finitely many mistakes              The theorem is proved by Rosenblatt et al 


Perceptron convergence theorem Given a dataset 
  
    
      
        D
      
    
      textstyle D 
  
  such that 
  
    
      
        
          max
          
             
            x
             
            y
             
              x     
            D
          
        
          x     
        x
        
            x     
          
             
          
        
         
        R
      
    
      textstyle  max    x y  in D   x       R 
  
  and it is linearly separable by some unit vector 
  
    
      
        
          w
          
              x     
          
        
      
    
      textstyle w     
  
  with margin 
  
    
      
          x b  
      
    
      textstyle  gamma  
  
  
  
    
      
          x b  
          
        
          min
          
             
            x
             
            y
             
              x     
            D
          
        
        y
         
        
          w
          
              x     
          
        
          x  c  
        x
         
      
    
      displaystyle  gamma    min    x y  in D y w     cdot x  
  

Then the perceptron     learning algorithm converges after making at most 
  
    
      
         
        R
        
           
        
          x b  
        
           
          
             
          
        
      
    
      textstyle  R  gamma       
  
 mistakes  for any learning rate  and any method of sampling from the dataset 

The following simple proof is due to Novikoff         The idea of the proof is that the weight vector is always adjusted by a bounded amount in a direction with which it has a negative dot product  and thus can be bounded above by O        t   where t is the number of changes to the weight vector  However  it can also be bounded below by O t  because if there exists an  unknown  satisfactory weight vector  then every change makes progress in this  unknown  direction by a positive amount that depends only on the input vector Proof
Suppose at step 
  
    
      
        t
      
    
      textstyle t 
  
  the perceptron with weight 
  
    
      
        
          w
          
            t
          
        
      
    
      textstyle w  t  
  
 makes a mistake on data point 
  
    
      
         
        x
         
        y
         
      
    
      textstyle  x y  
  
  then it updates to 
  
    
      
        
          w
          
            t
             
             
          
        
         
        
          w
          
            t
          
        
         
        r
         
        y
          x     
        
          f
          
            
              w
              
                t
              
            
          
        
         
        x
         
         
        x
      
    
      textstyle w  t    w  t  r y f  w  t   x  x 
  
 
If 
  
    
      
        y
         
         
      
    
      textstyle y   
  
  the argument is symmetric  so we omit it 
WLOG  
  
    
      
        y
         
         
      
    
      textstyle y   
  
  then 
  
    
      
        
          f
          
            
              w
              
                t
              
            
          
        
         
        x
         
         
         
      
    
      textstyle f  w  t   x    
  
  
  
    
      
        
          f
          
            
              w
              
                  x     
              
            
          
        
         
        x
         
         
         
      
    
      textstyle f  w      x    
  
  and 
  
    
      
        
          w
          
            t
             
             
          
        
         
        
          w
          
            t
          
        
         
        r
        x
      
    
      textstyle w  t    w  t  rx 
  
 
By assumption  we have separation with margins  
  
    
      
        
          w
          
              x     
          
        
          x  c  
        x
          x     
          x b  
      
    
      displaystyle w     cdot x geq  gamma  
  
 Thus 

  
    
      
        
          w
          
              x     
          
        
          x  c  
        
          w
          
            t
             
             
          
        
          x     
        
          w
          
              x     
          
        
          x  c  
        
          w
          
            t
          
        
         
        
          w
          
              x     
          
        
          x  c  
         
        r
        x
         
          x     
        r
          x b  
      
    
      displaystyle w     cdot w  t    w     cdot w  t  w     cdot  rx  geq r gamma  
  

Also 
  
    
      
          x     
        
          w
          
            t
             
             
          
        
        
            x     
          
             
          
          
             
          
        
          x     
          x     
        
          w
          
            t
          
        
        
            x     
          
             
          
          
             
          
        
         
          x     
        
          w
          
            t
          
        
         
        r
        x
        
            x     
          
             
          
          
             
          
        
          x     
          x     
        
          w
          
            t
          
        
        
            x     
          
             
          
          
             
          
        
         
         
        r
         
        
          w
          
            t
          
        
          x  c  
        x
         
         
        
          r
          
             
          
        
          x     
        x
        
            x     
          
             
          
          
             
          
        
      
    
      displaystyle   w  t                w  t              w  t  rx             w  t             r w  t  cdot x  r      x           
  
 and since the perceptron made a mistake  
  
    
      
        
          w
          
            t
          
        
          x  c  
        x
          x     
         
      
    
      textstyle w  t  cdot x leq   
  
  and so

  
    
      
          x     
        
          w
          
            t
             
             
          
        
        
            x     
          
             
          
          
             
          
        
          x     
          x     
        
          w
          
            t
          
        
        
            x     
          
             
          
          
             
          
        
          x     
          x     
        x
        
            x     
          
             
          
          
             
          
        
          x     
        
          r
          
             
          
        
        
          R
          
             
          
        
      
    
      displaystyle   w  t                w  t            leq   x           leq r    R     
  

Since we started with 
  
    
      
        
          w
          
             
          
        
         
         
      
    
      textstyle w       
  
  after making 
  
    
      
        N
      
    
      textstyle N 
  
 mistakes  
  
    
      
          x     
        w
        
            x     
          
             
          
        
          x     
        
          
            N
            
              r
              
                 
              
            
            
              R
              
                 
              
            
          
        
      
    
      displaystyle   w       leq   sqrt  Nr    R       
  
 but also

  
    
      
          x     
        w
        
            x     
          
             
          
        
          x     
        w
          x  c  
        
          w
          
              x     
          
        
          x     
        N
        r
          x b  
      
    
      displaystyle   w       geq w cdot w     geq Nr gamma  
  

Combining the two  we have 
  
    
      
        N
          x     
         
        R
        
           
        
          x b  
        
           
          
             
          
        
      
    
      textstyle N leq  R  gamma       
  



Two classes of points  and two of the infinitely many linear boundaries that separate them  Even though the boundaries are at nearly right angles to one another  the perceptron algorithm has no way of choosing between them 
While the perceptron algorithm is guaranteed to converge on some solution in the case of a linearly separable training set  it may still pick any solution and problems may admit many solutions of varying quality              The perceptron of optimal stability  nowadays better known as the linear support vector machine  was designed to solve this problem  Krauth and Mezard                    

Perceptron cycling theorem edit 
When the dataset is not linearly separable  then there is no way for a single perceptron to converge  However  we still have            


Perceptron cycling theorem If the dataset 
  
    
      
        D
      
    
      displaystyle D 
  
 has only finitely many points  then there exists an upper bound number 
  
    
      
        M
      
    
      displaystyle M 
  
  such that for any starting weight vector 
  
    
      
        
          w
          
             
          
        
      
    
      displaystyle w     
  
 all weight vector 
  
    
      
        
          w
          
            t
          
        
      
    
      displaystyle w  t  
  
 has norm bounded by 
  
    
      
          x     
        
          w
          
            t
          
        
          x     
          x     
          x     
        
          w
          
             
          
        
          x     
         
        M
      
    
      displaystyle   w  t    leq   w       M 
  


This is proved first by Bradley Efron             
Learning a Boolean function edit 
Consider a dataset where the 
  
    
      
        x
      
    
      displaystyle x 
  
 are from 
  
    
      
         
          x     
         
         
         
         
        
           
          
            n
          
        
      
    
      displaystyle            n  
  
  that is  the vertices of an n dimensional hypercube centered at origin  and 
  
    
      
        y
         
          x b  
         
        
          x
          
            i
          
        
         
      
    
      displaystyle y  theta  x  i   
  
  That is  all data points with positive 
  
    
      
        
          x
          
            i
          
        
      
    
      displaystyle x  i  
  
 have 
  
    
      
        y
         
         
      
    
      displaystyle y   
  
  and vice versa  By the perceptron convergence theorem  a perceptron would converge after making at most 
  
    
      
        n
      
    
      displaystyle n 
  
 mistakes 
If we were to write a logical program to perform the same task  each positive example shows that one of the coordinates is the right one  and each negative example shows that its complement is a positive example  By collecting all the known positive examples  we eventually eliminate all but one coordinate  at which point the dataset is learned             
This bound is asymptotically tight in terms of the worst case  In the worst case  the first presented example is entirely new  and gives 
  
    
      
        n
      
    
      displaystyle n 
  
 bits of information  but each subsequent example would differ minimally from previous examples  and gives   bit each  After 
  
    
      
        n
         
         
      
    
      displaystyle n   
  
 examples  there are 
  
    
      
         
        n
      
    
      displaystyle  n 
  
 bits of information  which is sufficient for the perceptron  with 
  
    
      
         
        n
      
    
      displaystyle  n 
  
 bits of information              
However  it is not tight in terms of expectation if the examples are presented uniformly at random  since the first would give 
  
    
      
        n
      
    
      displaystyle n 
  
 bits  the second 
  
    
      
        n
        
           
        
         
      
    
      displaystyle n   
  
 bits  and so on  taking 
  
    
      
        O
         
        ln
          x     
        n
         
      
    
      displaystyle O  ln n  
  
 examples in total             

Variants edit 
The pocket algorithm with ratchet  Gallant        solves the stability problem of perceptron learning by keeping the best solution seen so far  in its pocket   The pocket algorithm then returns the solution in the pocket  rather than the last solution  It can be used also for non separable data sets  where the aim is to find a perceptron with a small number of misclassifications  However  these solutions appear purely stochastically and hence the pocket algorithm neither approaches them gradually in the course of learning  nor are they guaranteed to show up within a given number of learning steps 
The Maxover algorithm  Wendemuth        is  robust  in the sense that it will converge regardless of  prior  knowledge of linear separability of the data set              In the linearly separable case  it will solve the training problem   if desired  even with optimal stability  maximum margin between the classes   For non separable data sets  it will return a solution with a computable small number of misclassifications              In all cases  the algorithm gradually approaches the solution in the course of learning  without memorizing previous states and without stochastic jumps  Convergence is to global optimality for separable data sets and to local optimality for non separable data sets 
The Voted Perceptron  Freund and Schapire         is a variant using multiple weighted perceptrons  The algorithm starts a new perceptron every time an example is wrongly classified  initializing the weights vector with the final weights of the last perceptron  Each perceptron will also be given another weight corresponding to how many examples do they correctly classify before wrongly classifying one  and at the end the output will be a weighted vote on all perceptrons 
In separable problems  perceptron training can also aim at finding the largest separating margin between the classes  The so called perceptron of optimal stability can be determined by means of iterative training and optimization schemes  such as the Min Over algorithm  Krauth and Mezard                     or the AdaTron  Anlauf and Biehl                      AdaTron uses the fact that the corresponding quadratic optimization problem is convex  The perceptron of optimal stability  together with the kernel trick  are the conceptual foundations of the support vector machine 
The 
  
    
      
          x b  
      
    
      displaystyle  alpha  
  
 perceptron further used a pre processing layer of fixed random weights  with thresholded output units  This enabled the perceptron to classify analogue patterns  by projecting them into a binary space  In fact  for a projection space of sufficiently high dimension  patterns can become linearly separable 
Another way to solve nonlinear problems without using multiple layers is to use higher order networks  sigma pi unit   In this type of network  each element in the input vector is extended with each pairwise combination of multiplied inputs  second order   This can be extended to an n order network 
It should be kept in mind  however  that the best classifier is not necessarily that which classifies all the training data perfectly  Indeed  if we had the prior constraint that the data come from equi variant Gaussian distributions  the linear separation in the input space is optimal  and the nonlinear solution is overfitted 
Other linear classification algorithms include Winnow  support vector machine  and logistic regression 

Multiclass perceptron edit 
Like most other techniques for training linear classifiers  the perceptron generalizes naturally to multiclass classification   Here  the input 
  
    
      
        x
      
    
      displaystyle x 
  
 and the output 
  
    
      
        y
      
    
      displaystyle y 
  
 are drawn from arbitrary sets  A feature representation function 
  
    
      
        f
         
        x
         
        y
         
      
    
      displaystyle f x y  
  
 maps each possible input output pair to a finite dimensional real valued feature vector   As before  the feature vector is multiplied by a weight vector 
  
    
      
        w
      
    
      displaystyle w 
  
  but now the resulting score is used to choose among many possible outputs 


  
    
      
        
          
            
              y
                x e 
            
          
        
         
        
          argmax
          
            y
          
        
          x     
        f
         
        x
         
        y
         
          x  c  
        w
         
      
    
      displaystyle   hat  y    operatorname  argmax    y f x y  cdot w  
  

Learning again iterates over the examples  predicting an output for each  leaving the weights unchanged when the predicted output matches the target  and changing them when it does not   The update becomes 


  
    
      
        
          w
          
            t
             
             
          
        
         
        
          w
          
            t
          
        
         
        f
         
        x
         
        y
         
          x     
        f
         
        x
         
        
          
            
              y
                x e 
            
          
        
         
         
      
    
      displaystyle w  t    w  t  f x y  f x   hat  y     
  

This multiclass feedback formulation reduces to the original perceptron when 
  
    
      
        x
      
    
      displaystyle x 
  
 is a real valued vector  
  
    
      
        y
      
    
      displaystyle y 
  
 is chosen from 
  
    
      
         
         
         
         
         
      
    
      displaystyle         
  
  and 
  
    
      
        f
         
        x
         
        y
         
         
        y
        x
      
    
      displaystyle f x y  yx 
  
 
For certain problems  input output representations and features can be chosen so that 
  
    
      
        
          
            a
            r
            g
            m
            a
            x
          
          
            y
          
        
        f
         
        x
         
        y
         
          x  c  
        w
      
    
      displaystyle  mathrm  argmax    y f x y  cdot w 
  
 can be found efficiently even though 
  
    
      
        y
      
    
      displaystyle y 
  
 is chosen from a very large or even infinite set 
Since       perceptron training has become popular in the field of natural language processing for such tasks as part of speech tagging and syntactic parsing  Collins         It has also been applied to large scale machine learning problems in a distributed computing setting             

References edit 


  a b Freund  Y   Schapire  R  E           Large margin classification using the perceptron algorithm   PDF   Machine Learning                   doi         A                S CID              

  a b Bishop  Christopher M          Pattern Recognition and Machine Learning  Springer  ISBN                    

  Hecht Nielsen  Robert         Neurocomputing  Reprint  with corrections      ed    Reading  Mass   Menlo Park  Calif   New York  etc    Addison Wesley  p     Figure     caption  ISBN                        

  Block  H  D                 The Perceptron  A Model for Brain Functioning  I   Reviews of Modern Physics                   Bibcode     RvMP          B  doi         RevModPhys         ISSN                

  McCulloch  W  Pitts  W          A Logical Calculus of Ideas Immanent in Nervous Activity   Bulletin of Mathematical Biophysics                  doi         BF         

  a b Rosenblatt  Frank          The Perceptron a perceiving and recognizing automaton   PDF   Report           Cornell Aeronautical Laboratory 

  Rosenblatt  Frank  March         Perceptron Simulation Experiments   Proceedings of the IRE                   doi         JRPROC              ISSN                

  a b Nilsson  Nils J                  Perceptrons   The Quest for Artificial Intelligence  Cambridge  Cambridge University Press  ISBN                        

  a b O Connor  Jack                Undercover Algorithm  A Secret Chapter in the Early History of Artificial Intelligence and Satellite Imagery   International Journal of Intelligence and CounterIntelligence        doi                                ISSN                 S CID                

  Rosenblatt  F           The perceptron  A probabilistic model for information storage and organization in the brain   Psychological Review                   doi         h         ISSN                 PMID               

  Frank Rosenblatt   Two Theorems of Statistical Separability in the Perceptron   Symposium on the Mechanization of Thought  National Physical Laboratory  Teddington  UK  November       vol     H  M  Stationery Office  London       

  Rosenblatt  Frank  and CORNELL UNIV ITHACA NY  Cognitive Systems Research Program  Technical report  Cornell University           

  Muerle  John Ludwig  and CORNELL AERONAUTICAL LAB INC BUFFALO NY  Project Para  Perceiving and Recognition Automata  Cornell Aeronautical Laboratory  Incorporated       

  Penn  Jonathan               Inventing Intelligence  On the History of Complex Information Processing and Artificial Intelligence in the United States in the Mid Twentieth Century  Thesis    object Object   doi          cam       

  Guice  Jon          Controversy and the State  Lord ARPA and Intelligent Computing   Social Studies of Science                   doi                             ISSN                 JSTOR              PMID               

  a b c Hay  John Cameron         Mark I perceptron operators  manual  Project PARA     PDF   Buffalo  Cornell Aeronautical Laboratory  Archived from the original  PDF  on            

   Perceptron  Mark I   National Museum of American History  Retrieved            

  Anderson  James A   Rosenfeld  Edward  eds          Talking Nets  An Oral History of Neural Networks  The MIT Press  doi         mitpress                ISBN                        

  Olazaran  Mikel          A Sociological Study of the Official History of the Perceptrons Controversy   Social Studies of Science                   doi                             JSTOR              S CID               

   Perception Concepts to Photo Interpretation   www cia gov  Retrieved            

  Irwin  Julia A                 Artificial Worlds and Perceptronic Objects  The CIA s Mid century Automatic Target Recognition   Grey Room             doi         grey a        ISSN                

  Principles of neurodynamics  Perceptrons and the theory of brain mechanisms  by Frank Rosenblatt  Report Number VG      G    Cornell Aeronautical Laboratory  published on    March       The work reported in this volume has been carried out under Contract Nonr            Project PARA  at C A L  and Contract Nonr          at Cornell Univensity 

  a b Sejnowski  Terrence J          The Deep Learning Revolution  MIT Press  p           ISBN                        

  Rosenblatt  Frank          A Description of the Tobermory Perceptron   Cognitive Research Program  Report No     Collected Technical Papers  Vol     Edited by Frank Rosenblatt  Ithaca  NY  Cornell University 

  a b Nagy  George        System and circuit designs for the Tobermory perceptron  Technical report number    Cognitive Systems Research Program  Cornell University  Ithaca New York 

  Nagy  George   Neural networks then and now   IEEE Transactions on Neural Networks                     

  Aizerman  M  A   Braverman  E  M   Rozonoer  L  I           Theoretical foundations of the potential function method in pattern recognition learning   Automation and Remote Control              

  Mohri  Mehryar  Rostamizadeh  Afshin          Perceptron Mistake Bounds   arXiv            cs LG  

      Foundations of Machine Learning  MIT Press  Chapter    

  Cash  Sydney  Yuste  Rafael          Linear Summation of Excitatory Inputs by CA  Pyramidal Neurons   Neuron                   doi         S                      PMID               

  Liou  D  R   Liou  J  W   Liou  C  Y          Learning Behaviors of Perceptron  iConcept Press  ISBN                        

  a b MacKay  David               Information Theory  Inference and Learning Algorithms  Cambridge University Press  p            ISBN                    

  Cover  Thomas M   June         Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition   IEEE Transactions on Electronic Computers  EC                  doi         PGEC              ISSN                

  a b   ma  Ji    Orponen  Pekka                General Purpose Computation with Neural Networks  A Survey of Complexity Theoretic Results   Neural Computation                      doi                             ISSN                 PMID               

   Introduction to Machine Learning  Chapter    Perceptron   openlearninglibrary mit edu  Retrieved            

  Novikoff  Albert J           On convergence proofs for perceptrons   Office of Naval Research 

  Bishop  Christopher M                Chapter    Linear Models for Classification   Pattern Recognition and Machine Learning  Springer Science Business Media  LLC  p            ISBN                       

  a b Krauth  W   Mezard  M           Learning algorithms with optimal stability in neural networks   Journal of Physics A  Mathematical and General           L      L     Bibcode     JPhA     L    K  doi                             

  Block  H  D   Levin  S  A           On the boundedness of an iterative procedure for solving a system of linear inequalities   Proceedings of the American Mathematical Society                   doi         S                          ISSN                

  Efron  Bradley   The perceptron correction procedure in nonseparable situations   Rome Air Dev  Center Tech  Doc  Rept        

  a b Simon  Herbert A   Laird  John E                 Limits on Speed of Concept Attainment   The Sciences of the Artificial  reissue of the third edition with a new introduction by John Laird  Reissue      ed    Cambridge  Massachusetts London  England  The MIT Press  ISBN                        

  Wendemuth  A           Learning the Unlearnable   Journal of Physics A  Mathematical and General                      Bibcode     JPhA          W  doi                             

  Wendemuth  A           Performance of robust training algorithms for neural networks   Journal of Physics A  Mathematical and General                      Bibcode     JPhA          W  doi                             

  Anlauf  J  K   Biehl  M           The AdaTron  an Adaptive Perceptron algorithm   Europhysics Letters                   Bibcode     EL            A  doi                             S CID                

  McDonald  R   Hall  K   Mann  G           Distributed Training Strategies for the Structured Perceptron   PDF   Human Language Technologies  The      Annual Conference of the North American Chapter of the ACL  Association for Computational Linguistics  pp               


Further reading edit 
Aizerman  M  A  and Braverman  E  M  and Lev I  Rozonoer  Theoretical foundations of the potential function method in pattern recognition learning  Automation and Remote Control                   
Rosenblatt  Frank         The Perceptron  A Probabilistic Model for Information Storage and Organization in the Brain  Cornell Aeronautical Laboratory  Psychological Review  v    No     pp                doi         h        
Rosenblatt  Frank         Principles of Neurodynamics  Washington  DC  Spartan Books 
Minsky  M  L  and Papert  S  A        Perceptrons  Cambridge  MA  MIT Press 
Gallant  S  I          Perceptron based learning algorithms  IEEE Transactions on Neural Networks  vol     no     pp               
Olazaran Rodriguez  Jose Miguel  A historical sociology of neural network research  PhD Dissertation  University of Edinburgh       
Mohri  Mehryar and Rostamizadeh  Afshin         Perceptron Mistake Bounds arXiv                 
Novikoff  A  B          On convergence proofs on perceptrons  Symposium on the Mathematical Theory of Automata               Polytechnic Institute of Brooklyn 
Widrow  B   Lehr  M A       years of Adaptive Neural Networks  Perceptron  Madaline  and Backpropagation   Proc  IEEE  vol     no    pp                         
Collins  M        Discriminative training methods for hidden Markov models  Theory and experiments with the perceptron algorithm in Proceedings of the Conference on Empirical Methods in Natural Language Processing  EMNLP      
Yin  Hongfeng         Perceptron Based Algorithms and Analysis  Spectrum Library  Concordia University  Canada
External links edit 
A Perceptron implemented in MATLAB to learn binary NAND function
Chapter   Weighted networks   the perceptron and chapter   Perceptron learning of Neural Networks   A Systematic Introduction by Ra l Rojas  ISBN                        
History of perceptrons
Mathematics of multilayer perceptrons
Applying a perceptron model using scikit learn   https   scikit learn org stable modules generated sklearn linear model Perceptron html
vteDifferentiable computingGeneral
Differentiable programming
Information geometry
Statistical manifold
Automatic differentiation
Neuromorphic computing
Pattern recognition
Ricci calculus
Computational learning theory
Inductive bias
Hardware
IPU
TPU
VPU
Memristor
SpiNNaker
Software libraries
TensorFlow
PyTorch
Keras
scikit learn
Theano
JAX
Flux jl
MindSpore

 Portals
Computer programming
Technology

Authority control databases  National GermanyUnited StatesJapanIsrael





Retrieved from  https   en wikipedia org w index php title Perceptron amp oldid