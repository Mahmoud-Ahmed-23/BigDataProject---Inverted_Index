Tasks in machine learning
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte

In machine learning  a common task is the study and construction of algorithms that can learn from and make predictions on data             Such algorithms function by making data driven predictions or decisions             through building a mathematical model from input data  These input data used to build the model are usually divided into multiple data sets  In particular  three data sets are commonly used in different stages of the creation of the model  training  validation  and test sets 
The model is initially fit on a training data set             which is a set of examples used to fit the parameters  e g  weights of connections between neurons in artificial neural networks  of the model             The model  e g  a naive Bayes classifier  is trained on the training data set using a supervised learning method  for example using optimization methods such as gradient descent or stochastic gradient descent  In practice  the training data set often consists of pairs of an input vector  or scalar  and the corresponding output vector  or scalar   where the answer key is commonly denoted as the target  or label   The current model is run with the training data set and produces a result  which is then compared with the target  for each input vector in the training data set  Based on the result of the comparison and the specific learning algorithm being used  the parameters of the model are adjusted  The model fitting can include both variable selection and parameter estimation 
Successively  the fitted model is used to predict the responses for the observations in a second data set called the validation data set             The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model s hyperparameters             e g  the number of hidden units layers and layer widths in a neural network              Validation data sets can be used for regularization by early stopping  stopping training when the error on the validation data set increases  as this is a sign of over fitting to the training data set             
This simple procedure is complicated in practice by the fact that the validation data set s error may fluctuate during training  producing multiple local minima  This complication has led to the creation of many ad hoc rules for deciding when over fitting has truly begun            
Finally  the test data set is a data set used to provide an unbiased evaluation of a final model fit on the training data set             If the data in the test data set has never been used in training  for example in cross validation   the test data set is also called a holdout data set  The term  validation set  is sometimes used instead of  test set  in some literature  e g   if the original data set was partitioned into only two subsets  the test set might be referred to as the validation set             
Deciding the sizes and strategies for data set division in training  test and validation sets is very dependent on the problem and data available            


Training data set edit 
Simplified example of training a neural network in object detection  The network is trained by multiple images that are known to depict starfish and sea urchins  which are correlated with  nodes  that represent visual features  The starfish match with a ringed texture and a star outline  whereas most sea urchins match with a striped texture and oval shape  However  the instance of a ring textured sea urchin creates a weakly weighted association between them Subsequent run of the network on an input image  left              The network correctly detects the starfish  However  the weakly weighted association between ringed texture and sea urchin also confers a weak signal to the latter from one of two intermediate nodes  In addition  a shell that was not included in the training gives a weak signal for the oval shape  also resulting in a weak signal for the sea urchin output  These weak signals may result in a false positive result for sea urchin In reality  textures and outlines would not be represented by single nodes  but rather by associated weight patterns of multiple nodes 
A training data set is a data set of examples used during the learning process and is used to fit the parameters  e g   weights  of  for example  a classifier                        
For classification tasks  a supervised learning algorithm looks at the training data set to determine  or learn  the optimal combinations of variables that will generate a good predictive model              The goal is to produce a trained  fitted  model that generalizes well to new  unknown data              The fitted model is evaluated using  new  examples from the held out data sets  validation and test data sets  to estimate the model s accuracy in classifying new data             To reduce the risk of issues such as over fitting  the examples in the validation and test data sets should not be used to train the model            
Most approaches that search through training data for empirical relationships tend to overfit the data  meaning that they can identify and exploit apparent relationships in the training data that do not hold in general 
When a training set is continuously expanded with new data  then this is incremental learning 

Validation data set edit 
A validation data set is a data set of examples used to tune the hyperparameters  i e  the architecture  of a model  It is sometimes also called the development set or the  dev set               An example of a hyperparameter for artificial neural networks includes the number of hidden units in each layer                         It  as well as the testing set  as mentioned below   should follow the same probability distribution as the training data set 
In order to avoid overfitting  when any classification parameter needs to be adjusted  it is necessary to have a validation data set in addition to the training and test data sets  For example  if the most suitable classifier for the problem is sought  the training data set is used to train the different candidate classifiers  the validation data set is used to compare their performances and decide which one to take and  finally  the test data set is used to obtain the performance characteristics such as accuracy  sensitivity  specificity  F measure  and so on  The validation data set functions as a hybrid  it is training data used for testing  but neither as part of the low level training nor as part of the final testing 
The basic process of using a validation data set for model selection  as part of training data set  validation data set  and test data set  is                         


Since our goal is to find the network having the best performance on new data  the simplest approach to the comparison of different networks is to evaluate the error function using data which is independent of that used for training  Various networks are trained by minimization of an appropriate error function defined with respect to a training data set  The performance of the networks is then compared by evaluating the error function using an independent validation set  and the network having the smallest error with respect to the validation set is selected  This approach is called the hold out method  Since this procedure can itself lead to some overfitting to the validation set  the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set 
An application of this process is in early stopping  where the candidate models are successive iterations of the same network  and training stops when the error on the validation set grows  choosing the previous model  the one with minimum error  

Test data set edit 
A test data set is a data set that is independent of the training data set  but that follows the same probability distribution as the training data set  If a model fit to the training data set also fits the test data set well  minimal overfitting has taken place  see figure below   A better fitting of the training data set as opposed to the test data set usually points to over fitting 
A test set is therefore a set of examples used only to assess the performance  i e  generalization  of a fully specified classifier                         To do this  the final model is used to predict classifications of examples in the test set  Those predictions are compared to the examples  true classifications to assess the model s accuracy             
In a scenario where both validation and test data sets are used  the test data set is typically used to assess the final model that is selected during the validation process  In the case where the original data set is partitioned into two subsets  training and test data sets   the test data set might assess the model only once  e g   in the holdout method               Note that some sources advise against such a method              However  when using a method such as cross validation  two partitions can be sufficient and effective since results are averaged after repeated rounds of model training and testing to help reduce bias and variability                        


A training set  left  and a test set  right  from the same statistical population are shown as blue points   Two predictive models are fit to the training data   Both fitted models are plotted with both the training and test sets   In the training set  the MSE of the fit shown in orange is   whereas the MSE for the fit shown in green is     In the test set  the MSE for the fit shown in orange is    and the MSE for the fit shown in green is      The orange curve severely overfits the training data  since its MSE increases by almost a factor of four when comparing the test set to the training set   The green curve overfits the training data much less  as its MSE increases by less than a factor of   
Confusion in terminology edit 
Testing is trying something to find out about it   To put to the proof  to prove the truth  genuineness  or quality of by experiment  according to the Collaborative International Dictionary of English  and to validate is to prove that something is valid   To confirm  to render valid  Collaborative International Dictionary of English   With this perspective  the most common use of the terms test set and validation set is the one here described  However  in both industry and academia  they are sometimes used interchanged  by considering that the internal process is testing different models to improve  test set as a development set  and the final model is the one that needs to be validated before real use with an unseen data  validation set    The literature on machine learning often reverses the meaning of  validation  and  test  sets  This is the most blatant example of the terminological confusion that pervades artificial intelligence research               Nevertheless  the important concept that must be kept is that the final set  whether called test or validation  should only be used in the final experiment 

Cross validation edit 
In order to get more stable results and use all valuable data for training  a data set can be repeatedly split into several training and a validation data sets  This is known as cross validation  To confirm the model s performance  an additional test data set held out from cross validation is normally used 
It is possible to use cross validation on training and validation sets  and within each training set have further cross validation for a test set for hyperparameter tuning  This is known as nested cross validation 

Causes of error edit 
Comic strip demonstrating a fictional erroneous computer output  making a coffee   million degrees  from a previous definition of  extra hot    This can be classified as both a failure in logic and a failure to include various relevant environmental conditions             
Omissions in the training of algorithms are a major cause of erroneous outputs              Types of such omissions include             

Particular circumstances or variations were not included 
Obsolete data
Ambiguous input information
Inability to change to new environments
Inability to request help from a human or another AI system when needed
An example of an omission of particular circumstances is a case where a boy was able to unlock the phone because his mother registered her face under indoor  nighttime lighting  a condition which was not appropriately included in the training of the system                         
Usage of relatively irrelevant input can include situations where algorithms use the background rather than the object of interest for object detection  such as being trained by pictures of sheep on grasslands  leading to a risk that a different object will be interpreted as a sheep if located on a grassland             

See also edit 
Statistical classification
List of datasets for machine learning research
Hierarchical classification
References edit 


  Ron Kohavi  Foster Provost          Glossary of terms   Machine Learning               doi         A               

  Bishop  Christopher M          Pattern Recognition and Machine Learning  New York  Springer  p       vii  ISBN                     Pattern recognition has its origins in engineering  whereas machine learning grew out of computer science  However  these activities can be viewed as two facets of the same field  and together they have undergone substantial development over the past ten years 

  a b James  Gareth         An Introduction to Statistical Learning  with Applications in R  Springer  p            ISBN                     

  a b Ripley  Brian         Pattern Recognition and Neural Networks  Cambridge University Press  p            ISBN                     

  a b c d e f Brownlee  Jason                What is the Difference Between Test and Validation Datasets    Retrieved            

  a b Prechelt  Lutz  Genevi ve B  Orr                Early Stopping   But When    In Gr goire Montavon  Klaus Robert M ller  eds    Neural Networks  Tricks of the Trade  Lecture Notes in Computer Science  Springer Berlin Heidelberg  pp              doi                              ISBN                        

   Machine learning   Is there a rule of thumb for how to divide a dataset into training and validation sets    Stack Overflow  Retrieved            

  Ferrie  C    amp  Kaiser  S          Neural Networks for Babies  Sourcebooks  ISBN                       cite book     CS  maint  multiple names  authors list  link 

  a b c Ripley  B D         Pattern Recognition and Neural Networks  Cambridge  Cambridge University Press  p     

  a b c d  Subject  What are the population  sample  training set  design set  validation set  and test set    Neural Network FAQ  part   of    Introduction  txt   comp ai neural nets  Sarle  W S   ed         last modified            

  a b Larose  D  T   Larose  C  D          Discovering knowledge in data        an introduction to data mining  Hoboken  Wiley  doi                        ISBN                         OCLC                

  a b c Xu  Yun  Goodacre  Royston          On Splitting Training and Validation Set  A Comparative Study of Cross Validation  Bootstrap and Systematic Sampling for Estimating the Generalization Performance of Supervised Learning   Journal of Analysis and Testing         Springer Science and Business Media LLC           doi         s                  ISSN              X  PMC               PMID               

   Deep Learning   Coursera  Retrieved            

  Bishop  C M          Neural Networks for Pattern Recognition  Oxford  Oxford University Press  p     

  Kohavi  Ron                A Study of Cross Validation and Bootstrap for Accuracy Estimation and Model Selection         cite journal    Cite journal requires       journal   help 

  Ripley  Brian D                 Glossary   Pattern recognition and neural networks  Cambridge University Press  ISBN                     OCLC                

  a b c d e Chanda SS  Banerjee DN          Omission and commission errors underlying AI failures   AI Soc                doi         s                x  PMC               PMID                

  Greenberg A                Watch a    Year Old s Face Unlock His Mom s iPhone X   Wired 


vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects






Retrieved from  https   en wikipedia org w index php title Training  validation  and test data sets amp oldid            training set