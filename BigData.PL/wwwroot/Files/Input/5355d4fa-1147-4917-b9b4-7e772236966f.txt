Solving multiple machine learning tasks at the same time
Multi task learning  MTL  is a subfield of machine learning in which multiple learning tasks are solved at the same time  while exploiting commonalities and differences across tasks  This can result in improved learning efficiency and prediction accuracy for the task specific models  when compared to training the models separately                                   
Inherently  Multi task learning is a multi objective optimization problem having trade offs between different tasks            
Early versions of MTL were called  hints                        

In a widely cited      paper  Rich Caruana gave the following characterization Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias  It does this by learning tasks in parallel while using a shared representation  what is learned for each task can help other tasks be learned better            
In the classification context  MTL aims to improve the performance of multiple classification tasks by learning them jointly  One example is a spam filter  which can be treated as distinct but related classification tasks across different users  To make this more concrete  consider that different people have different distributions of features which distinguish spam emails from legitimate ones  for example an English speaker may find that all emails in Russian are spam  not so for Russian speakers  Yet there is a definite commonality in this classification task across users  for example one common feature might be text related to money transfer  Solving each user s spam classification problem jointly via MTL can let the solutions inform each other and improve performance      citation needed      Further examples of settings for MTL include multiclass classification and multi label classification            
Multi task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly  One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled             However  as discussed below  MTL has also been shown to be beneficial for learning unrelated tasks                       


Methods edit 
The key challenge in multi task learning  is how to combine learning signals from multiple tasks into a single model  This may strongly depend on how well different task agree with each other  or contradict each other  There are several ways to address this challenge   

Task grouping and overlap edit 
Within the MTL paradigm  information can be shared across some or all of the tasks  Depending on the structure of task relatedness  one may want to share information selectively across the tasks  For example  tasks may be grouped or exist in a hierarchy  or be related according to some general metric  Suppose  as developed more formally below  that the parameter vector modeling each task is a linear combination of some underlying basis  Similarity in terms of this basis can indicate the relatedness of the tasks  For example  with sparsity  overlap of nonzero coefficients across tasks indicates commonality  A task grouping then corresponds to those tasks lying in a subspace generated by some subset of basis elements  where tasks in different groups may be disjoint or overlap arbitrarily in terms of their bases              Task relatedness can be imposed a priori or learned from the data                         Hierarchical task relatedness can also be exploited implicitly without assuming a priori knowledge or learning relations explicitly                         For example  the explicit learning of sample relevance across tasks can be done to guarantee the effectiveness of joint learning across multiple domains            

Exploiting unrelated tasks edit 
One can attempt learning a group of principal tasks using a group of auxiliary tasks  unrelated to the principal ones  In many applications  joint learning of unrelated tasks which use the same input data can be beneficial  The reason is that prior knowledge about task relatedness can lead to sparser and more informative representations for each task grouping  essentially by screening out idiosyncrasies of the data distribution  Novel methods which builds on a prior multitask methodology by favoring a shared low dimensional representation within each task grouping have been proposed  The programmer can impose a penalty on tasks from different groups which encourages the two representations to be orthogonal  Experiments on synthetic and real data have indicated that incorporating unrelated tasks can result in significant improvements over standard multi task learning methods            

Transfer of knowledge edit 
Related to multi task learning is the concept of knowledge transfer  Whereas traditional multi task learning implies that a shared representation is developed concurrently across tasks  transfer of knowledge implies a sequentially shared representation  Large scale machine learning projects such as the deep convolutional neural network GoogLeNet              an image based object classifier  can develop robust representations which may be useful to further algorithms learning related tasks  For example  the pre trained model can be used as a feature extractor to perform pre processing for another learning algorithm  Or the pre trained model can be used to initialize a model with similar architecture which is then fine tuned to learn a different classification task             

Multiple non stationary tasks edit 
Traditionally Multi task learning and transfer of knowledge are applied to stationary learning settings  Their extension to non stationary environments is termed Group online adaptive learning  GOAL               Sharing information could be particularly useful if learners operate in continuously changing environments  because a learner could benefit from previous experience of another learner to quickly adapt to their new environment  Such group adaptive learning has numerous applications  from predicting financial time series  through content recommendation systems  to visual understanding for adaptive autonomous agents 

Multi task optimization edit 
Multi task optimization focuses on solving optimizing the whole process                           The paradigm has been inspired by the well established concepts of transfer learning             and multi task learning in predictive analytics             
The key motivation behind multi task optimization is that if optimization tasks are related to each other in terms of their optimal solutions or the general characteristics of their function landscapes              the search progress can be transferred to substantially accelerate the search on the other 
The success of the paradigm is not necessarily limited to one way knowledge transfers from simpler to more complex tasks  In practice an attempt is to intentionally solve a more difficult task that may unintentionally solve several smaller problems             
There is a direct relationship between multitask optimization and multi objective optimization             
In some cases  the simultaneous training of seemingly related tasks may hinder performance compared to single task models              Commonly  MTL models employ task specific modules on top of a joint feature representation obtained using a shared module  Since this joint representation must capture useful features across all tasks  MTL may hinder individual task performance if the different tasks seek conflicting representation  i e   the gradients of different tasks point to opposing directions or differ significantly in magnitude  This phenomenon is commonly referred to as negative transfer  To mitigate this issue  various MTL optimization methods have been proposed  Commonly  the per task gradients are combined into a joint update direction through various aggregation algorithms or heuristics 
There are several common approaches for multi task optimization  Bayesian optimization   evolutionary computation  and approaches based on Game theory             

Multi task Bayesian optimization edit 
Multi task Bayesian optimization is a modern model based approach that leverages the concept of knowledge transfer to speed up the automatic hyperparameter optimization process of machine learning algorithms              The method builds a multi task Gaussian process model on the data originating from different searches progressing in tandem              The captured inter task dependencies are thereafter utilized to better inform the subsequent sampling of candidate solutions in respective search spaces 

Evolutionary multi tasking edit 
Evolutionary multi tasking has been explored as a means of exploiting the implicit parallelism of population based search algorithms to simultaneously progress multiple distinct optimization tasks  By mapping all tasks to a unified search space  the evolving population of candidate solutions can harness the hidden relationships between them through continuous genetic transfer  This is induced when solutions associated with different tasks crossover                          Recently  modes of knowledge transfer that are different from direct solution crossover have been explored                         

Game theoretic optimization edit 
Game theoretic approaches to multi task optimization propose to view the optimization problem as a game  where each task is a player  All players compete through the reward matrix of the game  and try to reach a solution that satisfies all players  all tasks   This view provide insight about how to build efficient algorithms based on gradient descent optimization  GD   which is particularly important for training deep neural networks               In GD for MTL  the problem is that each task provides its own loss  and it is not clear how to combine all losses and create a single unified gradient  leading to several different aggregation strategies                                      This aggregation problem can be solved by defining a game matrix where the reward of each player is the agreement of its own gradient with the common gradient  and then setting the common gradient to be the Nash Cooperative bargaining             of that system 

Applications edit 
Algorithms for multi task optimization span a wide array of real world applications  Recent studies highlight the potential for speed ups in the optimization of engineering design parameters by conducting related designs jointly in a multi task manner              In machine learning  the transfer of optimized features across related data sets can enhance the efficiency of the training process as well as improve the generalization capability of learned models                          In addition  the concept of multi tasking has led to advances in automatic hyperparameter optimization of machine learning models and ensemble learning                         
Applications have also been reported in cloud computing              with future developments geared towards cloud based on demand optimization services that can cater to multiple customers simultaneously                          Recent work has additionally shown applications in chemistry              In addition  some recent works have applied multi task optimization algorithms in industrial manufacturing                         

Mathematics edit 
Reproducing Hilbert space of vector valued functions  RKHSvv  edit 
The MTL problem can be cast within the context of RKHSvv  a complete inner product space of vector valued functions equipped with a reproducing kernel   In particular  recent focus has been on cases where task structure can be identified via a separable kernel  described below  The presentation here derives from Ciliberto et al                   

RKHSvv concepts edit 
Suppose the training data set is 
  
    
      
        
          
            
              S
            
          
          
            t
          
        
         
         
         
        
          x
          
            i
          
          
            t
          
        
         
        
          y
          
            i
          
          
            t
          
        
         
        
           
          
            i
             
             
          
          
            
              n
              
                t
              
            
          
        
      
    
      displaystyle   mathcal  S    t     x  i   t  y  i   t      i     n  t   
  
  with 
  
    
      
        
          x
          
            i
          
          
            t
          
        
          x     
        
          
            X
          
        
      
    
      displaystyle x  i   t  in   mathcal  X   
  
  
  
    
      
        
          y
          
            i
          
          
            t
          
        
          x     
        
          
            Y
          
        
      
    
      displaystyle y  i   t  in   mathcal  Y   
  
  where t indexes task  and 
  
    
      
        t
          x     
         
         
         
         
         
         
        T
      
    
      displaystyle t in       T 
  
  Let 
  
    
      
        n
         
        
            x     
          
            t
             
             
          
          
            T
          
        
        
          n
          
            t
          
        
      
    
      displaystyle n  sum   t     T n  t  
  
  In this setting there is a consistent input and output space and the same loss function 
  
    
      
        
          
            L
          
        
         
        
          R
        
          xd  
        
          R
        
          x     
        
          
            R
          
          
             
          
        
      
    
      displaystyle   mathcal  L    mathbb  R   times  mathbb  R   rightarrow  mathbb  R       
  
 for each task    This results in the regularized machine learning problem  


  
    
      
        
          min
          
            f
              x     
            
              
                H
              
            
          
        
        
            x     
          
            t
             
             
          
          
            T
          
        
        
          
             
            
              n
              
                t
              
            
          
        
        
            x     
          
            i
             
             
          
          
            
              n
              
                t
              
            
          
        
        
          
            L
          
        
         
        
          y
          
            i
          
          
            t
          
        
         
        
          f
          
            t
          
        
         
        
          x
          
            i
          
          
            t
          
        
         
         
         
          x bb 
        
           
        
        
           
        
        f
        
           
        
        
          
             
          
          
            
              H
            
          
          
             
          
        
      
    
      displaystyle  min   f in   mathcal  H    sum   t     T   frac     n  t    sum   i     n  t    mathcal  L   y  i   t  f  t  x  i   t     lambda   f     mathcal  H       
  
   
where 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is a vector valued reproducing kernel Hilbert space with functions 
  
    
      
        f
         
        
          
            X
          
        
          x     
        
          
            
              Y
            
          
          
            T
          
        
      
    
      displaystyle f   mathcal  X   rightarrow   mathcal  Y    T  
  
 having components 
  
    
      
        
          f
          
            t
          
        
         
        
          
            X
          
        
          x     
        
          
            Y
          
        
      
    
      displaystyle f  t    mathcal  X   rightarrow   mathcal  Y   
  
 
The reproducing kernel for the space 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 of functions   
  
    
      
        f
         
        
          
            X
          
        
          x     
        
          
            R
          
          
            T
          
        
      
    
      displaystyle f   mathcal  X   rightarrow  mathbb  R    T  
  
 is a symmetric matrix valued function 
  
    
      
          x    
         
        
          
            X
          
        
          xd  
        
          
            X
          
        
          x     
        
          
            R
          
          
            T
              xd  
            T
          
        
      
    
      displaystyle  Gamma    mathcal  X   times   mathcal  X   rightarrow  mathbb  R    T times T  
  
    such that 
  
    
      
          x    
         
          x  c  
         
        x
         
        c
          x     
        
          
            H
          
        
      
    
      displaystyle  Gamma   cdot  x c in   mathcal  H   
  
 and the following reproducing property holds  


  
    
      
          x  e  
        f
         
        x
         
         
        c
        
            x  e  
          
            
              
                R
              
              
                T
              
            
          
        
         
          x  e  
        f
         
          x    
         
        x
         
          x  c  
         
        c
        
            x  e  
          
            
              H
            
          
        
      
    
      displaystyle  langle f x  c rangle    mathbb  R    T    langle f  Gamma  x  cdot  c rangle    mathcal  H   
  
    The reproducing kernel gives rise to a representer theorem showing that any solution to equation   has the form  

  
    
      
        f
         
        x
         
         
        
            x     
          
            t
             
             
          
          
            T
          
        
        
            x     
          
            i
             
             
          
          
            
              n
              
                t
              
            
          
        
          x    
         
        x
         
        
          x
          
            i
          
          
            t
          
        
         
        
          c
          
            i
          
          
            t
          
        
      
    
      displaystyle f x   sum   t     T  sum   i     n  t   Gamma  x x  i   t  c  i   t  
  
   
Separable kernels edit 
The form of the kernel        induces both the representation of the feature space and structures the output across tasks  A natural simplification is to choose a separable kernel  which factors into separate kernels on the input space X and on the tasks 
  
    
      
         
         
         
         
         
         
         
        T
         
      
    
      displaystyle         T   
  
  In this case the kernel relating scalar components 
  
    
      
        
          f
          
            t
          
        
      
    
      displaystyle f  t  
  
 and 
  
    
      
        
          f
          
            s
          
        
      
    
      displaystyle f  s  
  
 is given by 
  
    
      
          x b  
         
         
        
          x
          
            i
          
        
         
        t
         
         
         
        
          x
          
            j
          
        
         
        s
         
         
         
        k
         
        
          x
          
            i
          
        
         
        
          x
          
            j
          
        
         
        
          k
          
            T
          
        
         
        s
         
        t
         
         
        k
         
        
          x
          
            i
          
        
         
        
          x
          
            j
          
        
         
        
          A
          
            s
             
            t
          
        
      
    
      textstyle  gamma   x  i  t   x  j  s   k x  i  x  j  k  T  s t  k x  i  x  j  A  s t  
  
  For vector valued functions  
  
    
      
        f
          x     
        
          
            H
          
        
      
    
      displaystyle f in   mathcal  H   
  
  we can write 
  
    
      
          x    
         
        
          x
          
            i
          
        
         
        
          x
          
            j
          
        
         
         
        k
         
        
          x
          
            i
          
        
         
        
          x
          
            j
          
        
         
        A
      
    
      displaystyle  Gamma  x  i  x  j   k x  i  x  j  A 
  
  where k is a scalar reproducing kernel  and A is a symmetric positive semi definite 
  
    
      
        T
          xd  
        T
      
    
      displaystyle T times T 
  
 matrix  Henceforth denote 
  
    
      
        
          S
          
             
          
          
            T
          
        
         
         
        
          PSD matrices
        
         
          x     
        
          
            R
          
          
            T
              xd  
            T
          
        
      
    
      displaystyle S      T      text PSD matrices     subset  mathbb  R    T times T  
  
  
This factorization property  separability  implies the input feature space representation does not vary by task  That is  there is no interaction between the input kernel and the task kernel  The structure on tasks is represented solely by A  Methods for non separable kernels        is a current field of research 
For the separable case  the representation theorem is reduced to 
  
    
      
        f
         
        x
         
         
        
            x     
          
            i
             
             
          
          
            N
          
        
        k
         
        x
         
        
          x
          
            i
          
        
         
        A
        
          c
          
            i
          
        
      
    
      textstyle f x   sum   i     N k x x  i  Ac  i  
  
  The model output on the training data is then KCA   where K is the 
  
    
      
        n
          xd  
        n
      
    
      displaystyle n times n 
  
 empirical kernel matrix with entries 
  
    
      
        
          K
          
            i
             
            j
          
        
         
        k
         
        
          x
          
            i
          
        
         
        
          x
          
            j
          
        
         
      
    
      textstyle K  i j  k x  i  x  j   
  
  and C  is the 
  
    
      
        n
          xd  
        T
      
    
      displaystyle n times T 
  
 matrix of rows 
  
    
      
        
          c
          
            i
          
        
      
    
      displaystyle c  i  
  
 
With the separable kernel  equation    can be rewritten as


  
    
      
        
          min
          
            C
              x     
            
              
                R
              
              
                n
                  xd  
                T
              
            
          
        
        V
         
        Y
         
        K
        C
        A
         
         
          x bb 
        t
        r
         
        K
        C
        A
        
          C
          
              x  a  
          
        
         
      
    
      displaystyle  min   C in  mathbb  R    n times T  V Y KCA   lambda tr KCAC   top    
  
  P
where V is a  weighted  average of L applied entry wise to Y and KCA   The weight is zero if 
  
    
      
        
          Y
          
            i
          
          
            t
          
        
      
    
      displaystyle Y  i   t  
  
 is a missing observation  
Note the second term in P can be derived as follows 


  
    
      
        
          
            
              
                  x     
                f
                
                    x     
                  
                    
                      H
                    
                  
                  
                     
                  
                
              
              
                
                 
                
                  
                      x  e  
                    
                      
                          x     
                        
                          i
                           
                           
                        
                        
                          n
                        
                      
                      k
                       
                        x  c  
                       
                      
                        x
                        
                          i
                        
                      
                       
                      A
                      
                        c
                        
                          i
                        
                      
                       
                      
                          x     
                        
                          j
                           
                           
                        
                        
                          n
                        
                      
                      k
                       
                        x  c  
                       
                      
                        x
                        
                          j
                        
                      
                       
                      A
                      
                        c
                        
                          j
                        
                      
                    
                      x  e  
                  
                  
                    
                      H
                    
                  
                
              
            
            
              
              
                
                 
                
                    x     
                  
                    i
                     
                    j
                     
                     
                  
                  
                    n
                  
                
                  x  e  
                k
                 
                  x  c  
                 
                
                  x
                  
                    i
                  
                
                 
                A
                
                  c
                  
                    i
                  
                
                 
                k
                 
                  x  c  
                 
                
                  x
                  
                    j
                  
                
                 
                A
                
                  c
                  
                    j
                  
                
                
                    x  e  
                  
                    
                      H
                    
                  
                
              
              
                
                   bilinearity 
                
              
            
            
              
              
                
                 
                
                    x     
                  
                    i
                     
                    j
                     
                     
                  
                  
                    n
                  
                
                  x  e  
                k
                 
                
                  x
                  
                    i
                  
                
                 
                
                  x
                  
                    j
                  
                
                 
                A
                
                  c
                  
                    i
                  
                
                 
                
                  c
                  
                    j
                  
                
                
                    x  e  
                  
                    
                      
                        R
                      
                      
                        T
                      
                    
                  
                
              
              
                
                   reproducing property 
                
              
            
            
              
              
                
                 
                
                    x     
                  
                    i
                     
                    j
                     
                     
                  
                  
                    n
                  
                
                k
                 
                
                  x
                  
                    i
                  
                
                 
                
                  x
                  
                    j
                  
                
                 
                
                  c
                  
                    i
                  
                  
                      x  a  
                  
                
                A
                
                  c
                  
                    j
                  
                
                 
                t
                r
                 
                K
                C
                A
                
                  C
                  
                      x  a  
                  
                
                 
              
            
          
        
      
    
      displaystyle   begin aligned   f     mathcal  H       amp   left langle  sum   i     n k  cdot  x  i  Ac  i   sum   j     n k  cdot  x  j  Ac  j  right rangle    mathcal  H     amp   sum   i j     n  langle k  cdot  x  i  Ac  i  k  cdot  x  j  Ac  j  rangle    mathcal  H   amp   text  bilinearity      amp   sum   i j     n  langle k x  i  x  j  Ac  i  c  j  rangle    mathbb  R    T   amp   text  reproducing property      amp   sum   i j     n k x  i  x  j  c  i    top  Ac  j  tr KCAC   top    end aligned   
  

Known task structure edit 
Task structure representations edit 
There are three largely equivalent ways to represent task structure  through a regularizer  through an output metric  and through an output mapping 


Regularizer With the separable kernel  it can be shown  below  that 
  
    
      
        
           
        
        
           
        
        f
        
           
        
        
          
             
          
          
            
              H
            
          
          
             
          
        
         
        
            x     
          
            s
             
            t
             
             
          
          
            T
          
        
        
          A
          
            t
             
            s
          
          
              x     
          
        
          x  e  
        
          f
          
            s
          
        
         
        
          f
          
            t
          
        
        
            x  e  
          
            
              
                
                  H
                
              
              
                k
              
            
          
        
      
    
      textstyle   f     mathcal  H        sum   s t     T A  t s    dagger   langle f  s  f  t  rangle     mathcal  H    k   
  
  where 
  
    
      
        
          A
          
            t
             
            s
          
          
              x     
          
        
      
    
      displaystyle A  t s    dagger   
  
 is the  
  
    
      
        t
         
        s
      
    
      displaystyle t s 
  
 element of the pseudoinverse of 
  
    
      
        A
      
    
      displaystyle A 
  
  and 
  
    
      
        
          
            
              H
            
          
          
            k
          
        
      
    
      displaystyle   mathcal  H    k  
  
 is the RKHS based on the scalar kernel 
  
    
      
        k
      
    
      displaystyle k 
  
  and 
  
    
      
        
          f
          
            t
          
        
         
        x
         
         
        
            x     
          
            i
             
             
          
          
            n
          
        
        k
         
        x
         
        
          x
          
            i
          
        
         
        
          A
          
            t
          
          
              x  a  
          
        
        
          c
          
            i
          
        
      
    
      textstyle f  t  x   sum   i     n k x x  i  A  t    top  c  i  
  
  This formulation shows that 
  
    
      
        
          A
          
            t
             
            s
          
          
              x     
          
        
      
    
      displaystyle A  t s    dagger   
  
 controls the weight of the penalty associated with 
  
    
      
          x  e  
        
          f
          
            s
          
        
         
        
          f
          
            t
          
        
        
            x  e  
          
            
              
                
                  H
                
              
              
                k
              
            
          
        
      
    
      textstyle  langle f  s  f  t  rangle     mathcal  H    k   
  
   Note that 
  
    
      
          x  e  
        
          f
          
            s
          
        
         
        
          f
          
            t
          
        
        
            x  e  
          
            
              
                
                  H
                
              
              
                k
              
            
          
        
      
    
      textstyle  langle f  s  f  t  rangle     mathcal  H    k   
  
 arises from 
  
    
      
        
           
        
        
           
        
        
          f
          
            t
          
        
        
           
        
        
          
             
          
          
            
              
                
                  H
                
              
              
                k
              
            
          
        
         
          x  e  
        
          f
          
            t
          
        
         
        
          f
          
            t
          
        
        
            x  e  
          
            
              
                
                  H
                
              
              
                k
              
            
          
        
      
    
      textstyle   f  t       mathcal  H    k    langle f  t  f  t  rangle     mathcal  H    k   
  
  

Proof

  
    
      
        
          
            
              
                  x     
                f
                
                    x     
                  
                    
                      H
                    
                  
                  
                     
                  
                
              
              
                
                 
                
                  
                      x  e  
                    
                      
                          x     
                        
                          i
                           
                           
                        
                        
                          n
                        
                      
                        x b  
                       
                       
                      
                        x
                        
                          i
                        
                      
                       
                      
                        t
                        
                          i
                        
                      
                       
                       
                        x  c  
                       
                      
                        c
                        
                          i
                        
                        
                          
                            t
                            
                              i
                            
                          
                        
                      
                       
                      
                          x     
                        
                          j
                           
                           
                        
                        
                          n
                        
                      
                        x b  
                       
                       
                      
                        x
                        
                          j
                        
                      
                       
                      
                        t
                        
                          j
                        
                      
                       
                       
                        x  c  
                       
                      
                        c
                        
                          j
                        
                        
                          
                            t
                            
                              j
                            
                          
                        
                      
                    
                      x  e  
                  
                  
                    
                      H
                    
                  
                
              
            
            
              
              
                
                 
                
                    x     
                  
                    i
                     
                    j
                     
                     
                  
                  
                    n
                  
                
                
                  c
                  
                    i
                  
                  
                    
                      t
                      
                        i
                      
                    
                  
                
                
                  c
                  
                    j
                  
                  
                    
                      t
                      
                        j
                      
                    
                  
                
                  x b  
                 
                 
                
                  x
                  
                    i
                  
                
                 
                
                  t
                  
                    i
                  
                
                 
                 
                 
                
                  x
                  
                    j
                  
                
                 
                
                  t
                  
                    j
                  
                
                 
                 
              
            
            
              
              
                
                 
                
                    x     
                  
                    i
                     
                    j
                     
                     
                  
                  
                    n
                  
                
                
                    x     
                  
                    s
                     
                    t
                     
                     
                  
                  
                    T
                  
                
                
                  c
                  
                    i
                  
                  
                    t
                  
                
                
                  c
                  
                    j
                  
                  
                    s
                  
                
                k
                 
                
                  x
                  
                    i
                  
                
                 
                
                  x
                  
                    j
                  
                
                 
                
                  A
                  
                    s
                     
                    t
                  
                
              
            
            
              
              
                
                 
                
                    x     
                  
                    i
                     
                    j
                     
                     
                  
                  
                    n
                  
                
                k
                 
                
                  x
                  
                    i
                  
                
                 
                
                  x
                  
                    j
                  
                
                 
                  x  e  
                
                  c
                  
                    i
                  
                
                 
                A
                
                  c
                  
                    j
                  
                
                
                    x  e  
                  
                    
                      
                        R
                      
                      
                        T
                      
                    
                  
                
              
            
            
              
              
                
                 
                
                    x     
                  
                    i
                     
                    j
                     
                     
                  
                  
                    n
                  
                
                k
                 
                
                  x
                  
                    i
                  
                
                 
                
                  x
                  
                    j
                  
                
                 
                  x  e  
                
                  c
                  
                    i
                  
                
                 
                A
                
                  A
                  
                      x     
                  
                
                A
                
                  c
                  
                    j
                  
                
                
                    x  e  
                  
                    
                      
                        R
                      
                      
                        T
                      
                    
                  
                
              
            
            
              
              
                
                 
                
                    x     
                  
                    i
                     
                    j
                     
                     
                  
                  
                    n
                  
                
                k
                 
                
                  x
                  
                    i
                  
                
                 
                
                  x
                  
                    j
                  
                
                 
                  x  e  
                A
                
                  c
                  
                    i
                  
                
                 
                
                  A
                  
                      x     
                  
                
                A
                
                  c
                  
                    j
                  
                
                
                    x  e  
                  
                    
                      
                        R
                      
                      
                        T
                      
                    
                  
                
              
            
            
              
              
                
                 
                
                    x     
                  
                    i
                     
                    j
                     
                     
                  
                  
                    n
                  
                
                
                    x     
                  
                    s
                     
                    t
                     
                     
                  
                  
                    T
                  
                
                 
                A
                
                  c
                  
                    i
                  
                
                
                   
                  
                    t
                  
                
                 
                A
                
                  c
                  
                    j
                  
                
                
                   
                  
                    s
                  
                
                k
                 
                
                  x
                  
                    i
                  
                
                 
                
                  x
                  
                    j
                  
                
                 
                
                  A
                  
                    s
                     
                    t
                  
                  
                      x     
                  
                
              
            
            
              
              
                
                 
                
                    x     
                  
                    s
                     
                    t
                     
                     
                  
                  
                    T
                  
                
                
                  A
                  
                    s
                     
                    t
                  
                  
                      x     
                  
                
                  x  e  
                
                    x     
                  
                    i
                     
                     
                  
                  
                    n
                  
                
                k
                 
                
                  x
                  
                    i
                  
                
                 
                  x  c  
                 
                 
                A
                
                  c
                  
                    i
                  
                
                
                   
                  
                    t
                  
                
                 
                
                    x     
                  
                    j
                     
                     
                  
                  
                    n
                  
                
                k
                 
                
                  x
                  
                    j
                  
                
                 
                  x  c  
                 
                 
                A
                
                  c
                  
                    j
                  
                
                
                   
                  
                    s
                  
                
                
                    x  e  
                  
                    
                      
                        
                          H
                        
                      
                      
                        k
                      
                    
                  
                
              
            
            
              
              
                
                 
                
                    x     
                  
                    s
                     
                    t
                     
                     
                  
                  
                    T
                  
                
                
                  A
                  
                    s
                     
                    t
                  
                  
                      x     
                  
                
                  x  e  
                
                  f
                  
                    t
                  
                
                 
                
                  f
                  
                    s
                  
                
                
                    x  e  
                  
                    
                      
                        
                          H
                        
                      
                      
                        k
                      
                    
                  
                
              
            
          
        
      
    
      displaystyle   begin aligned   f     mathcal  H       amp   left langle  sum   i     n  gamma   x  i  t  i    cdot  c  i   t  i    sum   j     n  gamma   x  j  t  j    cdot  c  j   t  j   right rangle    mathcal  H     amp   sum   i j     n c  i   t  i  c  j   t  j   gamma   x  i  t  i    x  j  t  j      amp   sum   i j     n  sum   s t     T c  i   t c  j   s k x  i  x  j  A  s t    amp   sum   i j     n k x  i  x  j   langle c  i  Ac  j  rangle    mathbb  R    T     amp   sum   i j     n k x  i  x  j   langle c  i  AA   dagger  Ac  j  rangle    mathbb  R    T     amp   sum   i j     n k x  i  x  j   langle Ac  i  A   dagger  Ac  j  rangle    mathbb  R    T     amp   sum   i j     n  sum   s t     T  Ac  i    t  Ac  j    s k x  i  x  j  A  s t    dagger     amp   sum   s t     T A  s t    dagger   langle  sum   i     n k x  i   cdot   Ac  i    t   sum   j     n k x  j   cdot   Ac  j    s  rangle     mathcal  H    k     amp   sum   s t     T A  s t    dagger   langle f  t  f  s  rangle     mathcal  H    k   end aligned   
  





Output metric an alternative output metric on 
  
    
      
        
          
            
              Y
            
          
          
            T
          
        
      
    
      displaystyle   mathcal  Y    T  
  
 can be induced by the inner product 
  
    
      
          x  e  
        
          y
          
             
          
        
         
        
          y
          
             
          
        
        
            x  e  
          
              x    
          
        
         
          x  e  
        
          y
          
             
          
        
         
          x    
        
          y
          
             
          
        
        
            x  e  
          
            
              
                R
              
              
                T
              
            
          
        
      
    
      displaystyle  langle y     y     rangle    Theta    langle y      Theta y     rangle    mathbb  R    T   
  
  With the squared loss there is an equivalence between the separable kernels 
  
    
      
        k
         
          x  c  
         
          x  c  
         
        
          I
          
            T
          
        
      
    
      displaystyle k  cdot   cdot  I  T  
  
 under the alternative metric  and 
  
    
      
        k
         
          x  c  
         
          x  c  
         
          x    
      
    
      displaystyle k  cdot   cdot   Theta  
  
  under the canonical metric 



Output mapping Outputs can be mapped as  
  
    
      
        L
         
        
          
            
              Y
            
          
          
            T
          
        
          x     
        
          
            
              
                Y
                  x e 
              
            
          
        
      
    
      displaystyle L   mathcal  Y    T  rightarrow   mathcal   tilde  Y    
  
  to a higher dimensional space to encode complex structures such as trees  graphs and strings   For linear maps L  with appropriate choice of separable kernel  it can be shown that  
  
    
      
        A
         
        
          L
          
              x  a  
          
        
        L
      
    
      displaystyle A L   top  L 
  
 


Task structure examples edit 
Via the regularizer formulation  one can represent a variety of task structures easily  

Letting 
  
    
      
        
          A
          
              x     
          
        
         
          x b  
        
          I
          
            T
          
        
         
         
          x b  
          x     
          x bb 
         
        
          
             
            T
          
        
        
           
        
        
          
             
          
          
              x  a  
          
        
      
    
      textstyle A   dagger    gamma I  T    gamma   lambda    frac     T   mathbf      mathbf        top   
  
  where 
  
    
      
        
          I
          
            T
          
        
      
    
      displaystyle I  T  
  
 is the TxT identity matrix  and 
  
    
      
        
           
        
        
          
             
          
          
              x  a  
          
        
      
    
      textstyle  mathbf      mathbf        top   
  
 is the TxT matrix of ones  is equivalent to letting        control the variance 
  
    
      
        
            x     
          
            t
          
        
        
           
        
        
           
        
        
          f
          
            t
          
        
          x     
        
          
            
              f
                xaf 
            
          
        
        
           
        
        
          
             
          
          
            
              
                
                  H
                
              
              
                k
              
            
          
        
      
    
      textstyle  sum   t   f  t    bar  f        mathcal  H    k   
  
  of tasks from their mean 
  
    
      
        
          
             
            T
          
        
        
            x     
          
            t
          
        
        
          f
          
            t
          
        
      
    
      textstyle   frac     T   sum   t f  t  
  
  For example  blood levels of some biomarker may be taken on T patients at 
  
    
      
        
          n
          
            t
          
        
      
    
      displaystyle n  t  
  
 time points during the course of a day and interest may lie in regularizing the variance of the predictions across patients 
Letting 
  
    
      
        
          A
          
              x     
          
        
         
          x b  
        
          I
          
            T
          
        
         
         
          x b  
          x     
          x bb 
         
        M
      
    
      displaystyle A   dagger    alpha I  T    alpha   lambda  M 
  
   where 
  
    
      
        
          M
          
            t
             
            s
          
        
         
        
          
             
            
              
                 
              
              
                G
                
                  r
                
              
              
                 
              
            
          
        
        
          I
        
         
        t
         
        s
          x     
        
          G
          
            r
          
        
         
      
    
      displaystyle M  t s    frac      G  r     mathbb  I   t s in G  r   
  
 is equivalent to letting 
  
    
      
          x b  
      
    
      displaystyle  alpha  
  
 control the variance measured with respect to a group mean  
  
    
      
        
            x     
          
            r
          
        
        
            x     
          
            t
              x     
            
              G
              
                r
              
            
          
        
        
           
        
        
           
        
        
          f
          
            t
          
        
          x     
        
          
             
            
              
                 
              
              
                G
                
                  r
                
              
              
                 
              
            
          
        
        
            x     
          
            s
              x     
            
              G
              
                r
              
            
             
          
        
        
          f
          
            s
          
        
        
           
        
        
           
        
      
    
      displaystyle  sum   r  sum   t in G  r    f  t    frac      G  r     sum   s in G  r   f  s    
  
   Here 
  
    
      
        
           
        
        
          G
          
            r
          
        
        
           
        
      
    
      displaystyle  G  r   
  
 the cardinality of group r  and 
  
    
      
        
          I
        
      
    
      displaystyle  mathbb  I   
  
 is the indicator function   For example  people in different political parties  groups  might be regularized together with respect to predicting the favorability rating of a politician  Note that this penalty reduces to the first when all tasks are in the same group 
Letting 
  
    
      
        
          A
          
              x     
          
        
         
          x b  
        
          I
          
            T
          
        
         
         
          x b  
          x     
          x bb 
         
        L
      
    
      displaystyle A   dagger    delta I  T    delta   lambda  L 
  
  where 
  
    
      
        L
         
        D
          x     
        M
      
    
      displaystyle L D M 
  
 is the Laplacian for the graph with adjacency matrix M giving pairwise similarities of tasks  This is equivalent to giving a larger penalty to the distance separating tasks t and s when they are more similar  according to the weight 
  
    
      
        
          M
          
            t
             
            s
          
        
      
    
      displaystyle M  t s  
  
   i e  
  
    
      
          x b  
      
    
      displaystyle  delta  
  
 regularizes 
  
    
      
        
            x     
          
            t
             
            s
          
        
        
           
        
        
           
        
        
          f
          
            t
          
        
          x     
        
          f
          
            s
          
        
        
           
        
        
          
             
          
          
            
              
                
                  H
                
              
              
                k
              
            
          
          
             
          
        
        
          M
          
            t
             
            s
          
        
      
    
      displaystyle  sum   t s   f  t  f  s       mathcal  H    k      M  t s  
  
 
All of the above choices of A also induce the additional regularization term  
  
    
      
          x bb 
        
            x     
          
            t
          
        
        
           
        
        
           
        
        f
        
           
        
        
          
             
          
          
            
              
                
                  H
                
              
              
                k
              
            
          
          
             
          
        
      
    
      textstyle  lambda  sum   t   f      mathcal  H    k       
  
 which penalizes complexity in f more broadly 
Learning tasks together with their structure edit 
Learning problem P can be generalized to admit learning task matrix A as follows 


  
    
      
        
          min
          
            C
              x     
            
              
                R
              
              
                n
                  xd  
                T
              
            
             
            A
              x     
            
              S
              
                 
              
              
                T
              
            
          
        
        V
         
        Y
         
        K
        C
        A
         
         
          x bb 
        t
        r
         
        K
        C
        A
        
          C
          
              x  a  
          
        
         
         
        F
         
        A
         
      
    
      displaystyle  min   C in  mathbb  R    n times T  A in S      T  V Y KCA   lambda tr KCAC   top    F A  
  
  Q
Choice of 
  
    
      
        F
         
        
          S
          
             
          
          
            T
          
        
          x     
        
          
            R
          
          
             
          
        
      
    
      displaystyle F S      T  rightarrow  mathbb  R       
  
 must be designed to learn matrices A of a given type  See  Special cases  below 

Optimization of Q edit 
Restricting to the case of convex losses and coercive penalties Ciliberto et al  have shown that although Q is not convex jointly in C and A  a related problem is jointly convex 
Specifically on the convex set 
  
    
      
        
          
            C
          
        
         
         
         
        C
         
        A
         
          x     
        
          
            R
          
          
            n
              xd  
            T
          
        
          xd  
        
          S
          
             
          
          
            T
          
        
        
           
        
        R
        a
        n
        g
        e
         
        
          C
          
              x  a  
          
        
        K
        C
         
          x     
        R
        a
        n
        g
        e
         
        A
         
         
      
    
      displaystyle   mathcal  C      C A  in  mathbb  R    n times T  times S      T  Range C   top  KC  subseteq Range A    
  
  the equivalent problem


  
    
      
        
          min
          
            C
             
            A
              x     
            
              
                C
              
            
          
        
        V
         
        Y
         
        K
        C
         
         
          x bb 
        t
        r
         
        
          A
          
              x     
          
        
        
          C
          
              x  a  
          
        
        K
        C
         
         
        F
         
        A
         
      
    
      displaystyle  min   C A in   mathcal  C   V Y KC   lambda tr A   dagger  C   top  KC  F A  
  
  R
is convex with the same minimum value  And if 
  
    
      
         
        
          C
          
            R
          
        
         
        
          A
          
            R
          
        
         
      
    
      displaystyle  C  R  A  R   
  
 is a minimizer for R then 
  
    
      
         
        
          C
          
            R
          
        
        
          A
          
            R
          
          
              x     
          
        
         
        
          A
          
            R
          
        
         
      
    
      displaystyle  C  R A  R    dagger   A  R   
  
 is a minimizer for Q 
R may be solved by a barrier method on a closed set by introducing the following perturbation 


  
    
      
        
          min
          
            C
              x     
            
              
                R
              
              
                n
                  xd  
                T
              
            
             
            A
              x     
            
              S
              
                 
              
              
                T
              
            
          
        
        V
         
        Y
         
        K
        C
         
         
          x bb 
        t
        r
         
        
          A
          
              x     
          
        
         
        
          C
          
              x  a  
          
        
        K
        C
         
        
            x b  
          
             
          
        
        
          I
          
            T
          
        
         
         
         
        F
         
        A
         
      
    
      displaystyle  min   C in  mathbb  R    n times T  A in S      T  V Y KC   lambda tr A   dagger   C   top  KC  delta     I  T    F A  
  
  S
The perturbation via the barrier 
  
    
      
        
            x b  
          
             
          
        
        t
        r
         
        
          A
          
              x     
          
        
         
      
    
      displaystyle  delta     tr A   dagger    
  
 forces the objective functions to be equal to 
  
    
      
         
          x   e 
      
    
      displaystyle   infty  
  
 on the boundary of 
  
    
      
        
          R
          
            n
              xd  
            T
          
        
          xd  
        
          S
          
             
          
          
            T
          
        
      
    
      displaystyle R  n times T  times S      T  
  
  
S can be solved with a block coordinate descent method  alternating in C and A  This results in a sequence of minimizers 
  
    
      
         
        
          C
          
            m
          
        
         
        
          A
          
            m
          
        
         
      
    
      displaystyle  C  m  A  m   
  
 in S that converges to the solution in R as 
  
    
      
        
            x b  
          
            m
          
        
          x     
         
      
    
      displaystyle  delta   m  rightarrow   
  
  and hence gives the solution to Q 

Special cases edit 
Spectral penalties   Dinnuzo et al             suggested setting F as the Frobenius norm 
  
    
      
        
          
            t
            r
             
            
              A
              
                  x  a  
              
            
            A
             
          
        
      
    
      displaystyle   sqrt  tr A   top  A    
  
  They optimized Q directly using block coordinate descent  not accounting for difficulties at the boundary of 
  
    
      
        
          
            R
          
          
            n
              xd  
            T
          
        
          xd  
        
          S
          
             
          
          
            T
          
        
      
    
      displaystyle  mathbb  R    n times T  times S      T  
  
 
Clustered tasks learning   Jacob et al             suggested to learn A in the setting where T  tasks are organized in R disjoint clusters  In this case let 
  
    
      
        E
          x     
         
         
         
         
        
           
          
            T
              xd  
            R
          
        
      
    
      displaystyle E in          T times R  
  
 be the matrix with 
  
    
      
        
          E
          
            t
             
            r
          
        
         
        
          I
        
         
        
          task  xa  
        
        t
          x     
        
          group  xa  
        
        r
         
      
    
      displaystyle E  t r   mathbb  I     text task   t in   text group   r  
  
  Setting 
  
    
      
        M
         
        I
          x     
        
          E
          
              x     
          
        
        
          E
          
            T
          
        
      
    
      displaystyle M I E   dagger  E  T  
  
  and  
  
    
      
        U
         
        
          
             
            T
          
        
        
          
              
          
          
              x  a  
          
        
      
    
      displaystyle U   frac     T   mathbf         top   
  
  the task matrix 
  
    
      
        
          A
          
              x     
          
        
      
    
      displaystyle A   dagger   
  
  can be parameterized as a function of 
  
    
      
        M
      
    
      displaystyle M 
  
  
  
    
      
        
          A
          
              x     
          
        
         
        M
         
         
        
            x f  
          
            M
          
        
        U
         
        
            x f  
          
            B
          
        
         
        M
          x     
        U
         
         
          x f  
         
        I
          x     
        M
         
      
    
      displaystyle A   dagger   M   epsilon   M U  epsilon   B  M U   epsilon  I M  
  
   with terms that penalize the average  between clusters variance and within clusters variance respectively of the task predictions  M is not convex  but there is a convex relaxation 
  
    
      
        
          
            
              S
            
          
          
            c
          
        
         
         
        M
          x     
        
          S
          
             
          
          
            T
          
        
         
        I
          x     
        M
          x     
        
          S
          
             
          
          
            T
          
        
          x     
        t
        r
         
        M
         
         
        r
         
      
    
      displaystyle   mathcal  S    c    M in S      T  I M in S      T  land tr M  r   
  
  In this formulation   
  
    
      
        F
         
        A
         
         
        
          I
        
         
        A
         
        M
         
          x     
         
        A
         
        M
          x     
        
          
            
              S
            
          
          
            C
          
        
         
         
      
    
      displaystyle F A   mathbb  I   A M  in   A M in   mathcal  S    C     
  
 

Generalizations edit 
Non convex penalties   Penalties can be constructed such that A is constrained to be a graph Laplacian  or that A has low rank factorization  However these penalties are not convex  and the analysis of the barrier method proposed by Ciliberto et al  does not go through in these cases 
Non separable kernels   Separable kernels are limited  in particular they do not account for structures in the interaction space between the input and output domains jointly  Future work is needed to develop models for these kernels 

Software package edit 
A Matlab package called Multi Task Learning via StructurAl Regularization  MALSAR                implements the following multi task learning algorithms  Mean Regularized Multi Task Learning                          Multi Task Learning with Joint Feature Selection              Robust Multi Task Feature Learning              Trace Norm Regularized Multi Task Learning              Alternating Structural Optimization                          Incoherent Low Rank and Sparse Learning              Robust Low Rank Multi Task Learning  Clustered Multi Task Learning                          Multi Task Learning with Graph Structures  

Literature edit 
Multi Target Prediction  A Unifying View on Problems and Methods Willem Waegeman  Krzysztof Dembczynski  Eyke Huellermeier https   arxiv org abs           v 
See also edit 

Artificial intelligence
Artificial neural network
Automated machine learning  AutoML 
Evolutionary computation
Foundation model
General game playing
Human based genetic algorithm
Kernel methods for vector output
Multiple criteria decision analysis
Multi objective optimization
Multicriteria classification
Robot learning
Transfer learning
James Stein estimator

References edit 


  Baxter  J          A model of inductive bias learning  Journal of Artificial Intelligence Research              On line paper

  Thrun  S          Is learning the n th thing any easier than learning the first   In Advances in Neural Information Processing Systems    pp            MIT Press  Paper at Citeseer

  a b Caruana  R           Multi task learning   PDF   Machine Learning             doi         A               

  Multi Task Learning as Multi Objective Optimization
Part of Advances in Neural Information Processing Systems     NeurIPS        https   proceedings neurips cc paper      hash    aca a e   e   f  a  c f  edce Abstract html

  Suddarth  S   Kergosien  Y          Rule injection hints as a means of improving network performance and learning time  EURASIP Workshop  Neural Networks pp           Lecture Notes in Computer Science  Springer 

  Abu Mostafa  Y  S           Learning from hints in neural networks   Journal of Complexity                  doi                 x          y 

  a b c Ciliberto  C           Convex Learning of Multiple Tasks and their Structure   arXiv             cs LG  

  a b c d Hajiramezanali  E   amp  Dadaneh  S  Z   amp  Karbalayghareh  A   amp  Zhou  Z   amp  Qian  X  Bayesian multi domain learning for cancer subtype discovery from next generation sequencing count data    nd Conference on Neural Information Processing Systems  NIPS        Montr al  Canada  arXiv           

  a b Romera Paredes  B   Argyriou  A   Bianchi Berthouze  N    amp  Pontil  M          Exploiting Unrelated Tasks in Multi Task Learning  http   jmlr csail mit edu proceedings papers v   romera   romera   pdf

  Kumar  A    amp  Daume III  H          Learning Task Grouping and Overlap in Multi Task Learning  http   icml cc      papers     pdf

  Jawanpuria  P    amp  Saketha Nath  J          A Convex Feature Learning Formulation for Latent Task Structure Discovery  http   icml cc      papers    pdf

  Zweig  A   amp  Weinshall  D  Hierarchical Regularization Cascade for Joint Learning  Proceedings  of   th International Conference on Machine Learning  ICML   Atlanta GA  June       http   www cs huji ac il  daphna papers Zweig ICML     pdf

  Szegedy  Christian  Wei Liu  Youssef  Yangqing Jia  Tomaso  Sermanet  Pierre  Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru  Vanhoucke  Vincent  Rabinovich  Andrew          Going deeper with convolutions        IEEE Conference on Computer Vision and Pattern Recognition  CVPR   pp            arXiv            doi         CVPR               ISBN                         S CID                

  Roig  Gemma   Deep Learning Overview   PDF   Archived from the original  PDF  on             Retrieved            

  Zweig  A   amp  Chechik  G  Group online adaptive learning  Machine Learning  DOI         s                   August       http   rdcu be uFSv

  a b Gupta  Abhishek  Ong  Yew Soon  Feng  Liang          Insights on Transfer Optimization  Because Experience is the Best Teacher   IEEE Transactions on Emerging Topics in Computational Intelligence            doi         TETCI               hdl               S CID               

  a b c Gupta  Abhishek  Ong  Yew Soon  Feng  Liang          Multifactorial Evolution  Toward Evolutionary Multitasking   IEEE Transactions on Evolutionary Computation                   doi         TEVC               hdl               S CID               

  Pan  Sinno Jialin  Yang  Qiang          A Survey on Transfer Learning   IEEE Transactions on Knowledge and Data Engineering                      doi         TKDE           S CID             

  Caruana  R    Multitask Learning   pp         in Sebastian Thrun  Lorien Pratt  eds   Learning to Learn         Springer ISBN                   

  Cheng  Mei Ying  Gupta  Abhishek  Ong  Yew Soon  Ni  Zhi Wei          Coevolutionary multitasking for concurrent global optimization  With case studies in complex engineering design   Engineering Applications of Artificial Intelligence             doi         j engappai              S CID               

  Cabi  Serkan  Sergio G mez Colmenarejo  Hoffman  Matthew W   Denil  Misha  Wang  Ziyu  Nando de Freitas          The Intentional Unintentional Agent  Learning to Solve Many Continuous Control Tasks Simultaneously   arXiv             cs AI  

  J   Y  Li  Z   H  Zhan  Y  Li and J  Zhang   Multiple Tasks for Multiple Objectives  A New Multiobjective Optimization Method via Multitask Optimization   in IEEE Transactions on Evolutionary Computation  doi         TEVC             

  Standley  Trevor  Zamir  Amir R   Chen  Dawn  Guibas  Leonidas  Malik  Jitendra  Savarese  Silvio                Learning the Pareto Front with Hypernetworks   International Conference on Machine Learning  ICML              arXiv            

  Swersky  K   Snoek  J    amp  Adams  R  P          Multi task bayesian optimization  Advances in neural information processing systems  pp             

  Bonilla  E  V   Chai  K  M    amp  Williams  C          Multi task Gaussian process prediction  Advances in neural information processing systems  pp           

  a b Ong  Y  S    amp  Gupta  A          Evolutionary multitasking  a computer science view of cognitive multitasking  Cognitive Computation                

  Feng  Liang  Zhou  Lei  Zhong  Jinghui  Gupta  Abhishek  Ong  Yew Soon  Tan  Kay Chen  Qin  A  K           Evolutionary Multitasking via Explicit Autoencoding   IEEE Transactions on Cybernetics                     doi         TCYB               PMID                S CID               

  Jiang  Yi  Zhan  Zhi Hui  Tan  Kay Chen  Zhang  Jun  January         Block Level Knowledge Transfer for Evolutionary Multitask Optimization   IEEE Transactions on Cybernetics                   doi         TCYB               ISSN                 PMID               

  Goodfellow  Ian  Bengio  Yoshua  Courville  Aaron         Deep Learning  MIT Press  ISBN                        

  Liu  L   Li  Y   Kuang  Z   Xue  J   Chen  Y   Yang  W   Liao  Q   Zhang  W                 Towards Impartial Multi task Learning   In  Proceedings of the International Conference on Learning Representations  ICLR        ICLR  Virtual event          Retrieved            

  Tianhe  Yu  Saurabh  Kumar  Abhishek  Gupta  Sergey  Levine  Karol  Hausman  Chelsea  Finn          Gradient Surgery for Multi Task Learning   Advances in Neural Information Processing Systems      arXiv            

  Liu  Bo  Liu  Xingchao  Jin  Xiaojie  Stone  Peter  Liu  Qiang                Conflict Averse Gradient Descent for Multi task Learning   arXiv             cs LG  

  Aviv Navon  Aviv Shamsian  Idan Achituve  Haggai Maron  Kenji Kawaguchi  Gal Chechik  Ethan Fetaya           Multi Task Learning as a Bargaining Game  International conference on machine learning 

  Chandra  R   Gupta  A   Ong  Y  S    amp  Goh  C  K         October   Evolutionary multi task learning for modular training of feedforward neural networks  In International Conference on Neural Information Processing  pp          Springer  Cham 

  Yosinski  J   Clune  J   Bengio  Y    amp  Lipson  H          How transferable are features in deep neural networks  In Advances in neural information processing systems  pp             

  Wen  Yu Wei  Ting  Chuan Kang          Learning ensemble of decision trees through multifactorial genetic programming        IEEE Congress on Evolutionary Computation  CEC   pp                  doi         CEC               ISBN                         S CID              

  Zhang  Boyu  Qin  A  K   Sellis  Timos          Evolutionary feature subspaces generation for ensemble classification   Proceedings of the Genetic and Evolutionary Computation Conference  pp                doi                          ISBN                         S CID               

  Bao  Liang  Qi  Yutao  Shen  Mengqing  Bu  Xiaoxuan  Yu  Jusheng  Li  Qian  Chen  Ping          An Evolutionary Multitasking Algorithm for Cloud Computing Service Composition   Services   SERVICES       Lecture Notes in Computer Science  Vol              pp                doi                               ISBN                        

  Tang  J   Chen  Y   Deng  Z   Xiang  Y    amp  Joy  C  P          A Group based Approach to Improve Multifactorial Evolutionary Algorithm  In IJCAI  pp             

  Felton  Kobi  Wigh  Daniel  Lapkin  Alexei          Multi task Bayesian Optimization of Chemical Reactions   chemRxiv  doi          chemrxiv          v  

  Jiang  Yi  Zhan  Zhi Hui  Tan  Kay Chen  Zhang  Jun  October         A Bi Objective Knowledge Transfer Framework for Evolutionary Many Task Optimization   IEEE Transactions on Evolutionary Computation                     doi         TEVC               ISSN              X 

  Jiang  Yi  Zhan  Zhi Hui  Tan  Kay Chen  Kwong  Sam  Zhang  Jun          Knowledge Structure Preserving Based Evolutionary Many Task Optimization   IEEE Transactions on Evolutionary Computation                   doi         TEVC               ISSN              X 

  Dinuzzo  Francesco          Learning output kernels with block coordinate descent   PDF   Proceedings of the   th International Conference on Machine Learning  ICML      Archived from the original  PDF  on            

  Jacob  Laurent          Clustered multi task learning  A convex formulation   Advances in Neural Information Processing Systems  arXiv            Bibcode     arXiv         J 

  Zhou  J   Chen  J  and Ye  J  MALSAR  Multi tAsk Learning via StructurAl Regularization  Arizona State University        http   www public asu edu  jye   Software MALSAR  On line manual

  Evgeniou  T    amp  Pontil  M          Regularized multi task learning  Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining  pp           

  Evgeniou  T   Micchelli  C   Pontil  M           Learning multiple tasks with kernel methods   PDF   Journal of Machine Learning Research         

  Argyriou  A   Evgeniou  T   Pontil  M       a    Convex multi task feature learning   Machine Learning                   doi         s                 

  Chen  J   Zhou  J    amp  Ye  J          Integrating low rank and group sparse structures for robust multi task learning     dead link       Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining 

  Ji  S    amp  Ye  J          An accelerated gradient method for trace norm minimization  Proceedings of the   th Annual International Conference on Machine Learning  pp           

  Ando  R   Zhang  T           A framework for learning predictive structures from multiple tasks and unlabeled data   PDF   The Journal of Machine Learning Research               

  Chen  J   Tang  L   Liu  J    amp  Ye  J          A convex formulation for learning shared structures from multiple tasks  Proceedings of the   th Annual International Conference on Machine Learning  pp           

  Chen  J   Liu  J    amp  Ye  J          Learning incoherent sparse and low rank patterns from multiple tasks  Proceedings of the   th ACM SIGKDD international conference on Knowledge discovery and data mining  pp             

  Jacob  L   Bach  F    amp  Vert  J          Clustered multi task learning  A convex formulation  Advances in Neural Information Processing Systems      

  Zhou  J   Chen  J    amp  Ye  J          Clustered multi task learning via alternating structure optimization  Advances in Neural Information Processing Systems 


External links edit 
The Biosignals Intelligence Group at UIUC
Washington University in St  Louis Department of Computer Science
Software edit 
The Multi Task Learning via Structural Regularization Package
Online Multi Task Learning Toolkit  OMT  A general purpose online multi task learning toolkit based on conditional random field models and stochastic gradient descent training  C    NET 
vteOptimization  Algorithms  methods  and heuristicsUnconstrained nonlinearFunctions
Golden section search
Powell s method
Line search
Nelder Mead method
Successive parabolic interpolation
GradientsConvergence
Trust region
Wolfe conditions
Quasi Newton
Berndt Hall Hall Hausman
Broyden Fletcher Goldfarb Shanno and L BFGS
Davidon Fletcher Powell
Symmetric rank one  SR  
Other methods
Conjugate gradient
Gauss Newton
Gradient
Mirror
Levenberg Marquardt
Powell s dog leg method
Truncated Newton
Hessians
Newton s method
Optimization computes maxima and minima Constrained nonlinearGeneral
Barrier methods
Penalty methods
Differentiable
Augmented Lagrangian methods
Sequential quadratic programming
Successive linear programming
Convex optimizationConvex minimization
Cutting plane method
Reduced gradient  Frank Wolfe 
Subgradient method
Linear andquadraticInterior point
Affine scaling
Ellipsoid algorithm of Khachiyan
Projective algorithm of Karmarkar
Basis exchange
Simplex algorithm of Dantzig
Revised simplex algorithm
Criss cross algorithm
Principal pivoting algorithm of Lemke
Active set method
CombinatorialParadigms
Approximation algorithm
Dynamic programming
Greedy algorithm
Integer programming
Branch and bound cut
Graph algorithmsMinimum spanning tree
Bor vka
Prim
Kruskal

    Shortest path
Bellman Ford
SPFA
Dijkstra
Floyd Warshall
Network flows
Dinic
Edmonds Karp
Ford Fulkerson
Push relabel maximum flow
Metaheuristics
Evolutionary algorithm
Hill climbing
Local search
Parallel metaheuristics
Simulated annealing
Spiral optimization algorithm
Tabu search

Software






Retrieved from  https   en wikipedia org w index php title Multi task learning amp oldid