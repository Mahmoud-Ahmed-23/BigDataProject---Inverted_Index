Filename used to indicate portions for web crawling
For Wikipedia s robots txt file  see https   en wikipedia org robots txt 



robots txtRobots Exclusion ProtocolExample of a simple robots txt file  indicating that a user agent called  Mallorybot  is not allowed to crawl any of the website s pages  and that other user agents cannot crawl more than one page every    seconds  and are not allowed to crawl the  secret  folderStatusProposed StandardFirst published     published  formally standardized in     Authors
Martijn Koster  original author 
Gary Illyes  Henner Zeller  Lizzi Sassman  IETF contributors 
Websiterobotstxt org  RFC     
robots txt is the filename used for implementing the Robots Exclusion Protocol  a standard used by websites to indicate to visiting web crawlers and other web robots which portions of the website they are allowed to visit 
The standard  developed in       relies on voluntary compliance  Malicious bots can use the file as a directory of which pages to visit  though standards bodies discourage countering this with security through obscurity  Some archival sites ignore robots txt  The standard was used in the     s to mitigate server overload  In the     s  websites began denying bots that collect information for generative artificial intelligence 
The  robots txt  file can be used in conjunction with sitemaps  another robot inclusion standard for websites 


History edit 
The standard was proposed by Martijn Koster                        when working for Nexor            in February                 on the www talk mailing list  the main communication channel for WWW related activities at the time  Charles Stross claims to have provoked Koster to suggest robots txt  after he wrote a badly behaved web crawler that inadvertently caused a denial of service attack on Koster s server            
The standard  initially RobotsNotWanted txt  allowed web developers to specify which bots should not access their website or which pages bots should not access  The internet was small enough in      to maintain a complete list of all bots  server overload was a primary concern  By June      it had become a de facto standard             most complied  including those operated by search engines such as WebCrawler  Lycos  and AltaVista            
On July          Google announced the proposal of the Robots Exclusion Protocol as an official standard under Internet Engineering Task Force             A proposed standard            was published in September      as RFC      

Standard edit 
When a site owner wishes to give instructions to web robots they place a text file called robots txt in the root of the web site hierarchy  e g  https   www example com robots txt   This text file contains the instructions in a specific format  see examples below   Robots that choose to follow the instructions try to fetch this file and read the instructions before fetching any other file from the website  If this file does not exist  web robots assume that the website owner does not wish to place any limitations on crawling the entire site 
A robots txt file contains instructions for bots indicating which web pages they can and cannot access  Robots txt files are particularly important for web crawlers from search engines such as Google 
A robots txt file on a website will function as a request that specified robots ignore specified files or directories when crawling a site  This might be  for example  out of a preference for privacy from search engine results  or the belief that the content of the selected directories might be misleading or irrelevant to the categorization of the site as a whole  or out of a desire that an application only operates on certain data  Links to pages listed in robots txt can still appear in search results if they are linked to from a page that is crawled             
A robots txt file covers one origin  For websites with multiple subdomains  each subdomain must have its own robots txt file  If example com had a robots txt file but a example com did not  the rules that would apply for example com would not apply to a example com  In addition  each protocol and port needs its own robots txt file  http   example com robots txt does not apply to pages under http   example com       or https   example com  

Compliance edit 
The robots txt protocol is widely complied with by bot operators             

Search engines edit 
Some major search engines following this standard include Ask              AOL              Baidu              Bing               DuckDuckGo              Kagi              Google              Yahoo               and Yandex             

Archival sites edit 
Some web archiving projects ignore robots txt  Archive Team uses the file to discover more links  such as sitemaps              Co founder Jason Scott said that  unchecked  and left alone  the robots txt file ensures no mirroring or reference for items that may have general use and meaning beyond the website s context               In       the Internet Archive announced that it would stop complying with robots txt directives                         According to Digital Trends  this followed widespread use of robots txt to remove historical sites from search engine results  and contrasted with the nonprofit s aim to archive  snapshots  of the internet as it previously existed             

Artificial intelligence edit 
Starting in the     s  web operators began using robots txt to deny access to bots collecting training data for generative AI  In       Originality AI found that     of the thousand most visited websites blocked OpenAI s GPTBot in their robots txt file and    blocked Google s Google Extended  Many robots txt files named GPTBot as the only bot explicitly disallowed on all pages  Denying access to GPTBot was common among news websites such as the BBC and The New York Times  In       blog host Medium announced it would deny access to all artificial intelligence web crawlers as  AI companies have leached value from writers in order to spam Internet readers             
GPTBot complies with the robots txt standard and gives advice to web operators about how to disallow it  but The Verge     s David Pierce said this only began after  training the underlying models that made it so powerful   Also  some bots are used both for search engines and artificial intelligence  and it may be impossible to block only one of these options                 Media reported that companies like Anthropic and Perplexity ai circumvented robots txt by renaming or spinning up new scrapers to replace the ones that appeared on popular blocklists             

Security edit 
Despite the use of the terms allow and disallow  the protocol is purely advisory and relies on the compliance of the web robot  it cannot enforce any of what is stated in the file               Malicious web robots are unlikely to honor robots txt  some may even use the robots txt as a guide to find disallowed links and go straight to them  While this is sometimes claimed to be a security risk              this sort of security through obscurity is discouraged by standards bodies  The National Institute of Standards and Technology  NIST  in the United States specifically recommends against this practice   System security should not depend on the secrecy of the implementation or its components               In the context of robots txt files  security through obscurity is not recommended as a security technique             

Alternatives edit 
Many robots also pass a special user agent to the web server when fetching content              A web administrator could also configure the server to automatically return failure  or pass alternative content  when it detects a connection using one of the robots                         
Some sites  such as Google  host a humans txt file that displays information meant for humans to read              Some sites such as GitHub redirect humans txt to an About page             
Previously  Google had a joke file hosted at  killer robots txt instructing the Terminator not to kill the company founders Larry Page and Sergey Brin                         

Examples edit 
This example tells all robots that they can visit all files because the wildcard   stands for all robots and the Disallow directive has no value  meaning no pages are disallowed  Search engine giant Google open sourced their robots txt parser              and recommends testing and validating rules on the robots txt file using community built testers such as Tame the Bots              and Real Robots Txt              

User agent   
Disallow  

This example has the same effect  allowing all files rather than prohibiting none 

User agent   
Allow   

The same result can be accomplished with an empty or missing robots txt file 
This example tells all robots to stay out of a website 

User agent   
Disallow   

This example tells all robots not to enter three directories 

User agent   
Disallow   cgi bin 
Disallow   tmp 
Disallow   junk 

This example tells all robots to stay away from one specific file 

User agent   
Disallow   directory file html

All other files in the specified directory will be processed 

This example tells one specific robot to stay out of a website 

User agent  BadBot   replace  BadBot  with the actual user agent of the bot
Disallow   

This example tells two specific robots not to enter one specific directory 

User agent  BadBot   replace  BadBot  with the actual user agent of the bot
User agent  Googlebot
Disallow   private 

Example demonstrating how comments can be used 

  Comments appear after the     symbol at the start of a line  or after a directive
User agent      match all bots
Disallow      keep them out

It is also possible to list multiple robots with their own rules  The actual robot string is defined by the crawler  A few robot operators  such as Google  support several user agent strings that allow the operator to deny access to a subset of their services by using specific user agent strings             
Example demonstrating multiple user agents 

User agent  googlebot          all Google services
Disallow   private             disallow this directory

User agent  googlebot news     only the news service
Disallow                       disallow everything

User agent                     any robot
Disallow   something           disallow this directory

The use of the wildcard   in rules edit 
The directive Disallow   something  blocks all files and subdirectories starting with  something  
In contrast using a wildcard   if supported by the crawler   allows for more complex patterns in specifying paths and files to allow or disallow from crawling  for example Disallow   something   other blocks URLs such as 

 something foo other
 something bar other

It would not prevent the crawling of  something foo else  as that would not match the pattern 
The wildcard   allows greater flexibility but may not be recognized by all crawlers  although it is part of the Robots Exclusion Protocol RFC             
A wildcard at the end of a rule in effect does nothing  as that is the standard behaviour 

Nonstandard extensions edit 
Crawl delay directive edit 
The crawl delay value is supported by some crawlers to throttle their visits to the host  Since this value is not part of the standard  its interpretation is dependent on the crawler reading it  It is used when the multiple burst of visits from bots is slowing down the host  Yandex interprets the value as the number of seconds to wait between subsequent visits              Bing defines crawl delay as the size of a time window  from   to    seconds  during which BingBot will access a web site only once              Google ignores this directive              but provides an interface in its search console for webmasters  to control the Googlebot s subsequent visits             

User agent  bingbot
Allow   
Crawl delay    

Sitemap edit 
Some crawlers support a Sitemap directive  allowing multiple Sitemaps in the same robots txt in the form Sitemap  full url             

Sitemap  http   www example com sitemap xml
Universal     match edit 
The Robot Exclusion Standard does not mention the     character in the Disallow  statement             

Meta tags and headers edit 
In addition to root level robots txt files  robots exclusion directives can be applied at a more granular level through the use of Robots meta tags and X Robots Tag HTTP headers  The robots meta tag cannot be used for non HTML files such as images  text files  or PDF documents  On the other hand  the X Robots Tag can be added to non HTML files by using  htaccess and httpd conf files             

A  noindex  meta tag edit 
 lt meta name  quot robots quot  content  quot noindex quot    gt 

A  noindex  HTTP response header edit 
X Robots Tag  noindex

The X Robots Tag is only effective after the page has been requested and the server responds  and the robots meta tag is only effective after the page has loaded  whereas robots txt is effective before the page is requested  Thus if a page is excluded by a robots txt file  any robots meta tags or X Robots Tag headers are effectively ignored because the robot will not see them in the first place             

Maximum size of a robots txt file edit 
The Robots Exclusion Protocol requires crawlers to parse at least     kibibytes         bytes  of robots txt files              which Google maintains as a     kibibyte file size restriction for robots txt files             

See also edit 

Internet portal

ads txt  a standard for listing authorized ad sellers
security txt  a file to describe the process for security researchers to follow in order to report security vulnerabilities
eBay v  Bidder s Edge
Automated Content Access Protocol   A failed proposal to extend robots txt
BotSeer   Now inactive search engine for robots txt files
Distributed web crawling
Focused crawler
Internet Archive
Meta elements for search engines
National Digital Library Program  NDLP 
National Digital Information Infrastructure and Preservation Program  NDIIPP 
nofollow
noindex
Perma cc
Sitemaps
Spider trap
Web archiving
Web crawler

References edit 


   Historical   Greenhills co uk  Archived from the original on             Retrieved            

  Fielding  Roy          Maintaining Distributed Hypertext Infostructures  Welcome to MOMspider s Web   PostScript   First International Conference on the World Wide Web  Geneva  Archived from the original on             Retrieved September          

   The Web Robots Pages   Robotstxt org              Archived from the original on             Retrieved            

  Koster  Martijn     February         Important  Spiders  Robots and Web Wanderers   www talk mailing list  Archived from the original  Hypermail archived message  on October          

   How I got here in the end  part five   things can only get better     Charlie s Diary     June       Archived from the original on             Retrieved    April      

  a b c d e Pierce  David     February         The text file that runs the internet   The Verge  Retrieved    March      

  Barry Schwartz     June         Robots txt Celebrates    Years Of Blocking Search Engines   Search Engine Land  Archived from the original on             Retrieved            

   Formalizing the Robots Exclusion Protocol Specification   Official Google Webmaster Central Blog  Archived from the original on             Retrieved            

  Koster  M   Illyes  G   Zeller  H   Sassman  L   September        Robots Exclusion Protocol  Internet Engineering Task Force  doi          RFC      RFC       Proposed Standard 
                        

   Uncrawled URLs in search results   YouTube  Oct          Archived from the original on             Retrieved            

   About Ask com  Webmasters   About ask com  Archived from the original on    January       Retrieved    February      

   About AOL Search   Search aol com  Archived from the original on    December       Retrieved    February      

   Baiduspider   Baidu com  Archived from the original on   August       Retrieved    February      

   Robots Exclusion Protocol  joining together to provide better documentation   Blogs bing com    June       Archived from the original on             Retrieved    February      

   DuckDuckGo Bot   DuckDuckGo com  Archived from the original on    February       Retrieved    April      

   Kagi Search KagiBot   Kagi Search  Archived from the original on    April       Retrieved    November      

  a b  Webmasters  Robots txt Specifications   Google Developers  Archived from the original on             Retrieved    February      

   Submitting your website to Yahoo  Search   Archived from the original on             Retrieved    February      

  a b  Using robots txt   Help yandex com  Archived from the original on             Retrieved    February      

   ArchiveBot  Bad behavior   wiki archiveteam org  Archive Team  Archived from the original on    October       Retrieved    October      

  Jason Scott   Robots txt is a suicide note   Archive Team  Archived from the original on             Retrieved    February      

   Robots txt meant for search engines don t work well for web archives   Internet Archive Blogs   blog archive org     April       Archived from the original on             Retrieved            

  Jones  Brad     April         The Internet Archive Will Ignore Robots txt Files to Maintain Accuracy   Digital Trends  Archived from the original on             Retrieved   May      

  Koebler  Jason                Websites are Blocking the Wrong AI Scrapers  Because AI Companies Keep Making New Ones        Media  Retrieved            

   Block URLs with robots txt  Learn about robots txt files   Archived from the original on             Retrieved            

   Robots txt tells hackers the places you don t want them to look   The Register  Archived from the original on             Retrieved August          

  Scarfone  K  A   Jansen  W   Tracy  M   July         Guide to General Server Security   PDF   National Institute of Standards and Technology  doi         NIST SP          Archived  PDF  from the original on             Retrieved August          

  Sverre H  Huseby         Innocent Code  A Security Wake Up Call for Web Programmers  John Wiley  amp  Sons  pp              ISBN                     Archived from the original on             Retrieved            

   List of User Agents  Spiders  Robots  Browser    User agents org  Archived from the original on             Retrieved            

   Access Control   Apache HTTP Server   Httpd apache org  Archived from the original on             Retrieved            

   Deny Strings for Filtering Rules        The Official Microsoft IIS Site   Iis net              Archived from the original on             Retrieved            

   Google humans txt   Archived from the original on January           Retrieved October         

   Github humans txt   GitHub  Archived from the original on May           Retrieved October         

  Newman  Lily Hay                Is This a Google Easter Egg or Proof That Skynet Is Actually Plotting World Domination    Slate Magazine  Archived from the original on             Retrieved            

    killer robots txt               Archived from the original on             Retrieved            

   Google Robots txt Parser and Matcher Library   Retrieved April          

   Robots txt Testing  amp  Validator Tool   Tame the Bots   Retrieved April          

   Robots txt parser based on Google s open source parser from Will Critchlow  CEO of SearchPilot   Retrieved April          

  Koster  Martijn  Illyes  Gary  Zeller  Henner  Sassman  Lizzi  September        Robots Exclusion Protocol  Report   Internet Engineering Task Force 

   To crawl or not to crawl  that is BingBot s question     May       Archived from the original on             Retrieved   February      

   How Google interprets the robots txt specification   Google Search Central              Retrieved            

   Change Googlebot crawl rate   Search Console Help   support google com  Archived from the original on             Retrieved    October      

   Yahoo  Search Blog   Webmasters can now auto discover with Sitemaps   Archived from the original on             Retrieved            

   Robots txt Specifications   Google Developers  Archived from the original on November          Retrieved February          

  a b  Robots meta tag and X Robots Tag HTTP header specifications   Webmasters   Google Developers   Archived from the original on             Retrieved            

  Koster  M   Illyes  G   Zeller  H   Sassman  L   September        Robots Exclusion Protocol  Internet Engineering Task Force  doi          RFC      RFC       Proposed Standard  sec       Limits 
                        

   How Google Interprets the robots txt Specification   Documentation   Google Developers  Archived from the original on             Retrieved            


Further reading edit 
Allyn  Bobby    July         Artificial Intelligence Web Crawlers Are Running Amok   All Things Considered  NPR  Archived from the original on   July       Retrieved   July      
External links edit 
Official website
Retrieved from  https   en wikipedia org w index php title Robots txt amp oldid