Iterative method for finding maximum likelihood estimates in statistical models
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
In statistics  an expectation maximization  EM  algorithm is an iterative method to find  local  maximum likelihood or maximum a posteriori  MAP  estimates of parameters in statistical models  where the model depends on unobserved latent variables             The EM iteration alternates between performing an expectation  E  step  which creates a function for the expectation of the log likelihood evaluated using the current estimate for the parameters  and a maximization  M  step  which computes parameters maximizing the expected log likelihood found on the E step  These parameter estimates are then used to determine the distribution of the latent variables in the next E step  It can be used  for example  to estimate a mixture of gaussians  or to solve the multiple linear regression problem            

EM clustering of Old Faithful eruption data  The random initial model  which  due to the different scales of the axes  appears to be two very flat and wide ellipses  is fit to the observed data  In the first iterations  the model changes substantially  but then converges to the two modes of the geyser  Visualized using ELKI 

History edit 
The EM algorithm was explained and given its name in a classic      paper by Arthur Dempster  Nan Laird  and Donald Rubin             They pointed out that the method had been  proposed many times in special circumstances  by earlier authors  One of the earliest is the gene counting method for estimating allele frequencies by Cedric Smith             Another was proposed by H O  Hartley in       and Hartley and Hocking in       from which many of the ideas in the Dempster Laird Rubin paper originated             Another one by S K Ng  Thriyambakam Krishnan and G J McLachlan in                  Hartley s ideas can be broadened to any grouped discrete distribution  A very detailed treatment of the EM method for exponential families was published by Rolf Sundberg in his thesis and several papers                                   following his collaboration with Per Martin L f and Anders Martin L f                                                              The Dempster Laird Rubin paper in      generalized the method and sketched a convergence analysis for a wider class of problems  The Dempster Laird Rubin paper established the EM method as an important tool of statistical analysis  See also Meng and van Dyk        
The convergence analysis of the Dempster Laird Rubin algorithm was flawed and a correct convergence analysis was published by C  F  Jeff Wu in                  
Wu s proof established the EM method s convergence also outside of the exponential family  as claimed by Dempster Laird Rubin             

Introduction edit 
The EM algorithm is used to find  local  maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly  Typically these models involve latent variables in addition to unknown parameters and known data observations  That is  either missing values exist among the data  or the model can be formulated more simply by assuming the existence of further unobserved data points  For example  a mixture model can be described more simply by assuming that each observed data point has a corresponding unobserved data point  or latent variable  specifying the mixture component to which each data point belongs 
Finding a maximum likelihood solution typically requires taking the derivatives of the likelihood function with respect to all the unknown values  the parameters and the latent variables  and simultaneously solving the resulting equations  In statistical models with latent variables  this is usually impossible  Instead  the result is typically a set of interlocking equations in which the solution to the parameters requires the values of the latent variables and vice versa  but substituting one set of equations into the other produces an unsolvable equation 
The EM algorithm proceeds from the observation that there is a way to solve these two sets of equations numerically  One can simply pick arbitrary values for one of the two sets of unknowns  use them to estimate the second set  then use these new values to find a better estimate of the first set  and then keep alternating between the two until the resulting values both converge to fixed points  It s not obvious that this will work  but it can be proven in this context  Additionally  it can be proven that the derivative of the likelihood is  arbitrarily close to  zero at that point  which in turn means that the point is either a local maximum or a saddle point              In general  multiple maxima may occur  with no guarantee that the global maximum will be found  Some likelihoods also have singularities in them  i e   nonsensical maxima  For example  one of the solutions that may be found by EM in a mixture model involves setting one of the components to have zero variance and the mean parameter for the same component to be equal to one of the data points 

Description edit 
The symbols edit 
Given the statistical model which generates a set 
  
    
      
        
          X
        
      
    
      displaystyle  mathbf  X   
  
 of observed data  a set of unobserved latent data or missing values 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
  and a vector of unknown parameters 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
  along with a likelihood function 
  
    
      
        L
         
        
            x b  
        
         
        
          X
        
         
        
          Z
        
         
         
        p
         
        
          X
        
         
        
          Z
        
          x     
        
            x b  
        
         
      
    
      displaystyle L   boldsymbol   theta     mathbf  X    mathbf  Z    p  mathbf  X    mathbf  Z   mid   boldsymbol   theta     
  
  the maximum likelihood estimate  MLE  of the unknown parameters is determined by maximizing the marginal likelihood of the observed data


  
    
      
        L
         
        
            x b  
        
         
        
          X
        
         
         
        p
         
        
          X
        
          x     
        
            x b  
        
         
         
          x   b 
        p
         
        
          X
        
         
        
          Z
        
          x     
        
            x b  
        
         
        
        d
        
          Z
        
         
          x   b 
        p
         
        
          X
        
          x     
        
          Z
        
         
        
            x b  
        
         
        p
         
        
          Z
        
          x     
        
            x b  
        
         
        
        d
        
          Z
        
      
    
      displaystyle L   boldsymbol   theta     mathbf  X    p  mathbf  X   mid   boldsymbol   theta      int p  mathbf  X    mathbf  Z   mid   boldsymbol   theta      d mathbf  Z    int p  mathbf  X   mid  mathbf  Z     boldsymbol   theta    p  mathbf  Z   mid   boldsymbol   theta      d mathbf  Z   
  

However  this quantity is often intractable since 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
 is unobserved and the distribution of 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
 is unknown before attaining 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
 

The EM algorithm edit 
The EM algorithm seeks to find the maximum likelihood estimate of the marginal likelihood by iteratively applying these two steps 

Expectation step  E step   Define 
  
    
      
        Q
         
        
            x b  
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
      
    
      displaystyle Q   boldsymbol   theta    mid   boldsymbol   theta      t    
  
 as the expected value of the log likelihood function of 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
  with respect to the current conditional distribution of 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
 given 
  
    
      
        
          X
        
      
    
      displaystyle  mathbf  X   
  
 and the current estimates of the parameters 
  
    
      
        
          
              x b  
          
          
             
            t
             
          
        
      
    
      displaystyle   boldsymbol   theta      t   
  
 

  
    
      
        Q
         
        
            x b  
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
         
        
          E
          
            
              Z
            
              x   c 
            p
             
              x  c  
            
               
            
            
              X
            
             
            
              
                  x b  
              
              
                 
                t
                 
              
            
             
          
        
          x     
        
           
          
            log
              x     
            p
             
            
              X
            
             
            
              Z
            
            
               
            
            
                x b  
            
             
          
           
        
        
      
    
      displaystyle Q   boldsymbol   theta    mid   boldsymbol   theta      t     operatorname  E     mathbf  Z   sim p  cdot   mathbf  X     boldsymbol   theta      t     left  log p  mathbf  X    mathbf  Z     boldsymbol   theta     right    
  

Maximization step  M step   Find the parameters that maximize this quantity 

  
    
      
        
          
              x b  
          
          
             
            t
             
             
             
          
        
         
        
          
            
              a
              r
              g
              
              m
              a
              x
            
              x b  
          
        
          xa  
        Q
         
        
            x b  
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
        
      
    
      displaystyle   boldsymbol   theta      t       underset   boldsymbol   theta     operatorname  arg  max      Q   boldsymbol   theta    mid   boldsymbol   theta      t      
  

More succinctly  we can write it as one equation 
  
    
      
        
          
              x b  
          
          
             
            t
             
             
             
          
        
         
        
          
            
              a
              r
              g
              
              m
              a
              x
            
              x b  
          
        
        
          E
          
            
              Z
            
              x   c 
            p
             
              x  c  
            
               
            
            
              X
            
             
            
              
                  x b  
              
              
                 
                t
                 
              
            
             
          
        
          x     
        
           
          
            log
              x     
            p
             
            
              X
            
             
            
              Z
            
            
               
            
            
                x b  
            
             
          
           
        
        
      
    
      displaystyle   boldsymbol   theta      t       underset   boldsymbol   theta     operatorname  arg  max     operatorname  E     mathbf  Z   sim p  cdot   mathbf  X     boldsymbol   theta      t     left  log p  mathbf  X    mathbf  Z     boldsymbol   theta     right    
  


Interpretation of the variables edit 
The typical models to which EM is applied use 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
 as a latent variable indicating membership in one of a set of groups 

The observed data points 
  
    
      
        
          X
        
      
    
      displaystyle  mathbf  X   
  
 may be discrete  taking values in a finite or countably infinite set  or continuous  taking values in an uncountably infinite set   Associated with each data point may be a vector of observations 
The missing values  aka latent variables  
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
 are discrete  drawn from a fixed number of values  and with one latent variable per observed unit 
The parameters are continuous  and are of two kinds  Parameters that are associated with all data points  and those associated with a specific value of a latent variable  i e   associated with all data points whose corresponding latent variable has that value  
However  it is possible to apply EM to other sorts of models 
The motivation is as follows  If the value of the parameters 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
 is known  usually the value of the latent variables 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
 can be found by maximizing the log likelihood over all possible values of 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
  either simply by iterating over 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
 or through an algorithm such as the Viterbi algorithm for hidden Markov models  Conversely  if we know the value of the latent variables 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
  we can find an estimate of the parameters 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
 fairly easily  typically by simply grouping the observed data points according to the value of the associated latent variable and averaging the values  or some function of the values  of the points in each group  This suggests an iterative algorithm  in the case where both 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
 and 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
 are unknown 

First  initialize the parameters 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
 to some random values 
Compute the probability of each possible value of 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
   given 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
 
Then  use the just computed values of 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
 to compute a better estimate for the parameters 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
 
Iterate steps   and   until convergence 
The algorithm as just described monotonically approaches a local minimum of the cost function 

Properties edit 
Although an EM iteration does increase the observed data  i e   marginal  likelihood function  no guarantee exists that the sequence converges to a maximum likelihood estimator  For multimodal distributions  this means that an EM algorithm may converge to a local maximum of the observed data likelihood function  depending on starting values  A variety of heuristic or metaheuristic approaches exist to escape a local maximum  such as random restart hill climbing  starting with several different random initial estimates 
  
    
      
        
          
              x b  
          
          
             
            t
             
          
        
      
    
      displaystyle   boldsymbol   theta      t   
  
   or applying simulated annealing methods 
EM is especially useful when the likelihood is an exponential family  see Sundberg        Ch     for a comprehensive treatment              the E step becomes the sum of expectations of sufficient statistics  and the M step involves maximizing a linear function  In such a case  it is usually possible to derive closed form expression updates for each step  using the Sundberg formula              proved and published by Rolf Sundberg  based on unpublished results of Per Martin L f and Anders Martin L f                                                                        
The EM method was modified to compute maximum a posteriori  MAP  estimates for Bayesian inference in the original paper by Dempster  Laird  and Rubin 
Other methods exist to find maximum likelihood estimates  such as gradient descent  conjugate gradient  or variants of the Gauss Newton algorithm  Unlike EM  such methods typically require the evaluation of first and or second derivatives of the likelihood function 

Proof of correctness edit 
Expectation Maximization works to improve 
  
    
      
        Q
         
        
            x b  
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
      
    
      displaystyle Q   boldsymbol   theta    mid   boldsymbol   theta      t    
  
 rather than directly improving 
  
    
      
        log
          x     
        p
         
        
          X
        
          x     
        
            x b  
        
         
      
    
      displaystyle  log p  mathbf  X   mid   boldsymbol   theta     
  
  Here it is shown that improvements to the former imply improvements to the latter             
For any 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
 with non zero probability 
  
    
      
        p
         
        
          Z
        
          x     
        
          X
        
         
        
            x b  
        
         
      
    
      displaystyle p  mathbf  Z   mid  mathbf  X     boldsymbol   theta     
  
  we can write


  
    
      
        log
          x     
        p
         
        
          X
        
          x     
        
            x b  
        
         
         
        log
          x     
        p
         
        
          X
        
         
        
          Z
        
          x     
        
            x b  
        
         
          x     
        log
          x     
        p
         
        
          Z
        
          x     
        
          X
        
         
        
            x b  
        
         
         
      
    
      displaystyle  log p  mathbf  X   mid   boldsymbol   theta      log p  mathbf  X    mathbf  Z   mid   boldsymbol   theta      log p  mathbf  Z   mid  mathbf  X     boldsymbol   theta      
  

We take the expectation over possible values of the unknown data 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
 under the current parameter estimate 
  
    
      
        
            x b  
          
             
            t
             
          
        
      
    
      displaystyle  theta    t   
  
 by multiplying both sides by 
  
    
      
        p
         
        
          Z
        
          x     
        
          X
        
         
        
          
              x b  
          
          
             
            t
             
          
        
         
      
    
      displaystyle p  mathbf  Z   mid  mathbf  X     boldsymbol   theta      t    
  
 and summing  or integrating  over 
  
    
      
        
          Z
        
      
    
      displaystyle  mathbf  Z   
  
  The left hand side is the expectation of a constant  so we get 


  
    
      
        
          
            
              
                log
                  x     
                p
                 
                
                  X
                
                  x     
                
                    x b  
                
                 
              
              
                
                 
                
                    x     
                  
                    
                      Z
                    
                  
                
                p
                 
                
                  Z
                
                  x     
                
                  X
                
                 
                
                  
                      x b  
                  
                  
                     
                    t
                     
                  
                
                 
                log
                  x     
                p
                 
                
                  X
                
                 
                
                  Z
                
                  x     
                
                    x b  
                
                 
                  x     
                
                    x     
                  
                    
                      Z
                    
                  
                
                p
                 
                
                  Z
                
                  x     
                
                  X
                
                 
                
                  
                      x b  
                  
                  
                     
                    t
                     
                  
                
                 
                log
                  x     
                p
                 
                
                  Z
                
                  x     
                
                  X
                
                 
                
                    x b  
                
                 
              
            
            
              
              
                
                 
                Q
                 
                
                    x b  
                
                  x     
                
                  
                      x b  
                  
                  
                     
                    t
                     
                  
                
                 
                 
                H
                 
                
                    x b  
                
                  x     
                
                  
                      x b  
                  
                  
                     
                    t
                     
                  
                
                 
                 
              
            
          
        
      
    
      displaystyle   begin aligned  log p  mathbf  X   mid   boldsymbol   theta     amp   sum    mathbf  Z   p  mathbf  Z   mid  mathbf  X     boldsymbol   theta      t    log p  mathbf  X    mathbf  Z   mid   boldsymbol   theta      sum    mathbf  Z   p  mathbf  Z   mid  mathbf  X     boldsymbol   theta      t    log p  mathbf  Z   mid  mathbf  X     boldsymbol   theta       amp  Q   boldsymbol   theta    mid   boldsymbol   theta      t    H   boldsymbol   theta    mid   boldsymbol   theta      t     end aligned   
  

where 
  
    
      
        H
         
        
            x b  
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
      
    
      displaystyle H   boldsymbol   theta    mid   boldsymbol   theta      t    
  
 is defined by the negated sum it is replacing 
This last equation holds for every value of 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
 including 
  
    
      
        
            x b  
        
         
        
          
              x b  
          
          
             
            t
             
          
        
      
    
      displaystyle   boldsymbol   theta      boldsymbol   theta      t   
  
 


  
    
      
        log
          x     
        p
         
        
          X
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
         
        Q
         
        
          
              x b  
          
          
             
            t
             
          
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
         
        H
         
        
          
              x b  
          
          
             
            t
             
          
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
         
      
    
      displaystyle  log p  mathbf  X   mid   boldsymbol   theta      t    Q   boldsymbol   theta      t   mid   boldsymbol   theta      t    H   boldsymbol   theta      t   mid   boldsymbol   theta      t     
  

and subtracting this last equation from the previous equation gives


  
    
      
        log
          x     
        p
         
        
          X
        
          x     
        
            x b  
        
         
          x     
        log
          x     
        p
         
        
          X
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
         
        Q
         
        
            x b  
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
          x     
        Q
         
        
          
              x b  
          
          
             
            t
             
          
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
         
        H
         
        
            x b  
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
          x     
        H
         
        
          
              x b  
          
          
             
            t
             
          
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
         
      
    
      displaystyle  log p  mathbf  X   mid   boldsymbol   theta      log p  mathbf  X   mid   boldsymbol   theta      t    Q   boldsymbol   theta    mid   boldsymbol   theta      t    Q   boldsymbol   theta      t   mid   boldsymbol   theta      t    H   boldsymbol   theta    mid   boldsymbol   theta      t    H   boldsymbol   theta      t   mid   boldsymbol   theta      t     
  

However  Gibbs  inequality tells us that 
  
    
      
        H
         
        
            x b  
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
          x     
        H
         
        
          
              x b  
          
          
             
            t
             
          
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
      
    
      displaystyle H   boldsymbol   theta    mid   boldsymbol   theta      t    geq H   boldsymbol   theta      t   mid   boldsymbol   theta      t    
  
  so we can conclude that


  
    
      
        log
          x     
        p
         
        
          X
        
          x     
        
            x b  
        
         
          x     
        log
          x     
        p
         
        
          X
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
          x     
        Q
         
        
            x b  
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
          x     
        Q
         
        
          
              x b  
          
          
             
            t
             
          
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
         
      
    
      displaystyle  log p  mathbf  X   mid   boldsymbol   theta      log p  mathbf  X   mid   boldsymbol   theta      t    geq Q   boldsymbol   theta    mid   boldsymbol   theta      t    Q   boldsymbol   theta      t   mid   boldsymbol   theta      t     
  

In words  choosing 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
 to improve 
  
    
      
        Q
         
        
            x b  
        
          x     
        
          
              x b  
          
          
             
            t
             
          
        
         
      
    
      displaystyle Q   boldsymbol   theta    mid   boldsymbol   theta      t    
  
 causes 
  
    
      
        log
          x     
        p
         
        
          X
        
          x     
        
            x b  
        
         
      
    
      displaystyle  log p  mathbf  X   mid   boldsymbol   theta     
  
 to improve at least as much 

As a maximization maximization procedure edit 
The EM algorithm can be viewed as two alternating maximization steps  that is  as an example of coordinate descent                          Consider the function 


  
    
      
        F
         
        q
         
          x b  
         
          
        
          E
          
            q
          
        
          x     
         
        log
          x     
        L
         
          x b  
         
        x
         
        Z
         
         
         
        H
         
        q
         
         
      
    
      displaystyle F q  theta     operatorname  E    q   log L  theta  x Z   H q   
  

where q is an arbitrary probability distribution over the unobserved data z and H q  is the entropy of the distribution q  This function can be written as


  
    
      
        F
         
        q
         
          x b  
         
         
          x     
        
          D
          
            
              K
              L
            
          
        
        
          
             
          
        
        q
          x     
        
          p
          
            Z
              x     
            X
          
        
         
          x  c  
          x     
        x
         
          x b  
         
        
          
             
          
        
         
        log
          x     
        L
         
          x b  
         
        x
         
         
      
    
      displaystyle F q  theta    D   mathrm  KL     big   q parallel p  Z mid X   cdot  mid x  theta    big     log L  theta  x   
  

where  
  
    
      
        
          p
          
            Z
              x     
            X
          
        
         
          x  c  
          x     
        x
         
          x b  
         
      
    
      displaystyle p  Z mid X   cdot  mid x  theta   
  
 is the conditional distribution of the unobserved data given the observed data 
  
    
      
        x
      
    
      displaystyle x 
  
 and 
  
    
      
        
          D
          
            K
            L
          
        
      
    
      displaystyle D  KL  
  
 is the Kullback Leibler divergence 
Then the steps in the EM algorithm may be viewed as 

Expectation step  Choose 
  
    
      
        q
      
    
      displaystyle q 
  
 to maximize 
  
    
      
        F
      
    
      displaystyle F 
  
 

  
    
      
        
          q
          
             
            t
             
          
        
         
        
          
            a
            r
            g
            
            m
            a
            x
          
          
            q
          
        
          x     
          xa  
        F
         
        q
         
        
            x b  
          
             
            t
             
          
        
         
      
    
      displaystyle q   t    operatorname  arg  max    q   F q  theta    t    
  

Maximization step  Choose 
  
    
      
          x b  
      
    
      displaystyle  theta  
  
 to maximize 
  
    
      
        F
      
    
      displaystyle F 
  
 

  
    
      
        
            x b  
          
             
            t
             
             
             
          
        
         
        
          
            a
            r
            g
            
            m
            a
            x
          
          
              x b  
          
        
          x     
          xa  
        F
         
        
          q
          
             
            t
             
          
        
         
          x b  
         
      
    
      displaystyle  theta    t      operatorname  arg  max     theta    F q   t    theta   
  

Applications edit 
EM is frequently used for parameter estimation of mixed models                          notably in quantitative genetics             
In psychometrics  EM is an important tool for estimating item parameters and latent abilities of item response theory models 
With the ability to deal with missing data and observe unidentified variables  EM is becoming a useful tool to price and manage risk of a portfolio      citation needed     
The EM algorithm  and its faster variant ordered subset expectation maximization  is also widely used in medical image reconstruction  especially in positron emission tomography  single photon emission computed tomography  and x ray computed tomography  See below for other faster variants of EM 
In structural engineering  the Structural Identification using Expectation Maximization  STRIDE              algorithm is an output only method for identifying natural vibration properties of a structural system using sensor data  see Operational Modal Analysis  
EM is also used for data clustering  In natural language processing  two prominent instances of the algorithm are the Baum Welch algorithm for hidden Markov models  and the inside outside algorithm for unsupervised induction of probabilistic context free grammars 
In the analysis of intertrade waiting times i e  the time between subsequent trades in shares of stock at a stock exchange the EM algorithm has proved to be very useful             
Filtering and smoothing EM algorithms edit 
A Kalman filter is typically used for on line state estimation and a minimum variance smoother may be employed for off line or batch state estimation  However  these minimum variance solutions require estimates of the state space model parameters  EM algorithms can be used for solving joint state and parameter estimation problems 
Filtering and smoothing EM algorithms arise by repeating this two step procedure 

E step
Operate a Kalman filter or a minimum variance smoother designed with current parameter estimates to obtain updated state estimates 
M step
Use the filtered or smoothed state estimates within maximum likelihood calculations to obtain updated parameter estimates 
Suppose that a Kalman filter or minimum variance smoother operates on measurements of a single input single output system that possess additive white noise  An updated measurement noise variance estimate can be obtained from the maximum likelihood calculation


  
    
      
        
          
            
              
                  x c  
                  x e 
              
            
          
          
            v
          
          
             
          
        
         
        
          
             
            N
          
        
        
            x     
          
            k
             
             
          
          
            N
          
        
        
          
             
            
              z
              
                k
              
            
              x     
            
              
                
                  
                    x
                      x e 
                  
                
              
              
                k
              
            
             
          
          
             
          
        
         
      
    
      displaystyle   widehat   sigma     v        frac     N   sum   k     N   z  k    widehat  x    k         
  

where 
  
    
      
        
          
            
              
                x
                  x e 
              
            
          
          
            k
          
        
      
    
      displaystyle   widehat  x    k  
  
 are scalar output estimates calculated by a filter or a smoother from N scalar measurements 
  
    
      
        
          z
          
            k
          
        
      
    
      displaystyle z  k  
  
  The above update can also be applied to updating a Poisson measurement noise intensity  Similarly  for a first order auto regressive process  an updated process noise variance estimate can be calculated by


  
    
      
        
          
            
              
                  x c  
                  x e 
              
            
          
          
            w
          
          
             
          
        
         
        
          
             
            N
          
        
        
            x     
          
            k
             
             
          
          
            N
          
        
        
          
             
            
              
                
                  
                    x
                      x e 
                  
                
              
              
                k
                 
                 
              
            
              x     
            
              
                
                  F
                    x e 
                
              
            
            
              
                
                  
                    x
                      x e 
                  
                
              
              
                k
              
            
             
          
          
             
          
        
         
      
    
      displaystyle   widehat   sigma     w        frac     N   sum   k     N     widehat  x    k      widehat  F    widehat  x    k         
  

where 
  
    
      
        
          
            
              
                x
                  x e 
              
            
          
          
            k
          
        
      
    
      displaystyle   widehat  x    k  
  
 and 
  
    
      
        
          
            
              
                x
                  x e 
              
            
          
          
            k
             
             
          
        
      
    
      displaystyle   widehat  x    k    
  
 are scalar state estimates calculated by a filter or a smoother  The updated model coefficient estimate is obtained via


  
    
      
        
          
            
              F
                x e 
            
          
        
         
        
          
            
              
                  x     
                
                  k
                   
                   
                
                
                  N
                
              
              
                
                   
                  
                    
                      
                        
                          x
                            x e 
                        
                      
                    
                    
                      k
                       
                       
                    
                  
                    x     
                  
                    
                      
                        F
                          x e 
                      
                    
                  
                  
                    
                      
                        
                          x
                            x e 
                        
                      
                    
                    
                      k
                    
                  
                   
                
                
                   
                
              
            
            
              
                  x     
                
                  k
                   
                   
                
                
                  N
                
              
              
                
                  
                    
                      x
                        x e 
                    
                  
                
                
                  k
                
                
                   
                
              
            
          
        
         
      
    
      displaystyle   widehat  F     frac   sum   k     N     widehat  x    k      widehat  F    widehat  x    k          sum   k     N   widehat  x    k         
  

The convergence of parameter estimates such as those above are well studied                                                 

Variants edit 
A number of methods have been proposed to accelerate the sometimes slow convergence of the EM algorithm  such as those using conjugate gradient and modified Newton s methods  Newton Raphson               Also  EM can be used with constrained estimation methods 
Parameter expanded expectation maximization  PX EM  algorithm often provides speed up by  us ing  a  covariance adjustment  to correct the analysis of the M step  capitalising on extra information captured in the imputed complete data              
Expectation conditional maximization  ECM  replaces each M step with a sequence of conditional maximization  CM  steps in which each parameter  i is maximized individually  conditionally on the other parameters remaining fixed              Itself can be extended into the Expectation conditional maximization either  ECME  algorithm             
This idea is further extended in generalized expectation maximization  GEM  algorithm  in which is sought only an increase in the objective function F for both the E step and M step as described in the As a maximization maximization procedure section              GEM is further developed in a distributed environment and shows promising results             
It is also possible to consider the EM algorithm as a subclass of the MM  Majorize Minimize or Minorize Maximize  depending on context  algorithm              and therefore use any machinery developed in the more general case 

  EM algorithm edit 
The Q function used in the EM algorithm is based on the log likelihood  Therefore  it is regarded as the log EM algorithm  The use of the log likelihood can be generalized to that of the   log likelihood ratio  Then  the   log likelihood ratio of the observed data can be exactly expressed as equality by using the Q function of the   log likelihood ratio and the   divergence  Obtaining this Q function is a generalized E step  Its maximization is a generalized M step  This pair is called the   EM algorithm            
which contains the log EM algorithm as its subclass  Thus  the   EM algorithm by Yasuo Matsuyama is an exact generalization of the log EM algorithm  No computation of gradient or Hessian matrix is needed  The   EM shows faster convergence than the log EM algorithm by choosing an appropriate    The   EM algorithm leads to a faster version of the Hidden Markov model estimation algorithm   HMM 
            

Relation to variational Bayes methods edit 
EM is a partially non Bayesian  maximum likelihood method  Its final result gives a probability distribution over the latent variables  in the Bayesian style  together with a point estimate for    either a maximum likelihood estimate or a posterior mode   A fully Bayesian version of this may be wanted  giving a probability distribution over   and the latent variables  The Bayesian approach to inference is simply to treat   as another latent variable  In this paradigm  the distinction between the E and M steps disappears  If using the factorized Q approximation as described above  variational Bayes   solving can iterate over each latent variable  now including    and optimize them one at a time  Now  k steps per iteration are needed  where k is the number of latent variables  For graphical models this is easy to do as each variable s new Q depends only on its Markov blanket  so local message passing can be used for efficient inference 

Geometric interpretation edit 
Further information  Information geometry
In information geometry  the E step and the M step are interpreted as projections under dual affine connections  called the e connection and the m connection  the Kullback Leibler divergence can also be understood in these terms 

Examples edit 
Gaussian mixture edit 
Comparison of k means and EM on artificial data visualized with ELKI  Using the variances  the EM algorithm can describe the normal distributions exactly  while k means splits the data in Voronoi cells  The cluster center is indicated by the lighter  bigger symbol 
An animation demonstrating the EM algorithm fitting a two component Gaussian mixture model to the Old Faithful dataset  The algorithm steps through from a random initialization to convergence  
Let 
  
    
      
        
          x
        
         
         
        
          
            x
          
          
             
          
        
         
        
          
            x
          
          
             
          
        
         
          x     
         
        
          
            x
          
          
            n
          
        
         
      
    
      displaystyle  mathbf  x     mathbf  x        mathbf  x        ldots   mathbf  x    n   
  
 be a sample of 
  
    
      
        n
      
    
      displaystyle n 
  
 independent observations from a mixture of two multivariate normal distributions of dimension 
  
    
      
        d
      
    
      displaystyle d 
  
  and let 
  
    
      
        
          z
        
         
         
        
          z
          
             
          
        
         
        
          z
          
             
          
        
         
          x     
         
        
          z
          
            n
          
        
         
      
    
      displaystyle  mathbf  z    z     z      ldots  z  n   
  
 be the latent variables that determine the component from which the observation originates             


  
    
      
        
          X
          
            i
          
        
          x     
         
        
          Z
          
            i
          
        
         
         
         
          x   c 
        
          
            
              N
            
          
          
            d
          
        
         
        
          
              x bc 
          
          
             
          
        
         
        
            x a  
          
             
          
        
         
      
    
      displaystyle X  i  mid  Z  i     sim   mathcal  N    d    boldsymbol   mu         Sigma       
  
 and 
  
    
      
        
          X
          
            i
          
        
          x     
         
        
          Z
          
            i
          
        
         
         
         
          x   c 
        
          
            
              N
            
          
          
            d
          
        
         
        
          
              x bc 
          
          
             
          
        
         
        
            x a  
          
             
          
        
         
         
      
    
      displaystyle X  i  mid  Z  i     sim   mathcal  N    d    boldsymbol   mu         Sigma        
  

where


  
    
      
        P
          x     
         
        
          Z
          
            i
          
        
         
         
         
         
        
            x c  
          
             
          
        
        
      
    
      displaystyle  operatorname  P   Z  i      tau        
  
 and 
  
    
      
        P
          x     
         
        
          Z
          
            i
          
        
         
         
         
         
        
            x c  
          
             
          
        
         
         
          x     
        
            x c  
          
             
          
        
         
      
    
      displaystyle  operatorname  P   Z  i      tau         tau       
  

The aim is to estimate the unknown parameters representing the mixing value between the Gaussians and the means and covariances of each 


  
    
      
          x b  
         
        
          
             
          
        
        
            x c  
        
         
        
          
              x bc 
          
          
             
          
        
         
        
          
              x bc 
          
          
             
          
        
         
        
            x a  
          
             
          
        
         
        
            x a  
          
             
          
        
        
          
             
          
        
         
      
    
      displaystyle  theta    big     boldsymbol   tau      boldsymbol   mu          boldsymbol   mu         Sigma       Sigma       big     
  

where the incomplete data likelihood function is


  
    
      
        L
         
          x b  
         
        
          x
        
         
         
        
            x   f 
          
            i
             
             
          
          
            n
          
        
        
            x     
          
            j
             
             
          
          
             
          
        
        
            x c  
          
            j
          
        
          xa  
        f
         
        
          
            x
          
          
            i
          
        
         
        
          
              x bc 
          
          
            j
          
        
         
        
            x a  
          
            j
          
        
         
         
      
    
      displaystyle L  theta   mathbf  x     prod   i     n  sum   j        tau   j   f  mathbf  x    i    boldsymbol   mu     j   Sigma   j    
  

and the complete data likelihood function is


  
    
      
        L
         
          x b  
         
        
          x
        
         
        
          z
        
         
         
        p
         
        
          x
        
         
        
          z
        
          x     
          x b  
         
         
        
            x   f 
          
            i
             
             
          
          
            n
          
        
        
            x   f 
          
            j
             
             
          
          
             
          
        
          xa  
         
        f
         
        
          
            x
          
          
            i
          
        
         
        
          
              x bc 
          
          
            j
          
        
         
        
            x a  
          
            j
          
        
         
        
            x c  
          
            j
          
        
        
           
          
            
              I
            
             
            
              z
              
                i
              
            
             
            j
             
          
        
         
      
    
      displaystyle L  theta   mathbf  x    mathbf  z    p  mathbf  x    mathbf  z   mid  theta    prod   i     n  prod   j          f  mathbf  x    i    boldsymbol   mu     j   Sigma   j   tau   j     mathbb  I   z  i  j    
  

or


  
    
      
        L
         
          x b  
         
        
          x
        
         
        
          z
        
         
         
        exp
          x     
        
           
          
            
                x     
              
                i
                 
                 
              
              
                n
              
            
            
                x     
              
                j
                 
                 
              
              
                 
              
            
            
              I
            
             
            
              z
              
                i
              
            
             
            j
             
            
              
                 
              
            
            log
              x     
            
                x c  
              
                j
              
            
              x     
            
              
                
                   
                   
                
              
            
            log
              x     
            
               
            
            
                x a  
              
                j
              
            
            
               
            
              x     
            
              
                
                   
                   
                
              
            
             
            
              
                x
              
              
                i
              
            
              x     
            
              
                  x bc 
              
              
                j
              
            
            
               
              
                  x  a  
              
            
            
                x a  
              
                j
              
              
                  x     
                 
              
            
             
            
              
                x
              
              
                i
              
            
              x     
            
              
                  x bc 
              
              
                j
              
            
             
              x     
            
              
                
                  d
                   
                
              
            
            log
              x     
             
             
              x c  
             
            
              
                 
              
            
          
           
        
         
      
    
      displaystyle L  theta   mathbf  x    mathbf  z     exp  left   sum   i     n  sum   j        mathbb  I   z  i  j   big    log  tau   j    tfrac         log   Sigma   j     tfrac          mathbf  x    i    boldsymbol   mu     j     top   Sigma   j        mathbf  x    i    boldsymbol   mu     j     tfrac  d      log   pi    big    right    
  

where 
  
    
      
        
          I
        
      
    
      displaystyle  mathbb  I   
  
 is an indicator function and 
  
    
      
        f
      
    
      displaystyle f 
  
 is the probability density function of a multivariate normal 
In the last equality  for each i  one indicator 
  
    
      
        
          I
        
         
        
          z
          
            i
          
        
         
        j
         
      
    
      displaystyle  mathbb  I   z  i  j  
  
 is equal to zero  and one indicator is equal to one  The inner sum thus reduces to one term 

E step edit 
Given our current estimate of the parameters   t   the conditional distribution of the Zi is determined by Bayes  theorem to be the proportional height of the normal density weighted by   


  
    
      
        
          T
          
            j
             
            i
          
          
             
            t
             
          
        
          
        P
          x     
         
        
          Z
          
            i
          
        
         
        j
          x     
        
          X
          
            i
          
        
         
        
          
            x
          
          
            i
          
        
         
        
            x b  
          
             
            t
             
          
        
         
         
        
          
            
              
                  x c  
                
                  j
                
                
                   
                  t
                   
                
              
                xa  
              f
               
              
                
                  x
                
                
                  i
                
              
               
              
                
                    x bc 
                
                
                  j
                
                
                   
                  t
                   
                
              
               
              
                  x a  
                
                  j
                
                
                   
                  t
                   
                
              
               
            
            
              
                  x c  
                
                   
                
                
                   
                  t
                   
                
              
                xa  
              f
               
              
                
                  x
                
                
                  i
                
              
               
              
                
                    x bc 
                
                
                   
                
                
                   
                  t
                   
                
              
               
              
                  x a  
                
                   
                
                
                   
                  t
                   
                
              
               
               
              
                  x c  
                
                   
                
                
                   
                  t
                   
                
              
                xa  
              f
               
              
                
                  x
                
                
                  i
                
              
               
              
                
                    x bc 
                
                
                   
                
                
                   
                  t
                   
                
              
               
              
                  x a  
                
                   
                
                
                   
                  t
                   
                
              
               
            
          
        
         
      
    
      displaystyle T  j i    t     operatorname  P   Z  i  j mid X  i   mathbf  x    i   theta    t      frac   tau   j    t    f  mathbf  x    i    boldsymbol   mu     j    t    Sigma   j    t      tau        t    f  mathbf  x    i    boldsymbol   mu          t    Sigma        t     tau        t    f  mathbf  x    i    boldsymbol   mu          t    Sigma        t       
  

These are called the  membership probabilities   which are normally considered the output of the E step  although this is not the Q function of below  
This E step corresponds with setting up this function for Q 


  
    
      
        
          
            
              
                Q
                 
                  x b  
                  x     
                
                    x b  
                  
                     
                    t
                     
                  
                
                 
              
              
                
                 
                
                  E
                  
                    
                      Z
                    
                      x     
                    
                      X
                    
                     
                    
                      x
                    
                     
                    
                      
                          x b  
                      
                      
                         
                        t
                         
                      
                    
                  
                
                  x     
                 
                log
                  x     
                L
                 
                  x b  
                 
                
                  x
                
                 
                
                  Z
                
                 
                 
              
            
            
              
              
                
                 
                
                  E
                  
                    
                      Z
                    
                      x     
                    
                      X
                    
                     
                    
                      x
                    
                     
                    
                      
                          x b  
                      
                      
                         
                        t
                         
                      
                    
                  
                
                  x     
                 
                log
                  x     
                
                    x   f 
                  
                    i
                     
                     
                  
                  
                    n
                  
                
                L
                 
                  x b  
                 
                
                  
                    x
                  
                  
                    i
                  
                
                 
                
                  Z
                  
                    i
                  
                
                 
                 
              
            
            
              
              
                
                 
                
                  E
                  
                    
                      Z
                    
                      x     
                    
                      X
                    
                     
                    
                      x
                    
                     
                    
                      
                          x b  
                      
                      
                         
                        t
                         
                      
                    
                  
                
                  x     
                 
                
                    x     
                  
                    i
                     
                     
                  
                  
                    n
                  
                
                log
                  x     
                L
                 
                  x b  
                 
                
                  
                    x
                  
                  
                    i
                  
                
                 
                
                  Z
                  
                    i
                  
                
                 
                 
              
            
            
              
              
                
                 
                
                    x     
                  
                    i
                     
                     
                  
                  
                    n
                  
                
                
                  E
                  
                    
                      Z
                      
                        i
                      
                    
                      x     
                    
                      X
                      
                        i
                      
                    
                     
                    
                      x
                      
                        i
                      
                    
                     
                    
                      
                          x b  
                      
                      
                         
                        t
                         
                      
                    
                  
                
                  x     
                 
                log
                  x     
                L
                 
                  x b  
                 
                
                  
                    x
                  
                  
                    i
                  
                
                 
                
                  Z
                  
                    i
                  
                
                 
                 
              
            
            
              
              
                
                 
                
                    x     
                  
                    i
                     
                     
                  
                  
                    n
                  
                
                
                    x     
                  
                    j
                     
                     
                  
                  
                     
                  
                
                P
                 
                
                  Z
                  
                    i
                  
                
                 
                j
                  x     
                
                  X
                  
                    i
                  
                
                 
                
                  
                    x
                  
                  
                    i
                  
                
                 
                
                    x b  
                  
                     
                    t
                     
                  
                
                 
                log
                  x     
                L
                 
                
                    x b  
                  
                    j
                  
                
                 
                
                  
                    x
                  
                  
                    i
                  
                
                 
                j
                 
              
            
            
              
              
                
                 
                
                    x     
                  
                    i
                     
                     
                  
                  
                    n
                  
                
                
                    x     
                  
                    j
                     
                     
                  
                  
                     
                  
                
                
                  T
                  
                    j
                     
                    i
                  
                  
                     
                    t
                     
                  
                
                
                  
                     
                  
                
                log
                  x     
                
                    x c  
                  
                    j
                  
                
                  x     
                
                  
                    
                       
                       
                    
                  
                
                log
                  x     
                
                   
                
                
                    x a  
                  
                    j
                  
                
                
                   
                
                  x     
                
                  
                    
                       
                       
                    
                  
                
                 
                
                  
                    x
                  
                  
                    i
                  
                
                  x     
                
                  
                      x bc 
                  
                  
                    j
                  
                
                
                   
                  
                      x  a  
                  
                
                
                    x a  
                  
                    j
                  
                  
                      x     
                     
                  
                
                 
                
                  
                    x
                  
                  
                    i
                  
                
                  x     
                
                  
                      x bc 
                  
                  
                    j
                  
                
                 
                  x     
                
                  
                    
                      d
                       
                    
                  
                
                log
                  x     
                 
                 
                  x c  
                 
                
                  
                     
                  
                
                 
              
            
          
        
      
    
      displaystyle   begin aligned Q  theta  mid  theta    t    amp   operatorname  E     mathbf  Z   mid  mathbf  X    mathbf  x    mathbf   theta      t     log L  theta   mathbf  x    mathbf  Z       amp   operatorname  E     mathbf  Z   mid  mathbf  X    mathbf  x    mathbf   theta      t     log  prod   i     n L  theta   mathbf  x    i  Z  i      amp   operatorname  E     mathbf  Z   mid  mathbf  X    mathbf  x    mathbf   theta      t     sum   i     n  log L  theta   mathbf  x    i  Z  i      amp   sum   i     n  operatorname  E    Z  i  mid X  i  x  i   mathbf   theta      t     log L  theta   mathbf  x    i  Z  i      amp   sum   i     n  sum   j       P Z  i  j mid X  i   mathbf  x    i   theta    t    log L  theta   j   mathbf  x    i  j    amp   sum   i     n  sum   j       T  j i    t    big    log  tau   j    tfrac         log   Sigma   j     tfrac          mathbf  x    i    boldsymbol   mu     j     top   Sigma   j        mathbf  x    i    boldsymbol   mu     j     tfrac  d      log   pi    big     end aligned   
  

The expectation of 
  
    
      
        log
          x     
        L
         
          x b  
         
        
          
            x
          
          
            i
          
        
         
        
          Z
          
            i
          
        
         
      
    
      displaystyle  log L  theta   mathbf  x    i  Z  i   
  
 inside the sum is taken with respect to the probability density function 
  
    
      
        P
         
        
          Z
          
            i
          
        
          x     
        
          X
          
            i
          
        
         
        
          
            x
          
          
            i
          
        
         
        
            x b  
          
             
            t
             
          
        
         
      
    
      displaystyle P Z  i  mid X  i   mathbf  x    i   theta    t    
  
  which might be different for each  
  
    
      
        
          
            x
          
          
            i
          
        
      
    
      displaystyle  mathbf  x    i  
  
 of the training set  Everything in the E step is known before the step is taken except 
  
    
      
        
          T
          
            j
             
            i
          
        
      
    
      displaystyle T  j i  
  
  which is computed according to the equation at the beginning of the E step section 
This full conditional expectation does not need to be calculated in one step  because   and     appear in separate linear terms and can thus be maximized independently 

M step edit 

  
    
      
        Q
         
          x b  
          x     
        
            x b  
          
             
            t
             
          
        
         
      
    
      displaystyle Q  theta  mid  theta    t    
  
 being quadratic in form means that determining the maximizing values of 
  
    
      
          x b  
      
    
      displaystyle  theta  
  
 is relatively straightforward  Also  
  
    
      
          x c  
      
    
      displaystyle  tau  
  
  
  
    
      
         
        
          
              x bc 
          
          
             
          
        
         
        
            x a  
          
             
          
        
         
      
    
      displaystyle    boldsymbol   mu         Sigma       
  
 and 
  
    
      
         
        
          
              x bc 
          
          
             
          
        
         
        
            x a  
          
             
          
        
         
      
    
      displaystyle    boldsymbol   mu         Sigma       
  
 may all be maximized independently since they all appear in separate linear terms 
To begin  consider 
  
    
      
          x c  
      
    
      displaystyle  tau  
  
  which has the constraint 
  
    
      
        
            x c  
          
             
          
        
         
        
            x c  
          
             
          
        
         
         
      
    
      displaystyle  tau       tau        
  
 


  
    
      
        
          
            
              
                
                  
                      x c  
                  
                  
                     
                    t
                     
                     
                     
                  
                
              
              
                
                 
                
                  
                    
                      a
                      r
                      g
                      
                      m
                      a
                      x
                    
                      x c  
                  
                
                  xa  
                Q
                 
                  x b  
                  x     
                
                    x b  
                  
                     
                    t
                     
                  
                
                 
              
            
            
              
              
                
                 
                
                  
                    
                      a
                      r
                      g
                      
                      m
                      a
                      x
                    
                      x c  
                  
                
                  xa  
                
                   
                  
                    
                       
                      
                        
                            x     
                          
                            i
                             
                             
                          
                          
                            n
                          
                        
                        
                          T
                          
                             
                             
                            i
                          
                          
                             
                            t
                             
                          
                        
                      
                       
                    
                    log
                      x     
                    
                        x c  
                      
                         
                      
                    
                     
                    
                       
                      
                        
                            x     
                          
                            i
                             
                             
                          
                          
                            n
                          
                        
                        
                          T
                          
                             
                             
                            i
                          
                          
                             
                            t
                             
                          
                        
                      
                       
                    
                    log
                      x     
                    
                        x c  
                      
                         
                      
                    
                  
                   
                
                 
              
            
          
        
      
    
      displaystyle   begin aligned   boldsymbol   tau      t     amp    underset   boldsymbol   tau     operatorname  arg  max      Q  theta  mid  theta    t      amp    underset   boldsymbol   tau     operatorname  arg  max       left   left  sum   i     n T    i    t   right  log  tau       left  sum   i     n T    i    t   right  log  tau      right    end aligned   
  

This has the same form as the maximum likelihood estimate for the binomial distribution  so


  
    
      
        
            x c  
          
            j
          
          
             
            t
             
             
             
          
        
         
        
          
            
              
                  x     
                
                  i
                   
                   
                
                
                  n
                
              
              
                T
                
                  j
                   
                  i
                
                
                   
                  t
                   
                
              
            
            
              
                  x     
                
                  i
                   
                   
                
                
                  n
                
              
               
              
                T
                
                   
                   
                  i
                
                
                   
                  t
                   
                
              
               
              
                T
                
                   
                   
                  i
                
                
                   
                  t
                   
                
              
               
            
          
        
         
        
          
             
            n
          
        
        
            x     
          
            i
             
             
          
          
            n
          
        
        
          T
          
            j
             
            i
          
          
             
            t
             
          
        
         
      
    
      displaystyle  tau   j    t       frac   sum   i     n T  j i    t     sum   i     n  T    i    t   T    i    t        frac     n   sum   i     n T  j i    t    
  

For the next estimates of 
  
    
      
         
        
          
              x bc 
          
          
             
          
        
         
        
            x a  
          
             
          
        
         
      
    
      displaystyle    boldsymbol   mu         Sigma       
  
 


  
    
      
        
          
            
              
                 
                
                  
                      x bc 
                  
                  
                     
                  
                  
                     
                    t
                     
                     
                     
                  
                
                 
                
                    x a  
                  
                     
                  
                  
                     
                    t
                     
                     
                     
                  
                
                 
              
              
                
                 
                
                  
                    
                      a
                      r
                      g
                      
                      m
                      a
                      x
                    
                    
                      
                        
                            x bc 
                        
                        
                           
                        
                      
                       
                      
                          x a  
                        
                           
                        
                      
                    
                  
                
                  xa  
                Q
                 
                  x b  
                  x     
                
                    x b  
                  
                     
                    t
                     
                  
                
                 
              
            
            
              
              
                
                 
                
                  
                    
                      a
                      r
                      g
                      
                      m
                      a
                      x
                    
                    
                      
                        
                            x bc 
                        
                        
                           
                        
                      
                       
                      
                          x a  
                        
                           
                        
                      
                    
                  
                
                  xa  
                
                    x     
                  
                    i
                     
                     
                  
                  
                    n
                  
                
                
                  T
                  
                     
                     
                    i
                  
                  
                     
                    t
                     
                  
                
                
                   
                  
                      x     
                    
                      
                        
                           
                           
                        
                      
                    
                    log
                      x     
                    
                       
                    
                    
                        x a  
                      
                         
                      
                    
                    
                       
                    
                      x     
                    
                      
                        
                           
                           
                        
                      
                    
                     
                    
                      
                        x
                      
                      
                        i
                      
                    
                      x     
                    
                      
                          x bc 
                      
                      
                         
                      
                    
                    
                       
                      
                          x  a  
                      
                    
                    
                        x a  
                      
                         
                      
                      
                          x     
                         
                      
                    
                     
                    
                      
                        x
                      
                      
                        i
                      
                    
                      x     
                    
                      
                          x bc 
                      
                      
                         
                      
                    
                     
                  
                   
                
              
            
          
        
         
      
    
      displaystyle   begin aligned    boldsymbol   mu          t      Sigma        t      amp    underset    boldsymbol   mu         Sigma        operatorname  arg  max      Q  theta  mid  theta    t      amp    underset    boldsymbol   mu         Sigma        operatorname  arg  max       sum   i     n T    i    t   left     tfrac         log   Sigma         tfrac          mathbf  x    i    boldsymbol   mu           top   Sigma            mathbf  x    i    boldsymbol   mu         right   end aligned    
  

This has the same form as a weighted maximum likelihood estimate for a normal distribution  so


  
    
      
        
          
              x bc 
          
          
             
          
          
             
            t
             
             
             
          
        
         
        
          
            
              
                  x     
                
                  i
                   
                   
                
                
                  n
                
              
              
                T
                
                   
                   
                  i
                
                
                   
                  t
                   
                
              
              
                
                  x
                
                
                  i
                
              
            
            
              
                  x     
                
                  i
                   
                   
                
                
                  n
                
              
              
                T
                
                   
                   
                  i
                
                
                   
                  t
                   
                
              
            
          
        
      
    
      displaystyle   boldsymbol   mu          t       frac   sum   i     n T    i    t   mathbf  x    i    sum   i     n T    i    t     
  
 and 
  
    
      
        
            x a  
          
             
          
          
             
            t
             
             
             
          
        
         
        
          
            
              
                  x     
                
                  i
                   
                   
                
                
                  n
                
              
              
                T
                
                   
                   
                  i
                
                
                   
                  t
                   
                
              
               
              
                
                  x
                
                
                  i
                
              
                x     
              
                
                    x bc 
                
                
                   
                
                
                   
                  t
                   
                   
                   
                
              
               
               
              
                
                  x
                
                
                  i
                
              
                x     
              
                
                    x bc 
                
                
                   
                
                
                   
                  t
                   
                   
                   
                
              
              
                 
                
                    x  a  
                
              
            
            
              
                  x     
                
                  i
                   
                   
                
                
                  n
                
              
              
                T
                
                   
                   
                  i
                
                
                   
                  t
                   
                
              
            
          
        
      
    
      displaystyle  Sigma        t       frac   sum   i     n T    i    t    mathbf  x    i    boldsymbol   mu          t       mathbf  x    i    boldsymbol   mu          t        top     sum   i     n T    i    t     
  

and  by symmetry 


  
    
      
        
          
              x bc 
          
          
             
          
          
             
            t
             
             
             
          
        
         
        
          
            
              
                  x     
                
                  i
                   
                   
                
                
                  n
                
              
              
                T
                
                   
                   
                  i
                
                
                   
                  t
                   
                
              
              
                
                  x
                
                
                  i
                
              
            
            
              
                  x     
                
                  i
                   
                   
                
                
                  n
                
              
              
                T
                
                   
                   
                  i
                
                
                   
                  t
                   
                
              
            
          
        
      
    
      displaystyle   boldsymbol   mu          t       frac   sum   i     n T    i    t   mathbf  x    i    sum   i     n T    i    t     
  
 and 
  
    
      
        
            x a  
          
             
          
          
             
            t
             
             
             
          
        
         
        
          
            
              
                  x     
                
                  i
                   
                   
                
                
                  n
                
              
              
                T
                
                   
                   
                  i
                
                
                   
                  t
                   
                
              
               
              
                
                  x
                
                
                  i
                
              
                x     
              
                
                    x bc 
                
                
                   
                
                
                   
                  t
                   
                   
                   
                
              
               
               
              
                
                  x
                
                
                  i
                
              
                x     
              
                
                    x bc 
                
                
                   
                
                
                   
                  t
                   
                   
                   
                
              
              
                 
                
                    x  a  
                
              
            
            
              
                  x     
                
                  i
                   
                   
                
                
                  n
                
              
              
                T
                
                   
                   
                  i
                
                
                   
                  t
                   
                
              
            
          
        
         
      
    
      displaystyle  Sigma        t       frac   sum   i     n T    i    t    mathbf  x    i    boldsymbol   mu          t       mathbf  x    i    boldsymbol   mu          t        top     sum   i     n T    i    t      
  

Termination edit 
Conclude the iterative process if 
  
    
      
        
          E
          
            Z
              x     
            
                x b  
              
                 
                t
                 
              
            
             
            
              x
            
          
        
         
        log
          x     
        L
         
        
            x b  
          
             
            t
             
          
        
         
        
          x
        
         
        
          Z
        
         
         
          x     
        
          E
          
            Z
              x     
            
                x b  
              
                 
                t
                  x     
                 
                 
              
            
             
            
              x
            
          
        
         
        log
          x     
        L
         
        
            x b  
          
             
            t
              x     
             
             
          
        
         
        
          x
        
         
        
          Z
        
         
         
         
          x b  
      
    
      displaystyle E  Z mid  theta    t    mathbf  x     log L  theta    t    mathbf  x    mathbf  Z     leq E  Z mid  theta    t      mathbf  x     log L  theta    t      mathbf  x    mathbf  Z      varepsilon  
  
 for 
  
    
      
          x b  
      
    
      displaystyle  varepsilon  
  
 below some preset threshold 

Generalization edit 
The algorithm illustrated above can be generalized for mixtures of more than two multivariate normal distributions 

Truncated and censored regression edit 
The EM algorithm has been implemented in the case where an underlying linear regression model exists explaining the variation of some quantity  but where the values actually observed are censored or truncated versions of those represented in the model              Special cases of this model include censored or truncated observations from one normal distribution             

Alternatives edit 
EM typically converges to a local optimum  not necessarily the global optimum  with no bound on the convergence rate in general  It is possible that it can be arbitrarily poor in high dimensions and there can be an exponential number of local optima  Hence  a need exists for alternative methods for guaranteed learning  especially in the high dimensional setting  Alternatives to EM exist with better guarantees for consistency  which are termed moment based approaches             or the so called spectral techniques                          Moment based approaches to learning the parameters of a probabilistic model enjoy guarantees such as global convergence under certain conditions unlike EM which is often plagued by the issue of getting stuck in local optima  Algorithms with guarantees for learning can be derived for a number of important models such as mixture models  HMMs etc  For these spectral methods  no spurious local optima occur  and the true parameters can be consistently estimated under some regularity conditions      citation needed     

See also edit 
mixture distribution
compound distribution
density estimation
Principal component analysis
total absorption spectroscopy
The EM algorithm can be viewed as a special case of the majorize minimization  MM  algorithm             
References edit 


  Meng  X  L   van Dyk  D           The EM algorithm   an old folk song sung to a fast new tune   J  Royal Statist  Soc  B                   doi                          S CID               

  Jeongyeol Kwon  Constantine Caramanis

Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics  PMLR                     

  
Dempster  A P   Laird  N M   Rubin  D B           Maximum Likelihood from Incomplete Data via the EM Algorithm   Journal of the Royal Statistical Society  Series B                doi         j                tb      x  JSTOR               MR              

  Ceppelini  R M           The estimation of gene frequencies in a random mating population   Ann  Hum  Genet                  doi         j                tb      x  PMID                S CID               

  Hartley  Herman Otto          Maximum Likelihood estimation from incomplete data   Biometrics                   doi                  JSTOR              

  Ng  Shu Kay  Krishnan  Thriyambakam  McLachlan  Geoffrey J                 The EM Algorithm   Handbook of Computational Statistics  Berlin  Heidelberg  Springer Berlin Heidelberg  pp                doi                              ISBN                         S CID                retrieved           

  Sundberg  Rolf          Maximum likelihood theory for incomplete data from an exponential family   Scandinavian Journal of Statistics                JSTOR               MR              

  a b 
Rolf Sundberg        Maximum likelihood theory and applications for distributions generated when observing a function of an exponential family variable  Dissertation  Institute for Mathematical Statistics  Stockholm University 

  a b Sundberg  Rolf          An iterative method for solution of the likelihood equations for incomplete data from exponential families   Communications in Statistics   Simulation and Computation                doi                            MR              

  See the acknowledgement by Dempster  Laird and Rubin on pages      and    

  a b Per Martin L f        Statistics from the point of view of statistical mechanics  Lecture notes  Mathematical Institute  Aarhus University    Sundberg formula   credited to Anders Martin L f  

  a b Per Martin L f        Statistiska Modeller  Statistical Models   Anteckningar fr n seminarier l s ret            Lecture notes             with the assistance of Rolf Sundberg  Stockholm University  

  a b Martin L f  P  The notion of redundancy and its use as a quantitative measure of the deviation between a statistical hypothesis and a set of observational data  With a discussion by F  Abildg rd  A  P  Dempster  D  Basu  D  R  Cox  A  W  F  Edwards  D  A  Sprott  G  A  Barnard  O  Barndorff Nielsen  J  D  Kalbfleisch and G  Rasch and a reply by the author  Proceedings of Conference on Foundational Questions in Statistical Inference  Aarhus         pp        Memoirs  No     Dept  Theoret  Statist   Inst  Math   Univ  Aarhus  Aarhus       

  a b Martin L f  Per          The notion of redundancy and its use as a quantitative measure of the discrepancy between a statistical hypothesis and a set of observational data   Scand  J  Statist              

  a b c 
Wu  C  F  Jeff  Mar         On the Convergence Properties of the EM Algorithm   Annals of Statistics                  doi         aos             JSTOR               MR              

  Sundberg  Rolf         Statistical Modelling by Exponential Families  Cambridge University Press  ISBN                    

  Laird  Nan          Sundberg formulas   Encyclopedia of Statistical Sciences  Wiley  doi                    ess     pub   ISBN                 

  Little  Roderick J A   Rubin  Donald B          Statistical Analysis with Missing Data  Wiley Series in Probability and Mathematical Statistics  New York  John Wiley  amp  Sons  pp                ISBN                        

  a b Neal  Radford  Hinton  Geoffrey          A view of the EM algorithm that justifies incremental  sparse  and other variants   In Michael I  Jordan  ed    Learning in Graphical Models  PDF   Cambridge  MA  MIT Press  pp                ISBN                         Retrieved            

  a b Hastie  Trevor  Tibshirani  Robert  Friedman  Jerome              The EM algorithm   The Elements of Statistical Learning  New York  Springer  pp                ISBN                        

  Lindstrom  Mary J  Bates  Douglas M          Newton Raphson and EM Algorithms for Linear Mixed Effects Models for Repeated Measures Data   Journal of the American Statistical Association                  doi                                

  Van Dyk  David A          Fitting Mixed Effects Models Using Efficient EM Type Algorithms   Journal of Computational and Graphical Statistics                doi                  JSTOR              

  Diffey  S  M  Smith  A  B  Welsh  A  H  Cullis  B  R          A new REML  parameter expanded  EM algorithm for linear mixed models   Australian  amp  New Zealand Journal of Statistics               doi         anzs        hdl             

  Matarazzo  T  J   and Pakzad  S  N           STRIDE for Structural Identification using Expectation Maximization  Iterative Output Only Method for Modal Identification   Journal of Engineering Mechanics http   ascelibrary org doi abs          ASCE EM                  

  Kreer  Markus  Kizilersu  Ayse  Thomas  Anthony W           Censored expectation maximization algorithm for mixtures  Application to intertrade waiting times   Physica A  Statistical Mechanics and Its Applications                   Bibcode     PhyA          K  doi         j physa              ISSN                 S CID                

  Einicke  G  A   Malos  J  T   Reid  D  C   Hainsworth  D  W   January         Riccati Equation and EM Algorithm Convergence for Inertial Navigation Alignment   IEEE Trans  Signal Process                   Bibcode     ITSP          E  doi         TSP               S CID              

  Einicke  G  A   Falco  G   Malos  J  T   May         EM Algorithm State Matrix Estimation for Navigation   IEEE Signal Processing Letters                   Bibcode     ISPL          E  doi         LSP               S CID               

  Einicke  G  A   Falco  G   Dunn  M  T   Reid  D  C   May         Iterative Smoother Based Variance Estimation   IEEE Signal Processing Letters                   Bibcode     ISPL          E  doi         LSP               S CID               

  Einicke  G  A   Sep         Iterative Filtering and Smoothing of Measurements Possessing Poisson Noise   IEEE Transactions on Aerospace and Electronic Systems                     Bibcode     ITAES         E  doi         TAES              S CID               

  Jamshidian  Mortaza  Jennrich  Robert I           Acceleration of the EM Algorithm by using Quasi Newton Methods   Journal of the Royal Statistical Society  Series B                   doi                          MR               S CID                

  Liu  C          Parameter expansion to accelerate EM  The PX EM algorithm   Biometrika                   CiteSeerX                       doi         biomet          

  Meng  Xiao Li  Rubin  Donald B           Maximum likelihood estimation via the ECM algorithm  A general framework   Biometrika                   doi         biomet           MR               S CID               

  Liu  Chuanhai  Rubin  Donald B          The ECME Algorithm  A Simple Extension of EM and ECM with Faster Monotone Convergence   Biometrika               doi         biomet           JSTOR              

  
Jiangtao Yin  Yanfeng Zhang  Lixin Gao          Accelerating Expectation Maximization Algorithms with Frequent Updates   PDF   Proceedings of the IEEE International Conference on Cluster Computing 

  Hunter DR and Lange K         A Tutorial on MM Algorithms  The American Statistician           

  
Matsuyama  Yasuo          The   EM algorithm  Surrogate likelihood maximization using   logarithmic information measures   IEEE Transactions on Information Theory                   doi         TIT             

  
Matsuyama  Yasuo          Hidden Markov model estimation based on alpha EM algorithm  Discrete and continuous alpha HMMs   International Joint Conference on Neural Networks          

  a b Wolynetz  M S           Maximum likelihood estimation in a linear model from confined and censored normal data   Journal of the Royal Statistical Society  Series C                   doi                  JSTOR              

  Pearson  Karl          Contributions to the Mathematical Theory of Evolution   Philosophical Transactions of the Royal Society of London A               Bibcode     RSPTA         P  doi         rsta            ISSN                 JSTOR            

  Shaban  Amirreza  Mehrdad  Farajtabar  Bo  Xie  Le  Song  Byron  Boots          Learning Latent Variable Models by Improving Spectral Solutions with Exterior Point Method   PDF   UAI           Archived from the original  PDF  on             Retrieved            

  Balle  Borja Quattoni  Ariadna Carreras  Xavier               Local Loss Optimization in Operator Models  A New Insight into Spectral Learning  OCLC                  cite book     CS  maint  multiple names  authors list  link 

  Lange  Kenneth   The MM Algorithm   PDF  


Further reading edit 
Hogg  Robert  McKean  Joseph  Craig  Allen         Introduction to Mathematical Statistics  Upper Saddle River  NJ  Pearson Prentice Hall  pp               
Dellaert  Frank  February        The Expectation Maximization Algorithm  PDF   Technical Report number GIT GVU         Georgia Tech College of Computing  gives an easier explanation of EM algorithm as to lowerbound maximization 
Bishop  Christopher M          Pattern Recognition and Machine Learning  Springer  ISBN                        
Gupta  M  R   Chen  Y           Theory and Use of the EM Algorithm   Foundations and Trends in Signal Processing                  CiteSeerX                       doi                     A well written short book on EM  including detailed derivation of EM for GMMs  HMMs  and Dirichlet 
Bilmes  Jeff         A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models  Technical Report TR          International Computer Science Institute  includes a simplified derivation of the EM equations for Gaussian Mixtures and Gaussian Mixture Hidden Markov Models 
McLachlan  Geoffrey J   Krishnan  Thriyambakam         The EM Algorithm and Extensions   nd      ed    Hoboken  Wiley  ISBN                        
External links edit 
Various  D   D and  D demonstrations of EM together with Mixture Modeling are provided as part of the paired SOCR activities and applets  These applets and activities show empirically the properties of the EM algorithm for parameter estimation in diverse settings 
Class hierarchy in C    GPL  including Gaussian Mixtures
The on line textbook  Information Theory  Inference  and Learning Algorithms  by David J C  MacKay includes simple examples of the EM algorithm such as clustering using the soft k means algorithm  and emphasizes the variational view of the EM algorithm  as described in Chapter      of version      fourth edition  
Variational Algorithms for Approximate Bayesian Inference  by M  J  Beal includes comparisons of EM to Variational Bayesian EM and derivations of several models including Variational Bayesian HMMs  chapters  
The Expectation Maximization Algorithm  A short tutorial  A self contained derivation of the EM Algorithm by Sean Borman 
The EM Algorithm  by Xiaojin Zhu 
EM algorithm and variants  an informal tutorial by Alexis Roche  A concise and very clear description of EM and many interesting variants 





Retrieved from  https   en wikipedia org w index php title Expectation maximization algorithm amp oldid