This article needs to be updated  Please help update this article to reflect recent events or newly available information    May      
Game playing artificial intelligence
This article is part of the series onChess programming
Board representations
 x  
Bitboards

Evaluation functions
Deep neural networks  Transformers 
Attention
Efficiently updatable neural networks
Handcrafted evaluation functions
Piece square tables
Reinforcement learning
Stochastic gradient descent
Supervised learning
Texel tuning
Unsupervised learning

Graph and tree search algorithms
Minimax
Alpha beta pruning
Principal variation search
Quiescence search
Monte Carlo tree search

Chess computers
Belle
ChessMachine
ChipTest
Cray Blitz
Deep Blue
Deep Thought
HiTech
Hydra
Mephisto
Saitek

Chess engines
AlphaZero
Chess Tiger
Crafty
CuckooChess
Deep Fritz
Dragon by Komodo Chess
Fairy Max
Fritz
Fruit
GNU Chess
HIARCS
Houdini
Ikarus
Junior
KnightCap
Komodo
Leela Chess Zero
MChess Pro
Mittens
MuZero
Naum
REBEL
Rybka
Shredder
Sjeng
SmarThink
Stockfish
Torch
Turochamp
Zappa
vte
MuZero is a computer program developed by artificial intelligence research company DeepMind to master games without knowing their rules                                   Its release in      included benchmarks of its performance in go  chess  shogi  and a standard suite of Atari games  The algorithm uses an approach similar to AlphaZero  It matched AlphaZero s performance in chess and shogi  improved on its performance in Go  setting a new world record   and improved on the state of the art in mastering a suite of    Atari games  the Arcade Learning Environment   a visually complex domain 
MuZero was trained via self play  with no access to rules  opening books  or endgame tablebases  The trained algorithm used the same convolutional and residual architecture as AlphaZero  but with    percent fewer computation steps per node in the search tree            
MuZero s capacity to plan and learn effectively without explicit rules makes it a groundbreaking achievement in reinforcement learning and AI  pushing the boundaries of what is possible in artificial intelligence 


History edit 
MuZero really is discovering for itself how to build a model and understand it just from first principles         David Silver  DeepMind  Wired           
On November           the DeepMind team released a preprint introducing MuZero 

Derivation from AlphaZero edit 
Further information  AlphaZeroMuZero  MZ  is a combination of the high performance planning of the AlphaZero  AZ  algorithm with approaches to model free reinforcement learning  The combination allows for more efficient training in classical planning regimes  such as Go  while also handling domains with much more complex inputs at each stage  such as visual video games 
MuZero was derived directly from AZ code  sharing its rules for setting hyperparameters  Differences between the approaches include            

AZ s planning process uses a simulator  The simulator knows the rules of the game  It has to be explicitly programmed  A neural network then predicts the policy and value of a future position  Perfect knowledge of game rules is used in modeling state transitions in the search tree  actions available at each node  and termination of a branch of the tree  MZ does not have access to the rules  and instead learns one with neural networks 
AZ has a single model for the game  from board state to predictions   MZ has separate models for representation of the current state  from board state into its internal embedding   dynamics of states  how actions change representations of board states   and prediction of policy and value of a future position  given a state s representation  
MZ s hidden model may be complex  and it may turn out it can host computation  exploring the details of the hidden model in a trained instance of MZ is a topic for future exploration 
MZ does not expect a two player game where winners take all  It works with standard reinforcement learning scenarios  including single agent environments with continuous intermediate rewards  possibly of arbitrary magnitude and with time discounting  AZ was designed for two player games that could be won  drawn  or lost 
Comparison with R D  edit 
The previous state of the art technique for learning to play the suite of Atari games was R D   the Recurrent Replay Distributed DQN            
MuZero surpassed both R D  s mean and median performance across the suite of games  though it did not do better in every game 

Training and results edit 
MuZero used    third generation tensor processing units  TPUs  for training  and      TPUs for selfplay for board games  with     simulations per step and   TPUs for training and    TPUs for selfplay for Atari games  with    simulations per step 
AlphaZero used    second generation TPUs for training  and      first generation TPUs for selfplay  As TPU design has improved  third generation chips are  x as powerful individually as second generation chips  with further advances in bandwidth and networking across chips in a pod   these are comparable training setups 
R D  was trained for   days through  M training steps 

Initial results edit 
MuZero matched AlphaZero s performance in chess and Shogi after roughly   million training steps  It matched AZ s performance in Go after         training steps and surpassed it by   million steps  It matched R D  s mean and median performance across the Atari game suite after     thousand training steps and surpassed it by   million steps  though it never performed well on   games in the suite 

Reactions and related work edit 
MuZero was viewed as a significant advancement over AlphaZero  and a generalizable step forward in unsupervised learning techniques                        The work was seen as advancing understanding of how to compose systems from smaller components  a systems level development more than a pure machine learning development             
While only pseudocode was released by the development team  Werner Duvaud produced an open source implementation based on that             
MuZero has been used as a reference implementation in other work  for instance as a way to generate model based behavior             
In late       a more efficient variant of MuZero was proposed  named EfficientZero  It  achieves       percent mean human performance and       percent median performance on the Atari    k benchmark with only two hours of real time game experience              
In early       a variant of MuZero was proposed to play stochastic games  for example       backgammon   called Stochastic MuZero  which uses afterstate dynamics and chance codes to account for the stochastic nature of the environment when training the dynamics network             

See also edit 
General game playing
Unsupervised learning
References edit 


  Wiggers  Kyle     November         DeepMind s MuZero teaches itself how to win at Atari  chess  shogi  and Go   VentureBeat  Retrieved    July      

  Friedel  Frederic   MuZero figures out chess  rules and all   ChessBase GmbH  Retrieved    July      

  Rodriguez  Jesus   DeepMind Unveils MuZero  a New Agent that Mastered Chess  Shogi  Atari and Go Without Knowing the Rules   KDnuggets  Retrieved    July      

  Schrittwieser  Julian  Antonoglou  Ioannis  Hubert  Thomas  Simonyan  Karen  Sifre  Laurent  Schmitt  Simon  Guez  Arthur  Lockhart  Edward  Hassabis  Demis  Graepel  Thore  Lillicrap  Timothy          Mastering Atari  Go  chess and shogi by planning with a learned model   Nature                       arXiv             Bibcode     Natur         S  doi         s                   PMID                S CID                

   What AlphaGo Can Teach Us About How People Learn   Wired  ISSN                 Retrieved            

  Silver  David  Hubert  Thomas  Schrittwieser  Julian  Antonoglou  Ioannis  Lai  Matthew  Guez  Arthur  Lanctot  Marc  Sifre  Laurent  Kumaran  Dharshan  Graepel  Thore  Lillicrap  Timothy  Simonyan  Karen  Hassabis  Demis    December         Mastering Chess and Shogi by Self Play with a General Reinforcement Learning Algorithm   arXiv             cs AI  

  Kapturowski  Steven  Ostrovski  Georg  Quan  John  Munos  Remi  Dabney  Will  RECURRENT EXPERIENCE REPLAY IN DISTRIBUTED REINFORCEMENT LEARNING  ICLR              via Open Review 

  Shah  Rohin     November              AN           Solving Atari and Go with learned game models  and thoughts from a MIRI employee   LessWrong       www lesswrong com  Retrieved            

  Wu  Jun   Reinforcement Learning  Deep Learning s Partner   Forbes  Retrieved            

   Machine Learning  amp  Robotics  My  biased       State of the Field   cachestocaches com  Retrieved            

  Duvaud  Werner               werner duvaud muzero general  retrieved           

  van Seijen  Harm  Nekoei  Hadi  Racah  Evan  Chandar  Sarath                The LoCA Regret  A Consistent Metric to Evaluate Model Based Behavior in Reinforcement Learning   arXiv             cs stat  

  Ye  Weirui  Liu  Shaohuai  Kurutach  Thanard  Abbeel  Pieter  Gao  Yang                Mastering Atari Games with Limited Data   arXiv             cs LG  

  Antonoglou  Ioannis  Schrittwieser  Julian  Ozair  Serjil  Hubert  Thomas  Silver  David                Planning in Stochastic Environments with a Learned Model   Retrieved            


External links edit 
Initial MuZero preprint
Open source implementations
vteGoogle AI
Google
Google Brain
Google DeepMind
Computer programsAlphaGoVersions
AlphaGo       
Master       
AlphaGo Zero       
AlphaZero       
MuZero       
Competitions
Fan Hui       
Lee Sedol       
Ke Jie       
In popular culture
AlphaGo       
The MANIAC       
Other
AlphaFold       
AlphaStar       
AlphaDev       
AlphaGeometry       
Machine learningNeural networks
Inception       
WaveNet       
MobileNet       
Transformer       
EfficientNet       
Gato       
Other
Quantum Artificial Intelligence Lab
TensorFlow
Tensor Processing Unit
Generative AIChatbots
Assistant       
Sparrow       
Gemini       
Language models
BERT       
XLNet       
T        
LaMDA       
Chinchilla       
PaLM       
Gemini       
VideoPoet       
Other
DreamBooth       
NotebookLM       
Vids       
See also
 Attention Is All You Need 
Future of Go Summit
Generative pre trained transformer
Google Labs
Google Pixel
Google Workspace
Robot Constitution

 Category
 Commons

vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects






Retrieved from  https   en wikipedia org w index php title MuZero amp oldid