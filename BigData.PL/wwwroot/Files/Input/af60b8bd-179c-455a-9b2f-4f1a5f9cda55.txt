Not to be confused with Query evaluation 
For other uses  see Question and Answer 
Computer science discipline
This article needs to be updated  Please help update this article to reflect recent events or newly available information    March      
Question answering  QA  is a computer science discipline within the fields of information retrieval and natural language processing  NLP  that is concerned with building systems that automatically answer questions that are posed by humans in a natural language            


Overview edit 
A question answering implementation  usually a computer program  may construct its answers by querying a structured database of knowledge or information  usually a knowledge base  More commonly  question answering systems can pull answers from an unstructured collection of natural language documents 
Some examples of natural language document collections used for question answering systems include 

a local     clarification needed      collection of reference texts
internal organization     ambiguous      documents and web pages
compiled newswire reports
a set of Wikipedia pages           
a subset of World Wide Web pages
Types of question answering edit 
Question answering research attempts to develop ways of answering a wide range of question types  including fact  list  definition  how  why  hypothetical  semantically constrained  and cross lingual questions 

Answering questions related to an article in order to evaluate reading comprehension is one of the simpler form of question answering  since a given article is relatively short compared to the domains of other types of question answering problems  An example of such a question is  What did Albert Einstein win the Nobel Prize for   after an article about this subject is given to the system 
Closed book question answering is when a system has memorized some facts during training and can answer questions without explicitly being given a context  This is similar to humans taking closed book exams 
Closed domain question answering deals with questions under a specific domain  for example  medicine or automotive maintenance  and can exploit domain specific knowledge frequently formalized in ontologies  Alternatively   closed domain  might refer to a situation where only a limited type of questions are accepted  such as questions asking for descriptive rather than procedural information  Question answering systems in the context of     vague      machine reading applications have also been constructed in the medical domain  for instance related to     vague      Alzheimer s disease            
Open domain question answering deals with questions about nearly anything and can only rely on general ontologies and world knowledge  Systems designed for open domain question answering usually have much more data available from which to extract the answer  An example of an open domain question is  What did Albert Einstein win the Nobel Prize for   while no article about this subject is given to the system 
Another way to categorize question answering systems is by the technical approach used  There are a number of different types of QA systems  including 

rule based systems 
statistical systems  and
hybrid systems 
Rule based systems use a set of rules to determine the correct answer to a question  Statistical systems use statistical methods to find the most likely answer to a question  Hybrid systems use a combination of rule based and statistical methods 

History edit 
Two early question answering systems were BASEBALL            and LUNAR             BASEBALL answered questions about Major League Baseball over a period of one year     ambiguous       LUNAR answered questions about the geological analysis of rocks returned by the Apollo Moon missions  Both question answering systems were very effective in their chosen domains  LUNAR was demonstrated at a lunar science convention in      and it was able to answer     of the questions in its domain that were posed by people untrained on the system  Further restricted domain question answering systems were developed in the following years  The common feature of all these systems is that they had a core database or knowledge system that was hand written by experts of the chosen domain  The language abilities of BASEBALL and LUNAR used techniques similar to ELIZA and DOCTOR  the first chatterbot programs 
SHRDLU was a successful question answering program developed by Terry Winograd in the late     s and early     s  It simulated the operation of a robot in a toy world  the  blocks world    and it offered the possibility of asking the robot questions about the state of the world  The strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program 
In the     s  knowledge bases were developed that targeted narrower domains of knowledge  The question answering systems developed to interface with these expert systems produced more repeatable     clarification needed      and valid responses to questions within an area of knowledge  These expert systems closely resembled modern question answering systems except in their internal architecture  Expert systems rely heavily on expert constructed and organized knowledge bases  whereas many modern question answering systems rely on statistical processing of a large  unstructured  natural language text corpus 
The     s and     s saw the development of comprehensive theories in computational linguistics  which led to the development of ambitious projects in text comprehension and question answering  One example was the Unix Consultant  UC   developed by Robert Wilensky at U C  Berkeley in the late     s  The system answered questions pertaining to the Unix operating system  It had a comprehensive  hand crafted knowledge base of its domain  and it aimed at phrasing the answer to accommodate various types of users  Another project was LILOG  a text understanding system that operated on the domain of tourism information in a German city  The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations  but they helped the development of theories on computational linguistics and reasoning 
Specialized natural language question answering systems have been developed  such as EAGLi for health and life scientists            

Applications edit 
QA systems are used in a variety of applications  including 

Fact checking if a fact is verified  by posing a question like  is fact X true or false 
customer service 
technical support 
market research 
generating reports or conducting research 
Architecture edit 
As of          update       question answering systems typically included a question classifier module that determined the type of question and the type of answer            
Different types of question answering systems employ different architectures  For example  modern open domain question answering systems may use a retriever reader architecture  The retriever is aimed at retrieving relevant documents related to a given question  while the reader is used to infer the answer from the retrieved documents  Systems such as GPT    T              and BART            use an end to end     jargon      architecture in which a transformer based     jargon      architecture stores large scale textual data in the underlying parameters  Such models can answer questions without accessing any external knowledge sources 

Question answering methods edit 
Question answering is dependent on a good search corpus  without documents containing the answer  there is little any question answering system can do  Larger collections generally mean better question answering performance  unless the question domain is orthogonal to the collection  Data redundancy in massive collections  such as the web  means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents              leading to two benefits 

If the right information appears in many forms  the question answering system needs to perform fewer complex NLP techniques to understand the text 
Correct answers can be filtered from false positives because the system can rely on versions of the correct answer appearing more times in the corpus than incorrect ones 
Some question answering systems rely heavily on automated reasoning                         

Open domain question answering edit 
This section needs additional citations for verification  Please help improve this article by adding citations to reliable sources     in this section  Unsourced material may be challenged and removed    January        Learn how and when to remove this message 
In information retrieval  an open domain question answering system tries to return an answer in response to the user s question  The returned answer is in the form of short texts rather than a list of relevant documents              The system finds answers by using a combination of techniques from computational linguistics  information retrieval  and knowledge representation 
The system takes a natural language question as an input rather than a set of keywords  for example   When is the national day of China   It then transforms this input sentence into a query in its logical form  Accepting natural language questions makes the system more user friendly  but harder to implement  as there are a variety of question types and the system will have to identify the correct one in order to give a sensible answer  Assigning a question type to the question is a crucial task  the entire answer extraction process relies on finding the correct question type and hence the correct answer type 
Keyword extraction is the first step in identifying the input question type              In some cases  words clearly indicate the question type  e g    Who    Where    When   or  How many  these words might suggest to the system that the answers should be of type  Person    Location    Date   or  Number   respectively  POS  part of speech  tagging and syntactic parsing techniques can also determine the answer type  In the example above  the subject is  Chinese National Day   the predicate is  is  and the adverbial modifier is  when   therefore the answer type is  Date   Unfortunately  some interrogative words like  Which    What   or  How  do not correspond to unambiguous answer types  Each can represent more than one type  In situations like this  other words in the question need to be considered  A lexical dictionary such as WordNet can be used for understanding the context 
Once the system identifies the question type  it uses an information retrieval system to find a set of documents that contain the correct keywords  A tagger and NP Verb Group chunker can verify whether the correct entities and relations are mentioned in the found documents  For questions such as  Who  or  Where   a named entity recogniser finds relevant  Person  and  Location  names from the retrieved documents  Only the relevant paragraphs are selected for ranking      clarification needed     
A vector space model can classify the candidate answers  Check     who       if the answer is of the correct type as determined in the question type analysis stage  An inference technique can validate the candidate answers  A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate the more and the closer the better  The answer is then translated by parsing into a compact and meaningful representation  In the previous example  the expected output answer is   st Oct  

Mathematical question answering edit 
An open source  math aware  question answering system called MathQA  based on Ask Platypus and Wikidata  was published in                   MathQA takes an English or Hindi natural language question as input and returns a mathematical formula retrieved from Wikidata as a succinct answer  translated into a computable form that allows the user to insert values for the variables  The system retrieves names and values of variables and common constants from Wikidata if those are available  It is claimed that the system outperforms a commercial computational mathematical knowledge engine on a test set              MathQA is hosted by Wikimedia at https   mathqa wmflabs org   In       it was extended to answer    math question types             
MathQA methods need to combine natural and formula language  One possible approach is to perform supervised annotation via Entity Linking  The  ARQMath Task  at CLEF                  was launched to address the problem of linking newly posted questions from the platform Math Stack Exchange to existing ones that were already answered by the community  Providing hyperlinks to already answered  semantically related questions helps users to get answers earlier but is a challenging problem because semantic relatedness is not trivial              The lab was motivated by the fact that     of mathematical queries in general purpose search engines are expressed as well formed questions              The challenge contained two separate sub tasks  Task     Answer retrieval  matching old post answers to newly posed questions  and Task     Formula retrieval  matching old post formulae to new questions  Starting with the domain of mathematics  which involves formula language  the goal is to later extend the task to other domains  e g   STEM disciplines  such as chemistry  biology  etc    which employ other types of special notation  e g   chemical formulae                          
The inverse of mathematical question answering mathematical question generation has also been researched  The PhysWikiQuiz physics question generation and test engine retrieves mathematical formulae from Wikidata together with semantic information about their constituting identifiers  names and values of variables               The formulae are then rearranged to generate a set of formula variants  Subsequently  the variables are substituted with random values to generate a large number of different questions suitable for individual student tests  PhysWikiquiz is hosted by Wikimedia at https   physwikiquiz wmflabs org  

Progress edit 
Question answering systems have been extended in recent     may be outdated as of April           years to encompass additional domains of knowledge             For example  systems have been developed to automatically answer temporal and geospatial questions  questions of definition and terminology  biographical questions  multilingual questions  and questions about the content of audio  images              and video              Current question answering research topics include 

interactivity clarification of questions or answers     further explanation needed                 
answer reuse or caching            
semantic parsing            
answer presentation     further explanation needed                 
knowledge representation and semantic entailment            
social media analysis     further explanation needed      with question answering systems
sentiment analysis            
utilization of thematic roles            
Image captioning for visual question answering            
Embodied question answering            
In       Watson  a question answering computer system developed by IBM  competed in two exhibition matches of Jeopardy  against Brad Rutter and Ken Jennings  winning by a significant margin             
Facebook Research made their DrQA system             available under an open source license  This system uses Wikipedia as knowledge source             The open source framework Haystack by deepset combines open domain question answering with generative question answering and supports the domain adaptation     clarification needed      of the underlying     clarification needed      language models for industry use cases     vague      
                        
Large Language Models  LLMs      like GPT        Gemini     are examples of successful QA systems that are enabling more sophisticated understanding and generation of text  When coupled with Multimodal     QA Systems  which can process and understand information from various modalities like text  images  and audio  LLMs significantly improve the capabilities of QA systems 

References edit 


  Philipp Cimiano  Christina Unger  John McCrae    March        Ontology Based Interpretation of Natural Language  Morgan  amp  Claypool Publishers  ISBN                        

  a b Chen  Danqi  Fisch  Adam  Weston  Jason  Bordes  Antoine          Reading Wikipedia to Answer Open Domain Questions   arXiv             cs CL  

  Roser Morante  Martin Krallinger  Alfonso Valencia and  Walter Daelemans  Machine Reading of Biomedical Texts about Alzheimer s Disease  CLEF      Evaluation Labs and Workshop  September         

  GREEN JR  Bert F  et      al           Baseball  an automatic question answerer   PDF   Western Joint IRE AIEE ACM Computer Conference          

  Woods  William A  Kaplan  R           Lunar rocks in natural English  Explorations in natural language question answering   Linguistic Structures Processing               

   EAGLi platform   Question Answering in MEDLINE   candy hesge ch  Retrieved            

  Hirschman  L   amp  Gaizauskas  R         Natural Language Question Answering  The View from Here  Natural Language Engineering                     Cambridge University Press 

  Raffel  Colin  Shazeer  Noam  Roberts  Adam  Lee  Katherine  Narang  Sharan  Matena  Michael  Zhou  Yanqi  Li  Wei  Liu  Peter J           Exploring the Limits of Transfer Learning with a Unified Text to Text Transformer   arXiv             cs LG  

  Lewis  Mike  Liu  Yinhan  Goyal  Naman  Ghazvininejad  Marjan  Mohamed  Abdelrahman  Levy  Omer  Stoyanov  Ves  Zettlemoyer  Luke          BART  Denoising Sequence to Sequence Pre training for Natural Language Generation  Translation  and Comprehension   arXiv             cs CL  

  Lin  J          The Web as a Resource for Question Answering  Perspectives and Challenges  In Proceedings of the Third International Conference on Language Resources and Evaluation  LREC       

  Moldovan  Dan  et al   Cogex  A logic prover for question answering   Proceedings of the      Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology Volume    Association for Computational Linguistics       

  Furbach  Ulrich  Ingo Gl ckner  and Bj rn Pelzer   An application of automated reasoning in natural language question answering   Ai Communications                        

  Sun  Haitian  Dhingra  Bhuwan  Zaheer  Manzil  Mazaitis  Kathryn  Salakhutdinov  Ruslan  Cohen  William          Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text   Proceedings of the      Conference on Empirical Methods in Natural Language Processing  Brussels  Belgium  pp                  arXiv             doi          v  D         S CID                 cite book     CS  maint  location missing publisher  link 

  Harabagiu  Sanda  Hickl  Andrew          Methods for using textual entailment in open domain question answering   Proceedings of the   st International Conference on Computational Linguistics and the   th annual meeting of the ACL   ACL      pp                doi                         

  a b Moritz Schubotz  Philipp Scharpf  et      al      September         Introducing MathQA  a Math Aware question answering system   Information Discovery and Delivery          Emerald Publishing Limited           arXiv             doi         IDD              

  Scharpf  P  Schubotz  M  Gipp  B  Mining Mathematical Documents for Question Answering via Unsupervised Formula Labeling ACM IEEE Joint Conference on Digital Libraries       

  a b Zanibbi  Richard  Oard  Douglas W   Agarwal  Anurag  Mansouri  Behrooz          Overview of ARQMath       CLEF Lab on Answer Retrieval for Questions on Math   Experimental IR Meets Multilinguality  Multimodality  and Interaction  Lecture Notes in Computer Science  vol              Cham  Springer International Publishing  pp                doi                               ISBN                         S CID                 retrieved           

  a b Scharpf  et      al                ARQMath Lab  An Incubator for Semantic Formula Search in zbMATH Open   OCLC                 

  Mansouri  Behrooz  Zanibbi  Richard  Oard  Douglas W   June         Characterizing Searches for Mathematical Concepts        ACM IEEE Joint Conference on Digital Libraries  JCDL   IEEE  pp              doi         jcdl             ISBN                         S CID                

  Scharpf  Philipp  Schubotz  Moritz  Spitz  Andreas  Greiner Petter  Andre  Gipp  Bela          Collaborative and AI aided Exam Question Generation using Wikidata in Education   arXiv             doi          RG                  S CID                   cite journal    Cite journal requires       journal   help 

  Pa ca  Marius          Book Review New Directions in Question Answering Mark T  Maybury  editor   MITRE Corporation  Menlo Park  CA  AAAI Press and Cambridge  MA  The MIT Press        xi     pp  paperbound  ISBN                                 Computational Linguistics                   doi                             S CID               

  a b Anderson  Peter  et al   Bottom up and top down attention for image captioning and visual question answering   Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition       

  Zhu  Linchao  Xu  Zhongwen  Yang  Yi  Hauptmann  Alexander G           Uncovering Temporal Context for Video Question and Answering   arXiv             cs CV  

  Quarteroni  Silvia  and Suresh Manandhar   Designing an interactive open domain question answering system   Natural Language Engineering                    

  Light  Marc  et al   Reuse in Question Answering  A Preliminary Study   New Directions in Question Answering       

  Yih  Wen tau  Xiaodong He  and Christopher Meek   Semantic parsing for single relation question answering   Proceedings of the   nd Annual Meeting of the Association for Computational Linguistics  Volume    Short Papers        

  Perera  R   Nand  P  and Naeem  A        Utilizing typed dependency subtree patterns for answer sentence generation in question answering systems 

  de Salvo Braz  Rodrigo  et al   An inference model for semantic entailment in natural language   Machine Learning Challenges Workshop  Springer  Berlin  Heidelberg       

   BitCrawl by Hobson Lane   Archived from the original on October           Retrieved              cite web     CS  maint  bot  original URL status unknown  link 

  Perera  R  and Perera  U        Towards a thematic role based target identification model for question answering  Archived            at the Wayback Machine

  Das  Abhishek  et al   Embodied question answering   Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition       

  Markoff  John                On  Jeopardy   Watson Win is All but Trivial   The New York Times 

   DrQA  

  Tunstall  Lewis    July        Natural Language Processing with Transformers  Building Language Applications with Hugging Face   nd      ed    O Reilly UK Ltd  p       Chapter    ISBN                     

   Haystack documentation   deepset  Retrieved   November      


Further reading edit 
Dragomir R  Radev  John Prager  and Valerie Samn  Ranking suspected answers to natural language questions using predictive annotation Archived            at the Wayback Machine  In Proceedings of the  th Conference on Applied Natural Language Processing  Seattle  WA  May      
John Prager  Eric Brown  Anni Coden  and Dragomir Radev  Question answering by predictive annotation Archived            at the Wayback Machine  In Proceedings    rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval  Athens  Greece  July      
Hutchins  W  John  Harold L  Somers         An Introduction to Machine Translation  London  Academic Press  ISBN                        
L  Fortnow  Steve Homer                A Short History of Computational Complexity   In D  van Dalen  J  Dawson  and A  Kanamori  editors  The History of Mathematical Logic  North Holland  Amsterdam 
Tunstall  Lewis    July        Natural Language Processing with Transformers  Building Language Applications with Hugging Face   nd      ed    O Reilly UK Ltd  p       Chapter    ISBN                     
External links edit 
Question Answering Evaluation at TREC
Question Answering Evaluation at CLEF
vteNatural language processingGeneral terms
AI complete
Bag of words
n gram
Bigram
Trigram
Computational linguistics
Natural language understanding
Stop words
Text processing
Text analysis
Argument mining
Collocation extraction
Concept mining
Coreference resolution
Deep linguistic processing
Distant reading
Information extraction
Named entity recognition
Ontology learning
Parsing
Semantic parsing
Syntactic parsing
Part of speech tagging
Semantic analysis
Semantic role labeling
Semantic decomposition
Semantic similarity
Sentiment analysis
Terminology extraction
Text mining
Textual entailment
Truecasing
Word sense disambiguation
Word sense induction
Text segmentation
Compound term processing
Lemmatisation
Lexical analysis
Text chunking
Stemming
Sentence segmentation
Word segmentation

Automatic summarization
Multi document summarization
Sentence extraction
Text simplification
Machine translation
Computer assisted
Example based
Rule based
Statistical
Transfer based
Neural
Distributional semantics models
BERT
Document term matrix
Explicit semantic analysis
fastText
GloVe
Language model  large 
Latent semantic analysis
Seq seq
Word embedding
Word vec
Language resources datasets and corporaTypes andstandards
Corpus linguistics
Lexical resource
Linguistic Linked Open Data
Machine readable dictionary
Parallel text
PropBank
Semantic network
Simple Knowledge Organization System
Speech corpus
Text corpus
Thesaurus  information retrieval 
Treebank
Universal Dependencies
Data
BabelNet
Bank of English
DBpedia
FrameNet
Google Ngram Viewer
UBY
WordNet
Wikidata
Automatic identificationand data capture
Speech recognition
Speech segmentation
Speech synthesis
Natural language generation
Optical character recognition
Topic model
Document classification
Latent Dirichlet allocation
Pachinko allocation
Computer assistedreviewing
Automated essay scoring
Concordancer
Grammar checker
Predictive text
Pronunciation assessment
Spell checker
Natural languageuser interface
Chatbot
Interactive fiction  cf  Syntax guessing 
Question answering
Virtual assistant
Voice user interface
Related
Formal semantics
Hallucination
Natural Language Toolkit
spaCy






Retrieved from  https   en wikipedia org w index php title Question answering amp oldid