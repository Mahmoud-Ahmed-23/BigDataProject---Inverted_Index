Paradigm in machine learning that uses no classification labels
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
Unsupervised learning is a framework in machine learning where  in contrast to supervised learning  algorithms learn patterns exclusively from unlabeled data             Other frameworks in the spectrum of supervisions include weak  or semi supervision  where a small portion of the data is tagged  and self supervision  Some researchers consider self supervised learning a form of unsupervised learning            
Conceptually  unsupervised learning divides into the aspects of data  training  algorithm  and downstream applications  Typically  the dataset is harvested cheaply  in the wild   such as massive text corpus obtained by web crawling  with only minor filtering  such as Common Crawl   This compares favorably to supervised learning  where the dataset  such as the ImageNet      is typically constructed manually  which is much more expensive 
There were algorithms designed specifically for unsupervised learning  such as clustering algorithms like k means  dimensionality reduction techniques like principal component analysis  PCA   Boltzmann machine learning  and autoencoders  After the rise of deep learning  most large scale unsupervised learning have been done by training general purpose neural network architectures by gradient descent  adapted to performing unsupervised learning by designing an appropriate training procedure 
Sometimes a trained model can be used as is  but more often they are modified for downstream applications  For example  the generative pretraining method trains a model to generate a textual dataset  before finetuning it for other applications  such as text classification                        As another example  autoencoders are trained to good features  which can then be used as a module for other models  such as in a latent diffusion model 


Tasks edit 
Tendency for a task to employ supervised vs  unsupervised methods  Task names straddling circle boundaries is intentional  It shows that the classical division of imaginative tasks  left  employing unsupervised methods is blurred in today s learning schemes Tasks are often categorized as discriminative  recognition  or generative  imagination    Often but not always  discriminative tasks use supervised methods and generative tasks use unsupervised  see Venn diagram   however  the separation is very hazy   For example  object recognition favors supervised learning but unsupervised learning can also cluster objects into groups   Furthermore  as progress marches onward  some tasks employ both methods  and some tasks swing from one to another   For example  image recognition started off as heavily supervised  but became hybrid by employing unsupervised pre training  and then moved towards supervision again with the advent of dropout  ReLU  and adaptive learning rates 
A typical generative task is as follows  At each step  a datapoint is sampled from the dataset  and part of the data is removed  and the model must infer the removed part  This is particularly clear for the denoising autoencoders and BERT 

Neural network architectures edit 
Training edit 
During the learning phase  an unsupervised network tries to mimic the data it s given and uses the error in its mimicked output to correct itself  i e  correct its weights and biases   Sometimes the error is expressed as a low probability that the erroneous output occurs  or it might be expressed as an unstable high energy state in the network 
In contrast to supervised methods  dominant use of backpropagation  unsupervised learning also employs other methods  including  Hopfield learning rule  Boltzmann learning rule  Contrastive Divergence  Wake Sleep  Variational Inference  Maximum Likelihood  Maximum A Posteriori  Gibbs Sampling  and backpropagating reconstruction errors or hidden state reparameterizations  See the table below for more details 

Energy edit 
An energy function is a macroscopic measure of a network s activation state   In Boltzmann machines  it plays the role of the Cost function   This analogy with physics is inspired by Ludwig Boltzmann s analysis of a gas  macroscopic energy from the microscopic probabilities of particle motion 
  
    
      
        p
          x   d 
        
          e
          
              x     
            E
            
               
            
            k
            T
          
        
      
    
      displaystyle p propto e   E kT  
  
  where k is the Boltzmann constant and T is temperature  In the RBM network the relation is 
  
    
      
        p
         
        
          e
          
              x     
            E
          
        
        
           
        
        Z
      
    
      displaystyle p e   E  Z 
  
             where 
  
    
      
        p
      
    
      displaystyle p 
  
 and 
  
    
      
        E
      
    
      displaystyle E 
  
 vary over every possible activation pattern and 
  
    
      
        
          
            Z
             
            
                x     
              
                
                  
                    All Patterns
                  
                
              
            
            
              e
              
                  x     
                E
                 
                
                  pattern
                
                 
              
            
          
        
      
    
      displaystyle  textstyle  Z  sum    scriptscriptstyle   text All Patterns   e   E   text pattern      
  
  To be more precise  
  
    
      
        p
         
        a
         
         
        
          e
          
              x     
            E
             
            a
             
          
        
        
           
        
        Z
      
    
      displaystyle p a  e   E a   Z 
  
  where 
  
    
      
        a
      
    
      displaystyle a 
  
 is an activation pattern of all neurons  visible and hidden   Hence  some early neural networks bear the name Boltzmann Machine   Paul Smolensky calls 
  
    
      
          x     
        E
        
      
    
      displaystyle  E   
  
 the Harmony  A network seeks low energy which is high Harmony 

Networks edit 
This table shows connection diagrams of various unsupervised networks  the details of which will be given in the section Comparison of Networks   Circles are neurons and edges between them are connection weights   As network design changes  features are added on to enable new capabilities or removed to make learning faster   For instance  neurons change between deterministic  Hopfield  and stochastic  Boltzmann  to allow robust output  weights are removed within a layer  RBM  to hasten learning  or connections are allowed to become asymmetric  Helmholtz  




Hopfield
Boltzmann
RBM
Stacked Boltzmann


A network based on magnetic domains in iron with a single self connected layer   It can be used as a content addressable memory 

Network is separated into   layers  hidden vs  visible   but still using symmetric   way weights   Following Boltzmann s thermodynamics  individual  probabilities give rise to macroscopic energies 

Restricted Boltzmann Machine   This is a Boltzmann machine where lateral connections within a layer are prohibited to make analysis tractable 

This network has multiple RBM s to encode a hierarchy of hidden features   After a single RBM is trained  another blue hidden layer  see left RBM  is added  and the top   layers are trained as a red  amp  blue RBM   Thus the middle layers of an RBM acts as hidden or visible  depending on the training phase it is in 




Helmholtz
Autoencoder
VAE


Instead of the bidirectional symmetric connection of the stacked Boltzmann machines  we have separate one way connections to form a loop   It does both generation and discrimination 

A feed forward network that aims to find a good middle layer representation of its input world   This network is deterministic  so it is not as robust as its successor the VAE 

Applies Variational Inference to the Autoencoder   The middle layer is a set of means  amp  variances for Gaussian distributions   The stochastic nature allows for more robust imagination than the deterministic autoencoder 

Of the networks bearing people s names  only Hopfield worked directly with neural networks   Boltzmann and Helmholtz came before artificial neural networks  but their work in physics and physiology inspired the analytical methods that were used 

History edit 



    
Ising magnetic model proposed by WA Little           de      for cognition


    
Kunihiko Fukushima introduces the neocognitron  which is later called a convolutional neural network  It is mostly used in SL  but deserves a mention here 


    
Ising variant Hopfield net described as CAMs and classifiers by John Hopfield 


    
Ising variant Boltzmann machine with probabilistic neurons described by Hinton  amp  Sejnowski following Sherington  amp  Kirkpatrick s      work 


    
Paul Smolensky publishes Harmony Theory  which is an RBM with practically the same Boltzmann energy function  Smolensky did not give a practical training scheme  Hinton did in mid     s 


    
Schmidthuber introduces the LSTM neuron for languages 


    
Dayan  amp  Hinton introduces Helmholtz machine


    
Kingma  Rezende   amp  co  introduced Variational Autoencoders as Bayesian graphical probability network  with neural nets as components 

Specific Networks edit 
Here  we highlight some characteristics of select networks   The details of each are given in the comparison table below 



Hopfield Network
Ferromagnetism inspired Hopfield networks   A neuron correspond to an iron domain with binary magnetic moments Up and Down  and neural connections correspond to the domain s influence on each other   Symmetric connections enable a global energy formulation  During inference the network updates each state using the standard activation step function  Symmetric weights and the right energy functions guarantees convergence to a stable activation pattern   Asymmetric weights are difficult to analyze   Hopfield nets are used as Content Addressable Memories  CAM  

Boltzmann Machine
These are stochastic Hopfield nets  Their state value is sampled from this pdf as follows  suppose a binary neuron fires with the Bernoulli probability p          and rests with p           One samples from it by taking a uniformly distributed random number y  and plugging it into the inverted cumulative distribution function  which in this case is the step function thresholded at      The inverse function       if x  lt          if x  gt        

Sigmoid Belief Net
Introduced by Radford Neal in       this network applies ideas from probabilistic graphical models to neural networks   A key difference is that nodes in graphical models have pre assigned meanings  whereas Belief Net neurons  features are determined after training  The network is a sparsely connected directed acyclic graph composed of binary stochastic neurons   The learning rule comes from Maximum Likelihood on p X     wij 
  
    
      
          x   d 
      
    
      displaystyle  propto  
  
 sj    si   pi   where pi             eweighted inputs into neuron i     sj s are activations from an unbiased sample of the posterior distribution and this is problematic due to the Explaining Away problem raised by Judea Perl   Variational Bayesian methods uses a surrogate posterior and blatantly disregard this complexity 

Deep Belief Network
Introduced by Hinton  this network is a hybrid of RBM and Sigmoid Belief Network   The top   layers is an RBM and the second layer downwards form a sigmoid belief network   One trains it by the stacked RBM method and then throw away the recognition weights below the top RBM   As of           layers seems to be the optimal depth            

Helmholtz machine
These are early inspirations for the Variational Auto Encoders  Its   networks combined into one forward weights operates recognition and backward weights implements imagination  It is perhaps the first network to do both  Helmholtz did not work in machine learning but he inspired the view of  statistical inference engine whose function is to infer probable causes of sensory input              the stochastic binary neuron outputs a probability that its state is   or    The data input is normally not considered a layer  but in the Helmholtz machine generation mode  the data layer receives input from the middle layer and has separate weights for this purpose  so it is considered a layer  Hence this network has   layers 

Variational autoencoder
These are inspired by Helmholtz machines and combines probability network with neural networks  An Autoencoder is a   layer CAM network  where the middle layer is supposed to be some internal representation of input patterns  The encoder neural network is a probability distribution q  z given x  and the decoder network is p  x given z   The weights are named phi  amp  theta rather than W and V as in Helmholtz a cosmetic difference  These   networks here can be fully connected  or use another NN scheme 

Comparison of networks edit 




Hopfield
Boltzmann
RBM
Stacked RBM
Helmholtz
Autoencoder
VAE


Usage  amp  notables
CAM  traveling salesman problem
CAM  The freedom of connections makes this network difficult to analyze 
pattern recognition  used in MNIST digits and speech 
recognition  amp  imagination    trained with unsupervised pre training and or supervised fine tuning 
imagination  mimicry
language  creative writing  translation   vision  enhancing blurry images
generate realistic data


Neuron
deterministic binary state  Activation        or     if x is negative    otherwise  
stochastic binary Hopfield neuron
  same   extended to real valued in mid     s 
  same
  same
language  LSTM  vision  local receptive fields  usually real valued relu activation 
middle layer neurons encode means  amp  variances for Gaussians   In run mode  inference   the output of the middle layer are sampled values from the Gaussians 


Connections
  layer with symmetric weights  No self connections 
  layers    hidden  amp    visible  symmetric weights 
  same  no lateral connections within a layer 
top layer is undirected  symmetric   other layers are   way  asymmetric 
  layers  asymmetric weights    networks combined into   
  layers  The input is considered a layer even though it has no inbound weights  recurrent layers for NLP  feedforward convolutions for vision  input  amp  output have the same neuron counts 
  layers  input  encoder  distribution sampler decoder  the sampler is not considered a layer


Inference  amp  energy
Energy is given by Gibbs probability measure       
  
    
      
        E
         
          x     
        
          
             
             
          
        
        
            x     
          
            i
             
            j
          
        
        
          
            w
            
              i
              j
            
          
          
            
              s
              
                i
              
            
          
          
            
              s
              
                j
              
            
          
        
         
        
            x     
          
            i
          
        
        
          
              x b  
            
              i
            
          
        
        
          
            s
            
              i
            
          
        
      
    
      displaystyle E    frac         sum   i j  w  ij  s  i   s  j     sum   i   theta   i   s  i   
  

  same
  same

minimize KL divergence
inference is only feed forward  previous UL networks ran forwards AND backwards
minimize error   reconstruction error   KLD


Training
 wij   si sj  for       neuron
 wij   e  pij   p ij   This is derived from minimizing KLD  e   learning rate  p    predicted and p   actual distribution 

 wij   e    lt  vi hj  gt data    lt  vi hj  gt equilibrium     This is a form of contrastive divergence w  Gibbs Sampling     lt  gt   are expectations 
  similar  train   layer at a time   approximate equilibrium state with a   segment pass   no back propagation 
wake sleep   phase training
back propagate the reconstruction error
reparameterize hidden state for backprop


Strength
resembles physical systems so it inherits their equations
  same  hidden neurons act as internal representatation of the external world
faster more practical training scheme than Boltzmann machines
trains quickly   gives hierarchical layer of features
mildly anatomical  analyzable w  information theory  amp  statistical mechanics




Weakness

hard to train due to lateral connections
equilibrium requires too many iterations
integer  amp  real valued neurons are more complicated 




Hebbian Learning  ART  SOM edit 
The classical example of unsupervised learning in the study of neural networks is Donald Hebb s principle  that is  neurons that fire together wire together             In Hebbian learning  the connection is reinforced irrespective of an error  but is exclusively a function of the coincidence between action potentials between the two neurons             A similar version that modifies synaptic weights takes into account the time between the action potentials  spike timing dependent plasticity or STDP   Hebbian Learning has been hypothesized to underlie a range of cognitive functions  such as pattern recognition and experiential learning 
Among neural network models  the self organizing map  SOM  and adaptive resonance theory  ART  are commonly used in unsupervised learning algorithms  The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties  The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user defined constant called the vigilance parameter  ART networks are used for many pattern recognition tasks  such as automatic target recognition and seismic signal processing             

Probabilistic methods edit 
Two of the main methods used in unsupervised learning are principal component and cluster analysis  Cluster analysis is used in unsupervised learning to group  or segment  datasets with shared attributes in order to extrapolate algorithmic relationships              Cluster analysis is a branch of machine learning that groups the data that has not been labelled  classified or categorized  Instead of responding to feedback  cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data  This approach helps detect anomalous data points that do not fit into either group 
A central application of unsupervised learning is in the field of density estimation in statistics              though unsupervised learning encompasses many other domains involving summarizing and explaining data features  It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data  unsupervised learning intends to infer an a priori probability distribution  

Approaches edit 
Some of the most common algorithms used in unsupervised learning include      Clustering      Anomaly detection      Approaches for learning latent variable models  Each approach uses several methods as follows 

Clustering methods include  hierarchical clustering              k means              mixture models  model based clustering  DBSCAN  and OPTICS algorithm
Anomaly detection methods include  Local Outlier Factor  and Isolation Forest
Approaches for learning latent variable models such as Expectation maximization algorithm  EM   Method of moments  and Blind signal separation techniques  Principal component analysis  Independent component analysis  Non negative matrix factorization  Singular value decomposition 
Method of moments edit 
One of the statistical approaches for unsupervised learning is the method of moments  In the method of moments  the unknown parameters  of interest  in the model are related to the moments of one or more random variables  and thus  these unknown parameters can be estimated given the moments  The moments are usually estimated from samples empirically  The basic moments are first and second order moments  For a random vector  the first order moment is the mean vector  and the second order moment is the covariance matrix  when the mean is zero   Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi dimensional arrays 
In particular  the method of moments is shown to be effective in learning the parameters of latent variable models  Latent variable models are statistical models where in addition to the observed variables  a set of latent variables also exists which is not observed  A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words  observed variables  in the document based on the topic  latent variable  of the document  In the topic modeling  the words in the document are generated according to different statistical parameters when the topic of the document is changed  It is shown that method of moments  tensor decomposition techniques  consistently recover the parameters of a large class of latent variable models under some assumptions             
The Expectation maximization algorithm  EM  is also one of the most practical methods for learning latent variable models  However  it can get stuck in local optima  and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model  In contrast  for the method of moments  the global convergence is guaranteed under some conditions 

See also edit 
Automated machine learning
Cluster analysis
Model based clustering
Anomaly detection
Expectation maximization algorithm
Generative topographic map
Meta learning  computer science 
Multivariate analysis
Radial basis function network
Weak supervision
References edit 


  Wu  Wei   Unsupervised Learning   PDF   Archived  PDF  from the original on    April       Retrieved    April      

  Liu  Xiao  Zhang  Fanjin  Hou  Zhenyu  Mian  Li  Wang  Zhaoyu  Zhang  Jing  Tang  Jie          Self supervised Learning  Generative or Contrastive   IEEE Transactions on Knowledge and Data Engineering     arXiv             doi         TKDE               ISSN                

  Radford  Alec  Narasimhan  Karthik  Salimans  Tim  Sutskever  Ilya     June         Improving Language Understanding by Generative Pre Training   PDF   OpenAI  p           Archived  PDF  from the original on    January       Retrieved    January      

  Li  Zhuohan  Wallace  Eric  Shen  Sheng  Lin  Kevin  Keutzer  Kurt  Klein  Dan  Gonzalez  Joey                Train Big  Then Compress  Rethinking Model Size for Efficient Training and Inference of Transformers   Proceedings of the   th International Conference on Machine Learning  PMLR            

  Hinton  G           A Practical Guide to Training Restricted Boltzmann Machines   PDF   Neural Networks  Tricks of the Trade  Lecture Notes in Computer Science  Vol             Springer  pp                doi                               ISBN                         Archived  PDF  from the original on             Retrieved            

   Deep Belief Nets   video   September       Archived from the original on             Retrieved            

  Peter  Dayan  Hinton  Geoffrey E   Neal  Radford M   Zemel  Richard S           The Helmholtz machine   Neural Computation                  doi         neco               hdl                    D D  E  PMID               S CID               

  Buhmann  J   Kuhnel  H           Unsupervised and supervised data clustering with competitive neural networks        Proceedings           IJCNN International Joint Conference on Neural Networks  Vol          IEEE  pp                doi         ijcnn              ISBN                  S CID               

  Comesa a Campos  Alberto  Bouza Rodr guez  Jos  Benito  June         An application of Hebbian learning in the design process decision making   Journal of Intelligent Manufacturing                   doi         s               z  ISSN                 S CID                

  Carpenter  G A   amp  Grossberg  S           The ART of adaptive pattern recognition by a self organizing neural network   PDF   Computer                 doi               S CID                Archived from the original  PDF  on             Retrieved            

  Roman  Victor                Unsupervised Machine Learning  Clustering Analysis   Medium  Archived from the original on             Retrieved            

  Jordan  Michael I   Bishop  Christopher M              Intelligent Systems  Neural Networks   In Tucker  Allen B   ed    Computer Science Handbook   nd      ed    Chapman  amp  Hall CRC Press  doi                        ISBN                  X  Archived from the original on             Retrieved            

  Hastie  Tibshirani  amp  Friedman       pp              

  Garbade  Dr Michael J                 Understanding K means Clustering in Machine Learning   Medium  Archived from the original on             Retrieved            

  Anandkumar  Animashree  Ge  Rong  Hsu  Daniel  Kakade  Sham  Telgarsky  Matus          Tensor Decompositions for Learning Latent Variable Models   PDF   Journal of Machine Learning Research                 arXiv            Bibcode     arXiv         A  Archived  PDF  from the original on             Retrieved            


Further reading edit 

Bousquet  O   von Luxburg  U   Raetsch  G   eds          Advanced Lectures on Machine Learning  Springer  ISBN                     
Duda  Richard O   Hart  Peter E   Stork  David G           Unsupervised Learning and Clustering   Pattern classification   nd      ed    Wiley  ISBN                    
Hastie  Trevor  Tibshirani  Robert  Friedman  Jerome          Unsupervised Learning   The Elements of Statistical Learning  Data mining  Inference  and Prediction  Springer  pp                doi                               ISBN                         Archived from the original on             Retrieved            
Hinton  Geoffrey  Sejnowski  Terrence J   eds          Unsupervised Learning  Foundations of Neural Computation  MIT Press  ISBN                  X 

vteDifferentiable computingGeneral
Differentiable programming
Information geometry
Statistical manifold
Automatic differentiation
Neuromorphic computing
Pattern recognition
Ricci calculus
Computational learning theory
Inductive bias
Hardware
IPU
TPU
VPU
Memristor
SpiNNaker
Software libraries
TensorFlow
PyTorch
Keras
scikit learn
Theano
JAX
Flux jl
MindSpore

 Portals
Computer programming
Technology

Authority control databases  National Germany





Retrieved from  https   en wikipedia org w index php title Unsupervised learning amp oldid