Field of linguistics and computer science
This article is about computer processing  For human brain processing  see Language processing in the brain 
This article needs additional citations for verification  Please help improve this article by adding citations to reliable sources  Unsourced material may be challenged and removed Find sources        Natural language processing              news        newspapers        books        scholar        JSTOR   May        Learn how and when to remove this message 
Natural language processing  NLP  is a subfield of computer science and especially artificial intelligence  It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval  knowledge representation and computational linguistics  a subfield of linguistics 
Major tasks in natural language processing are speech recognition  text classification  natural language understanding  and natural language generation 


History edit 
Further information  History of natural language processing
Natural language processing has its roots in the     s             Already in       Alan Turing published an article titled  Computing Machinery and Intelligence  which proposed what is now called the Turing test as a criterion of intelligence  though at the time that was not articulated as a problem separate from artificial intelligence  The proposed test includes a task that involves the automated interpretation and generation of natural language 

Symbolic NLP      s   early     s  edit 
The premise of symbolic NLP is well summarized by John Searle s Chinese room experiment  Given a collection of rules  e g   a Chinese phrasebook  with questions and matching answers   the computer emulates natural language understanding  or other NLP tasks  by applying those rules to the data it confronts 

    s  The Georgetown experiment in      involved fully automatic translation of more than sixty Russian sentences into English  The authors claimed that within three or five years  machine translation would be a solved problem              However  real progress was much slower  and after the ALPAC report in       which found that ten years of research had failed to fulfill the expectations  funding for machine translation was dramatically reduced  Little further research in machine translation was conducted in America  though some research continued elsewhere  such as Japan and Europe             until the late     s when the first statistical machine translation systems were developed 
    s  Some notably successful natural language processing systems developed in the     s were SHRDLU  a natural language system working in restricted  blocks worlds  with restricted vocabularies  and ELIZA  a simulation of a Rogerian psychotherapist  written by Joseph Weizenbaum between      and       Using almost no information about human thought or emotion  ELIZA sometimes provided a startlingly human like interaction  When the  patient  exceeded the very small knowledge base  ELIZA might provide a generic response  for example  responding to  My head hurts  with  Why do you say your head hurts    Ross Quillian s successful work on natural language was demonstrated with a vocabulary of only twenty words  because that was all that would fit in a computer  memory at the time            
    s  During the     s  many programmers began to write  conceptual ontologies   which structured real world information into computer understandable data   Examples are MARGIE  Schank         SAM  Cullingford         PAM  Wilensky         TaleSpin  Meehan         QUALM  Lehnert         Politics  Carbonell         and Plot Units  Lehnert         During this time  the first chatterbots were written  e g   PARRY  
    s  The     s and early     s mark the heyday of symbolic methods in NLP  Focus areas of the time included research on rule based parsing  e g   the development of HPSG as a computational operationalization of generative grammar   morphology  e g   two level morphology              semantics  e g   Lesk algorithm   reference  e g   within Centering Theory             and other areas of natural language understanding  e g   in the Rhetorical Structure Theory   Other lines of research were continued  e g   the development of chatterbots with Racter and Jabberwacky  An important development  that eventually led to the statistical turn in the     s  was the rising importance of quantitative evaluation in this period            
Statistical NLP      s present  edit 
Up until the     s  most natural language processing systems were based on complex sets of hand written rules   Starting in the late     s  however  there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing   This was due to both the steady increase in computational power  see Moore s law  and the gradual lessening of the dominance of Chomskyan theories of linguistics  e g  transformational grammar   whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine learning approach to language processing             

    s  Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation  due especially to work at IBM Research  such as IBM alignment models   These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government   However  most other systems depended on corpora specifically developed for the tasks implemented by these systems  which was  and often continues to be  a major limitation in the success of these systems  As a result  a great deal of research has gone into methods of more effectively learning from limited amounts of data 
    s  With the growth of the web  increasing amounts of raw  unannotated  language data have become available since the mid     s  Research has thus increasingly focused on unsupervised and semi supervised learning algorithms   Such algorithms can learn from data that has not been hand annotated with the desired answers or using a combination of annotated and non annotated data   Generally  this task is much more difficult than supervised learning  and typically produces less accurate results for a given amount of input data   However  there is an enormous amount of non annotated data available  including  among other things  the entire content of the World Wide Web   which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical 
      word n gram model  at the time the best statistical algorithm  is outperformed by a multi layer perceptron  with a single hidden layer and context length of several words  trained on up to    million words  by Bengio et al             
      Tom   Mikolov  then a PhD student at Brno University of Technology  with co authors applied a simple recurrent neural network with a single hidden layer to language modelling              and in the following years he went on to develop Word vec  In the     s  representation learning and deep neural network style  featuring many hidden layers  machine learning methods became widespread in natural language processing  That popularity was due partly to a flurry of results showing that such techniques                         can achieve state of the art results in many natural language tasks  e g   in language modeling             and parsing                          This is increasingly important in medicine and healthcare  where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care             or protect patient privacy             
Approaches  Symbolic  statistical  neural networks edit 
Symbolic approach  i e   the hand coding of a set of rules for manipulating symbols  coupled with a dictionary lookup  was historically the first approach used both by AI in general and by NLP in particular                          such as by writing grammars or devising heuristic rules for stemming 
Machine learning approaches  which include both statistical and neural networks  on the other hand  have many advantages over the symbolic approach  

both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts  whereas the rule based approach needs to provide rules for both rare cases and common ones equally 
language models  produced by either statistical or neural networks methods  are more robust to both unfamiliar  e g  containing words or structures that have not been seen before  and erroneous input  e g  with misspelled words or words accidentally omitted  in comparison to the rule based systems  which are also more costly to produce 
the larger such a  probabilistic  language model is  the more accurate it becomes  in contrast to rule based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems 
Rule based systems are commonly used 

when the amount of training data is insufficient to successfully apply machine learning methods  e g   for the machine translation of low resource languages such as provided by the Apertium system 
for preprocessing in NLP pipelines  e g   tokenization  or
for postprocessing and transforming the output of NLP pipelines  e g   for knowledge extraction from syntactic parses 
Statistical approach edit 
In the late     s and mid     s  the statistical approach ended a period of AI winter  which was caused by the inefficiencies of the rule based approaches                         
The earliest decision trees  producing systems of hard if then rules  were still very similar to the old rule based approaches 
Only the introduction of hidden Markov models  applied to part of speech tagging  announced the end of the old rule based approach 

Neural networks edit 
Further information  Artificial neural network
A major drawback of statistical methods is that they require elaborate feature engineering  Since                   the statistical approach has been replaced by the neural networks approach  using semantic networks             and word embeddings to capture semantic properties of words   
Intermediate tasks  e g   part of speech tagging and dependency parsing  are not needed anymore  
Neural machine translation  based on then newly invented sequence to sequence transformations  made obsolete the intermediate steps  such as word alignment  previously necessary for statistical machine translation 

Common NLP tasks edit 
The following is a list of some of the most commonly researched tasks in natural language processing  Some of these tasks have direct real world applications  while others more commonly serve as subtasks that are used to aid in solving larger tasks 
Though natural language processing tasks are closely intertwined  they can be subdivided into categories for convenience  A coarse division is given below 

Text and speech processing edit 
Optical character recognition  OCR 
Given an image representing printed text  determine the corresponding text 
Speech recognition
Given a sound clip of a person or people speaking  determine the textual representation of the speech   This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed  AI complete   see above    In natural speech there are hardly any pauses between successive words  and thus speech segmentation is a necessary subtask of speech recognition  see below   In most spoken languages  the sounds representing successive letters blend into each other in a process termed coarticulation  so the conversion of the analog signal to discrete characters can be a very difficult process  Also  given that words in the same language are spoken by people with different accents  the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent 
Speech segmentation
Given a sound clip of a person or people speaking  separate it into words   A subtask of speech recognition and typically grouped with it 
Text to speech
Given a text  transform those units and produce a spoken representation  Text to speech can be used to aid the visually impaired             
Word segmentation  Tokenization 
Tokenization is a process used in text analysis that divides text into individual words or word fragments  This technique results in two key components  a word index and tokenized text  The word index is a list that maps unique words to specific numerical identifiers  and the tokenized text replaces each word with its corresponding numerical token  These numerical tokens are then used in various deep learning methods             
For a language like English  this is fairly trivial  since words are usually separated by spaces  However  some written languages like Chinese  Japanese and Thai do not mark word boundaries in such a fashion  and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language  Sometimes this process is also used in cases like bag of words  BOW  creation in data mining      citation needed     
Morphological analysis edit 
Lemmatization
The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma  Lemmatization is another technique for reducing words to their normalized form  But in this case  the transformation actually uses a dictionary to map words to their actual form             
Morphological segmentation
Separate words into individual morphemes and identify the class of the morphemes  The difficulty of this task depends greatly on the complexity of the morphology  i e   the structure of words  of the language being considered  English has fairly simple morphology  especially inflectional morphology  and thus it is often possible to ignore this task entirely and simply model all possible forms of a word  e g    open  opens  opened  opening   as separate words  In languages such as Turkish or Meitei  a highly agglutinated Indian language  however  such an approach is not possible  as each dictionary entry has thousands of possible word forms             
Part of speech tagging
Given a sentence  determine the part of speech  POS  for each word  Many words  especially common ones  can serve as multiple parts of speech  For example   book  can be a noun   the book on the table   or verb   to book a flight     set  can be a noun  verb or adjective  and  out  can be any of at least five different parts of speech 
Stemming
The process of reducing inflected  or sometimes derived  words to a base form  e g    close  will be the root for  closed    closing    close    closer  etc    Stemming yields similar results as lemmatization  but does so on grounds of rules  not a dictionary 
Syntactic analysis edit 
Part of a series onFormal languages
Key concepts
Formal system
Alphabet
Syntax
Formal semantics
Semantics  programming languages 
Formal grammar
Formation rule
Well formed formula
Automata theory
Regular expression
Production
Ground expression
Atomic formula

Applications
Formal methods
Propositional calculus
Predicate logic
Mathematical notation
Natural language processing
Programming language theory
Mathematical linguistics
Computational linguistics
Syntax analysis
Formal verification
Automated theorem proving
vte
Grammar induction            
Generate a formal grammar that describes a language s syntax 
Sentence breaking  also known as  sentence boundary disambiguation  
Given a chunk of text  find the sentence boundaries  Sentence boundaries are often marked by periods or other punctuation marks  but these same characters can serve other purposes  e g   marking abbreviations  
Parsing
Determine the parse tree  grammatical analysis  of a given sentence  The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses  perhaps surprisingly  for a typical sentence there may be thousands of potential parses  most of which will seem completely nonsensical to a human   There are two primary types of parsing  dependency parsing and constituency parsing  Dependency parsing focuses on the relationships between words in a sentence  marking things like primary objects and predicates   whereas constituency parsing focuses on building out the parse tree using a probabilistic context free grammar  PCFG   see also stochastic grammar  
Lexical semantics  of individual words in context  edit 
Lexical semantics
What is the computational meaning of individual words in context 
Distributional semantics
How can we learn semantic representations from data 
Named entity recognition  NER 
Given a stream of text  determine which items in the text map to proper names  such as people or places  and what the type of each such name is  e g  person  location  organization   Although capitalization can aid in recognizing named entities in languages such as English  this information cannot aid in determining the type of named entity  and in any case  is often inaccurate or insufficient   For example  the first letter of a sentence is also capitalized  and named entities often span several words  only some of which are capitalized   Furthermore  many other languages in non Western scripts  e g  Chinese or Arabic  do not have any capitalization at all  and even languages with capitalization may not consistently use it to distinguish names  For example  German capitalizes all nouns  regardless of whether they are names  and French and Spanish do not capitalize names that serve as adjectives  Another name for this task is token classification             
Sentiment analysis  see also Multimodal sentiment analysis 
Sentiment analysis is a computational method used to identify and classify the emotional intent behind text  This technique involves analyzing text to determine whether the expressed sentiment is positive  negative  or neutral  Models for sentiment classification typically utilize inputs such as word n grams  Term Frequency Inverse Document Frequency  TF IDF  features  hand generated features  or employ deep learning models designed to recognize both long term and short term dependencies in text sequences  The applications of sentiment analysis are diverse  extending to tasks such as categorizing customer reviews on various online platforms             
Terminology extraction
The goal of terminology extraction is to automatically extract relevant terms from a given corpus 
Word sense disambiguation  WSD 
Many words have more than one meaning  we have to select the meaning which makes the most sense in context   For this problem  we are typically given a list of words and associated word senses  e g  from a dictionary or an online resource such as WordNet 
Entity linking
Many words typically proper names refer to named entities  here we have to select the entity  a famous individual  a location  a company  etc   which is referred to in context 
Relational semantics  semantics of individual sentences  edit 
Relationship extraction
Given a chunk of text  identify the relationships among named entities  e g  who is married to whom  
Semantic parsing
Given a piece of text  typically a sentence   produce a formal representation of its semantics  either as a graph  e g   in AMR parsing  or in accordance with a logical formalism  e g   in DRT parsing   This challenge typically includes aspects of several more elementary NLP tasks from semantics  e g   semantic role labelling  word sense disambiguation  and can be extended to include full fledged discourse analysis  e g   discourse analysis  coreference  see Natural language understanding below  
Semantic role labelling  see also implicit semantic role labelling below 
Given a single sentence  identify and disambiguate semantic predicates  e g   verbal frames   then identify and classify the frame elements  semantic roles  
Discourse  semantics beyond individual sentences  edit 
Coreference resolution
Given a sentence or larger chunk of text  determine which words   mentions   refer to the same objects   entities    Anaphora resolution is a specific example of this task  and is specifically concerned with matching up pronouns with the nouns or names to which they refer  The more general task of coreference resolution also includes identifying so called  bridging relationships  involving referring expressions  For example  in a sentence such as  He entered John s house through the front door    the front door  is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John s house  rather than of some other structure that might also be referred to  
Discourse analysis
This rubric includes several related tasks   One task is discourse parsing  i e   identifying the discourse structure of a connected text  i e  the nature of the discourse relationships between sentences  e g  elaboration  explanation  contrast    Another possible task is recognizing and classifying the speech acts in a chunk of text  e g  yes no question  content question  statement  assertion  etc   
Implicit semantic role labelling
Given a single sentence  identify and disambiguate semantic predicates  e g   verbal frames  and their explicit semantic roles in the current sentence  see Semantic role labelling above   Then  identify semantic roles that are not explicitly realized in the current sentence  classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified  and resolve the former against the local text  A closely related task is zero anaphora resolution  i e   the extension of coreference resolution to pro drop languages 
Recognizing textual entailment
Given two text fragments  determine if one being true entails the other  entails the other s negation  or allows the other to be either true or false             
Topic segmentation and recognition
Given a chunk of text  separate it into segments each of which is devoted to a topic  and identify the topic of the segment 
Argument mining
The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs              Such argumentative structures include the premise  conclusions  the argument scheme and the relationship between the main and subsidiary argument  or the main and counter argument within discourse                         
Higher level NLP applications edit 
Automatic summarization  text summarization 
Produce a readable summary of a chunk of text   Often used to provide summaries of the text of a known type  such as research papers  articles in the financial section of a newspaper 
Grammatical error correction
Grammatical error detection and correction involves a great band width of problems on all levels of linguistic analysis  phonology orthography  morphology  syntax  semantics  pragmatics   Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language  It has thus been subject to a number of shared tasks since                                           As far as orthography  morphology  syntax and certain aspects of semantics are concerned  and due to the development of powerful neural language models such as GPT    this can now        be considered a largely solved problem and is being marketed in various commercial applications 
Logic translation
Translate a text from a natural language into formal logic 
Machine translation  MT 
Automatically translate text from one human language to another   This is one of the most difficult problems  and is a member of a class of problems colloquially termed  AI complete   i e  requiring all of the different types of knowledge that humans possess  grammar  semantics  facts about the real world  etc   to solve properly 
Natural language understanding  NLU 
Convert chunks of text into more formal representations such as first order logic structures that are easier for computer programs to manipulate  Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts  Introduction and creation of language metamodel and ontology are efficient however empirical solutions  An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed world assumption  CWA  vs  open world assumption  or subjective Yes No vs  objective True False is expected for the construction of a basis of semantics formalization             
Natural language generation  NLG  
Convert information from computer databases or semantic intents into readable human language 
Book generation
Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full fledged books  The first machine generated book was created by a rule based system in       Racter  The policeman s beard is half constructed               The first published work by a neural network was published in         the Road  marketed as a novel  contains sixty million words  Both these systems are basically elaborate but non sensical  semantics free  language models  The first machine generated science book was published in       Beta Writer  Lithium Ion Batteries  Springer  Cham               Unlike Racter and   the Road  this is grounded on factual knowledge and based on text summarization 
Document AI
A Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence  machine learning or NLP to quickly train a computer to extract the specific data they need from different document types  NLP powered Document AI enables non technical teams to quickly access information hidden in documents  for example  lawyers  business analysts and accountants             
Dialogue management
Computer systems intended to converse with a human 
Question answering
Given a human language question  determine its answer  Typical questions have a specific right answer  such as  What is the capital of Canada     but sometimes open ended questions are also considered  such as  What is the meaning of life    
Text to image generation
Given a description of an image  generate an image that matches the description             
Text to scene generation
Given a description of a scene  generate a  D model of the scene                         
Text to video
Given a description of a video  generate a video that matches the description                         
General tendencies and  possible  future directions edit 
Based on long standing trends in the field  it is possible to extrapolate future directions of NLP  As of       three trends among the topics of the long standing series of CoNLL Shared Tasks can be observed             

Interest on increasingly abstract   cognitive  aspects of natural language             shallow parsing           named entity recognition                   dependency syntax                  semantic role labelling          coreference           discourse parsing        semantic parsing  
Increasing interest in multilinguality  and  potentially  multimodality  English since       Spanish  Dutch since       German since       Bulgarian  Danish  Japanese  Portuguese  Slovenian  Swedish  Turkish since       Basque  Catalan  Chinese  Greek  Hungarian  Italian  Turkish since       Czech since       Arabic since                 languages                 languages 
Elimination of symbolic representations  rule based over supervised towards weakly supervised methods  representation learning and end to end systems 
Cognition edit 
Most higher level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language  More broadly speaking  the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP  see trends among CoNLL shared tasks above  
Cognition refers to  the mental action or process of acquiring knowledge and understanding through thought  experience  and the senses               Cognitive science is the interdisciplinary  scientific study of the mind and its processes              Cognitive linguistics is an interdisciplinary branch of linguistics  combining knowledge and research from both psychology and linguistics              Especially during the age of symbolic NLP  the area of computational linguistics maintained strong ties with cognitive studies 
As an example  George Lakoff offers a methodology to build natural language processing  NLP  algorithms through the perspective of cognitive science  along with the findings of cognitive linguistics              with two defining aspects 

Apply the theory of conceptual metaphor  explained by Lakoff as  the understanding of one idea  in terms of another  which provides an idea of the intent of the author              For example  consider the English word big  When used in a comparison   That is a big tree    the author s intent is to imply that the tree is physically large relative to other trees or the authors experience   When used metaphorically   Tomorrow is a big day    the author s intent to imply importance   The intent behind other usages  like in  She is a big person   will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information 
Assign relative measures of meaning to a word  phrase  sentence or piece of text based on the information presented before and after the piece of text being analyzed  e g   by means of a probabilistic context free grammar  PCFG   The mathematical equation for such algorithms is presented in  US Patent                     

  
    
      
        
          R
          M
          M
           
          t
          o
          k
          e
          
            n
            
              N
            
          
           
        
         
        
          P
          M
          M
           
          t
          o
          k
          e
          
            n
            
              N
            
          
           
        
          xd  
        
          
             
            
               
              d
            
          
        
        
           
          
            
                x     
              
                i
                 
                  x     
                d
              
              
                d
              
            
            
               
               
              P
              M
              M
               
              t
              o
              k
              e
              
                n
                
                  N
                
              
               
            
              xd  
            
              P
              F
               
              t
              o
              k
              e
              
                n
                
                  N
                    x     
                  i
                
              
               
              t
              o
              k
              e
              
                n
                
                  N
                
              
               
              t
              o
              k
              e
              
                n
                
                  N
                   
                  i
                
              
               
              
                 
                
                  i
                
              
            
          
           
        
      
    
      displaystyle  RMM token  N     PMM token  N    times   frac      d   left  sum   i  d   d    PMM token  N    times  PF token  N i  token  N  token  N i     i   right  
  

Where
RMM is the relative measure of meaning
token is any block of text  sentence  phrase or word
N is the number of tokens being analyzed
PMM is the probable measure of meaning based on a corpora
d is the non zero location of the token along the sequence of N tokens
PF is the probability function specific to a language
Ties with cognitive linguistics are part of the historical heritage of NLP  but they have been less frequently addressed since the statistical turn during the     s  Nevertheless  approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks  e g   of cognitive grammar              functional grammar              construction grammar              computational psycholinguistics and cognitive neuroscience  e g   ACT R   however  with limited uptake in mainstream NLP  as measured by presence on major conferences             of the ACL   More recently  ideas of cognitive NLP have been revived as an approach to achieve explainability  e g   under the notion of  cognitive AI               Likewise  ideas of cognitive NLP are inherent to neural models multimodal NLP  although rarely made explicit              and developments in artificial intelligence  specifically tools and technologies using large language model approaches             and new directions in artificial general intelligence based on the free energy principle             by British neuroscientist and theoretician at University College London Karl J  Friston 

See also edit 

  the Road
Artificial intelligence detection software
Automated essay scoring
Biomedical text mining
Compound term processing
Computational linguistics
Computer assisted reviewing
Controlled natural language
Deep learning
Deep linguistic processing
Distributional semantics
Foreign language reading aid
Foreign language writing aid
Information extraction
Information retrieval
Language and Communication Technologies
Language model
Language technology
Latent semantic indexing
Multi agent system
Native language identification
Natural language programming
Natural language understanding
Natural language search
Outline of natural language processing
Query expansion
Query understanding
Reification  linguistics 
Speech processing
Spoken dialogue systems
Text proofing
Text simplification
Transformer  machine learning model 
Truecasing
Question answering
Word vec

References edit 


   NLP  

  Hutchins  J           The history of machine translation in a nutshell   PDF       self published source     

   ALPAC  the  in famous report   John Hutchins  MT News International  no      June       pp       

  Crevier       pp               harvnb error  no target  CITEREFCrevier      help   see also Buchanan       p          harvnb error  no target  CITEREFBuchanan      help    Early programs were necessarily limited in scope by the size and speed of memory 

  Koskenniemi  Kimmo         Two level morphology  A general computational model of word form recognition and production  PDF   Department of General Linguistics  University of Helsinki

  Joshi  A  K    amp  Weinstein  S         August   Control of Inference  Role of Some Aspects of Discourse Structure Centering  In IJCAI  pp           

  Guida  G   Mauri  G   July         Evaluation of natural language processing systems  Issues and approaches   Proceedings of the IEEE                     doi         PROC             ISSN                 S CID               

  Chomskyan linguistics encourages the investigation of  corner cases  that stress the limits of its theoretical models  comparable to pathological phenomena in mathematics   typically created using thought experiments  rather than the systematic investigation of typical phenomena that occur in real world data  as is the case in corpus linguistics   The creation and use of such corpora of real world data is a fundamental part of machine learning algorithms for natural language processing   In addition  theoretical underpinnings of Chomskyan linguistics such as the so called  poverty of the stimulus  argument entail that general learning algorithms  as are typically used in machine learning  cannot be successful in language processing   As a result  the Chomskyan paradigm discouraged the application of such models to language processing 

  Bengio  Yoshua  Ducharme  R jean  Vincent  Pascal  Janvin  Christian  March            A neural probabilistic language model   The Journal of Machine Learning Research                       via ACM Digital Library 

  Mikolov  Tom    Karafi t  Martin  Burget  Luk     ernock   Jan  Khudanpur  Sanjeev     September         Recurrent neural network based language model   PDF   Interspeech       pp                  doi          Interspeech           S CID                  cite book          journal  ignored  help 

  Goldberg  Yoav          A Primer on Neural Network Models for Natural Language Processing   Journal of Artificial Intelligence Research               arXiv             doi         jair       S CID              

  Goodfellow  Ian  Bengio  Yoshua  Courville  Aaron         Deep Learning  MIT Press 

  Jozefowicz  Rafal  Vinyals  Oriol  Schuster  Mike  Shazeer  Noam  Wu  Yonghui         Exploring the Limits of Language Modeling  arXiv             Bibcode     arXiv         J 

  Choe  Do Kook  Charniak  Eugene   Parsing as Language Modeling   Emnlp       Archived from the original on             Retrieved            

  Vinyals  Oriol  et      al           Grammar as a Foreign Language   PDF   Nips      arXiv            Bibcode     arXiv         V 

  Turchin  Alexander  Florez Builes  Luisa F                 Using Natural Language Processing to Measure and Improve Quality of Diabetes Care  A Systematic Review   Journal of Diabetes Science and Technology                   doi                            ISSN                 PMC               PMID               

  Lee  Jennifer  Yang  Samuel  Holland Hall  Cynthia  Sezgin  Emre  Gill  Manjot  Linwood  Simon  Huang  Yungui  Hoffman  Jeffrey                Prevalence of Sensitive Terms in Clinical Notes Using Natural Language Processing Techniques  Observational Study   JMIR Medical Informatics          e       doi                ISSN                 PMC               PMID               

  Winograd  Terry         Procedures as a Representation for Data in a Computer Program for Understanding Natural Language  Thesis  

  Schank  Roger C   Abelson  Robert P          Scripts  Plans  Goals  and Understanding  An Inquiry Into Human Knowledge Structures  Hillsdale  Erlbaum  ISBN                    

  Mark Johnson  How the statistical revolution changes  computational  linguistics  Proceedings of the EACL      Workshop on the Interaction between Linguistics and Computational Linguistics 

  Philip Resnik  Four revolutions  Language Log  February         

  Socher  Richard   Deep Learning For NLP ACL      Tutorial   www socher org  Retrieved             This was an early Deep Learning tutorial at the ACL      and met with both interest and  at the time  skepticism by most participants  Until then  neural learning was basically rejected because of its lack of statistical interpretability  Until       deep learning had evolved into the major framework of NLP   Link is broken  try http   web stanford edu class cs   n  

  Segev  Elad         Semantic Network Analysis in Social Sciences  London  Routledge  ISBN                     Archived from the original on   December       Retrieved   December      

  Yi  Chucai  Tian  Yingli          Assistive Text Reading from Complex Background for Blind Persons   Camera Based Document Analysis and Recognition  Lecture Notes in Computer Science  vol             Springer Berlin Heidelberg  pp              CiteSeerX                      doi                              ISBN                   

  a b  Natural Language Processing  NLP    A Complete Guide   www deeplearning ai              Retrieved            

   What is Natural Language Processing  Intro to NLP in Machine Learning   GyanSetu               Retrieved            

  Kishorjit  N   Vidya  Raj RK   Nirmal  Y   Sivaji  B           Manipuri Morpheme Identification   PDF   Proceedings of the  rd Workshop on South and Southeast Asian Natural Language Processing  SANLP   COLING       Mumbai  December                cite journal     CS  maint  location  link 

  Klein  Dan  Manning  Christopher D           Natural language grammar induction using a constituent context model   PDF   Advances in Neural Information Processing Systems 

  Kariampuzha  William  Alyea  Gioconda  Qu  Sue  Sanjak  Jaleal  Math   Ewy  Sid  Eric  Chatelaine  Haley  Yadaw  Arjun  Xu  Yanji  Zhu  Qian          Precision information extraction for rare disease epidemiology at scale   Journal of Translational Medicine               doi         s                y  PMC               PMID               

  PASCAL Recognizing Textual Entailment Challenge  RTE    https   tac nist gov       RTE 

  Lippi  Marco  Torroni  Paolo                Argumentation Mining  State of the Art and Emerging Trends   ACM Transactions on Internet Technology                doi                  hdl               ISSN                 S CID              

   Argument Mining   IJCAI     Tutorial   www i s unice fr  Retrieved            

   NLP Approaches to Computational Argumentation   ACL       Berlin   Retrieved            

  Administration   Centre for Language Technology  CLT    Macquarie University  Retrieved            

   Shared Task  Grammatical Error Correction   www comp nus edu sg  Retrieved            

   Shared Task  Grammatical Error Correction   www comp nus edu sg  Retrieved            

  Duan  Yucong  Cruz  Christophe          Formalizing Semantic of Natural Language through Conceptualization from Existence   International Journal of Innovation  Management and Technology                Archived from the original on            

   U B U W E B         Racter   www ubu com  Retrieved            

  Writer  Beta         Lithium Ion Batteries  doi                            ISBN                         S CID                

   Document Understanding AI on Google Cloud  Cloud Next        YouTube   www youtube com     April       Archived from the original on             Retrieved            

  Robertson  Adi                OpenAI s DALL E AI image generator can now edit pictures  too   The Verge  Retrieved            

   The Stanford Natural Language Processing Group   nlp stanford edu  Retrieved            

  Coyne  Bob  Sproat  Richard                WordsEye   Proceedings of the   th annual conference on Computer graphics and interactive techniques  SIGGRAPH      New York  NY  USA  Association for Computing Machinery  pp                doi                        ISBN                         S CID              

   Google announces AI advances in text to video  language translation  more   VentureBeat              Retrieved            

  Vincent  James                Meta s new text to video AI generator is like DALL E for video   The Verge  Retrieved            

   Previous shared tasks   CoNLL   www conll org  Retrieved            

   Cognition   Lexico  Oxford University Press and Dictionary com  Archived from the original on July           Retrieved   May      

   Ask the Cognitive Scientist   American Federation of Teachers    August       Cognitive science is an interdisciplinary field of researchers from Linguistics  psychology  neuroscience  philosophy  computer science  and anthropology that seek to understand the mind 

  Robinson  Peter         Handbook of Cognitive Linguistics and Second Language Acquisition  Routledge  pp            ISBN                        

  Lakoff  George         Philosophy in the Flesh  The Embodied Mind and Its Challenge to Western Philosophy  Appendix  The Neural Theory of Language Paradigm  New York Basic Books  pp                ISBN                        

  Strauss  Claudia         A Cognitive Theory of Cultural Meaning  Cambridge University Press  pp                ISBN                        

  US     patent              

   Universal Conceptual Cognitive Annotation  UCCA    Universal Conceptual Cognitive Annotation  UCCA   Retrieved            

  Rodr guez  F  C    amp  Mairal Us n  R          Building an RRG computational grammar  Onomazein               

   Fluid Construction Grammar   A fully operational processing system for construction grammars   Retrieved            

   ACL Member Portal   The Association for Computational Linguistics Member Portal   www aclweb org  Retrieved            

   Chunks and Rules   W C  Retrieved            

  Socher  Richard  Karpathy  Andrej  Le  Quoc V   Manning  Christopher D   Ng  Andrew Y           Grounded Compositional Semantics for Finding and Describing Images with Sentences   Transactions of the Association for Computational Linguistics              doi         tacl a        S CID              

  Dasgupta  Ishita  Lampinen  Andrew K   Chan  Stephanie C  Y   Creswell  Antonia  Kumaran  Dharshan  McClelland  James L   Hill  Felix          Language models show human like content effects on reasoning  Dasgupta  Lampinen et al   arXiv             cs CL  

  Friston  Karl J          Active Inference  The Free Energy Principle in Mind  Brain  and Behavior  Chapter   The Generative Models of Active Inference  The MIT Press  ISBN                        


Further reading edit 

Bates  M          Models of natural language understanding   Proceedings of the National Academy of Sciences of the United States of America                      Bibcode     PNAS          B  doi         pnas             PMC             PMID              
Steven Bird  Ewan Klein  and Edward Loper         Natural Language Processing with Python  O Reilly Media  ISBN                        
Kenna Hughes Castleberry   A Murder Mystery Puzzle  The literary puzzle Cain s Jawbone  which has stumped humans for decades  reveals the limitations of natural language processing algorithms   Scientific American  vol       no     November        pp          This murder mystery competition has revealed that although NLP  natural language processing  models are capable of incredible feats  their abilities are very much limited by the amount of context they receive  This       could cause  difficulties  for researchers who hope to use them to do things such as analyze ancient languages  In some cases  there are few historical records on long gone civilizations to serve as training data for such a purpose    p      
Daniel Jurafsky and James H  Martin         Speech and Language Processing   nd edition  Pearson Prentice Hall  ISBN                        
Mohamed Zakaria Kurdi         Natural Language Processing and Computational Linguistics  speech  morphology  and syntax  Volume    ISTE Wiley  ISBN                     
Mohamed Zakaria Kurdi         Natural Language Processing and Computational Linguistics  semantics  discourse  and applications  Volume    ISTE Wiley  ISBN                     
Christopher D  Manning  Prabhakar Raghavan  and Hinrich Sch tze         Introduction to Information Retrieval  Cambridge University Press  ISBN                         Official html and pdf versions available without charge 
Christopher D  Manning and Hinrich Sch tze         Foundations of Statistical Natural Language Processing  The MIT Press  ISBN                        
David M  W  Powers and Christopher C  R  Turk         Machine Learning of Natural Language  Springer Verlag  ISBN                        

External links edit 
 Media related to Natural language processing at Wikimedia Commons
vteNatural language processingGeneral terms
AI complete
Bag of words
n gram
Bigram
Trigram
Computational linguistics
Natural language understanding
Stop words
Text processing
Text analysis
Argument mining
Collocation extraction
Concept mining
Coreference resolution
Deep linguistic processing
Distant reading
Information extraction
Named entity recognition
Ontology learning
Parsing
Semantic parsing
Syntactic parsing
Part of speech tagging
Semantic analysis
Semantic role labeling
Semantic decomposition
Semantic similarity
Sentiment analysis
Terminology extraction
Text mining
Textual entailment
Truecasing
Word sense disambiguation
Word sense induction
Text segmentation
Compound term processing
Lemmatisation
Lexical analysis
Text chunking
Stemming
Sentence segmentation
Word segmentation

Automatic summarization
Multi document summarization
Sentence extraction
Text simplification
Machine translation
Computer assisted
Example based
Rule based
Statistical
Transfer based
Neural
Distributional semantics models
BERT
Document term matrix
Explicit semantic analysis
fastText
GloVe
Language model  large 
Latent semantic analysis
Seq seq
Word embedding
Word vec
Language resources datasets and corporaTypes andstandards
Corpus linguistics
Lexical resource
Linguistic Linked Open Data
Machine readable dictionary
Parallel text
PropBank
Semantic network
Simple Knowledge Organization System
Speech corpus
Text corpus
Thesaurus  information retrieval 
Treebank
Universal Dependencies
Data
BabelNet
Bank of English
DBpedia
FrameNet
Google Ngram Viewer
UBY
WordNet
Wikidata
Automatic identificationand data capture
Speech recognition
Speech segmentation
Speech synthesis
Natural language generation
Optical character recognition
Topic model
Document classification
Latent Dirichlet allocation
Pachinko allocation
Computer assistedreviewing
Automated essay scoring
Concordancer
Grammar checker
Predictive text
Pronunciation assessment
Spell checker
Natural languageuser interface
Chatbot
Interactive fiction  cf  Syntax guessing 
Question answering
Virtual assistant
Voice user interface
Related
Formal semantics
Hallucination
Natural Language Toolkit
spaCy

Portal  Language
Authority control databases  National United StatesJapanCzech RepublicIsrael





Retrieved from  https   en wikipedia org w index php title Natural language processing amp oldid