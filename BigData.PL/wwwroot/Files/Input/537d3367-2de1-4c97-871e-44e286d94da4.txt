Method in natural language processing
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
Illustration of word embedding  Each word is a point in some space  The word embedding enables to perform semantic operator like obtaining the capital of a given country 
In natural language processing  a word embedding is a representation of a word  The embedding is used in text analysis  Typically  the representation is a real valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning             Word embeddings can be obtained using language modeling and feature learning techniques  where words or phrases from the vocabulary are mapped to vectors of real numbers 
Methods to generate this mapping include neural networks             dimensionality reduction on the word co occurrence matrix                                   probabilistic models             explainable knowledge base method             and explicit representation in terms of the context in which words appear            
Word and phrase embeddings  when used as the underlying input representation  have been shown to boost the performance in NLP tasks such as syntactic parsing            and sentiment analysis             


Development and history of the approach edit 
In distributional semantics  a quantitative methodological approach for understanding meaning in observed language  word embeddings or semantic feature space models have been used as a knowledge representation for some time              Such models aim to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data   The underlying idea that  a word is characterized by the company it keeps  was proposed in a      article by John Rupert Firth              but also has roots in the contemporaneous work on search systems             and in cognitive psychology             
The notion of a semantic space with lexical items  words or multi word terms  represented as vectors or embeddings is based on the computational challenges of capturing distributional characteristics and using them for practical application to measure similarity between words  phrases  or entire documents  The first generation of semantic space models is the vector space model for information retrieval                                      Such vector space models for words and their distributional data implemented in their simplest form results in a very sparse vector space of high dimensionality  cf  curse of dimensionality   Reducing the number of dimensions using linear algebraic methods such as singular value decomposition then led to the introduction of latent semantic analysis in the late     s and the random indexing approach for collecting word co occurrence contexts                                                  In       Bengio et al  provided in a series of papers titled  Neural probabilistic language models  to reduce the high dimensionality of word representations in contexts by  learning a distributed representation for words                                      
A study published in NeurIPS  NIPS       introduced the use of both word and document embeddings applying the method of kernel CCA to bilingual  and multi lingual  corpora  also providing an early example of self supervised learning of word embeddings             
Word embeddings come in two different styles  one in which words are expressed as vectors of co occurring words  and another in which words are expressed as vectors of linguistic contexts in which the words occur  these different styles are studied in Lavelli et al                     Roweis and Saul published in Science how to use  locally linear embedding   LLE  to discover representations of high dimensional data structures              Most new word embedding techniques after about      rely on a neural network architecture instead of more probabilistic and algebraic models  after foundational work done by Yoshua Bengio                 circular reference      and colleagues                         
The approach has been adopted by many research groups after theoretical advances in      had been made on the quality of vectors and the training speed of the model  as well as after hardware advances allowed for a broader parameter space to be explored profitably  In       a team at Google led by Tomas Mikolov created word vec  a word embedding toolkit that can train vector space models faster than previous approaches  The word vec approach has been widely used in experimentation and was instrumental in raising interest for word embeddings as a technology  moving the research strand out of specialised research into broader experimentation and eventually paving the way for practical application             

Polysemy and homonymy edit 
Historically  one of the main limitations of static word embeddings or word vector space models is that words with multiple meanings are conflated into a single representation  a single vector in the semantic space   In other words  polysemy and homonymy are not handled properly  For example  in the sentence  The club I tried yesterday was great    it is not clear if the term club is related to the word sense of a club sandwich  clubhouse  golf club  or any other sense that club might have  The necessity to accommodate multiple meanings per word in different vectors  multi sense embeddings  is the motivation for several contributions in NLP to split single sense embeddings into multi sense ones                         
Most approaches that produce multi sense embeddings can be divided into two main categories for their word sense representation  i e   unsupervised and knowledge based              Based on word vec skip gram  Multi Sense Skip Gram  MSSG              performs word sense discrimination and embedding simultaneously  improving its training time  while assuming a specific number of senses for each word  In the Non Parametric Multi Sense Skip Gram  NP MSSG  this number can vary depending on each word  Combining the prior knowledge of lexical databases  e g   WordNet  ConceptNet  BabelNet   word embeddings and word sense disambiguation  Most Suitable Sense Annotation  MSSA              labels word senses through an unsupervised and knowledge based approach  considering a word s context in a pre defined sliding window  Once the words are disambiguated  they can be used in a standard word embeddings technique  so multi sense embeddings are produced  MSSA architecture allows the disambiguation and annotation process to be performed recurrently in a self improving manner             
The use of multi sense embeddings is known to improve performance in several NLP tasks  such as part of speech tagging  semantic relation identification  semantic relatedness  named entity recognition and sentiment analysis                         
As of the late     s  contextually meaningful embeddings such as ELMo and BERT have been developed              Unlike static word embeddings  these embeddings are at the token level  in that each occurrence of a word has its own embedding  These embeddings better reflect the multi sense nature of words  because occurrences of a word in similar contexts are situated in similar regions of BERT s embedding space                         

For biological sequences  BioVectors edit 
Word embeddings for n grams in biological sequences  e g  DNA  RNA  and Proteins  for bioinformatics applications have been proposed by Asgari and Mofrad              Named bio vectors  BioVec  to refer to biological sequences in general with protein vectors  ProtVec  for proteins  amino acid sequences  and gene vectors  GeneVec  for gene sequences  this representation can be widely used in applications of deep learning in proteomics and genomics  The results presented by Asgari and Mofrad             suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns 

Game design edit 
Word embeddings with applications in game design have been proposed by Rabii and Cook             as a way to discover emergent gameplay using logs of gameplay data  The process requires transcribing actions that occur during a game within a formal language and then using the resulting text to create word embeddings  The results presented by Rabii and Cook             suggest that the resulting vectors can capture expert knowledge about games like chess that are not explicitly stated in the game s rules 

Sentence embeddings edit 
Main article  Sentence embedding
The idea has been extended to embeddings of entire sentences or even documents  e g  in the form of the thought vectors concept  In       some researchers suggested  skip thought vectors  as a means to improve the quality of machine translation              A more recent and popular approach for representing sentences is Sentence BERT  or SentenceTransformers  which modifies pre trained BERT with the use of siamese and triplet network structures             

Software edit 
Software for training and using word embeddings includes Tom   Mikolov s Word vec  Stanford University s GloVe              GN GloVe              Flair embeddings              AllenNLP s ELMo              BERT              fastText  Gensim              Indra              and Deeplearning j  Principal Component Analysis  PCA  and T Distributed Stochastic Neighbour Embedding  t SNE  are both used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters             

Examples of application edit 
For instance  the fastText is also used to calculate word embeddings for text corpora in Sketch Engine that are available online             

Ethical implications edit 
Word embeddings may contain the biases and stereotypes contained in the trained dataset  as Bolukbasi et al  points out in the      paper  Man is to Computer Programmer as Woman is to Homemaker  Debiasing Word Embeddings  that a publicly available  and popular  word vec embedding trained on Google News texts  a commonly used data corpus   which consists of text written by professional journalists  still shows disproportionate word associations reflecting gender and racial biases when extracting word analogies              For example  one of the analogies generated using the aforementioned word embedding is  man is to computer programmer as woman is to homemaker                          
Research done by Jieyu Zhou et al  shows that the applications of these trained word embeddings without careful oversight likely perpetuates existing bias in society  which is introduced through unaltered training data  Furthermore  word embeddings can even amplify these biases                           

See also edit 
Embedding  machine learning 
Brown clustering
Distributional relational database
References edit 


  Jurafsky  Daniel  H  James  Martin         Speech and language processing        an introduction to natural language processing  computational linguistics  and speech recognition  Upper Saddle River  N J   Prentice Hall  ISBN                        

  Mikolov  Tomas  Sutskever  Ilya  Chen  Kai  Corrado  Greg  Dean  Jeffrey          Distributed Representations of Words and Phrases and their Compositionality   arXiv            cs CL  

  Lebret  R mi  Collobert  Ronan          Word Emdeddings through Hellinger PCA   Conference of the European Chapter of the Association for Computational Linguistics  EACL   Vol             arXiv           

  Levy  Omer  Goldberg  Yoav         Neural Word Embedding as Implicit Matrix Factorization  PDF   NIPS 

  Li  Yitan  Xu  Linli         Word Embedding Revisited  A New Representation Learning and Explicit Matrix Factorization Perspective  PDF   Int l J  Conf  on Artificial Intelligence  IJCAI  

  Globerson  Amir          Euclidean Embedding of Co occurrence Data   PDF   Journal of Machine Learning Research 

  Qureshi  M  Atif  Greene  Derek                EVE  explainable vector based embedding technique using Wikipedia   Journal of Intelligent Information Systems               arXiv             doi         s               x  ISSN                 S CID               

  Levy  Omer  Goldberg  Yoav         Linguistic Regularities in Sparse and Explicit Word Representations  PDF   CoNLL  pp               

  Socher  Richard  Bauer  John  Manning  Christopher  Ng  Andrew         Parsing with compositional vector grammars  PDF   Proc  ACL Conf  Archived from the original  PDF  on             Retrieved            

  Socher  Richard  Perelygin  Alex  Wu  Jean  Chuang  Jason  Manning  Chris  Ng  Andrew  Potts  Chris         Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  PDF   EMNLP 

  Sahlgren  Magnus   A brief history of word embeddings  

  Firth  J R           A synopsis of linguistic theory             Studies in Linguistic Analysis        Reprinted in F R  Palmer  ed          Selected Papers of J R  Firth            London  Longman   cite book     CS  maint  publisher location  link 

  Luhn  H P           A New Method of Recording and Searching Information   American Documentation            doi         asi            

  Osgood  C E   Suci  G J   Tannenbaum  P H          The Measurement of Meaning  University of Illinois Press 

  Salton  Gerard          Some experiments in the generation of word and document associations   Proceedings of the December            fall joint computer conference on   AFIPS      Fall   pp                doi                          ISBN                     S CID                 cite book    ISBN   Date incompatibility  help 

  Salton  Gerard  Wong  A  Yang  C S          A Vector Space Model for Automatic Indexing   Communications of the ACM                    doi                        hdl            S CID              

  Dubin  David          The most influential paper Gerard Salton never wrote   Archived from the original on    October       Retrieved    October      

  Kanerva  Pentti  Kristoferson  Jan and Holst  Anders         Random Indexing of Text Samples for Latent Semantic Analysis  Proceedings of the   nd Annual Conference of the Cognitive Science Society  p             Mahwah  New Jersey  Erlbaum       

  Karlgren  Jussi  Sahlgren  Magnus         Uesaka  Yoshinori  Kanerva  Pentti  Asoh  Hideki  eds     From words to understanding   Foundations of Real World Intelligence  CSLI Publications          

  Sahlgren  Magnus        An Introduction to Random Indexing  Proceedings of the Methods and Applications of Semantic Indexing Workshop at the  th International Conference on Terminology and Knowledge Engineering  TKE       August     Copenhagen  Denmark

  Sahlgren  Magnus  Holst  Anders and Pentti Kanerva        Permutations as a Means to Encode Order in Word Space  In Proceedings of the   th Annual Conference of the Cognitive Science Society            

  Bengio  Yoshua  R jean  Ducharme  Pascal  Vincent          A Neural Probabilistic Language Model   PDF   NeurIPS 

  Bengio  Yoshua  Ducharme  R jean  Vincent  Pascal  Jauvin  Christian          A Neural Probabilistic Language Model   PDF   Journal of Machine Learning Research               

  Bengio  Yoshua  Schwenk  Holger  Sen cal  Jean S bastien  Morin  Fr deric  Gauvain  Jean Luc          A Neural Probabilistic Language Model   Studies in Fuzziness and Soft Computing  Vol            Springer  pp                doi                          ISBN                        

  Vinkourov  Alexei  Cristianini  Nello  Shawe Taylor  John         Inferring a semantic representation of text via cross language correlation analysis  PDF   Advances in Neural Information Processing Systems  Vol          

  Lavelli  Alberto  Sebastiani  Fabrizio  Zanoli  Roberto         Distributional term representations  an experimental comparison    th ACM International Conference on Information and Knowledge Management  pp                doi                         

  Roweis  Sam T   Saul  Lawrence K           Nonlinear Dimensionality Reduction by Locally Linear Embedding   Science                      Bibcode     Sci           R  CiteSeerX                       doi         science                PMID                S CID              

  he             

  Morin  Fredric  Bengio  Yoshua          Hierarchical probabilistic neural network language model   PDF   In Cowell  Robert G   Ghahramani  Zoubin  eds    Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics  Proceedings of Machine Learning Research  Vol       R   pp               

  Mnih  Andriy  Hinton  Geoffrey          A Scalable Hierarchical Distributed Language Model   Advances in Neural Information Processing Systems      NIPS        Curran Associates  Inc             

   word vec   Google Code Archive  Retrieved    July      

  Reisinger  Joseph  Mooney  Raymond J          Multi Prototype Vector Space Models of Word Meaning  Vol       Human Language Technologies  The      Annual Conference of the North American Chapter of the Association for Computational Linguistics  Los Angeles  California  Association for Computational Linguistics  pp                ISBN                         Retrieved October          

  Huang  Eric          Improving word representations via global context and multiple word prototypes  OCLC                

  Camacho Collados  Jose  Pilehvar  Mohammad Taher          From Word to Sense Embeddings  A Survey on Vector Representations of Meaning   arXiv             cs CL  

  Neelakantan  Arvind  Shankar  Jeevan  Passos  Alexandre  McCallum  Andrew          Efficient Non parametric Estimation of Multiple Embeddings per Word in Vector Space   Proceedings of the      Conference on Empirical Methods in Natural Language Processing  EMNLP   Stroudsburg  PA  USA  Association for Computational Linguistics  pp                  arXiv             doi         v  d         S CID               

  Ruas  Terry  Grosky  William  Aizawa  Akiko                Multi sense embeddings through a word sense disambiguation process   Expert Systems with Applications                arXiv             doi         j eswa              hdl                 ISSN                 S CID               

  Agre  Gennady  Petrov  Daniel  Keskinova  Simona                Word Sense Disambiguation Studio  A Flexible System for WSD Feature Extraction   Information              doi         info          ISSN                

  a b Akbik  Alan  Blythe  Duncan  Vollgraf  Roland          Contextual String Embeddings for Sequence Labeling   Proceedings of the   th International Conference on Computational Linguistics  Santa Fe  New Mexico  USA  Association for Computational Linguistics            

  Li  Jiwei  Jurafsky  Dan          Do Multi Sense Embeddings Improve Natural Language Understanding    Proceedings of the      Conference on Empirical Methods in Natural Language Processing  Stroudsburg  PA  USA  Association for Computational Linguistics  pp                  arXiv             doi          v  d         S CID              

  Devlin  Jacob  Chang  Ming Wei  Lee  Kenton  Toutanova  Kristina  June         Proceedings of the      Conference of the North   Proceedings of the      Conference of the North American Chapter of the Association for Computational Linguistics  Human Language Technologies  Volume    Long and Short Papers   Association for Computational Linguistics             doi          v  N         S CID               

  Lucy  Li  and David Bamman   Characterizing English variation across social media communities with BERT   Transactions of the Association for Computational Linguistics                   

  Reif  Emily  Ann Yuan  Martin Wattenberg  Fernanda B  Viegas  Andy Coenen  Adam Pearce  and Been Kim   Visualizing and measuring the geometry of BERT   Advances in Neural Information Processing Systems           

  a b Asgari  Ehsaneddin  Mofrad  Mohammad R K           Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics   PLOS ONE           e         arXiv             Bibcode     PLoSO         A  doi         journal pone          PMC               PMID               

  a b Rabii  Youn s  Cook  Michael                Revealing Game Dynamics via Word Embeddings of Gameplay Data   Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment                   doi         aiide v  i         ISSN                 S CID                

  Kiros  Ryan  Zhu  Yukun  Salakhutdinov  Ruslan  Zemel  Richard S   Torralba  Antonio  Urtasun  Raquel  Fidler  Sanja          skip thought vectors   arXiv             cs CL  

  Reimers  Nils  and Iryna Gurevych   Sentence BERT  Sentence Embeddings using Siamese BERT Networks   In Proceedings of the      Conference on Empirical Methods in Natural Language Processing and the  th International Joint Conference on Natural Language Processing  EMNLP IJCNLP   pp                  

   GloVe  

  Zhao  Jieyu  et      al                  Learning Gender Neutral Word Embeddings   arXiv             cs CL  

   Elmo      October      

  Pires  Telmo  Schlinger  Eva  Garrette  Dan                How multilingual is Multilingual BERT    arXiv             cs CL  

   Gensim  

   Indra   GitHub             

  Ghassemi  Mohammad  Mark  Roger  Nemati  Shamim          A visualization of evolving clinical sentiment using vector representations of clinical notes   PDF        Computing in Cardiology Conference  CinC   Vol             pp                doi         CIC               ISBN                         PMC               PMID               

   Embedding Viewer   Embedding Viewer  Lexical Computing  Archived from the original on   February       Retrieved   Feb      

  Bolukbasi  Tolga  Chang  Kai Wei  Zou  James  Saligrama  Venkatesh  Kalai  Adam          Man is to Computer Programmer as Woman is to Homemaker  Debiasing Word Embeddings   arXiv             cs CL  

  Bolukbasi  Tolga  Chang  Kai Wei  Zou  James  Saligrama  Venkatesh  Kalai  Adam                Man is to Computer Programmer as Woman is to Homemaker  Debiasing Word Embeddings   arXiv             cs CL  

  Dieng  Adji B   Ruiz  Francisco J  R   Blei  David M           Topic Modeling in Embedding Spaces   Transactions of the Association for Computational Linguistics              arXiv             doi         tacl a       

  Zhao  Jieyu  Wang  Tianlu  Yatskar  Mark  Ordonez  Vicente  Chang  Kai Wei          Men Also Like Shopping  Reducing Gender Bias Amplification using Corpus level Constraints   Proceedings of the      Conference on Empirical Methods in Natural Language Processing  pp                  doi          v  D        

  Petreski  Davor  Hashim  Ibrahim C                 Word embeddings are biased  But whose bias are they reflecting    AI  amp  Society                   doi         s                w  ISSN                 S CID                


vteNatural language processingGeneral terms
AI complete
Bag of words
n gram
Bigram
Trigram
Computational linguistics
Natural language understanding
Stop words
Text processing
Text analysis
Argument mining
Collocation extraction
Concept mining
Coreference resolution
Deep linguistic processing
Distant reading
Information extraction
Named entity recognition
Ontology learning
Parsing
Semantic parsing
Syntactic parsing
Part of speech tagging
Semantic analysis
Semantic role labeling
Semantic decomposition
Semantic similarity
Sentiment analysis
Terminology extraction
Text mining
Textual entailment
Truecasing
Word sense disambiguation
Word sense induction
Text segmentation
Compound term processing
Lemmatisation
Lexical analysis
Text chunking
Stemming
Sentence segmentation
Word segmentation

Automatic summarization
Multi document summarization
Sentence extraction
Text simplification
Machine translation
Computer assisted
Example based
Rule based
Statistical
Transfer based
Neural
Distributional semantics models
BERT
Document term matrix
Explicit semantic analysis
fastText
GloVe
Language model  large 
Latent semantic analysis
Seq seq
Word embedding
Word vec
Language resources datasets and corporaTypes andstandards
Corpus linguistics
Lexical resource
Linguistic Linked Open Data
Machine readable dictionary
Parallel text
PropBank
Semantic network
Simple Knowledge Organization System
Speech corpus
Text corpus
Thesaurus  information retrieval 
Treebank
Universal Dependencies
Data
BabelNet
Bank of English
DBpedia
FrameNet
Google Ngram Viewer
UBY
WordNet
Wikidata
Automatic identificationand data capture
Speech recognition
Speech segmentation
Speech synthesis
Natural language generation
Optical character recognition
Topic model
Document classification
Latent Dirichlet allocation
Pachinko allocation
Computer assistedreviewing
Automated essay scoring
Concordancer
Grammar checker
Predictive text
Pronunciation assessment
Spell checker
Natural languageuser interface
Chatbot
Interactive fiction  cf  Syntax guessing 
Question answering
Virtual assistant
Voice user interface
Related
Formal semantics
Hallucination
Natural Language Toolkit
spaCy

vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects






Retrieved from  https   en wikipedia org w index php title Word embedding amp oldid