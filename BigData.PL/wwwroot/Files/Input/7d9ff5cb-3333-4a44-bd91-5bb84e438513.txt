See also  Statistical learning theory
Theory of machine learningThis article needs additional citations for verification  Please help improve this article by adding citations to reliable sources  Unsourced material may be challenged and removed Find sources        Computational learning theory              news        newspapers        books        scholar        JSTOR   November        Learn how and when to remove this message 
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
In computer science  computational learning theory  or just learning theory  is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms            


Overview edit 
Theoretical results in machine learning mainly deal with a type of inductive learning called supervised learning   In supervised learning  an algorithm is given samples that are labeled in some useful way   For example  the samples might be descriptions of mushrooms  and the labels could be whether or not the mushrooms are edible   The algorithm takes these previously labeled samples and uses them to induce a classifier   This classifier is a function that assigns labels to samples  including samples that have not been seen previously by the algorithm   The goal of the supervised learning algorithm is to optimize some measure of performance such as minimizing the number of mistakes made on new samples 
In addition to performance bounds  computational learning theory studies the time complexity and feasibility of learning      citation needed      In
computational learning theory  a computation is considered feasible if it can be done in polynomial time      citation needed      There are two kinds of time
complexity results 

Positive results                  Showing that a certain class of functions is learnable in polynomial time 
Negative results                  Showing that certain classes cannot be learned in polynomial time            
Negative results often rely on commonly believed  but yet unproven assumptions      citation needed      such as 

Computational complexity   P   NP  the P versus NP problem  
Cryptographic   One way functions exist 
There are several different approaches to computational learning theory based on making different assumptions about the inference principles used to generalise from limited data  This includes different definitions of probability  see frequency probability  Bayesian probability  and different assumptions on the generation of samples      citation needed      The different approaches include 

Exact learning  proposed by Dana Angluin     citation needed      
Probably approximately correct learning  PAC learning   proposed by Leslie Valiant            
VC theory  proposed by Vladimir Vapnik and Alexey Chervonenkis            
Inductive inference as developed by Ray Solomonoff                       
Algorithmic learning theory  from the work of E  Mark Gold            
Online machine learning  from the work of Nick Littlestone     citation needed      
While its primary goal is to understand learning abstractly  computational learning theory has led to the development of practical algorithms  For example  PAC theory inspired boosting  VC theory led to support vector machines  and Bayesian inference led to belief networks 

See also edit 
Error tolerance  PAC learning 
Grammar induction
Information theory
Occam learning
Stability  learning theory 
References edit 


   ACL   Association for Computational Learning  

  Kearns  Michael  Vazirani  Umesh  August            An Introduction to Computational Learning Theory  MIT Press  ISBN                     

  Valiant  Leslie          A Theory of the Learnable   PDF   Communications of the ACM                      doi                    S CID                Archived from the original  PDF  on             Retrieved            

  Vapnik  V   Chervonenkis  A           On the uniform convergence of relative frequencies of events to their probabilities   PDF   Theory of Probability and Its Applications                   doi                 

  Solomonoff  Ray  March         A Formal Theory of Inductive Inference Part     Information and Control               doi         S                     

  Solomonoff  Ray          A Formal Theory of Inductive Inference Part     Information and Control                  doi         S                     

  Gold  E  Mark          Language identification in the limit   PDF   Information and Control                   doi         S                     


Further reading edit 
A description of some of these publications is given at important publications in machine learning 

Surveys edit 
Angluin  D        Computational learning theory  Survey and selected bibliography  In Proceedings of the Twenty Fourth Annual ACM Symposium on Theory of Computing  May        pages               http   portal acm org citation cfm id              
D  Haussler  Probably approximately correct learning  In AAAI    Proceedings of the Eight National Conference on Artificial Intelligence  Boston  MA  pages            American Association for Artificial Intelligence        http   citeseer ist psu edu haussler  probably html
Feature selection edit 
A  Dhagat and L  Hellerstein   PAC learning with irrelevant attributes   in  Proceedings of the IEEE Symp  on Foundation of Computer Science         http   citeseer ist psu edu dhagat  pac html
Optimal O notation learning edit 
Oded Goldreich  Dana Ron  On universal learning algorithms  http   citeseerx ist psu edu viewdoc summary doi               
Negative results edit 
M  Kearns and Leslie Valiant        Cryptographic limitations on learning boolean formulae and finite automata  In Proceedings of the   st Annual ACM Symposium on Theory of Computing  pages          New York  ACM  http   citeseer ist psu edu kearns  cryptographic html     dead link     
Error tolerance edit 
Michael Kearns and Ming Li  Learning in the presence of malicious errors  SIAM Journal on Computing                 August       http   citeseer ist psu edu kearns  learning html
Kearns  M          Efficient noise tolerant learning from statistical queries  In Proceedings of the Twenty Fifth Annual ACM Symposium on Theory of Computing  pages          http   citeseer ist psu edu kearns  efficient html
Equivalence edit 
D Haussler  M Kearns  N Littlestone and M  Warmuth  Equivalence of models for polynomial learnability  Proc   st ACM Workshop on Computational Learning Theory               
Pitt  L   Warmuth  M  K           Prediction Preserving Reducibility   Journal of Computer and System Sciences                   doi                            J 
External links edit 
Basics of Bayesian inference
vteDifferentiable computingGeneral
Differentiable programming
Information geometry
Statistical manifold
Automatic differentiation
Neuromorphic computing
Pattern recognition
Ricci calculus
Computational learning theory
Inductive bias
Hardware
IPU
TPU
VPU
Memristor
SpiNNaker
Software libraries
TensorFlow
PyTorch
Keras
scikit learn
Theano
JAX
Flux jl
MindSpore

 Portals
Computer programming
Technology






Retrieved from  https   en wikipedia org w index php title Computational learning theory amp oldid