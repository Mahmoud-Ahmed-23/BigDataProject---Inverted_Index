Attribute of machine learning models
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
The sample complexity of a machine learning algorithm represents the number of training samples that it needs in order to successfully learn a target function 
More precisely  the sample complexity is the number of training samples that we need to supply to the algorithm  so that the function returned by the algorithm is within an arbitrarily small error of the best possible function  with probability arbitrarily close to   
There are two variants of sample complexity 

The weak variant fixes a particular input output distribution 
The strong variant takes the worst case sample complexity over all input output distributions 
The No free lunch theorem  discussed below  proves that  in general  the strong sample complexity is infinite  i e  that there is no algorithm that can learn the globally optimal target function using a finite number of training samples 
However  if we are only interested in a particular class of target functions  e g   only linear functions  then the sample complexity is finite  and it depends linearly on the VC dimension on the class of target functions            


Definition edit 
Let 
  
    
      
        X
      
    
      displaystyle X 
  
 be a space which we call the input space  and 
  
    
      
        Y
      
    
      displaystyle Y 
  
 be a space which we call the output space  and let 
  
    
      
        Z
      
    
      displaystyle Z 
  
 denote the product 
  
    
      
        X
          xd  
        Y
      
    
      displaystyle X times Y 
  
  For example  in the setting of binary classification  
  
    
      
        X
      
    
      displaystyle X 
  
 is typically a finite dimensional vector space and 
  
    
      
        Y
      
    
      displaystyle Y 
  
 is the set 
  
    
      
         
          x     
         
         
         
         
      
    
      displaystyle          
  
 
Fix a hypothesis space 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 of functions 
  
    
      
        h
          x a 
        X
          x     
        Y
      
    
      displaystyle h colon X to Y 
  
  A learning algorithm over 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is a computable map from 
  
    
      
        Z
      
    
      displaystyle Z 
  
 to 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
  In other words  it is an algorithm that takes as input a finite sequence of training samples and outputs a function from 
  
    
      
        X
      
    
      displaystyle X 
  
 to 
  
    
      
        Y
      
    
      displaystyle Y 
  
  Typical learning algorithms include empirical risk minimization  without or with Tikhonov regularization 
Fix a loss function 
  
    
      
        
          
            L
          
        
          x a 
        Y
          xd  
        Y
          x     
        
          
            R
          
          
              x     
             
          
        
      
    
      displaystyle   mathcal  L   colon Y times Y to  mathbb  R     geq    
  
  for example  the square loss 
  
    
      
        
          
            L
          
        
         
        y
         
        
          y
            x     
        
         
         
         
        y
          x     
        
          y
            x     
        
        
           
          
             
          
        
      
    
      displaystyle   mathcal  L   y y    y y       
  
  where 
  
    
      
        h
         
        x
         
         
        
          y
            x     
        
      
    
      displaystyle h x  y  
  
  For a given distribution 
  
    
      
          x c  
      
    
      displaystyle  rho  
  
 on 
  
    
      
        X
          xd  
        Y
      
    
      displaystyle X times Y 
  
  the expected risk of a hypothesis  a function  
  
    
      
        h
          x     
        
          
            H
          
        
      
    
      displaystyle h in   mathcal  H   
  
 is


  
    
      
        
          
            E
          
        
         
        h
         
          
        
          
            E
          
          
              x c  
          
        
         
        
          
            L
          
        
         
        h
         
        x
         
         
        y
         
         
         
        
            x   b 
          
            X
              xd  
            Y
          
        
        
          
            L
          
        
         
        h
         
        x
         
         
        y
         
        
        d
          x c  
         
        x
         
        y
         
      
    
      displaystyle   mathcal  E   h    mathbb  E     rho     mathcal  L   h x  y    int   X times Y   mathcal  L   h x  y   d rho  x y  
  

In our setting  we have 
  
    
      
        h
         
        
          
            A
          
        
         
        
          S
          
            n
          
        
         
      
    
      displaystyle h   mathcal  A   S  n   
  
  where 
  
    
      
        
          
            A
          
        
      
    
      displaystyle   mathcal  A   
  
 is a learning algorithm and 
  
    
      
        
          S
          
            n
          
        
         
         
         
        
          x
          
             
          
        
         
        
          y
          
             
          
        
         
         
          x     
         
         
        
          x
          
            n
          
        
         
        
          y
          
            n
          
        
         
         
          x   c 
        
            x c  
          
            n
          
        
      
    
      displaystyle S  n    x     y       ldots   x  n  y  n    sim  rho   n  
  
 is a sequence of vectors which are all drawn independently from 
  
    
      
          x c  
      
    
      displaystyle  rho  
  
  Define the optimal risk
  
    
      
        
          
            
              E
            
          
          
            
              H
            
          
          
              x     
          
        
         
        
          
            inf
            
              h
                x     
              
                
                  H
                
              
            
          
        
        
          
            E
          
        
         
        h
         
         
      
    
      displaystyle   mathcal  E     mathcal  H         underset  h in   mathcal  H     inf     mathcal  E   h   
  
Set 
  
    
      
        
          h
          
            n
          
        
         
        
          
            A
          
        
         
        
          S
          
            n
          
        
         
      
    
      displaystyle h  n    mathcal  A   S  n   
  
  for each sample size 
  
    
      
        n
      
    
      displaystyle n 
  
  
  
    
      
        
          h
          
            n
          
        
      
    
      displaystyle h  n  
  
 is a random variable and depends on the random variable 
  
    
      
        
          S
          
            n
          
        
      
    
      displaystyle S  n  
  
  which is drawn from the distribution 
  
    
      
        
            x c  
          
            n
          
        
      
    
      displaystyle  rho   n  
  
  The algorithm 
  
    
      
        
          
            A
          
        
      
    
      displaystyle   mathcal  A   
  
 is called consistent if 
  
    
      
        
          
            E
          
        
         
        
          h
          
            n
          
        
         
      
    
      displaystyle   mathcal  E   h  n   
  
 probabilistically converges to 
  
    
      
        
          
            
              E
            
          
          
            
              H
            
          
          
              x     
          
        
      
    
      displaystyle   mathcal  E     mathcal  H       
  
  In other words  for all 
  
    
      
          x f  
         
          x b  
         gt 
         
      
    
      displaystyle  epsilon   delta  gt   
  
  there exists a positive integer 
  
    
      
        N
      
    
      displaystyle N 
  
  such that  for all sample sizes 
  
    
      
        n
          x     
        N
      
    
      displaystyle n geq N 
  
  we have

  
    
      
        
          Pr
          
            
                x c  
              
                n
              
            
          
        
         
        
          
            E
          
        
         
        
          h
          
            n
          
        
         
          x     
        
          
            
              E
            
          
          
            
              H
            
          
          
              x     
          
        
          x     
          x b  
         
         lt 
          x b  
         
      
    
      displaystyle  Pr    rho   n     mathcal  E   h  n     mathcal  E     mathcal  H       geq  varepsilon   lt  delta   
  
The sample complexity of 
  
    
      
        
          
            A
          
        
      
    
      displaystyle   mathcal  A   
  
 is then the minimum 
  
    
      
        N
      
    
      displaystyle N 
  
 for which this holds  as a function of 
  
    
      
          x c  
         
          x f  
      
    
      displaystyle  rho   epsilon  
  
  and 
  
    
      
          x b  
      
    
      displaystyle  delta  
  
  We write the sample complexity as 
  
    
      
        N
         
          x c  
         
          x f  
         
          x b  
         
      
    
      displaystyle N  rho   epsilon   delta   
  
 to emphasize that this value of 
  
    
      
        N
      
    
      displaystyle N 
  
 depends on 
  
    
      
          x c  
         
          x f  
      
    
      displaystyle  rho   epsilon  
  
  and 
  
    
      
          x b  
      
    
      displaystyle  delta  
  
  If 
  
    
      
        
          
            A
          
        
      
    
      displaystyle   mathcal  A   
  
 is not consistent  then we set 
  
    
      
        N
         
          x c  
         
          x f  
         
          x b  
         
         
          x   e 
      
    
      displaystyle N  rho   epsilon   delta    infty  
  
  If there exists an algorithm for which 
  
    
      
        N
         
          x c  
         
          x f  
         
          x b  
         
      
    
      displaystyle N  rho   epsilon   delta   
  
 is finite  then we say that the hypothesis space 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is learnable 
In others words  the sample complexity 
  
    
      
        N
         
          x c  
         
          x f  
         
          x b  
         
      
    
      displaystyle N  rho   epsilon   delta   
  
 defines the rate of consistency of the algorithm  given a desired accuracy 
  
    
      
          x f  
      
    
      displaystyle  epsilon  
  
 and confidence 
  
    
      
          x b  
      
    
      displaystyle  delta  
  
  one needs to sample 
  
    
      
        N
         
          x c  
         
          x f  
         
          x b  
         
      
    
      displaystyle N  rho   epsilon   delta   
  
 data points to guarantee that the risk of the output function is within 
  
    
      
          x f  
      
    
      displaystyle  epsilon  
  
 of the best possible  with probability at least 
  
    
      
         
          x     
          x b  
      
    
      displaystyle    delta  
  
             
In probably approximately correct  PAC  learning  one is concerned with whether the sample complexity is polynomial  that is  whether 
  
    
      
        N
         
          x c  
         
          x f  
         
          x b  
         
      
    
      displaystyle N  rho   epsilon   delta   
  
 is bounded by a polynomial in 
  
    
      
         
        
           
        
          x f  
      
    
      displaystyle    epsilon  
  
 and 
  
    
      
         
        
           
        
          x b  
      
    
      displaystyle    delta  
  
  If 
  
    
      
        N
         
          x c  
         
          x f  
         
          x b  
         
      
    
      displaystyle N  rho   epsilon   delta   
  
 is polynomial for some learning algorithm  then one says that the hypothesis space  
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is PAC learnable  This is a stronger notion than being learnable 

Unrestricted hypothesis space  infinite sample complexity edit 

One can ask whether there exists a learning algorithm so that the sample complexity is finite in the strong sense  that is  there is a bound on the number of samples needed so that the algorithm can learn any distribution over the input output space with a specified target error  More formally  one asks whether there exists a learning algorithm 
  
    
      
        
          
            A
          
        
      
    
      displaystyle   mathcal  A   
  
  such that  for all 
  
    
      
          x f  
         
          x b  
         gt 
         
      
    
      displaystyle  epsilon   delta  gt   
  
  there exists a positive integer 
  
    
      
        N
      
    
      displaystyle N 
  
 such that for all 
  
    
      
        n
          x     
        N
      
    
      displaystyle n geq N 
  
  we have

  
    
      
        
          sup
          
              x c  
          
        
        
           
          
            
              Pr
              
                
                    x c  
                  
                    n
                  
                
              
            
             
            
              
                E
              
            
             
            
              h
              
                n
              
            
             
              x     
            
              
                
                  E
                
              
              
                
                  H
                
              
              
                  x     
              
            
              x     
              x b  
             
          
           
        
         lt 
          x b  
         
      
    
      displaystyle  sup    rho   left  Pr    rho   n     mathcal  E   h  n     mathcal  E     mathcal  H       geq  varepsilon   right  lt  delta   
  

where 
  
    
      
        
          h
          
            n
          
        
         
        
          
            A
          
        
         
        
          S
          
            n
          
        
         
      
    
      displaystyle h  n    mathcal  A   S  n   
  
  with 
  
    
      
        
          S
          
            n
          
        
         
         
         
        
          x
          
             
          
        
         
        
          y
          
             
          
        
         
         
          x     
         
         
        
          x
          
            n
          
        
         
        
          y
          
            n
          
        
         
         
          x   c 
        
            x c  
          
            n
          
        
      
    
      displaystyle S  n    x     y       ldots   x  n  y  n    sim  rho   n  
  
 as above  The No Free Lunch Theorem says that without restrictions on the hypothesis space 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
  this is not the case  i e   there always exist  bad  distributions for which the sample complexity is arbitrarily large            
Thus  in order to make statements about the rate of convergence of the quantity

  
    
      
        
          sup
          
              x c  
          
        
        
           
          
            
              Pr
              
                
                    x c  
                  
                    n
                  
                
              
            
             
            
              
                E
              
            
             
            
              h
              
                n
              
            
             
              x     
            
              
                
                  E
                
              
              
                
                  H
                
              
              
                  x     
              
            
              x     
              x b  
             
          
           
        
         
      
    
      displaystyle  sup    rho   left  Pr    rho   n     mathcal  E   h  n     mathcal  E     mathcal  H       geq  varepsilon   right   
  

one must either

constrain the space of probability distributions 
  
    
      
          x c  
      
    
      displaystyle  rho  
  
  e g  via a parametric approach  or
constrain the space of hypotheses 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
  as in distribution free approaches 
Restricted hypothesis space  finite sample complexity edit 
The latter approach leads to concepts such as VC dimension and Rademacher complexity which control the complexity of the space 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
  A smaller hypothesis space introduces more bias into the inference process  meaning that 
  
    
      
        
          
            
              E
            
          
          
            
              H
            
          
          
              x     
          
        
      
    
      displaystyle   mathcal  E     mathcal  H       
  
 may be greater than the best possible risk in a larger space  However  by restricting the complexity of the hypothesis space it becomes possible for an algorithm to produce more uniformly consistent functions  This trade off leads to the concept of regularization            
It is a theorem from VC theory that the following three statements are equivalent for a hypothesis space 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 


  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is PAC learnable 
The VC dimension of 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is finite 

  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is a uniform Glivenko Cantelli class 
This gives a way to prove that certain hypothesis spaces are PAC learnable  and by extension  learnable 

An example of a PAC learnable hypothesis space edit 

  
    
      
        X
         
        
          
            R
          
          
            d
          
        
         
        Y
         
         
          x     
         
         
         
         
      
    
      displaystyle X  mathbb  R    d  Y          
  
  and let 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 be the space of affine functions on 
  
    
      
        X
      
    
      displaystyle X 
  
  that is  functions of the form 
  
    
      
        x
          x  a  
          x  e  
        w
         
        x
          x  e  
         
        b
      
    
      displaystyle x mapsto  langle w x rangle  b 
  
 for some 
  
    
      
        w
          x     
        
          
            R
          
          
            d
          
        
         
        b
          x     
        
          R
        
      
    
      displaystyle w in  mathbb  R    d  b in  mathbb  R   
  
  This is the linear classification with offset learning problem  Now  four coplanar points in a square cannot be shattered by any affine function  since no affine function can be positive on two diagonally opposite vertices and negative on the remaining two  Thus  the VC dimension of 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is 
  
    
      
        d
         
         
      
    
      displaystyle d   
  
  so it is finite  It follows by the above characterization of PAC learnable classes that 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is PAC learnable  and by extension  learnable 

Sample complexity bounds edit 

Suppose 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is a class of binary functions  functions to 
  
    
      
         
         
         
         
         
      
    
      displaystyle         
  
   Then  
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is 
  
    
      
         
          x f  
         
          x b  
         
      
    
      displaystyle   epsilon   delta   
  
 PAC learnable with a sample of size 
           

  
    
      
        N
         
        O
        
          
             
          
        
        
          
            
              V
              C
               
              
                
                  H
                
              
               
               
              ln
                x     
              
                
                   
                    x b  
                
              
            
              x f  
          
        
        
          
             
          
        
      
    
      displaystyle N O  bigg     frac  VC   mathcal  H     ln     over  delta     epsilon     bigg    
  

where 
  
    
      
        V
        C
         
        
          
            H
          
        
         
      
    
      displaystyle VC   mathcal  H    
  
 is the VC dimension of 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 
Moreover  any 
  
    
      
         
          x f  
         
          x b  
         
      
    
      displaystyle   epsilon   delta   
  
 PAC learning algorithm for 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 must have sample complexity            

  
    
      
        N
         
          x a  
        
          
             
          
        
        
          
            
              V
              C
               
              
                
                  H
                
              
               
               
              ln
                x     
              
                
                   
                    x b  
                
              
            
              x f  
          
        
        
          
             
          
        
      
    
      displaystyle N  Omega   bigg     frac  VC   mathcal  H     ln     over  delta     epsilon     bigg    
  

Thus  the sample complexity is a linear function of the VC dimension of the hypothesis space 
Suppose 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is a class of real valued functions with range in 
  
    
      
         
         
         
        T
         
      
    
      displaystyle    T  
  
  Then  
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 is 
  
    
      
         
          x f  
         
          x b  
         
      
    
      displaystyle   epsilon   delta   
  
 PAC learnable with a sample of size 
                      

  
    
      
        N
         
        O
        
          
             
          
        
        
          T
          
             
          
        
        
          
            
              P
              D
               
              
                
                  H
                
              
               
              ln
                x     
              
                
                  T
                    x f  
                
              
               
              ln
                x     
              
                
                   
                    x b  
                
              
            
            
                x f  
              
                 
              
            
          
        
        
          
             
          
        
      
    
      displaystyle N O  bigg   T      frac  PD   mathcal  H    ln  T  over  epsilon    ln     over  delta     epsilon         bigg    
  

where 
  
    
      
        P
        D
         
        
          
            H
          
        
         
      
    
      displaystyle PD   mathcal  H    
  
 is Pollard s pseudo dimension of 
  
    
      
        
          
            H
          
        
      
    
      displaystyle   mathcal  H   
  
 

Other settings edit 
In addition to the supervised learning setting  sample complexity is relevant to semi supervised learning problems including active learning             where the algorithm can ask for labels to specifically chosen inputs in order to reduce the cost of obtaining many labels  The concept of sample complexity also shows up in reinforcement learning             online learning  and unsupervised algorithms  e g  for dictionary learning            

Efficiency in robotics edit 
A high sample complexity means that many calculations are needed for running a Monte Carlo tree search              It is equivalent to a model free brute force search in the state space  In contrast  a high efficiency algorithm has a low sample complexity              Possible techniques for reducing the sample complexity are metric learning             and model based reinforcement learning             

See also edit 
Active learning  machine learning 
References edit 


  a b Vapnik  Vladimir         Statistical Learning Theory  New York  Wiley 

  a b Rosasco  Lorenzo         Consistency  Learnability  and Regularization  Lecture Notes for MIT Course       

  Steve Hanneke          The optimal sample complexity of PAC learning   J  Mach  Learn  Res                     arXiv            

  Ehrenfeucht  Andrzej  Haussler  David  Kearns  Michael  Valiant  Leslie          A general lower bound on the number of examples needed for learning   Information and Computation               doi                              

  Anthony  Martin  Bartlett  Peter L          Neural Network Learning  Theoretical Foundations  ISBN                    

  Morgenstern  Jamie  Roughgarden  Tim         On the Pseudo Dimension of Nearly Optimal Auctions  NIPS  Curran Associates  pp                arXiv            

  Balcan  Maria Florina  Hanneke  Steve  Wortman Vaughan  Jennifer          The true sample complexity of active learning   Machine Learning                     doi         s               y 

  Kakade  Sham         On the Sample Complexity of Reinforcement Learning  PDF   PhD Thesis  University College London  Gatsby Computational Neuroscience Unit 

  Vainsencher  Daniel  Mannor  Shie  Bruckstein  Alfred          The Sample Complexity of Dictionary Learning   PDF   Journal of Machine Learning Research                

  Kaufmann  Emilie and Koolen  Wouter M         Monte carlo tree search by best arm identification  Advances in Neural Information Processing Systems  pp                   cite conference     CS  maint  multiple names  authors list  link 

  Fidelman  Peggy and Stone  Peter         The chin pinch  A case study in skill learning on a legged robot  Robot Soccer World Cup  Springer  pp               cite conference     CS  maint  multiple names  authors list  link 

  Verma  Nakul and Branson  Kristin         Sample complexity of learning mahalanobis distance metrics  Advances in neural information processing systems  pp                   cite conference     CS  maint  multiple names  authors list  link 

  Kurutach  Thanard and Clavera  Ignasi and Duan  Yan and Tamar  Aviv and Abbeel  Pieter          Model ensemble trust region policy optimization   arXiv             cs LG    cite arXiv     CS  maint  multiple names  authors list  link 







Retrieved from  https   en wikipedia org w index php title Sample complexity amp oldid