Lexer  redirects here  For people with this name  see Lexer  surname  
Conversion of character sequences into token sequences in computer science
Lexical tokenization is conversion of a text into  semantically or syntactically  meaningful lexical tokens belonging to categories defined by a  lexer  program  In case of a natural language  those categories include nouns  verbs  adjectives  punctuations etc  In case of a programming language  the categories include identifiers  operators  grouping symbols and data types  Lexical tokenization is related to the type of tokenization used in large language models  LLMs  but with two differences  First  lexical tokenization is usually based on a lexical grammar  whereas LLM tokenizers are usually probability based  Second  LLM tokenizers perform a second step that converts the tokens into numerical values 


Rule based programs edit 
A rule based program  performing lexical tokenization  is called tokenizer             or scanner  although scanner is also a term for the first stage of a lexer  A lexer forms the first phase of a compiler frontend in processing  Analysis generally occurs in one pass  Lexers and parsers are most often used for compilers  but can be used for other computer language tools  such as prettyprinters or linters  Lexing can be divided into two stages  the scanning  which segments the input string into syntactic units called lexemes and categorizes these into token classes  and the evaluating  which converts lexemes into processed values 
Lexers are generally quite simple  with most of the complexity deferred to the syntactic analysis or semantic analysis phases  and can often be generated by a lexer generator  notably lex or derivatives  However  lexers can sometimes include some complexity  such as phrase structure processing to make input easier and simplify the parser  and may be written partly or fully by hand  either to support more features or for performance 

Disambiguation of  lexeme  edit 
See also  Word boundary  linguistics  and Word boundary  computing 
What is called  lexeme  in rule based natural language processing is not equal to what is called lexeme in linguistics  What is called  lexeme  in rule based natural language processing can be equal to the linguistic equivalent only in analytic languages  such as English  but not in highly synthetic languages  such as fusional languages  What is called a lexeme in rule based natural language processing is more similar to what is called a word in linguistics  not to be confused with a word in computer architecture   although in some cases it may be more similar to a morpheme 

Lexical token and lexical tokenization edit 
Not to be confused with Large language model        Tokenization  or tokenization  data security  
A lexical token is a string with an assigned and thus identified meaning  in contrast to the probabilistic token used in large language models  A lexical token consists of a token name and an optional token value  The token name is a category of a rule based lexical unit            


Examples of common tokens


Token name
 Lexical category 


Explanation
Sample token values


identifier
Names assigned by the programmer 
x  color  UP


keyword
Reserved words of the language 
if  while  return


separator punctuator
Punctuation characters and paired delimiters 
       


operator
Symbols that operate on arguments and produce results 
    lt    


literal
Numeric  logical  textual  and reference literals 
true      e     quot music quot 


comment
Line or block comments  Usually discarded 
   Retrieves user data        must be negative


whitespace
Groups of non printable characters  Usually discarded 
 

Consider this expression in the C programming language 

x   a   b     
The lexical analysis of this expression yields the following sequence of tokens 

  identifier  x    operator       identifier  a    operator       identifier  b    operator       literal       separator          
A token name is what might be termed a part of speech in linguistics 
Lexical tokenization is the conversion of a raw text into  semantically or syntactically  meaningful lexical tokens  belonging to categories defined by a  lexer  program  such as identifiers  operators  grouping symbols  and data types  The resulting tokens are then passed on to some other form of processing  The process can be considered a sub task of parsing input 
For example  in the text string 

The quick brown fox jumps over the lazy dog
the string is not implicitly segmented on spaces  as a natural language speaker would do  The raw input  the    characters  must be explicitly split into the   tokens with a given space delimiter  i e   matching the string     or regular expression   s      
When a token class represents more than one possible lexeme  the lexer often saves enough information to reproduce the original lexeme  so that it can be used in semantic analysis  The parser typically retrieves this information from the lexer and stores it in the abstract syntax tree  This is necessary in order to avoid information loss in the case where numbers may also be valid identifiers 
Tokens are identified based on the specific rules of the lexer  Some methods used to identify tokens include regular expressions  specific sequences of characters termed a flag  specific separating characters called delimiters  and explicit definition by a dictionary  Special characters  including punctuation characters  are commonly used by lexers to identify tokens because of their natural use in written and programming languages  A lexical analyzer generally does nothing with combinations of tokens  a task left for a parser  For example  a typical lexical analyzer recognizes parentheses as tokens but does nothing to ensure that each     is matched with a      
When a lexer feeds tokens to the parser  the representation used is typically an enumerated type  which is a list of number representations  For example   Identifier  can be represented with     Assignment operator  with     Addition operator  with    etc 
Tokens are often defined by regular expressions  which are understood by a lexical analyzer generator such as lex  or handcoded equivalent finite state automata  The lexical analyzer  generated automatically by a tool like lex or hand crafted  reads in a stream of characters  identifies the lexemes in the stream  and categorizes them into tokens  This is termed tokenizing  If the lexer finds an invalid token  it will report an error 
Following tokenizing is parsing  From there  the interpreted data may be loaded into data structures for general use  interpretation  or compiling 

Lexical grammar edit 
The specification of a programming language often includes a set of rules  the lexical grammar  which defines the lexical syntax  The lexical syntax is usually a regular language  with the grammar rules consisting of regular expressions  they define the set of possible character sequences  lexemes  of a token  A lexer recognizes strings  and for each kind of string found  the lexical program takes an action  most simply producing a token 
Two important common lexical categories are white space and comments  These are also defined in the grammar and processed by the lexer but may be discarded  not producing any tokens  and considered non significant  at most separating two tokens  as in if      x instead of ifx   There are two important exceptions to this  First  in off side rule languages that delimit blocks with indenting  initial whitespace is significant  as it determines block structure  and is generally handled at the lexer level  see phrase structure  below  Secondly  in some uses of lexers  comments and whitespace must be preserved   for examples  a prettyprinter also needs to output the comments and some debugging tools may provide messages to the programmer showing the original source code  In the     s  notably for ALGOL  whitespace and comments were eliminated as part of the line reconstruction phase  the initial phase of the compiler frontend   but this separate phase has been eliminated and these are now handled by the lexer 

Details edit 
Scanner edit 
The first stage  the scanner  is usually based on a finite state machine  FSM   It has encoded within it information on the possible sequences of characters that can be contained within any of the tokens it handles  individual instances of these character sequences are termed lexemes   For example  an integer lexeme may contain any sequence of numerical digit characters  In many cases  the first non whitespace character can be used to deduce the kind of token that follows and subsequent input characters are then processed one at a time until reaching a character that is not in the set of characters acceptable for that token  this is termed the maximal munch  or longest match  rule   In some languages  the lexeme creation rules are more complex and may involve backtracking over previously read characters  For example  in C  one  L  character is not enough to distinguish between an identifier that begins with  L  and a wide character string literal 

Evaluator edit 
A lexeme  however  is only a string of characters known to be of a certain kind  e g   a string literal  a sequence of letters   In order to construct a token  the lexical analyzer needs a second stage  the evaluator  which goes over the characters of the lexeme to produce a value  The lexeme s type combined with its value is what properly constitutes a token  which can be given to a parser  Some tokens such as parentheses do not really have values  and so the evaluator function for these can return nothing  Only the type is needed  Similarly  sometimes evaluators can suppress a lexeme entirely  concealing it from the parser  which is useful for whitespace and comments  The evaluators for identifiers are usually simple  literally representing the identifier   but may include some unstropping  The evaluators for integer literals may pass the string on  deferring evaluation to the semantic analysis phase   or may perform evaluation themselves  which can be involved for different bases or floating point numbers  For a simple quoted string literal  the evaluator needs to remove only the quotes  but the evaluator for an escaped string literal incorporates a lexer  which unescapes the escape sequences 
For example  in the source code of a computer program  the string

net worth future    assets   liabilities  
might be converted into the following lexical token stream  whitespace is suppressed and special characters have no value 

IDENTIFIER net worth future
EQUALS
OPEN PARENTHESIS
IDENTIFIER assets
MINUS
IDENTIFIER liabilities
CLOSE PARENTHESIS
SEMICOLON

Lexers may be written by hand  This is practical if the list of tokens is small  but lexers generated by automated tooling as part of a compiler compiler toolchain are more practical for a larger number of potential tokens  These tools generally accept regular expressions that describe the tokens allowed in the input stream  Each regular expression is associated with a production rule in the lexical grammar of the programming language that evaluates the lexemes matching the regular expression  These tools may generate source code that can be compiled and executed or construct a state transition table for a finite state machine  which is plugged into template code for compiling and executing  
Regular expressions compactly represent patterns that the characters in lexemes might follow  For example  for an English based language  an IDENTIFIER token might be any English alphabetic character or an underscore  followed by any number of instances of ASCII alphanumeric characters and or underscores  This could be represented compactly by the string  a zA Z   a zA Z        This means  any character a z  A Z or    followed by   or more of a z  A Z    or      
Regular expressions and the finite state machines they generate are not powerful enough to handle recursive patterns  such as  n opening parentheses  followed by a statement  followed by n closing parentheses   They are unable to keep count  and verify that n is the same on both sides  unless a finite set of permissible values exists for n  It takes a full parser to recognize such patterns in their full generality  A parser can push parentheses on a stack and then try to pop them off and see if the stack is empty at the end  see example            in the Structure and Interpretation of Computer Programs book  

Obstacles edit 
Typically  lexical tokenization occurs at the word level  However  it is sometimes difficult to define what is meant by a  word   Often  a tokenizer relies on simple heuristics  for example 

Punctuation and whitespace may or may not be included in the resulting list of tokens 
All contiguous strings of alphabetic characters are part of one token  likewise with numbers 
Tokens are separated by whitespace characters  such as a space or line break  or by punctuation characters 
In languages that use inter word spaces  such as most that use the Latin alphabet  and most programming languages   this approach is fairly straightforward  However  even here there are many edge cases such as contractions  hyphenated words  emoticons  and larger constructs such as URIs  which for some purposes may count as single tokens   A classic example is  New York based   which a naive tokenizer may break at the space even though the better break is  arguably  at the hyphen 
Tokenization is particularly difficult for languages written in scriptio continua  which exhibit no word boundaries  such as Ancient Greek  Chinese             or Thai  Agglutinative languages  such as Korean  also make tokenization tasks complicated 
Some ways to address the more difficult problems include developing more complex heuristics  querying a table of common special cases  or fitting the tokens to a language model that identifies collocations in a later processing step 

Lexer generator edit 
See also  Parser generator
Lexers are often generated by a lexer generator  analogous to parser generators  and such tools often come together  The most established is lex  paired with the yacc parser generator  or rather some of their many reimplementations  like flex  often paired with GNU Bison   These generators are a form of domain specific language  taking in a lexical specification   generally regular expressions with some markup   and emitting a lexer 
These tools yield very fast development  which is very important in early development  both to get a working lexer and because a language specification may change often  Further  they often provide advanced features  such as pre  and post conditions which are hard to program by hand  However  an automatically generated lexer may lack flexibility  and thus may require some manual modification  or an all manually written lexer 
Lexer performance is a concern  and optimizing is worthwhile  more so in stable languages where the lexer runs very often  such as C or HTML   lex flex generated lexers are reasonably fast  but improvements of two to three times are possible using more tuned generators  Hand written lexers are sometimes used  but modern lexer generators produce faster lexers than most hand coded ones  The lex flex family of generators uses a table driven approach which is much less efficient than the directly coded approach      dubious             discuss      With the latter approach the generator produces an engine that directly jumps to follow up states via goto statements  Tools like re c            have proven to produce engines that are between two and three times faster than flex produced engines      citation needed      It is in general difficult to hand write analyzers that perform better than engines generated by these latter tools 

Phrase structure edit 
Lexical analysis mainly segments the input stream of characters into tokens  simply grouping the characters into pieces and categorizing them  However  the lexing may be significantly more complex  most simply  lexers may omit tokens or insert added tokens  Omitting tokens  notably whitespace and comments  is very common when these are not needed by the compiler  Less commonly  added tokens may be inserted  This is done mainly to group tokens into statements  or statements into blocks  to simplify the parser 

Line continuation edit 
Line continuation is a feature of some languages where a newline is normally a statement terminator  Most often  ending a line with a backslash  immediately followed by a newline  results in the line being continued   the following line is joined to the prior line  This is generally done in the lexer  The backslash and newline are discarded  rather than the newline being tokenized  Examples include bash             other shell scripts and Python            

Semicolon insertion edit 
Many languages use the semicolon as a statement terminator  Most often this is mandatory  but in some languages the semicolon is optional in many contexts  This is mainly done at the lexer level  where the lexer outputs a semicolon into the token stream  despite one not being present in the input character stream  and is termed semicolon insertion or automatic semicolon insertion  In these cases  semicolons are part of the formal phrase grammar of the language  but may not be found in input text  as they can be inserted by the lexer  Optional semicolons or other terminators or separators are also sometimes handled at the parser level  notably in the case of trailing commas or semicolons 
Semicolon insertion is a feature of BCPL and its distant descendant Go             though it is absent in B or C             Semicolon insertion is present in JavaScript  though the rules are somewhat complex and much criticized  to avoid bugs  some recommend always using semicolons  while others use initial semicolons  termed defensive semicolons  at the start of potentially ambiguous statements 
Semicolon insertion  in languages with semicolon terminated statements  and line continuation  in languages with newline terminated statements  can be seen as complementary  Semicolon insertion adds a token even though newlines generally do not generate tokens  while line continuation prevents a token from being generated even though newlines generally do generate tokens 

Off side rule edit 
Further information  Off side rule
The off side rule  blocks determined by indenting  can be implemented in the lexer  as in Python  where increasing the indenting results in the lexer emitting an INDENT token and decreasing the indenting results in the lexer emitting one or more DEDENT tokens              These tokens correspond to the opening brace   and closing brace   in languages that use braces for blocks and means that the phrase grammar does not depend on whether braces or indenting are used  This requires that the lexer hold state  namely a stack of indent levels  and thus can detect changes in indenting when this changes  and thus the lexical grammar is not context free  INDENT DEDENT depend on the contextual information of prior indent levels 

Context sensitive lexing edit 
Generally lexical grammars are context free  or almost so  and thus require no looking back or ahead  or backtracking  which allows a simple  clean  and efficient implementation  This also allows simple one way communication from lexer to parser  without needing any information flowing back to the lexer 
There are exceptions  however  Simple examples include semicolon insertion in Go  which requires looking back one token  concatenation of consecutive string literals in Python             which requires holding one token in a buffer before emitting it  to see if the next token is another string literal   and the off side rule in Python  which requires maintaining a count of indent level  indeed  a stack of each indent level   These examples all only require lexical context  and while they complicate a lexer somewhat  they are invisible to the parser and later phases 
A more complex example is the lexer hack in C  where the token class of a sequence of characters cannot be determined until the semantic analysis phase since typedef names and variable names are lexically identical but constitute different token classes  Thus in the hack  the lexer calls the semantic analyzer  say  symbol table  and checks if the sequence requires a typedef name  In this case  information must flow back not from the parser only  but from the semantic analyzer back to the lexer  which complicates design 

See also edit 
Lexical frequency analysis
Lexicalization
Lexical semantics
List of parser generators
References edit 


   Anatomy of a Compiler and The Tokenizer   www cs man ac uk 

  page       Compilers Principles  Techniques   amp  Tools   nd Ed    WorldCat  by Aho  Lam  Sethi and Ullman  as quoted in https   stackoverflow com questions          what is the difference between token and lexeme

   Structure and Interpretation of Computer Programs   mitpress mit edu  Archived from the original on             Retrieved            

  Huang  C   Simon  P   Hsieh  S    amp  Prevot  L         Rethinking Chinese Word Segmentation  Tokenization  Character Classification  or Word break Identification

  Bumbulis  P   Cowan  D  D   Mar Dec         RE C  A more versatile scanner generator   ACM Letters on Programming Languages and Systems                  doi                        S CID               

  Bash Reference Manual          Escape Character

  a b        Documentation   docs python org 

  Effective Go   Semicolons 

   Semicolons in Go   golang nuts  Rob  Commander  Pike          

   Lexical analysis  gt  Indentation   The Python Language Reference  Retrieved    June      


Sources edit 

Compiling with C  and Java  Pat Terry        ISBN               X
Algorithms   Data Structures   Programs  Niklaus Wirth        ISBN                   
Compiler Construction  Niklaus Wirth        ISBN                   
Sebesta  R  W          Concepts of programming languages  Seventh edition  pp            Boston  Pearson Addison Wesley 

External links edit 
Yang  W   Tsay  Chey Woei  Chan  Jien Tsai          On the applicability of the longest match rule in lexical analysis   Computer Languages  Systems  amp  Structures                   doi         S                      NSC         E         and NSC         E         
Trim  Craig  Jan             The Art of Tokenization   Developer Works  IBM  Archived from the original on            
Word Mention Segmentation Task  an analysis
vteNatural language processingGeneral terms
AI complete
Bag of words
n gram
Bigram
Trigram
Computational linguistics
Natural language understanding
Stop words
Text processing
Text analysis
Argument mining
Collocation extraction
Concept mining
Coreference resolution
Deep linguistic processing
Distant reading
Information extraction
Named entity recognition
Ontology learning
Parsing
Semantic parsing
Syntactic parsing
Part of speech tagging
Semantic analysis
Semantic role labeling
Semantic decomposition
Semantic similarity
Sentiment analysis
Terminology extraction
Text mining
Textual entailment
Truecasing
Word sense disambiguation
Word sense induction
Text segmentation
Compound term processing
Lemmatisation
Lexical analysis
Text chunking
Stemming
Sentence segmentation
Word segmentation

Automatic summarization
Multi document summarization
Sentence extraction
Text simplification
Machine translation
Computer assisted
Example based
Rule based
Statistical
Transfer based
Neural
Distributional semantics models
BERT
Document term matrix
Explicit semantic analysis
fastText
GloVe
Language model  large 
Latent semantic analysis
Seq seq
Word embedding
Word vec
Language resources datasets and corporaTypes andstandards
Corpus linguistics
Lexical resource
Linguistic Linked Open Data
Machine readable dictionary
Parallel text
PropBank
Semantic network
Simple Knowledge Organization System
Speech corpus
Text corpus
Thesaurus  information retrieval 
Treebank
Universal Dependencies
Data
BabelNet
Bank of English
DBpedia
FrameNet
Google Ngram Viewer
UBY
WordNet
Wikidata
Automatic identificationand data capture
Speech recognition
Speech segmentation
Speech synthesis
Natural language generation
Optical character recognition
Topic model
Document classification
Latent Dirichlet allocation
Pachinko allocation
Computer assistedreviewing
Automated essay scoring
Concordancer
Grammar checker
Predictive text
Pronunciation assessment
Spell checker
Natural languageuser interface
Chatbot
Interactive fiction  cf  Syntax guessing 
Question answering
Virtual assistant
Voice user interface
Related
Formal semantics
Hallucination
Natural Language Toolkit
spaCy






Retrieved from  https   en wikipedia org w index php title Lexical analysis amp oldid