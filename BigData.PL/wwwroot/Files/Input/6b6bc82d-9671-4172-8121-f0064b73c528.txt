Class of artificial neural network
Not to be confused with recursive neural network 
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
Recurrent neural networks  RNNs  are a class of artificial neural networks designed for processing sequential data  such as text  speech  and time series             where the order of elements is important  Unlike feedforward neural networks  which process inputs independently  RNNs utilize recurrent connections  where the output of a neuron at one time step is fed back as input to the network at the next time step  This enables RNNs to capture temporal dependencies and patterns within sequences 
The fundamental building block of RNNs is the recurrent unit  which maintains a hidden state a form of memory that is updated at each time step based on the current input and the previous hidden state  This feedback mechanism allows the network to learn from past inputs and incorporate that knowledge into its current processing  RNNs have been successfully applied to tasks such as unsegmented  connected handwriting recognition             speech recognition                        natural language processing  and neural machine translation                       
However  traditional RNNs suffer from the vanishing gradient problem  which limits their ability to learn long range dependencies  This issue was addressed by the development of the long short term memory  LSTM  architecture in       making it the standard RNN variant for handling long term dependencies  Later  Gated Recurrent Units  GRUs  were introduced as a more computationally efficient alternative 
In recent years  Transformers  which rely on self attention mechanisms instead of recurrence  have become the dominant architecture for many sequence processing tasks  particularly in natural language processing  due to their superior handling of long range dependencies and greater parallelizability  Nevertheless  RNNs remain relevant for applications where computational efficiency  real time processing  or the inherent sequential nature of data is crucial 


History edit 
Before modern edit 
One origin of RNN was neuroscience  The word  recurrent  is used to describe loop like structures in anatomy  In       Cajal observed  recurrent semicircles  in the cerebellar cortex formed by parallel fiber  Purkinje cells  and granule cells                        In       Lorente de N  discovered  recurrent  reciprocal connections  by Golgi s method  and proposed that excitatory loops explain certain aspects of the vestibulo ocular reflex                         During     s  multiple people proposed the existence of feedback in the brain  which was a contrast to the previous understanding of the neural system as a purely feedforward structure  Hebb considered  reverberating circuit  as an explanation for short term memory              The McCulloch and Pitts paper         which proposed the McCulloch Pitts neuron model  considered networks that contains cycles  The current activity of such networks can be affected by activity indefinitely far in the past              They were both interested in closed loops as possible explanations for e g  epilepsy and causalgia                          Recurrent inhibition was proposed in      as a negative feedback mechanism in motor control  Neural feedback loops were a common topic of discussion at the Macy conferences              See              for an extensive review of recurrent neural network models in neuroscience A close loop cross coupled perceptron network                                    Fig            
Frank Rosenblatt in      published  close loop cross coupled perceptrons   which are   layered perceptron networks whose middle layer contains recurrent connections that change by a Hebbian learning rule                                      Later  in Principles of Neurodynamics         he described  closed loop cross coupled  and  back coupled  perceptron networks  and made theoretical and experimental studies for Hebbian learning in these networks                         Chapter               and noted that a fully cross coupled perceptron network is equivalent to an infinitely deep feedforward network                         Section             
Similar networks were published by Kaoru Nakano in                              Shun ichi Amari in                   and William A  Little           de      in                   who was acknowledged by Hopfield in his      paper 
Another origin of RNN was statistical mechanics  The Ising model was developed by Wilhelm Lenz             and Ernst Ising             in the     s             as a simple statistical mechanical model of magnets at equilibrium  Glauber in      studied the Ising model evolving in time  as a process towards equilibrium  Glauber dynamics   adding in the component of time             
The Sherrington Kirkpatrick model of spin glass  published in                   is the Hopfield network with random initialization  Sherrington and Kirkpatrick found that it is highly likely for the energy function of the SK model to have many local minima  In the      paper  Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions              In a      paper he extended this to continuous activation functions              It became a standard model for the study of neural networks through statistical mechanics                         

Modern edit 
Modern RNN networks are mainly based on two architectures  LSTM and BRNN             
At the resurgence of neural networks in the     s  recurrent networks were studied again  They were sometimes called  iterated nets               Two early influential works were the Jordan network        and the Elman network         which applied RNN to study cognitive psychology  In       a neural history compressor system solved a  Very Deep Learning  task that required more than      subsequent layers in an RNN unfolded in time             
Long short term memory  LSTM  networks were invented by Hochreiter and Schmidhuber in      and set accuracy records in multiple applications domains                          It became the default choice for RNN architecture 
Bidirectional recurrent neural networks  BRNN  uses two RNN that processes the same input in opposite directions              These two are often combined  giving the bidirectional LSTM architecture 
Around       bidirectional LSTM started to revolutionize speech recognition  outperforming traditional models in certain speech applications                          They also improved large vocabulary speech recognition                       and text to speech synthesis             and was used in Google voice search  and dictation on Android devices              They broke records for improved machine translation              language modeling             and Multilingual Language Processing              Also  LSTM combined with convolutional neural networks  CNNs  improved automatic image captioning             
The idea of encoder decoder sequence transduction had been developed in the early     s  The papers most commonly cited as the originators that produced seq seq are two papers from                               A seq seq architecture employs two RNN  typically LSTM  an  encoder  and a  decoder   for sequence transduction  such as machine translation  They became state of the art in machine translation  and was instrumental in the development of attention mechanisms and Transformers 

Configurations edit 
Main article  Layer  deep learning An RNN based model can be factored into two parts  configuration and architecture  Multiple RNN can be combined in a data flow  and the data flow itself is the configuration  Each RNN itself may have any architecture  including LSTM  GRU  etc 
Standard edit 
Compressed  left  and unfolded  right  basic recurrent neural network
RNNs come in many variants  Abstractly speaking  an RNN is a function 
  
    
      
        
          f
          
              x b  
          
        
      
    
      displaystyle f   theta   
  
 of type 
  
    
      
         
        
          x
          
            t
          
        
         
        
          h
          
            t
          
        
         
          x  a  
         
        
          y
          
            t
          
        
         
        
          h
          
            t
             
             
          
        
         
      
    
      displaystyle  x  t  h  t   mapsto  y  t  h  t     
  
  where


  
    
      
        
          x
          
            t
          
        
      
    
      displaystyle x  t  
  
  input vector 

  
    
      
        
          h
          
            t
          
        
      
    
      displaystyle h  t  
  
  hidden vector 

  
    
      
        
          y
          
            t
          
        
      
    
      displaystyle y  t  
  
  output vector 

  
    
      
          x b  
      
    
      displaystyle  theta  
  
  neural network parameters 
In words  it is a neural network that maps an input 
  
    
      
        
          x
          
            t
          
        
      
    
      displaystyle x  t  
  
 into an output 
  
    
      
        
          y
          
            t
          
        
      
    
      displaystyle y  t  
  
  with the hidden vector 
  
    
      
        
          h
          
            t
          
        
      
    
      displaystyle h  t  
  
 playing the role of  memory   a partial record of all previous input output pairs  At each step  it transforms input to an output  and modifies its  memory  to help it to better perform future processing 
The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in  layers  and the drawing gives that appearance  However  what appears to be layers are  in fact  different steps in time   unfolded  to produce the appearance of layers 

Stacked RNN edit 
Stacked RNN A stacked RNN  or deep RNN  is composed of multiple RNNs stacked one above the other  Abstractly  it is structured as follows
Layer   has hidden vector 
  
    
      
        
          h
          
             
             
            t
          
        
      
    
      displaystyle h    t  
  
  parameters 
  
    
      
        
            x b  
          
             
          
        
      
    
      displaystyle  theta      
  
  and maps 
  
    
      
        
          f
          
            
                x b  
              
                 
              
            
          
        
         
         
        
          x
          
             
             
            t
          
        
         
        
          h
          
             
             
            t
          
        
         
          x  a  
         
        
          x
          
             
             
            t
          
        
         
        
          h
          
             
             
            t
             
             
          
        
         
      
    
      displaystyle f   theta        x    t  h    t   mapsto  x    t  h    t     
  
 
Layer   has hidden vector 
  
    
      
        
          h
          
             
             
            t
          
        
      
    
      displaystyle h    t  
  
  parameters 
  
    
      
        
            x b  
          
             
          
        
      
    
      displaystyle  theta      
  
  and maps 
  
    
      
        
          f
          
            
                x b  
              
                 
              
            
          
        
         
         
        
          x
          
             
             
            t
          
        
         
        
          h
          
             
             
            t
          
        
         
          x  a  
         
        
          x
          
             
             
            t
          
        
         
        
          h
          
             
             
            t
             
             
          
        
         
      
    
      displaystyle f   theta        x    t  h    t   mapsto  x    t  h    t     
  
 
   
Layer 
  
    
      
        n
      
    
      displaystyle n 
  
 has hidden vector 
  
    
      
        
          h
          
            n
             
            t
          
        
      
    
      displaystyle h  n t  
  
  parameters 
  
    
      
        
            x b  
          
            n
          
        
      
    
      displaystyle  theta   n  
  
  and maps 
  
    
      
        
          f
          
            
                x b  
              
                n
              
            
          
        
         
         
        
          x
          
            n
              x     
             
             
            t
          
        
         
        
          h
          
            n
             
            t
          
        
         
          x  a  
         
        
          x
          
            n
             
            t
          
        
         
        
          h
          
            n
             
            t
             
             
          
        
         
      
    
      displaystyle f   theta   n    x  n   t  h  n t   mapsto  x  n t  h  n t     
  
 
Each layer operates as a stand alone RNN  and each layer s output sequence is used as the input sequence to the layer above  There is no conceptual limit to the depth of stacked RNN 

Bidirectional edit 
Main article  Bidirectional recurrent neural networks
Bidirectional RNN A bidirectional RNN  biRNN  is composed of two RNNs  one processing the input sequence in one direction  and another in the opposite direction  Abstractly  it is structured as follows 
The forward RNN processes in one direction  
  
    
      
        
          f
          
              x b  
          
        
         
        
          x
          
             
          
        
         
        
          h
          
             
          
        
         
         
         
        
          y
          
             
          
        
         
        
          h
          
             
          
        
         
         
        
          f
          
              x b  
          
        
         
        
          x
          
             
          
        
         
        
          h
          
             
          
        
         
         
         
        
          y
          
             
          
        
         
        
          h
          
             
          
        
         
         
          x     
      
    
      displaystyle f   theta   x     h       y     h      f   theta   x     h       y     h       dots  
  

The backward RNN processes in the opposite direction 
  
    
      
        
          f
          
            
                x b  
                x     
            
          
            x     
        
         
        
          x
          
            N
          
        
         
        
          h
          
            N
          
            x     
        
         
         
         
        
          y
          
            N
          
            x     
        
         
        
          h
          
            N
              x     
             
          
            x     
        
         
         
        
          f
          
            
                x b  
                x     
            
          
            x     
        
         
        
          x
          
            N
              x     
             
          
        
         
        
          h
          
            N
              x     
             
          
            x     
        
         
         
         
        
          y
          
            N
              x     
             
          
            x     
        
         
        
          h
          
            N
              x     
             
          
            x     
        
         
         
          x     
      
    
      displaystyle f    theta    x  N  h  N     y   N  h  N      f    theta    x  N    h  N       y   N    h  N       dots  
  

The two output sequences are then concatenated to give the total output  
  
    
      
         
         
        
          y
          
             
          
        
         
        
          y
          
             
          
            x     
        
         
         
         
        
          y
          
             
          
        
         
        
          y
          
             
          
            x     
        
         
         
          x     
         
         
        
          y
          
            N
          
        
         
        
          y
          
            N
          
            x     
        
         
         
      
    
      displaystyle   y     y        y     y        dots   y  N  y  N     
  
 
Bidirectional RNN allows the model to process a token both in the context of what came before it and what came after it  By stacking multiple bidirectional RNNs together  the model can process a token increasingly contextually  The ELMo model                    is a stacked bidirectional LSTM which takes character level as inputs and produces word level embeddings 

Encoder decoder edit 
Main article  seq seq
A decoder without an encoder 
Encoder decoder RNN without attention mechanism 
Encoder decoder RNN with attention mechanism 
Two RNNs can be run front to back in an encoder decoder configuration  The encoder RNN processes an input sequence into a sequence of hidden vectors  and the decoder RNN processes the sequence of hidden vectors to an output sequence  with an optional attention mechanism  This was used to construct state of the art neural machine translators during the           period  This was an instrumental step towards the development of Transformers             

PixelRNN edit 
An RNN may process data with more than one dimension  PixelRNN processes two dimensional data  with many possible directions              For example  the row by row direction processes an 
  
    
      
        n
          xd  
        n
      
    
      displaystyle n times n 
  
 grid of vectors 
  
    
      
        
          x
          
            i
             
            j
          
        
      
    
      displaystyle x  i j  
  
 in the following order  
  
    
      
        
          x
          
             
             
             
          
        
         
        
          x
          
             
             
             
          
        
         
          x     
         
        
          x
          
             
             
            n
          
        
         
        
          x
          
             
             
             
          
        
         
        
          x
          
             
             
             
          
        
         
          x     
         
        
          x
          
             
             
            n
          
        
         
          x     
         
        
          x
          
            n
             
            n
          
        
      
    
      displaystyle x       x        dots  x    n  x       x        dots  x    n   dots  x  n n  
  
The diagonal BiLSTM uses two LSTMs to process the same grid  One processes it from the top left corner to the bottom right  such that it processes 
  
    
      
        
          x
          
            i
             
            j
          
        
      
    
      displaystyle x  i j  
  
 depending on its hidden state and cell state on the top and the left side  
  
    
      
        
          h
          
            i
              x     
             
             
            j
          
        
         
        
          c
          
            i
              x     
             
             
            j
          
        
      
    
      displaystyle h  i   j  c  i   j  
  
 and 
  
    
      
        
          h
          
            i
             
            j
              x     
             
          
        
         
        
          c
          
            i
             
            j
              x     
             
          
        
      
    
      displaystyle h  i j    c  i j    
  
  The other processes it from the top right corner to the bottom left 

Architectures edit 
Fully recurrent edit 
A fully connected RNN with   neurons 
Fully recurrent neural networks  FRNN  connect the outputs of all neurons to the inputs of all neurons  In other words  it is a fully connected network  This is the most general neural network topology  because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons 

A simple Elman network where 
  
    
      
        
            x c  
          
            h
          
        
         
        tanh
         
        
            x c  
          
            y
          
        
         
        
          Identity
        
      
    
      displaystyle  sigma   h   tanh   sigma   y    text Identity   
  
 
Hopfield edit 
Main article  Hopfield network
The Hopfield network is an RNN in which all connections across layers are equally sized  It requires stationary inputs and is thus not a general RNN  as it does not process sequences of patterns  However  it guarantees that it will converge  If the connections are trained using Hebbian learning  then the Hopfield network can perform as robust content addressable memory  resistant to connection alteration 

Elman networks and Jordan networks edit 
The Elman network
An Elman network is a three layer network  arranged horizontally as x  y  and z in the illustration  with the addition of a set of context units  u in the illustration   The middle  hidden  layer is connected to these context units fixed with a weight of one              At each time step  the input is fed forward and a learning rule is applied  The fixed back connections save a copy of the previous values of the hidden units in the context units  since they propagate over the connections before the learning rule is applied   Thus the network can maintain a sort of state  allowing it to perform tasks such as sequence prediction that are beyond the power of a standard multilayer perceptron 
Jordan networks are similar to Elman networks  The context units are fed from the output layer instead of the hidden layer  The context units in a Jordan network are also called the state layer  They have a recurrent connection to themselves             
Elman and Jordan networks are also known as  Simple recurrent networks   SRN  

Elman network            

  
    
      
        
          
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    h
                  
                
                 
                
                  W
                  
                    h
                  
                
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    h
                  
                
                
                  h
                  
                    t
                      x     
                     
                  
                
                 
                
                  b
                  
                    h
                  
                
                 
              
            
            
              
                
                  y
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    y
                  
                
                 
                
                  W
                  
                    y
                  
                
                
                  h
                  
                    t
                  
                
                 
                
                  b
                  
                    y
                  
                
                 
              
            
          
        
      
    
      displaystyle   begin aligned h  t  amp   sigma   h  W  h x  t  U  h h  t    b  h    y  t  amp   sigma   y  W  y h  t  b  y   end aligned   
  

Jordan network            

  
    
      
        
          
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    h
                  
                
                 
                
                  W
                  
                    h
                  
                
                
                  x
                  
                    t
                  
                
                 
                
                  U
                  
                    h
                  
                
                
                  s
                  
                    t
                  
                
                 
                
                  b
                  
                    h
                  
                
                 
              
            
            
              
                
                  y
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    y
                  
                
                 
                
                  W
                  
                    y
                  
                
                
                  h
                  
                    t
                  
                
                 
                
                  b
                  
                    y
                  
                
                 
              
            
            
              
                
                  s
                  
                    t
                  
                
              
              
                
                 
                
                    x c  
                  
                    s
                  
                
                 
                
                  W
                  
                    s
                     
                    s
                  
                
                
                  s
                  
                    t
                      x     
                     
                  
                
                 
                
                  W
                  
                    s
                     
                    y
                  
                
                
                  y
                  
                    t
                      x     
                     
                  
                
                 
                
                  b
                  
                    s
                  
                
                 
              
            
          
        
      
    
      displaystyle   begin aligned h  t  amp   sigma   h  W  h x  t  U  h s  t  b  h    y  t  amp   sigma   y  W  y h  t  b  y    s  t  amp   sigma   s  W  s s s  t    W  s y y  t    b  s   end aligned   
  

Variables and functions


  
    
      
        
          x
          
            t
          
        
      
    
      displaystyle x  t  
  
  input vector

  
    
      
        
          h
          
            t
          
        
      
    
      displaystyle h  t  
  
  hidden layer vector

  
    
      
        
          s
          
            t
          
        
      
    
      displaystyle s  t  
  
   state  vector 

  
    
      
        
          y
          
            t
          
        
      
    
      displaystyle y  t  
  
  output vector

  
    
      
        W
      
    
      displaystyle W 
  
  
  
    
      
        U
      
    
      displaystyle U 
  
 and 
  
    
      
        b
      
    
      displaystyle b 
  
  parameter matrices and vector

  
    
      
          x c  
      
    
      displaystyle  sigma  
  
  Activation functions
Long short term memory edit 
Main article  Long short term memory
Long short term memory unit
Long short term memory  LSTM  is the most widely used RNN architecture  It was designed to solve the vanishing gradient problem  LSTM is normally augmented by recurrent gates called  forget gates               LSTM prevents backpropagated errors from vanishing or exploding              Instead  errors can flow backward through unlimited numbers of virtual layers unfolded in space  That is  LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier  Problem specific LSTM like topologies can be evolved              LSTM works even given long delays between significant events and can handle signals that mix low and high frequency components 
Many applications use stacks of LSTMs              for which it is called  deep LSTM   LSTM can learn to recognize context sensitive languages unlike previous models based on hidden Markov models  HMM  and similar concepts             

Gated recurrent unit edit 
Main article  Gated recurrent unit
Gated recurrent unit
Gated recurrent unit  GRU   introduced in       was designed as a simplification of LSTM  They are used in the full form and several further simplified variants                          They have fewer parameters than LSTM  as they lack an output gate             
Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short term memory              There does not appear to be particular performance difference between LSTM and GRU                         

Bidirectional associative memory edit 
Main article  Bidirectional associative memory
Introduced by Bart Kosko              a bidirectional associative memory  BAM  network is a variant of a Hopfield network that stores associative data as a vector  The bidirectionality comes from passing information through a matrix and its transpose  Typically  bipolar encoding is preferred to binary encoding of the associative pairs  Recently  stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real world applications             
A BAM network has two layers  either of which can be driven as an input to recall an association and produce an output on the other layer             

Echo state edit 
Main article  Echo state network
Echo state networks  ESN  have a sparsely connected random hidden layer  The weights of output neurons are the only part of the network that can change  be trained   ESNs are good at reproducing certain time series              A variant for spiking neurons is known as a liquid state machine             

Recursive edit 
Main article  Recursive neural network
A recursive neural network             is created by applying the same set of weights recursively over a differentiable graph like structure by traversing the structure in topological order  Such networks are typically also trained by the reverse mode of automatic differentiation                          They can process distributed representations of structure  such as logical terms  A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain  Recursive neural networks have been applied to natural language processing              The Recursive Neural Tensor Network uses a tensor based composition function for all nodes in the tree             

Neural Turing machines edit 
Main articles  Neural Turing machine and Differentiable neural computer
Neural Turing machines  NTMs  are a method of extending recurrent neural networks by coupling them to external memory resources with which they interact  The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end to end  allowing it to be efficiently trained with gradient descent             
Differentiable neural computers  DNCs  are an extension of Neural Turing machines  allowing for the usage of fuzzy amounts of each memory address and a record of chronology             
Neural network pushdown automata  NNPDA  are similar to NTMs  but tapes are replaced by analog stacks that are differentiable and trained  In this way  they are similar in complexity to recognizers of context free grammars  CFGs              
Recurrent neural networks are Turing complete and can run arbitrary programs to process arbitrary sequences of inputs             

Training edit 
Teacher forcing edit 
Encoder decoder RNN without attention mechanism  Teacher forcing is shown in red An RNN can be trained into a conditionally generative model of sequences  aka autoregression 
Concretely  let us consider the problem of machine translation  that is  given a sequence 
  
    
      
         
        
          x
          
             
          
        
         
        
          x
          
             
          
        
         
          x     
         
        
          x
          
            n
          
        
         
      
    
      displaystyle  x     x      dots  x  n   
  
 of English words  the model is to produce a sequence 
  
    
      
         
        
          y
          
             
          
        
         
          x     
         
        
          y
          
            m
          
        
         
      
    
      displaystyle  y      dots  y  m   
  
 of French words  It is to be solved by a seq seq model 
Now  during training  the encoder half of the model would first ingest 
  
    
      
         
        
          x
          
             
          
        
         
        
          x
          
             
          
        
         
          x     
         
        
          x
          
            n
          
        
         
      
    
      displaystyle  x     x      dots  x  n   
  
  then the decoder half would start generating a sequence 
  
    
      
         
        
          
            
              
                y
                  x e 
              
            
          
          
             
          
        
         
        
          
            
              
                y
                  x e 
              
            
          
          
             
          
        
         
          x     
         
        
          
            
              
                y
                  x e 
              
            
          
          
            l
          
        
         
      
    
      displaystyle    hat  y         hat  y        dots    hat  y    l   
  
  The problem is that if the model makes a mistake early on  say at 
  
    
      
        
          
            
              
                y
                  x e 
              
            
          
          
             
          
        
      
    
      displaystyle   hat  y       
  
  then subsequent tokens are likely to also be mistakes  This makes it inefficient for the model to obtain a learning signal  since the model would mostly learn to shift 
  
    
      
        
          
            
              
                y
                  x e 
              
            
          
          
             
          
        
      
    
      displaystyle   hat  y       
  
 towards 
  
    
      
        
          y
          
             
          
        
      
    
      displaystyle y     
  
  but not the others 
Teacher forcing makes it so that the decoder uses the correct output sequence for generating the next entry in the sequence  So for example  it would see 
  
    
      
         
        
          y
          
             
          
        
         
          x     
         
        
          y
          
            k
          
        
         
      
    
      displaystyle  y      dots  y  k   
  
 in order to generate 
  
    
      
        
          
            
              
                y
                  x e 
              
            
          
          
            k
             
             
          
        
      
    
      displaystyle   hat  y    k    
  
 

Gradient descent edit 
Main articles  Gradient descent and Vanishing gradient problem
Gradient descent is a first order iterative optimization algorithm for finding the minimum of a function  In neural networks  it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight  provided the non linear activation functions are differentiable 
The standard method for training RNN by gradient descent is the  backpropagation through time   BPTT  algorithm  which is a special case of the general algorithm of backpropagation  A more computationally expensive online variant is called  Real Time Recurrent Learning  or RTRL                          which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors  Unlike BPTT  this algorithm is local in time but not local in space 
In this context  local in space means that a unit s weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector  Local in time means that the updates take place continually  on line  and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT  Biological neural networks appear to be local with respect to both time and space                         
For recursively computing the partial derivatives  RTRL has a time complexity of O number of hidden x number of weights  per time step for computing the Jacobian matrices  while BPTT only takes O number of weights  per time step  at the cost of storing all forward activations within the given time horizon              An online hybrid between BPTT and RTRL with intermediate complexity exists                          along with variants for continuous time             
A major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events                          LSTM combined with a BPTT RTRL hybrid learning method attempts to overcome these problems              This problem is also solved in the independently recurrent neural network  IndRNN              by reducing the context of a neuron to its own past state and the cross neuron information can then be explored in the following layers  Memories of different ranges including long term memory can be learned without the gradient vanishing and exploding problem 
The on line algorithm called causal recursive backpropagation  CRBP   implements and combines BPTT and RTRL paradigms for locally recurrent networks              It works with the most general locally recurrent networks  The CRBP algorithm can minimize the global error term  This fact improves the stability of the algorithm  providing a unifying view of gradient calculation techniques for recurrent networks with local feedback 
One approach to gradient information computation in RNNs with arbitrary architectures is based on signal flow graphs diagrammatic derivation              It uses the BPTT batch algorithm  based on Lee s theorem for network sensitivity calculations              It was proposed by Wan and Beaufays  while its fast online version was proposed by Campolucci  Uncini and Piazza             

Connectionist temporal classification edit 
The connectionist temporal classification  CTC              is a specialized loss function for training RNNs for sequence modeling problems where the timing is variable             

Global optimization methods edit 
Training the weights in a neural network can be modeled as a non linear global optimization problem  A target function can be formed to evaluate the fitness or error of a particular weight vector as follows  First  the weights in the network are set according to the weight vector  Next  the network is evaluated against the training sequence  Typically  the sum squared difference between the predictions and the target values specified in the training sequence is used to represent the error of the current weight vector  Arbitrary global optimization techniques may then be used to minimize this target function 
The most common global optimization method for training RNNs is genetic algorithms  especially in unstructured networks                                     
Initially  the genetic algorithm is encoded with the neural network weights in a predefined manner where one gene in the chromosome represents one weight link  The whole network is represented as a single chromosome  The fitness function is evaluated as follows 

Each weight encoded in the chromosome is assigned to the respective weight link of the network 
The training set is presented to the network which propagates the input signals forward 
The mean squared error is returned to the fitness function 
This function drives the genetic selection process 
Many chromosomes make up the population  therefore  many different neural networks are evolved until a stopping criterion is satisfied  A common stopping scheme is  

When the neural network has learned a certain percentage of the training data or
When the minimum value of the mean squared error is satisfied or
When the maximum number of training generations has been reached 
The fitness function evaluates the stopping criterion as it receives the mean squared error reciprocal from each network during training  Therefore  the goal of the genetic algorithm is to maximize the fitness function  reducing the mean squared error 
Other global  and or evolutionary  optimization techniques may be used to seek a good set of weights  such as simulated annealing or particle swarm optimization 

Other architectures edit 
Independently RNN  IndRNN  edit 
The independently recurrent neural network  IndRNN              addresses the gradient vanishing and exploding problems in the traditional fully connected RNN  Each neuron in one layer only receives its own past state as context information  instead of full connectivity to all other neurons in this layer  and thus neurons are independent of each other s history  The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short term memory  The cross neuron information is explored in the next layers  IndRNN can be robustly trained with non saturated nonlinear functions such as ReLU  Deep networks can be trained using skip connections 

Neural history compressor edit 
The neural history compressor is an unsupervised stack of RNNs              At the input level  it learns to predict its next input from the previous inputs  Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN  which therefore recomputes its internal state only rarely  Each higher level RNN thus studies a compressed representation of the information in the RNN below  This is done such that the input sequence can be precisely reconstructed from the representation at the highest level 
The system effectively minimizes the description length or the negative logarithm of the probability of the data              Given a lot of learnable predictability in the incoming data sequence  the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events 
It is possible to distill the RNN hierarchy into two RNNs  the  conscious  chunker  higher level  and the  subconscious  automatizer  lower level               Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer  then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker  This makes it easy for the automatizer to learn appropriate  rarely changing memories across long intervals  In turn  this helps the automatizer to make many of its once unpredictable inputs predictable  such that the chunker can focus on the remaining unpredictable events             
A generative model partially overcame the vanishing gradient problem             of automatic differentiation or backpropagation in neural networks in       In       such a system solved a  Very Deep Learning  task that required more than      subsequent layers in an RNN unfolded in time             

Second order RNNs edit 
Second order RNNs use higher order weights 
  
    
      
        w
        
          

          
          
            i
            j
            k
          
        
      
    
      displaystyle w    ijk  
  
 instead of the standard 
  
    
      
        w
        
          

          
          
            i
            j
          
        
      
    
      displaystyle w    ij  
  
 weights  and states can be a product  This allows a direct mapping to a finite state machine both in training  stability  and representation                          Long short term memory is an example of this but has no such formal mappings or proof of stability 

Hierarchical recurrent neural network edit 
Hierarchical recurrent neural networks  HRNN  connect their neurons in various ways to decompose hierarchical behavior into useful subprograms                           Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson  whose philosophical views have inspired hierarchical models              
Hierarchical recurrent neural networks are useful in forecasting  helping to predict disaggregated inflation components of the consumer price index  CPI   The HRNN model leverages information from higher levels in the CPI hierarchy to enhance lower level predictions  Evaluation of a substantial dataset from the US CPI U index demonstrates the superior performance of the HRNN model compared to various established inflation prediction methods              

Recurrent multilayer perceptron network edit 
Generally  a recurrent multilayer perceptron network  RMLP network  consists of cascaded subnetworks  each containing multiple layers of nodes  Each subnetwork is feed forward except for the last layer  which can have feedback connections  Each of these subnets is connected only by feed forward connections              

Multiple timescales model edit 
A multiple timescales recurrent neural network  MTRNN  is a neural based computational model that can simulate the functional hierarchy of the brain through self organization depending on the spatial connection between neurons and on distinct types of neuron activities  each with distinct time properties                            With such varied neuronal activities  continuous sequences of any set of behaviors are segmented into reusable primitives  which in turn are flexibly integrated into diverse sequential behaviors  The biological approval of such a type of hierarchy was discussed in the memory prediction theory of brain function by Hawkins in his book On Intelligence      citation needed      Such a hierarchy also agrees with theories of memory posited by philosopher Henri Bergson  which have been incorporated into an MTRNN model                           

Memristive networks edit 
Greg Snider of HP Labs describes a system of cortical computing with memristive nanodevices               The memristors  memory resistors  are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film  DARPA s SyNAPSE project has funded IBM Research and HP Labs  in collaboration with the Boston University Department of Cognitive and Neural Systems  CNS   to develop neuromorphic architectures that may be based on memristive systems 
Memristive networks are a particular type of physical neural network that have very similar properties to  Little  Hopfield networks  as they have continuous dynamics  a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the Ising model  In this sense  the dynamics of a memristive circuit have the advantage compared to a Resistor Capacitor network to have a more interesting non linear behavior  From this point of view  engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology 
The evolution of these networks can be studied analytically using variations of the Caravelli Traversa Di Ventra equation              

Continuous time edit 
A continuous time recurrent neural network  CTRNN  uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs  They are typically analyzed by dynamical systems theory  Many RNN models in neuroscience are continuous time             
For a neuron 
  
    
      
        i
      
    
      displaystyle i 
  
 in the network with activation 
  
    
      
        
          y
          
            i
          
        
      
    
      displaystyle y  i  
  
  the rate of change of activation is given by 


  
    
      
        
            x c  
          
            i
          
        
        
          
            
              
                y
                  x d  
              
            
          
          
            i
          
        
         
          x     
        
          y
          
            i
          
        
         
        
            x     
          
            j
             
             
          
          
            n
          
        
        
          w
          
            j
            i
          
        
          x c  
         
        
          y
          
            j
          
        
          x     
        
            x    
          
            j
          
        
         
         
        
          I
          
            i
          
        
         
        t
         
      
    
      displaystyle  tau   i   dot  y    i   y  i   sum   j     n w  ji  sigma  y  j   Theta   j   I  i  t  
  

Where 


  
    
      
        
            x c  
          
            i
          
        
      
    
      displaystyle  tau   i  
  
        Time constant of postsynaptic node

  
    
      
        
          y
          
            i
          
        
      
    
      displaystyle y  i  
  
        Activation of postsynaptic node

  
    
      
        
          
            
              
                y
                  x d  
              
            
          
          
            i
          
        
      
    
      displaystyle   dot  y    i  
  
        Rate of change of activation of postsynaptic node

  
    
      
        w
        
          

          
          
            j
            i
          
        
      
    
      displaystyle w    ji  
  
        Weight of connection from pre to postsynaptic node

  
    
      
          x c  
         
        x
         
      
    
      displaystyle  sigma  x  
  
        Sigmoid of x e g  
  
    
      
          x c  
         
        x
         
         
         
        
           
        
         
         
         
        
          e
          
              x     
            x
          
        
         
      
    
      displaystyle  sigma  x       e   x   
  
 

  
    
      
        
          y
          
            j
          
        
      
    
      displaystyle y  j  
  
        Activation of presynaptic node

  
    
      
        
            x    
          
            j
          
        
      
    
      displaystyle  Theta   j  
  
        Bias of presynaptic node

  
    
      
        
          I
          
            i
          
        
         
        t
         
      
    
      displaystyle I  i  t  
  
        Input  if any  to node
CTRNNs have been applied to evolutionary robotics where they have been used to address vision               co operation               and minimal cognitive behaviour              
Note that  by the Shannon sampling theorem  discrete time recurrent neural networks can be viewed as continuous time recurrent neural networks where the differential equations have transformed into equivalent difference equations               This transformation can be thought of as occurring after the post synaptic node activation functions 
  
    
      
        
          y
          
            i
          
        
         
        t
         
      
    
      displaystyle y  i  t  
  
 have been low pass filtered but prior to sampling 
They are in fact recursive neural networks with a particular structure  that of a linear chain  Whereas recursive neural networks operate on any hierarchical structure  combining child representations into parent representations  recurrent neural networks operate on the linear progression of time  combining the previous time step and a hidden representation into the representation for the current time step 
From a time series perspective  RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model  NARX                RNN has infinite impulse response whereas convolutional neural networks have finite impulse response  Both classes of networks exhibit temporal dynamic behavior               A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network  while an infinite impulse recurrent network is a directed cyclic graph that cannot be unrolled 
The effect of memory based learning for the recognition of sequences can also be implemented by a more biological based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity              
Additional stored states and the storage under direct control by the network can be added to both infinite impulse and finite impulse networks  Another network or graph can also replace the storage if that incorporates time delays or has feedback loops  Such controlled states are referred to as gated states or gated memory and are part of long short term memory networks  LSTMs  and gated recurrent units  This is also called Feedback Neural Network  FNN  

Libraries edit 
Modern libraries provide runtime optimized implementations of the above functionality or allow to speed up the slow loop by just in time compilation 

Apache Singa
Caffe  Created by the Berkeley Vision and Learning Center  BVLC   It supports both CPU and GPU  Developed in C    and has Python and MATLAB wrappers 
Chainer  Fully in Python  production support for CPU  GPU  distributed training 
Deeplearning j  Deep learning in Java and Scala on multi GPU enabled Spark 
Flux  includes interfaces for RNNs  including GRUs and LSTMs  written in Julia 
Keras  High level API  providing a wrapper to many other deep learning libraries 
Microsoft Cognitive Toolkit
MXNet  an open source deep learning framework used to train and deploy deep neural networks 
PyTorch  Tensors and Dynamic neural networks in Python with GPU acceleration 
TensorFlow  Apache     licensed Theano like library with support for CPU  GPU and Google s proprietary TPU               mobile
Theano  A deep learning library for Python with an API largely compatible with the NumPy library 
Torch  A scientific computing framework with support for machine learning algorithms  written in C and Lua 
Applications edit 
Applications of recurrent neural networks include 

Machine translation            
Robot control             
Time series prediction                                       
Speech recognition                                      
Speech synthesis             
Brain computer interfaces             
Time series anomaly detection             
Text to Video model             
Rhythm learning             
Music composition             
Grammar learning                                      
Handwriting recognition                          
Human action recognition             
Protein homology detection             
Predicting subcellular localization of proteins             
Several prediction tasks in the area of business process management             
Prediction in medical care pathways             
Predictions of fusion plasma disruptions in reactors  Fusion Recurrent Neural Network  FRNN  code               
References edit 


  Tealab  Ahmed                Time series forecasting using artificial neural networks methodologies  A systematic review   Future Computing and Informatics Journal                  doi         j fcij              ISSN                

  Graves  Alex  Liwicki  Marcus  Fernandez  Santiago  Bertolami  Roman  Bunke  Horst  Schmidhuber  J rgen          A Novel Connectionist System for Improved Unconstrained Handwriting Recognition   PDF   IEEE Transactions on Pattern Analysis and Machine Intelligence                   CiteSeerX                       doi         tpami           PMID                S CID               

  a b Sak  Ha im  Senior  Andrew  Beaufays  Fran oise          Long Short Term Memory recurrent neural network architectures for large scale acoustic modeling   PDF   Google Research 

  a b Li  Xiangang  Wu  Xihong                Constructing Long Short Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition   arXiv            cs CL  

  Dupond  Samuel          A thorough review on the current advance of neural network structures   Annual Reviews in Control              

  Abiodun  Oludare Isaac  Jantan  Aman  Omolara  Abiodun Esther  Dada  Kemi Victoria  Mohamed  Nachaat Abdelatif  Arshad  Humaira                State of the art in artificial neural network applications  A survey   Heliyon          e       Bibcode     Heliy         A  doi         j heliyon      e       ISSN                 PMC               PMID               

  Espinosa Sanchez  Juan Manuel  Gomez Marin  Alex  de Castro  Fernando                The Importance of Cajal s and Lorente de N  s Neuroscience to the Birth of Cybernetics   The Neuroscientist                 doi                            hdl               ISSN                 PMID               

  Ram n y Cajal  Santiago         Histologie du syst me nerveux de l homme  amp  des vert br s  Vol       II  Foyle Special Collections Library King s College London  Paris        A  Maloine  p           

  de N   R  Lorente                Vestibulo Ocular Reflex Arc   Archives of Neurology and Psychiatry               doi         archneurpsyc                      ISSN                

  Larriva Sahd  Jorge A                 Some predictions of Rafael Lorente de N     years later   Frontiers in Neuroanatomy          doi         fnana             ISSN                 PMC               PMID               

   reverberating circuit   Oxford Reference  Retrieved            

  McCulloch  Warren S   Pitts  Walter  December         A logical calculus of the ideas immanent in nervous activity   The Bulletin of Mathematical Biophysics                  doi         BF          ISSN                

  Moreno D az  Roberto  Moreno D az  Arminda  April         On the legacy of W S  McCulloch   Biosystems                   Bibcode     BiSys         M  doi         j biosystems              PMID               

  Arbib  Michael A  December         Warren McCulloch s Search for the Logic of the Nervous System   Perspectives in Biology and Medicine                   doi         pbm            ISSN                 PMID               

  Renshaw  Birdsey                Central Effects of Centripetal Impulses in Axons of Spinal Ventral Roots   Journal of Neurophysiology                  doi         jn               ISSN                 PMID               

  a b Grossberg  Stephen                Recurrent Neural Networks   Scholarpedia               Bibcode     SchpJ         G  doi         scholarpedia       ISSN                

  a b c Rosenblatt  Frank               DTIC AD         PRINCIPLES OF NEURODYNAMICS  PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS  Defense Technical Information Center 

  F  Rosenblatt   Perceptual Generalization over Transformation Groups   pp          in Self organizing Systems  Proceedings of an Inter disciplinary Conference    and   May       Edited by Marshall C  Yovitz and Scott Cameron  London  New York   etc    Pergamon Press        ix      p 

  Nakano  Kaoru          Learning Process in a Model of Associative Memory   Pattern Recognition and Machine Learning  pp                doi                               ISBN                        

  Nakano  Kaoru          Associatron A Model of Associative Memory   IEEE Transactions on Systems  Man  and Cybernetics  SMC                 doi         TSMC              

  Amari  Shun Ichi          Learning patterns and pattern sequences by self organizing nets of threshold elements   IEEE Transactions  C                 

  Little  W  A           The Existence of Persistent States in the Brain   Mathematical Biosciences                     doi                              

  Lenz  W           Beitr ge zum Verst ndnis der magnetischen Eigenschaften in festen K rpern   Physikalische Zeitschrift              

  Ising  E           Beitrag zur Theorie des Ferromagnetismus   Z  Phys                    Bibcode     ZPhy          I  doi         BF          S CID               

  Brush  Stephen G           History of the Lenz Ising Model   Reviews of Modern Physics                   Bibcode     RvMP          B  doi         RevModPhys        

  Glauber  Roy J   February         Roy J  Glauber  Time Dependent Statistics of the Ising Model    Journal of Mathematical Physics                  doi                    Retrieved            

  Sherrington  David  Kirkpatrick  Scott                Solvable Model of a Spin Glass   Physical Review Letters                      Bibcode     PhRvL         S  doi         PhysRevLett          ISSN                

  Hopfield  J  J           Neural networks and physical systems with emergent collective computational abilities   Proceedings of the National Academy of Sciences                     Bibcode     PNAS          H  doi         pnas            PMC              PMID              

  Hopfield  J  J           Neurons with graded response have collective computational properties like those of two state neurons   Proceedings of the National Academy of Sciences                      Bibcode     PNAS          H  doi         pnas             PMC              PMID              

  Engel  A   Broeck  C  van den         Statistical mechanics of learning  Cambridge  UK  New York  NY  Cambridge University Press  ISBN                        

  Seung  H  S   Sompolinsky  H   Tishby  N                 Statistical mechanics of learning from examples   Physical Review A                     Bibcode     PhRvA         S  doi         PhysRevA          PMID              

  Zhang  Aston  Lipton  Zachary  Li  Mu  Smola  Alexander J               Modern Recurrent Neural Networks   Dive into deep learning  Cambridge New York Port Melbourne New Delhi Singapore  Cambridge University Press  ISBN                        

  Rumelhart  David E   Hinton  Geoffrey E   Williams  Ronald J   October         Learning representations by back propagating errors   Nature                       Bibcode     Natur         R  doi               a   ISSN                

  a b Schmidhuber  J rgen         Habilitation thesis  System modeling and optimization  PDF       permanent dead link      Page     ff demonstrates credit assignment across the equivalent of       layers in an unfolded RNN 

  Sepp Hochreiter  J rgen Schmidhuber     August        Long Short Term Memory  Wikidata      Q        

  a b Hochreiter  Sepp  Schmidhuber  J rgen                Long Short Term Memory   Neural Computation                    doi         neco                PMID               S CID              

  Schuster  Mike  and Kuldip K  Paliwal   Bidirectional recurrent neural networks   Signal Processing  IEEE Transactions on                            Awni Hannun  Carl Case  Jared Casper  Bryan Catanzaro  Greg Diamos  Erich Elsen  Ryan

  Graves  Alex  Schmidhuber  J rgen                Framewise phoneme classification with bidirectional LSTM and other neural network architectures   Neural Networks  IJCNN                        CiteSeerX                       doi         j neunet              PMID                S CID              

  a b Fern ndez  Santiago  Graves  Alex  Schmidhuber  J rgen          An Application of Recurrent Neural Networks to Discriminative Keyword Spotting   Proceedings of the   th International Conference on Artificial Neural Networks  ICANN     Berlin  Heidelberg  Springer Verlag  pp                ISBN                        

  Fan  Bo  Wang  Lijuan  Soong  Frank K   Xie  Lei          Photo Real Talking Head with Deep Bidirectional LSTM   Proceedings of ICASSP      IEEE International Conference on Acoustics  Speech and Signal Processing  pp               doi         ICASSP               ISBN                        

  Sak  Ha im  Senior  Andrew  Rao  Kanishka  Beaufays  Fran oise  Schalkwyk  Johan  September         Google voice search  faster and more accurate  

  a b Sutskever  Ilya  Vinyals  Oriol  Le  Quoc V           Sequence to Sequence Learning with Neural Networks   PDF   Electronic Proceedings of the Neural Information Processing Systems Conference            arXiv            Bibcode     arXiv         S 

  Jozefowicz  Rafal  Vinyals  Oriol  Schuster  Mike  Shazeer  Noam  Wu  Yonghui                Exploring the Limits of Language Modeling   arXiv             cs CL  

  Gillick  Dan  Brunk  Cliff  Vinyals  Oriol  Subramanya  Amarnag                Multilingual Language Processing From Bytes   arXiv             cs CL  

  Vinyals  Oriol  Toshev  Alexander  Bengio  Samy  Erhan  Dumitru                Show and Tell  A Neural Image Caption Generator   arXiv            cs CV  

  Cho  Kyunghyun  van Merrienboer  Bart  Gulcehre  Caglar  Bahdanau  Dzmitry  Bougares  Fethi  Schwenk  Holger  Bengio  Yoshua                Learning Phrase Representations using RNN Encoder Decoder for Statistical Machine Translation   arXiv            cs CL  

  Sutskever  Ilya  Vinyals  Oriol  Le  Quoc Viet     Dec         Sequence to sequence learning with neural networks   arXiv            cs CL    first version posted to arXiv on    Sep      

  Peters ME  Neumann M  Iyyer M  Gardner M  Clark C  Lee K  Zettlemoyer L          Deep contextualized word representations   arXiv             cs CL  

  Vaswani  Ashish  Shazeer  Noam  Parmar  Niki  Uszkoreit  Jakob  Jones  Llion  Gomez  Aidan N  Kaiser    ukasz  Polosukhin  Illia          Attention is All you Need   Advances in Neural Information Processing Systems      Curran Associates  Inc 

  Oord  A ron van den  Kalchbrenner  Nal  Kavukcuoglu  Koray                Pixel Recurrent Neural Networks   Proceedings of the   rd International Conference on Machine Learning  PMLR            

  a b Cruse  Holk  Neural Networks as Cybernetic Systems   nd and revised edition

  Elman  Jeffrey L           Finding Structure in Time   Cognitive Science                   doi                            E 

  Jordan  Michael I                 Serial Order  A Parallel Distributed Processing Approach   Neural Network Models of Cognition   Biobehavioral Foundations  Advances in Psychology  Vol            pp                doi         s                      ISBN                         S CID               

  Gers  Felix A   Schraudolph  Nicol N   Schmidhuber  J rgen          Learning Precise Timing with LSTM Recurrent Networks   PDF   Journal of Machine Learning Research              Retrieved            

  a b c Hochreiter  Sepp         Untersuchungen zu dynamischen neuronalen Netzen  PDF   Diploma   Institut f  Informatik  Technische University Munich 

  Bayer  Justin  Wierstra  Daan  Togelius  Julian  Schmidhuber  J rgen                Evolving Memory Cell Structures for Sequence Learning   Artificial Neural Networks   ICANN       PDF   Lecture Notes in Computer Science  Vol             Berlin  Heidelberg  Springer  pp                doi                               ISBN                        

  Fern ndez  Santiago  Graves  Alex  Schmidhuber  J rgen          Sequence labelling in structured domains with hierarchical recurrent neural networks   PDF   Proceedings of the   th International Joint Conference on Artificial Intelligence  Ijcai       pp              CiteSeerX                     

  a b Gers  Felix A   Schmidhuber  J rgen          LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages   PDF   IEEE Transactions on Neural Networks                   doi                    PMID                S CID                Archived from the original  PDF  on             Retrieved            

  Heck  Joel  Salem  Fathi M                 Simplified Minimal Gated Unit Variations for Recurrent Neural Networks   arXiv             cs NE  

  Dey  Rahul  Salem  Fathi M                 Gate Variants of Gated Recurrent Unit  GRU  Neural Networks   arXiv             cs NE  

  Britz  Denny  October             Recurrent Neural Network Tutorial  Part     Implementing a GRU LSTM RNN with Python and Theano   WildML   Wildml com  Retrieved May          

  a b Chung  Junyoung  Gulcehre  Caglar  Cho  KyungHyun  Bengio  Yoshua          Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling   arXiv            cs NE  

  Gruber  N   Jockisch  A           Are GRU cells more specific and LSTM cells more sensitive in motive classification of text    Frontiers in Artificial Intelligence         doi         frai             PMC               PMID                S CID               

  Kosko  Bart          Bidirectional associative memories   IEEE Transactions on Systems  Man  and Cybernetics                 doi                   S CID               

  Rakkiyappan  Rajan  Chandrasekar  Arunachalam  Lakshmanan  Subramanian  Park  Ju H     January         Exponential stability for markovian jumping stochastic BAM neural networks with mode dependent probabilistic time varying delays and impulse control   Complexity                 Bibcode     Cmplx    c    R  doi         cplx       

  Rojas  R ul         Neural networks  a systematic introduction  Springer  p            ISBN                        

  Jaeger  Herbert  Haas  Harald                Harnessing Nonlinearity  Predicting Chaotic Systems and Saving Energy in Wireless Communication   Science                     Bibcode     Sci           J  CiteSeerX                       doi         science          PMID                S CID              

  Maass  Wolfgang  Natschl ger  Thomas  Markram  Henry          Real time computing without stable states  a new framework for neural computation based on perturbations   PDF   Neural Computation                      doi                             PMID                S CID              

  Goller  Christoph  K chler  Andreas          Learning task dependent distributed representations by backpropagation through structure   Proceedings of International Conference on Neural Networks  ICNN      Vol          p            CiteSeerX                      doi         ICNN              ISBN                         S CID              

  Linnainmaa  Seppo         The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors  MSc   in Finnish   University of Helsinki 

  Griewank  Andreas  Walther  Andrea         Evaluating Derivatives  Principles and Techniques of Algorithmic Differentiation  Second      ed    SIAM  ISBN                        

  Socher  Richard  Lin  Cliff  Ng  Andrew Y   Manning  Christopher D    Parsing Natural Scenes and Natural Language with Recursive Neural Networks   PDF     th International Conference on Machine Learning  ICML      

  Socher  Richard  Perelygin  Alex  Wu  Jean Y   Chuang  Jason  Manning  Christopher D   Ng  Andrew Y   Potts  Christopher   Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank   PDF   Emnlp      

  Graves  Alex  Wayne  Greg  Danihelka  Ivo          Neural Turing Machines   arXiv            cs NE  

  Graves  Alex  Wayne  Greg  Reynolds  Malcolm  Harley  Tim  Danihelka  Ivo  Grabska Barwi ska  Agnieszka  Colmenarejo  Sergio G mez  Grefenstette  Edward  Ramalho  Tiago                Hybrid computing using a neural network with dynamic external memory   Nature                       Bibcode     Natur         G  doi         nature       ISSN                 PMID                S CID                

  Sun  Guo Zheng  Giles  C  Lee  Chen  Hsing Hen          The Neural Network Pushdown Automaton  Architecture  Dynamics and Training   In Giles  C  Lee  Gori  Marco  eds    Adaptive Processing of Sequences and Data Structures  Lecture Notes in Computer Science  Berlin  Heidelberg  Springer  pp                CiteSeerX                      doi         bfb         ISBN                        

  Hy tyniemi  Heikki          Turing machines are recurrent neural networks   Proceedings of STeP     Publications of the Finnish Artificial Intelligence Society        

  Robinson  Anthony J   Fallside  Frank         The Utility Driven Dynamic Error Propagation Network  Technical Report CUED F INFENG TR    Department of Engineering  University of Cambridge 

  Williams  Ronald J   Zipser  D     February         Gradient based learning algorithms for recurrent networks and their computational complexity   In Chauvin  Yves  Rumelhart  David E   eds    Backpropagation  Theory  Architectures  and Applications  Psychology Press  ISBN                        

  Schmidhuber  J rgen                A Local Learning Algorithm for Dynamic Feedforward and Recurrent Networks   Connection Science                  doi                            S CID               

  Pr ncipe  Jos  C   Euliano  Neil R   Lefebvre  W  Curt         Neural and adaptive systems  fundamentals through simulations  Wiley  ISBN                        

  Yann  Ollivier  Tallec  Corentin  Charpiat  Guillaume                Training recurrent networks online without backtracking   arXiv             cs NE  

  Schmidhuber  J rgen                A Fixed Size Storage O n   Time Complexity Learning Algorithm for Fully Recurrent Continually Running Networks   Neural Computation                  doi         neco               S CID               

  Williams  Ronald J          Complexity of exact gradient computation algorithms for recurrent neural networks  Report   Technical Report NU CCS        Boston  MA   Northeastern University  College of Computer Science  Archived from the original on             Retrieved            

  Pearlmutter  Barak A                 Learning State Space Trajectories in Recurrent Neural Networks   Neural Computation                  doi         neco               S CID               

  Hochreiter  Sepp  et      al      January         Gradient flow in recurrent nets  the difficulty of learning long term dependencies   In Kolen  John F   Kremer  Stefan C   eds    A Field Guide to Dynamical Recurrent Networks  John Wiley  amp  Sons  ISBN                        

  a b Li  Shuai  Li  Wanqing  Cook  Chris  Zhu  Ce  Yanbo  Gao          Independently Recurrent Neural Network  IndRNN   Building a Longer and Deeper RNN   arXiv             cs CV  

  Campolucci  Paolo  Uncini  Aurelio  Piazza  Francesco  Rao  Bhaskar D           On Line Learning Algorithms for Locally Recurrent Neural Networks   IEEE Transactions on Neural Networks                   CiteSeerX                      doi                    PMID               

  Wan  Eric A   Beaufays  Fran oise          Diagrammatic derivation of gradient algorithms for neural networks   Neural Computation              doi         neco               S CID               

  a b Campolucci  Paolo  Uncini  Aurelio  Piazza  Francesco          A Signal Flow Graph Approach to On line Gradient Calculation   Neural Computation                     CiteSeerX                       doi                             PMID                S CID               

  Graves  Alex  Fern ndez  Santiago  Gomez  Faustino J           Connectionist temporal classification  Labelling unsegmented sequence data with recurrent neural networks   PDF   Proceedings of the International Conference on Machine Learning  pp                CiteSeerX                      doi                          ISBN                    

  Hannun  Awni                Sequence Modeling with CTC   Distill          e   doi          distill        ISSN                

  Gomez  Faustino J   Miikkulainen  Risto          Solving non Markovian control tasks with neuroevolution   PDF   IJCAI     Morgan Kaufmann  retrieved   August     

  Syed  Omar  May        Applying Genetic Algorithms to Recurrent Neural Networks for Learning Network Parameters and Architecture  MSc   Department of Electrical Engineering  Case Western Reserve University 

  Gomez  Faustino J   Schmidhuber  J rgen  Miikkulainen  Risto  June         Accelerated Neural Evolution Through Cooperatively Coevolved Synapses   PDF   Journal of Machine Learning Research             

  a b c d Schmidhuber  J rgen          Learning complex  extended sequences using the principle of history compression   PDF   Neural Computation                  doi         neco               S CID                    permanent dead link     

  Schmidhuber  J rgen          Deep Learning   Scholarpedia                  Bibcode     SchpJ         S  doi         scholarpedia       

  Giles  C  Lee  Miller  Clifford B   Chen  Dong  Chen  Hsing Hen  Sun  Guo Zheng  Lee  Yee Chun          Learning and Extracting Finite State Automata with Second Order Recurrent Neural Networks   PDF   Neural Computation                  doi         neco               S CID               

  Omlin  Christian W   Giles  C  Lee          Constructing Deterministic Finite State Automata in Recurrent Neural Networks   Journal of the ACM                   CiteSeerX                      doi                        S CID             

  Paine  Rainer W   Tani  Jun                How Hierarchical Control Self organizes in Artificial Adaptive Systems   Adaptive Behavior                   doi                             S CID              

  a b  Burns  Benureau  Tani        A Bergson Inspired Adaptive Time Constant for the Multiple Timescales Recurrent Neural Network Model  JNNS  

  Barkan  Oren  Benchimol  Jonathan  Caspi  Itamar  Cohen  Eliya  Hammer  Allon  Koenigstein  Noam          Forecasting CPI inflation components with Hierarchical Recurrent Neural Networks   International Journal of Forecasting                     arXiv             doi         j ijforecast             

  Tutschku  Kurt  June        Recurrent Multilayer Perceptrons for Identification and Control  The Road to Applications  Institute of Computer Science Research Report  Vol            University of W rzburg Am Hubland  CiteSeerX                     

  Yamashita  Yuichi  Tani  Jun                Emergence of Functional Hierarchy in a Multiple Timescale Neural Network Model  A Humanoid Robot Experiment   PLOS Computational Biology          e         Bibcode     PLSCB    E    Y  doi         journal pcbi          PMC               PMID               

  Alnajjar  Fady  Yamashita  Yuichi  Tani  Jun          The hierarchical and functional connectivity of higher order cognitive mechanisms  neurorobotic model to investigate the stability and flexibility of working memory   Frontiers in Neurorobotics        doi         fnbot             PMC               PMID               

   Proceedings of the   th Annual Conference of the Japanese Neural Network Society  October          PDF  

  Snider  Greg          Cortical computing with memristive nanodevices   Sci DAC Review             archived from the original on             retrieved           

  Caravelli  Francesco  Traversa  Fabio Lorenzo  Di Ventra  Massimiliano          The complex dynamics of memristive circuits  analytical results and universal slow relaxation   Physical Review E                  arXiv             Bibcode     PhRvE    b    C  doi         PhysRevE            PMID                S CID              

  Harvey  Inman  Husbands  Phil  Cliff  Dave          Seeing the light  Artificial evolution  real vision    rd international conference on Simulation of adaptive behavior  from animals to animats    pp              

  Quinn  Matt          Evolving communication without dedicated communication channels   Advances in Artificial Life   th European Conference  ECAL       pp                doi                     X     ISBN                        

  Beer  Randall D           The dynamics of adaptive behavior  A research program   Robotics and Autonomous Systems                     doi         S                     

  Sherstinsky  Alex               Bloem Reddy  Benjamin  Paige  Brooks  Kusner  Matt  Caruana  Rich  Rainforth  Tom  Teh  Yee Whye  eds    Deriving the Recurrent Neural Network Definition and RNN Unrolling Using Signal Processing  Critiquing and Correcting Trends in Machine Learning Workshop at NeurIPS      

  Siegelmann  Hava T   Horne  Bill G   Giles  C  Lee          Computational Capabilities of Recurrent NARX Neural Networks   IEEE Transactions on Systems  Man  and Cybernetics   Part B  Cybernetics                  CiteSeerX                      doi                      PMID               

  Miljanovic  Milos  Feb Mar         Comparative analysis of Recurrent and Finite Impulse Response Neural Networks in Time Series Prediction   PDF   Indian Journal of Computer and Engineering        

  Hodassman  Shiri  Meir  Yuval  Kisos  Karin  Ben Noam  Itamar  Tugendhaft  Yael  Goldental  Amir  Vardi  Roni  Kanter  Ido                Brain inspired neuronal silencing mechanism to enable reliable sequence identification   Scientific Reports                 arXiv             Bibcode     NatSR         H  doi         s                x  ISSN                 PMC               PMID               

  Metz  Cade  May             Google Built Its Very Own Chips to Power Its AI Bots   Wired 

  Mayer  Hermann  Gomez  Faustino J   Wierstra  Daan  Nagy  Istvan  Knoll  Alois  Schmidhuber  J rgen  October         A System for Robotic Heart Surgery that Learns to Tie Knots Using Recurrent Neural Networks        IEEE RSJ International Conference on Intelligent Robots and Systems  pp                CiteSeerX                       doi         IROS              ISBN                         S CID               

  Wierstra  Daan  Schmidhuber  J rgen  Gomez  Faustino J           Evolino  Hybrid Neuroevolution Optimal Linear Search for Sequence Learning   Proceedings of the   th International Joint Conference on Artificial Intelligence  IJCAI   Edinburgh  pp              OCLC               

  Petneh zi  G bor                Recurrent neural networks for time series forecasting   arXiv             cs LG  

  Hewamalage  Hansika  Bergmeir  Christoph  Bandara  Kasun          Recurrent Neural Networks for Time Series Forecasting  Current Status and Future Directions   International Journal of Forecasting               arXiv             doi         j ijforecast              S CID                

  Graves  Alex  Schmidhuber  J rgen          Framewise phoneme classification with bidirectional LSTM and other neural network architectures   Neural Networks                     CiteSeerX                       doi         j neunet              PMID                S CID              

  Graves  Alex  Mohamed  Abdel rahman  Hinton  Geoffrey E           Speech recognition with deep recurrent neural networks        IEEE International Conference on Acoustics  Speech and Signal Processing  pp               arXiv            Bibcode     arXiv         G  doi         ICASSP               ISBN                         S CID                

  Chang  Edward F   Chartier  Josh  Anumanchipalli  Gopala K      April         Speech synthesis from neural decoding of spoken sentences   Nature                     Bibcode     Natur         A  doi         s                  ISSN                 PMC               PMID                S CID                

  Moses  David A   Metzger  Sean L   Liu  Jessie R   Anumanchipalli  Gopala K   Makin  Joseph G   Sun  Pengfei F   Chartier  Josh  Dougherty  Maximilian E   Liu  Patricia M   Abrams  Gary M   Tu Chan  Adelyn  Ganguly  Karunesh  Chang  Edward F                 Neuroprosthesis for Decoding Speech in a Paralyzed Person with Anarthria   New England Journal of Medicine                    doi         NEJMoa         PMC               PMID               

  Malhotra  Pankaj  Vig  Lovekesh  Shroff  Gautam  Agarwal  Puneet  April         Long Short Term Memory Networks for Anomaly Detection in Time Series   European Symposium on Artificial Neural Networks  Computational Intelligence and Machine Learning   ESANN       Ciaco  pp              ISBN                        

   Papers with Code   DeepHS HDRVideo  Deep High Speed High Dynamic Range Video Reconstruction   paperswithcode com  Retrieved            

  Gers  Felix A   Schraudolph  Nicol N   Schmidhuber  J rgen          Learning precise timing with LSTM recurrent networks   PDF   Journal of Machine Learning Research             

  Eck  Douglas  Schmidhuber  J rgen                Learning the Long Term Structure of the Blues   Artificial Neural Networks   ICANN       Lecture Notes in Computer Science  Vol             Berlin  Heidelberg  Springer  pp                CiteSeerX                       doi                           ISBN                        

  Schmidhuber  J rgen  Gers  Felix A   Eck  Douglas          Learning nonregular languages  A comparison of simple recurrent networks and LSTM   Neural Computation                     CiteSeerX                      doi                             PMID                S CID               

  P rez Ortiz  Juan Antonio  Gers  Felix A   Eck  Douglas  Schmidhuber  J rgen          Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets   Neural Networks                   CiteSeerX                       doi         s                      PMID               

  Graves  Alex  Schmidhuber  J rgen          Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks   PDF   Advances in Neural Information Processing Systems  Vol           NIPS     MIT Press  pp               

  Graves  Alex  Fern ndez  Santiago  Liwicki  Marcus  Bunke  Horst  Schmidhuber  J rgen          Unconstrained Online Handwriting Recognition with Recurrent Neural Networks   Proceedings of the   th International Conference on Neural Information Processing Systems  Curran Associates  pp                ISBN                        

  Baccouche  Moez  Mamalet  Franck  Wolf  Christian  Garcia  Christophe  Baskurt  Atilla          Sequential Deep Learning for Human Action Recognition   In Salah  Albert Ali  Lepri  Bruno  eds    Human Behavior Unterstanding  Lecture Notes in Computer Science  Vol             Amsterdam  Netherlands  Springer  pp              doi                              ISBN                        

  Hochreiter  Sepp  Heusel  Martin  Obermayer  Klaus          Fast model based protein homology detection without alignment   Bioinformatics                      doi         bioinformatics btm     PMID               

  Thireou  Trias  Reczko  Martin  July         Bidirectional Long Short Term Memory Networks for Predicting the Subcellular Localization of Eukaryotic Proteins   IEEE ACM Transactions on Computational Biology and Bioinformatics                  doi         tcbb            PMID                S CID               

  Tax  Niek  Verenich  Ilya  La Rosa  Marcello  Dumas  Marlon          Predictive Business Process Monitoring with LSTM Neural Networks   Advanced Information Systems Engineering  Lecture Notes in Computer Science  Vol              pp                arXiv             doi                               ISBN                         S CID              

  Choi  Edward  Bahadori  Mohammad Taha  Schuetz  Andy  Stewart  Walter F   Sun  Jimeng          Doctor AI  Predicting Clinical Events via Recurrent Neural Networks   JMLR Workshop and Conference Proceedings               arXiv             Bibcode     arXiv         C  PMC               PMID               

   Artificial intelligence helps accelerate progress toward efficient fusion reactions   Princeton University  Retrieved            


Further reading edit 
Mandic  Danilo P   Chambers  Jonathon A          Recurrent Neural Networks for Prediction  Learning Algorithms  Architectures and Stability  Wiley  ISBN                        
Grossberg  Stephen                Recurrent Neural Networks   Scholarpedia               Bibcode     SchpJ         G  doi         scholarpedia       ISSN                
Recurrent Neural Networks  List of RNN papers by J rgen Schmidhuber s group at Dalle Molle Institute for Artificial Intelligence Research 
vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects

Authority control databases  National Germany





Retrieved from  https   en wikipedia org w index php title Recurrent neural network amp oldid