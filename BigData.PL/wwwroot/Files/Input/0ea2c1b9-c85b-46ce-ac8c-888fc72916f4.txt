Scientific study of digital information
Not to be confused with Information science 
Information theory
Entropy
Differential entropy
Conditional entropy
Joint entropy
Mutual information
Directed information
Conditional mutual information
Relative entropy
Entropy rate
Limiting density of discrete points

Asymptotic equipartition property
Rate distortion theory

Shannon s source coding theorem
Channel capacity
Noisy channel coding theorem
Shannon Hartley theorem
vte
Information theory is the mathematical study of the quantification  storage  and communication of information  The field was established and formalized by Claude Shannon in the     s             though early contributions were made in the     s through the works of Harry Nyquist and Ralph Hartley  It is at the intersection of electronic engineering  mathematics  statistics  computer science  neurobiology  physics  and electrical engineering                       
A key measure in information theory is entropy  Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process  For example  identifying the outcome of a fair coin flip  which has two equally likely outcomes  provides less information  lower entropy  less uncertainty  than identifying the outcome from a roll of a die  which has six equally likely outcomes   Some other important measures in information theory are mutual information  channel capacity  error exponents  and relative entropy  Important sub fields of information theory include source coding  algorithmic complexity theory  algorithmic information theory and information theoretic security 
Applications of fundamental topics of information theory include source coding data compression  e g  for ZIP files   and channel coding error detection and correction  e g  for DSL   Its impact has been crucial to the success of the Voyager missions to deep space             the invention of the compact disc  the feasibility of mobile phones and the development of the Internet and artificial intelligence                                   The theory has also found applications in other areas  including statistical inference             cryptography  neurobiology             perception             signal processing             linguistics  the evolution             and function             of molecular codes  bioinformatics   thermal physics              molecular dynamics              black holes  quantum computing  information retrieval  intelligence gathering  plagiarism detection              pattern recognition  anomaly detection              the analysis of music                          art creation              imaging system design              study of outer space              the dimensionality of space              and epistemology             


Overview edit 
Information theory studies the transmission  processing  extraction  and utilization of information  Abstractly  information can be thought of as the resolution of uncertainty  In the case of communication of information over a noisy channel  this abstract concept was formalized in      by Claude Shannon in a paper entitled A Mathematical Theory of Communication  in which information is thought of as a set of possible messages  and the goal is to send these messages over a noisy channel  and to have the receiver reconstruct the message with low probability of error  in spite of the channel noise  Shannon s main result  the noisy channel coding theorem  showed that  in the limit of many channel uses  the rate of information that is asymptotically achievable is equal to the channel capacity  a quantity dependent merely on the statistics of the channel over which the messages are sent            
Coding theory is concerned with finding explicit methods  called codes  for increasing the efficiency and reducing the error rate of data communication over noisy channels to near the channel capacity  These codes can be roughly subdivided into data compression  source coding  and error correction  channel coding  techniques  In the latter case  it took many years to find the methods Shannon s work proved were possible      citation needed     
A third class of information theory codes are cryptographic algorithms  both codes and ciphers   Concepts  methods and results from coding theory and information theory are widely used in cryptography and cryptanalysis  such as the unit ban      citation needed     

Historical background edit 
Main article  History of information theory
The landmark event establishing the discipline of information theory and bringing it to immediate worldwide attention was the publication of Claude E  Shannon s classic paper  A Mathematical Theory of Communication  in the Bell System Technical Journal in July and October       Historian James Gleick rated the paper as the most important development of       noting that the paper was  even more profound and more fundamental  than the transistor              He came to be known as the  father of information theory                                       Shannon outlined some of his initial ideas of information theory as early as      in a letter to Vannevar Bush             
Prior to this paper  limited information theoretic ideas had been developed at Bell Labs  all implicitly assuming events of equal probability  Harry Nyquist s      paper  Certain Factors Affecting Telegraph Speed  contains a theoretical section quantifying  intelligence  and the  line speed  at which it can be transmitted by a communication system  giving the relation W   K log m  recalling the Boltzmann constant   where W is the speed of transmission of intelligence  m is the number of different voltage levels to choose from at each time step  and K is a constant  Ralph Hartley s      paper  Transmission of Information  uses the word information as a measurable quantity  reflecting the receiver s ability to distinguish one sequence of symbols from any other  thus quantifying information as H   log Sn   n log S  where S was the number of possible symbols  and n the number of symbols in a transmission  The unit of information was therefore the decimal digit  which since has sometimes been called the hartley in his honor as a unit or scale or measure of information  Alan Turing in      used similar ideas as part of the statistical analysis of the breaking of the German second world war Enigma ciphers      citation needed     
Much of the mathematics behind information theory with events of different probabilities were developed for the field of thermodynamics by Ludwig Boltzmann and J  Willard Gibbs  Connections between information theoretic entropy and thermodynamic entropy  including the important contributions by Rolf Landauer in the     s  are explored in Entropy in thermodynamics and information theory      citation needed     
In Shannon s revolutionary and groundbreaking paper  the work for which had been substantially completed at Bell Labs by the end of       Shannon for the first time introduced the qualitative and quantitative model of communication as a statistical process underlying information theory  opening with the assertion  

 The fundamental problem of communication is that of reproducing at one point  either exactly or approximately  a message selected at another point  
With it came the ideas of  

the information entropy and redundancy of a source  and its relevance through the source coding theorem 
the mutual information  and the channel capacity of a noisy channel  including the promise of perfect loss free communication given by the noisy channel coding theorem 
the practical result of the Shannon Hartley law for the channel capacity of a Gaussian channel  as well as
the bit a new way of seeing the most fundamental unit of information      citation needed     
Quantities of information edit 
This section does not cite any sources  Please help improve this section by adding citations to reliable sources  Unsourced material may be challenged and removed    April        Learn how and when to remove this message Main article  Quantities of information
Information theory is based on probability theory and statistics  where quantified information is usually described in terms of bits  Information theory often concerns itself with measures of information of the distributions associated with random variables  One of the most important measures is called entropy  which forms the building block of many other measures  Entropy allows quantification of measure of information in a single random variable              Another useful concept is mutual information defined on two random variables  which describes the measure of information in common between those variables  which can be used to describe their correlation  The former quantity is a property of the probability distribution of a random variable and gives a limit on the rate at which data generated by independent samples with the given distribution can be reliably compressed  The latter is a property of the joint distribution of two random variables  and is the maximum rate of reliable communication across a noisy channel in the limit of long block lengths  when the channel statistics are determined by the joint distribution 
The choice of logarithmic base in the following formulae determines the unit of information entropy that is used  A common unit of information is the bit or shannon  based on the binary logarithm  Other units include the nat  which is based on the natural logarithm  and the decimal digit  which is based on the common logarithm 
In what follows  an expression of the form p log p is considered by convention to be equal to zero whenever p      This is justified because 
  
    
      
        
          lim
          
            p
              x     
             
             
          
        
        p
        log
          x     
        p
         
         
      
    
      displaystyle  lim   p rightarrow    p log p   
  
 for any logarithmic base 

Entropy of an information source edit 
Based on the probability mass function of each source symbol to be communicated  the Shannon entropy H  in units of bits  per symbol   is given by


  
    
      
        H
         
          x     
        
            x     
          
            i
          
        
        
          p
          
            i
          
        
        
          log
          
             
          
        
          x     
         
        
          p
          
            i
          
        
         
      
    
      displaystyle H   sum   i p  i  log      p  i   
  

where pi is the probability of occurrence of the i th possible value of the source symbol  This equation gives the entropy in the units of  bits   per symbol  because it uses a logarithm of base    and this base   measure of entropy has sometimes been called the shannon in his honor  Entropy is also commonly computed using the natural logarithm  base e  where e is Euler s number   which produces a measurement of entropy in nats per symbol and sometimes simplifies the analysis by avoiding the need to include extra constants in the formulas  Other bases are also possible  but less commonly used  For example  a logarithm of base          will produce a measurement in bytes per symbol  and a logarithm of base    will produce a measurement in decimal digits  or hartleys  per symbol 
Intuitively  the entropy HX of a discrete random variable X is a measure of the amount of uncertainty associated with the value of X when only its distribution is known 
The entropy of a source that emits a sequence of N symbols that are independent and identically distributed  iid  is N   H bits  per message of N symbols   If the source data symbols are identically distributed but not independent  the entropy of a message of length N will be less than N   H 

The entropy of a Bernoulli trial as a function of success probability  often called the binary entropy function  Hb p   The entropy is maximized at   bit per trial when the two possible outcomes are equally probable  as in an unbiased coin toss 
If one transmits      bits   s and  s   and the value of each of these bits is known to the receiver  has a specific value with certainty  ahead of transmission  it is clear that no information is transmitted  If  however  each bit is independently equally likely to be   or         shannons of information  more often called bits  have been transmitted  Between these two extremes  information can be quantified as follows  If 
  
    
      
        
          X
        
      
    
      displaystyle  mathbb  X   
  
 is the set of all messages  x        xn  that X could be  and p x  is the probability of some 
  
    
      
        x
          x     
        
          X
        
      
    
      displaystyle x in  mathbb  X   
  
  then the entropy  H  of X is defined             


  
    
      
        H
         
        X
         
         
        
          
            E
          
          
            X
          
        
         
        I
         
        x
         
         
         
          x     
        
            x     
          
            x
              x     
            
              X
            
          
        
        p
         
        x
         
        log
          x     
        p
         
        x
         
         
      
    
      displaystyle H X   mathbb  E    X  I x     sum   x in  mathbb  X   p x  log p x   
  

 Here  I x  is the self information  which is the entropy contribution of an individual message  and 
  
    
      
        
          
            E
          
          
            X
          
        
      
    
      displaystyle  mathbb  E    X  
  
 is the expected value   A property of entropy is that it is maximized when all the messages in the message space are equiprobable p x      n  i e   most unpredictable  in which case H X    log n 
The special case of information entropy for a random variable with two outcomes is the binary entropy function  usually taken to the logarithmic base    thus having the shannon  Sh  as unit 


  
    
      
        
          H
          
            
              b
            
          
        
         
        p
         
         
          x     
        p
        
          log
          
             
          
        
          x     
        p
          x     
         
         
          x     
        p
         
        
          log
          
             
          
        
          x     
         
         
          x     
        p
         
         
      
    
      displaystyle H   mathrm  b    p   p log     p    p  log        p   
  

Joint entropy edit 
The joint entropy of two discrete random variables X and Y is merely the entropy of their pairing   X  Y   This implies that if X and Y are independent  then their joint entropy is the sum of their individual entropies 
For example  if  X  Y  represents the position of a chess piece X the row and Y the column  then the joint entropy of the row of the piece and the column of the piece will be the entropy of the position of the piece 


  
    
      
        H
         
        X
         
        Y
         
         
        
          
            E
          
          
            X
             
            Y
          
        
         
          x     
        log
          x     
        p
         
        x
         
        y
         
         
         
          x     
        
            x     
          
            x
             
            y
          
        
        p
         
        x
         
        y
         
        log
          x     
        p
         
        x
         
        y
         
        
      
    
      displaystyle H X Y   mathbb  E    X Y    log p x y     sum   x y p x y  log p x y    
  

Despite similar notation  joint entropy should not be confused with cross entropy 

Conditional entropy  equivocation  edit 
The conditional entropy or conditional uncertainty of X given random variable Y  also called the equivocation of X about Y  is the average conditional entropy over Y             


  
    
      
        H
         
        X
        
           
        
        Y
         
         
        
          
            E
          
          
            Y
          
        
         
        H
         
        X
        
           
        
        y
         
         
         
          x     
        
            x     
          
            y
              x     
            Y
          
        
        p
         
        y
         
        
            x     
          
            x
              x     
            X
          
        
        p
         
        x
        
           
        
        y
         
        log
          x     
        p
         
        x
        
           
        
        y
         
         
          x     
        
            x     
          
            x
             
            y
          
        
        p
         
        x
         
        y
         
        log
          x     
        p
         
        x
        
           
        
        y
         
         
      
    
      displaystyle H X Y   mathbb  E    Y  H X y     sum   y in Y p y  sum   x in X p x y  log p x y    sum   x y p x y  log p x y   
  

Because entropy can be conditioned on a random variable or on that random variable being a certain value  care should be taken not to confuse these two definitions of conditional entropy  the former of which is in more common use  A basic property of this form of conditional entropy is that 


  
    
      
        H
         
        X
        
           
        
        Y
         
         
        H
         
        X
         
        Y
         
          x     
        H
         
        Y
         
         
        
      
    
      displaystyle H X Y  H X Y  H Y     
  

Mutual information  transinformation  edit 
Mutual information measures the amount of information that can be obtained about one random variable by observing another  It is important in communication where it can be used to maximize the amount of information shared between sent and received signals  The mutual information of X relative to Y is given by 


  
    
      
        I
         
        X
         
        Y
         
         
        
          
            E
          
          
            X
             
            Y
          
        
         
        S
        I
         
        x
         
        y
         
         
         
        
            x     
          
            x
             
            y
          
        
        p
         
        x
         
        y
         
        log
          x     
        
          
            
              p
               
              x
               
              y
               
            
            
              p
               
              x
               
              
              p
               
              y
               
            
          
        
      
    
      displaystyle I X Y   mathbb  E    X Y  SI x y    sum   x y p x y  log   frac  p x y   p x   p y    
  

where SI  Specific mutual Information  is the pointwise mutual information 
A basic property of the mutual information is that


  
    
      
        I
         
        X
         
        Y
         
         
        H
         
        X
         
          x     
        H
         
        X
        
           
        
        Y
         
         
        
      
    
      displaystyle I X Y  H X  H X Y     
  

That is  knowing Y  we can save an average of I X  Y  bits in encoding X compared to not knowing Y 
Mutual information is symmetric 


  
    
      
        I
         
        X
         
        Y
         
         
        I
         
        Y
         
        X
         
         
        H
         
        X
         
         
        H
         
        Y
         
          x     
        H
         
        X
         
        Y
         
         
        
      
    
      displaystyle I X Y  I Y X  H X  H Y  H X Y     
  

Mutual information can be expressed as the average Kullback Leibler divergence  information gain  between the posterior probability distribution of X given the value of Y and the prior distribution on X 


  
    
      
        I
         
        X
         
        Y
         
         
        
          
            E
          
          
            p
             
            y
             
          
        
         
        
          D
          
            
              K
              L
            
          
        
         
        p
         
        X
        
           
        
        Y
         
        y
         
          x     
        p
         
        X
         
         
         
         
      
    
      displaystyle I X Y   mathbb  E    p y   D   mathrm  KL    p X Y y   p X     
  

In other words  this is a measure of how much  on the average  the probability distribution on X will change if we are given the value of Y  This is often recalculated as the divergence from the product of the marginal distributions to the actual joint distribution 


  
    
      
        I
         
        X
         
        Y
         
         
        
          D
          
            
              K
              L
            
          
        
         
        p
         
        X
         
        Y
         
          x     
        p
         
        X
         
        p
         
        Y
         
         
         
      
    
      displaystyle I X Y  D   mathrm  KL    p X Y   p X p Y    
  

Mutual information is closely related to the log likelihood ratio test in the context of contingency tables and the multinomial distribution and to Pearson s    test  mutual information can be considered a statistic for assessing independence between a pair of variables  and has a well specified asymptotic distribution 

Kullback Leibler divergence  information gain  edit 
The Kullback Leibler divergence  or information divergence  information gain  or relative entropy  is a way of comparing two distributions  a  true  probability distribution        
  
    
      
        p
         
        X
         
      
    
      displaystyle p X  
  
         and an arbitrary probability distribution        
  
    
      
        q
         
        X
         
      
    
      displaystyle q X  
  
         If we compress data in a manner that assumes        
  
    
      
        q
         
        X
         
      
    
      displaystyle q X  
  
        is the distribution underlying some data  when  in reality         
  
    
      
        p
         
        X
         
      
    
      displaystyle p X  
  
        is the correct distribution  the Kullback Leibler divergence is the number of average additional bits per datum necessary for compression  It is thus defined


  
    
      
        
          D
          
            
              K
              L
            
          
        
         
        p
         
        X
         
          x     
        q
         
        X
         
         
         
        
            x     
          
            x
              x     
            X
          
        
          x     
        p
         
        x
         
        log
          x     
        
          q
           
          x
           
        
        
          x     
        
        
            x     
          
            x
              x     
            X
          
        
          x     
        p
         
        x
         
        log
          x     
        
          p
           
          x
           
        
         
        
            x     
          
            x
              x     
            X
          
        
        p
         
        x
         
        log
          x     
        
          
            
              p
               
              x
               
            
            
              q
               
              x
               
            
          
        
         
      
    
      displaystyle D   mathrm  KL    p X   q X    sum   x in X  p x  log  q x        sum   x in X  p x  log  p x    sum   x in X p x  log   frac  p x   q x     
  

Although it is sometimes used as a  distance metric   KL divergence is not a true metric since it is not symmetric and does not satisfy the triangle inequality  making it a semi quasimetric  
Another interpretation of the KL divergence is the  unnecessary surprise  introduced by a prior from the truth  suppose a number X is about to be drawn randomly from a discrete set with probability distribution        
  
    
      
        p
         
        x
         
      
    
      displaystyle p x  
  
         If Alice knows the true distribution        
  
    
      
        p
         
        x
         
      
    
      displaystyle p x  
  
         while Bob believes  has a prior  that the distribution is        
  
    
      
        q
         
        x
         
      
    
      displaystyle q x  
  
         then Bob will be more surprised than Alice  on average  upon seeing the value of X  The KL divergence is the  objective  expected value of Bob s  subjective  surprisal minus Alice s surprisal  measured in bits if the log is in base    In this way  the extent to which Bob s prior is  wrong  can be quantified in terms of how  unnecessarily surprised  it is expected to make him 

Directed Information edit 
Directed information  
  
    
      
        I
         
        
          X
          
            n
          
        
          x     
        
          Y
          
            n
          
        
         
      
    
      displaystyle I X  n  to Y  n   
  
  is an information theory measure that quantifies the information flow from the random process 
  
    
      
        
          X
          
            n
          
        
         
         
        
          X
          
             
          
        
         
        
          X
          
             
          
        
         
          x     
         
        
          X
          
            n
          
        
         
      
    
      displaystyle X  n    X     X      dots  X  n    
  
 to the random process 
  
    
      
        
          Y
          
            n
          
        
         
         
        
          Y
          
             
          
        
         
        
          Y
          
             
          
        
         
          x     
         
        
          Y
          
            n
          
        
         
      
    
      displaystyle Y  n    Y     Y      dots  Y  n    
  
  The term directed information was coined by James Massey and is defined as


  
    
      
        I
         
        
          X
          
            n
          
        
          x     
        
          Y
          
            n
          
        
         
          x   c 
        
            x     
          
            i
             
             
          
          
            n
          
        
        I
         
        
          X
          
            i
          
        
         
        
          Y
          
            i
          
        
        
           
        
        
          Y
          
            i
              x     
             
          
        
         
      
    
      displaystyle I X  n  to Y  n   triangleq  sum   i     n I X  i  Y  i  Y  i     
  
 
where 
  
    
      
        I
         
        
          X
          
            i
          
        
         
        
          Y
          
            i
          
        
        
           
        
        
          Y
          
            i
              x     
             
          
        
         
      
    
      displaystyle I X  i  Y  i  Y  i     
  
 is the conditional mutual information 
  
    
      
        I
         
        
          X
          
             
          
        
         
        
          X
          
             
          
        
         
         
         
         
         
        
          X
          
            i
          
        
         
        
          Y
          
            i
          
        
        
           
        
        
          Y
          
             
          
        
         
        
          Y
          
             
          
        
         
         
         
         
         
        
          Y
          
            i
              x     
             
          
        
         
      
    
      displaystyle I X     X         X  i  Y  i  Y     Y         Y  i     
  
 
In contrast to mutual information  directed information is not symmetric  The 
  
    
      
        I
         
        
          X
          
            n
          
        
          x     
        
          Y
          
            n
          
        
         
      
    
      displaystyle I X  n  to Y  n   
  
 measures the information bits that are transmitted causally     clarification needed      from 
  
    
      
        
          X
          
            n
          
        
      
    
      displaystyle X  n  
  
 to 
  
    
      
        
          Y
          
            n
          
        
      
    
      displaystyle Y  n  
  
  The Directed information has many applications in problems where causality plays an important role such as capacity of channel with feedback                          capacity of discrete memoryless networks with feedback              gambling with causal side information              compression with causal side information             
real time control communication settings                          and in statistical physics             

Other quantities edit 
Other important information theoretic quantities include the R nyi entropy and the Tsallis entropy  generalizations of the concept of entropy   differential entropy  a generalization of quantities of information to continuous distributions   and the conditional mutual information  Also  pragmatic information has been proposed as a measure of how much information has been used in making a decision 

Coding theory edit 
This section does not cite any sources  Please help improve this section by adding citations to reliable sources  Unsourced material may be challenged and removed    April        Learn how and when to remove this message Main article  Coding theory
A picture showing scratches on the readable surface of a CD R  Music and data CDs are coded using error correcting codes and thus can still be read even if they have minor scratches using error detection and correction 
Coding theory is one of the most important and direct applications of information theory  It can be subdivided into source coding theory and channel coding theory  Using a statistical description for data  information theory quantifies the number of bits needed to describe the data  which is the information entropy of the source 

Data compression  source coding   There are two formulations for the compression problem 
lossless data compression  the data must be reconstructed exactly 
lossy data compression  allocates bits needed to reconstruct the data  within a specified fidelity level measured by a distortion function  This subset of information theory is called rate distortion theory 
Error correcting codes  channel coding   While data compression removes as much redundancy as possible  an error correcting code adds just the right kind of redundancy  i e   error correction  needed to transmit the data efficiently and faithfully across a noisy channel 
This division of coding theory into compression and transmission is justified by the information transmission theorems  or source channel separation theorems that justify the use of bits as the universal currency for information in many contexts  However  these theorems only hold in the situation where one transmitting user wishes to communicate to one receiving user  In scenarios with more than one transmitter  the multiple access channel   more than one receiver  the broadcast channel  or intermediary  helpers   the relay channel   or more general networks  compression followed by transmission may no longer be optimal 

Source theory edit 
Any process that generates successive messages can be considered a source of information  A memoryless source is one in which each message is an independent identically distributed random variable  whereas the properties of ergodicity and stationarity impose less restrictive constraints  All such sources are stochastic  These terms are well studied in their own right outside information theory 

Rate edit 
Information rate is the average entropy per symbol  For memoryless sources  this is merely the entropy of each symbol  while  in the case of a stationary stochastic process  it is 


  
    
      
        r
         
        
          lim
          
            n
              x     
              x   e 
          
        
        H
         
        
          X
          
            n
          
        
        
           
        
        
          X
          
            n
              x     
             
          
        
         
        
          X
          
            n
              x     
             
          
        
         
        
          X
          
            n
              x     
             
          
        
         
          x     
         
         
      
    
      displaystyle r  lim   n to  infty  H X  n  X  n    X  n    X  n     ldots    
  

that is  the conditional entropy of a symbol given all the previous symbols generated  For the more general case of a process that is not necessarily stationary  the average rate is 


  
    
      
        r
         
        
          lim
          
            n
              x     
              x   e 
          
        
        
          
             
            n
          
        
        H
         
        
          X
          
             
          
        
         
        
          X
          
             
          
        
         
          x     
        
          X
          
            n
          
        
         
         
      
    
      displaystyle r  lim   n to  infty    frac     n  H X     X      dots X  n    
  

that is  the limit of the joint entropy per symbol  For stationary sources  these two expressions give the same result             
The information rate is defined as   


  
    
      
        r
         
        
          lim
          
            n
              x     
              x   e 
          
        
        
          
             
            n
          
        
        I
         
        
          X
          
             
          
        
         
        
          X
          
             
          
        
         
          x     
        
          X
          
            n
          
        
         
        
          Y
          
             
          
        
         
        
          Y
          
             
          
        
         
          x     
        
          Y
          
            n
          
        
         
         
      
    
      displaystyle r  lim   n to  infty    frac     n  I X     X      dots X  n  Y     Y      dots Y  n    
  

It is common in information theory to speak of the  rate  or  entropy  of a language  This is appropriate  for example  when the source of information is English prose  The rate of a source of information is related to its redundancy and how well it can be compressed  the subject of source coding 

Channel capacity edit 
Main article  Channel capacity
Communications over a channel is the primary motivation of information theory  However  channels often fail to produce exact reconstruction of a signal  noise  periods of silence  and other forms of signal corruption often degrade quality 
Consider the communications process over a discrete channel  A simple model of the process is shown below 


  
    
      
        
          
              x     
            
              
                Message
              
            
            
              W
            
          
        
        
          
            
              
                
                  Encoder
                
              
            
            
              
                
                  f
                  
                    n
                  
                
              
            
          
        
        
          
              x     
            
              
                
                  
                    
                      E
                      n
                      c
                      o
                      d
                      e
                      d
                    
                    
                      s
                      e
                      q
                      u
                      e
                      n
                      c
                      e
                    
                  
                
              
            
            
              
                X
                
                  n
                
              
            
          
        
        
          
            
              
                
                  Channel
                
              
            
            
              
                p
                 
                y
                
                   
                
                x
                 
              
            
          
        
        
          
              x     
            
              
                
                  
                    
                      R
                      e
                      c
                      e
                      i
                      v
                      e
                      d
                    
                    
                      s
                      e
                      q
                      u
                      e
                      n
                      c
                      e
                    
                  
                
              
            
            
              
                Y
                
                  n
                
              
            
          
        
        
          
            
              
                
                  Decoder
                
              
            
            
              
                
                  g
                  
                    n
                  
                
              
            
          
        
        
          
              x     
            
              
                
                  
                    
                      E
                      s
                      t
                      i
                      m
                      a
                      t
                      e
                      d
                    
                    
                      m
                      e
                      s
                      s
                      a
                      g
                      e
                    
                  
                
              
            
            
              
                
                  W
                    x e 
                
              
            
          
        
      
    
      displaystyle   xrightarrow   text Message    W    begin array   c    hline   text Encoder    f  n    hline  end array    xrightarrow   mathrm  Encoded  atop sequence     X  n     begin array   c    hline   text Channel    p y x    hline  end array    xrightarrow   mathrm  Received  atop sequence     Y  n     begin array   c    hline   text Decoder    g  n    hline  end array    xrightarrow   mathrm  Estimated  atop message      hat  W    
  

Here X represents the space of messages transmitted  and Y the space of messages received during a unit time over our channel  Let p y      x  be the conditional probability distribution function of Y given X  We will consider p y      x  to be an inherent fixed property of our communications channel  representing the nature of the noise of our channel   Then the joint distribution of X and Y is completely determined by our channel and by our choice of f x   the marginal distribution of messages we choose to send over the channel  Under these constraints  we would like to maximize the rate of information  or the signal  we can communicate over the channel  The appropriate measure for this is the mutual information  and this maximum mutual information is called the channel capacity and is given by 


  
    
      
        C
         
        
          max
          
            f
          
        
        I
         
        X
         
        Y
         
         
        
      
    
      displaystyle C  max   f I X Y     
  

This capacity has the following property related to communicating at information rate R  where R is usually bits per symbol   For any information rate R  lt  C and coding error    gt     for large enough N  there exists a code of length N and rate   R and a decoding algorithm  such that the maximal probability of block error is      that is  it is always possible to transmit with arbitrarily small block error  In addition  for any rate R  gt  C  it is impossible to transmit with arbitrarily small block error 
Channel coding is concerned with finding such nearly optimal codes that can be used to transmit data over a noisy channel with a small coding error at a rate near the channel capacity 

Capacity of particular channel models edit 
A continuous time analog communications channel subject to Gaussian noise see Shannon Hartley theorem 
A binary symmetric channel  BSC  with crossover probability p is a binary input  binary output channel that flips the input bit with probability p  The BSC has a capacity of           Hb p  bits per channel use  where Hb is the binary entropy function to the base   logarithm 

A binary erasure channel  BEC  with erasure probability p is a binary input  ternary output channel  The possible channel outputs are       and a third symbol  e  called an erasure  The erasure represents complete loss of information about an input bit  The capacity of the BEC is           p bits per channel use 

Channels with memory and directed information edit 
In practice many channels have memory  Namely  at time 
  
    
      
        i
      
    
      displaystyle i 
  
 the channel is given by the conditional probability
  
    
      
        P
         
        
          y
          
            i
          
        
        
           
        
        
          x
          
            i
          
        
         
        
          x
          
            i
              x     
             
          
        
         
        
          x
          
            i
              x     
             
          
        
         
         
         
         
         
        
          x
          
             
          
        
         
        
          y
          
            i
              x     
             
          
        
         
        
          y
          
            i
              x     
             
          
        
         
         
         
         
         
        
          y
          
             
          
        
         
      
    
      displaystyle P y  i  x  i  x  i    x  i        x     y  i    y  i        y      
  
 
It is often more comfortable to use the notation 
  
    
      
        
          x
          
            i
          
        
         
         
        
          x
          
            i
          
        
         
        
          x
          
            i
              x     
             
          
        
         
        
          x
          
            i
              x     
             
          
        
         
         
         
         
         
        
          x
          
             
          
        
         
      
    
      displaystyle x  i   x  i  x  i    x  i        x      
  
 and the channel become 
  
    
      
        P
         
        
          y
          
            i
          
        
        
           
        
        
          x
          
            i
          
        
         
        
          y
          
            i
              x     
             
          
        
         
      
    
      displaystyle P y  i  x  i  y  i     
  
 
In such a case the capacity is given by the mutual information rate when there is no feedback available and the Directed information rate in the case that either there is feedback or not                          if there is no feedback the directed information equals the mutual information  

Fungible information edit 
Fungible information is the information for which the means of encoding is not important              Classical information theorists and computer scientists are mainly concerned with information of this sort  It is sometimes referred as speakable information             

Applications to other fields edit 
Intelligence uses and secrecy applications edit 
This section does not cite any sources  Please help improve this section by adding citations to reliable sources  Unsourced material may be challenged and removed    April        Learn how and when to remove this message 
Information theoretic concepts apply to cryptography and cryptanalysis  Turing s information unit  the ban  was used in the Ultra project  breaking the German Enigma machine code and hastening the end of World War II in Europe  Shannon himself defined an important concept now called the unicity distance  Based on the redundancy of the plaintext  it attempts to give a minimum amount of ciphertext necessary to ensure unique decipherability 
Information theory leads us to believe it is much more difficult to keep secrets than it might first appear  A brute force attack can break systems based on asymmetric key algorithms or on most commonly used methods of symmetric key algorithms  sometimes called secret key algorithms   such as block ciphers  The security of all such methods comes from the assumption that no known attack can break them in a practical amount of time 
Information theoretic security refers to methods such as the one time pad that are not vulnerable to such brute force attacks  In such cases  the positive conditional mutual information between the plaintext and ciphertext  conditioned on the key  can ensure proper transmission  while the unconditional mutual information between the plaintext and ciphertext remains zero  resulting in absolutely secure communications  In other words  an eavesdropper would not be able to improve his or her guess of the plaintext by gaining knowledge of the ciphertext but not of the key  However  as in any other cryptographic system  care must be used to correctly apply even information theoretically secure methods  the Venona project was able to crack the one time pads of the Soviet Union due to their improper reuse of key material 

Pseudorandom number generation edit 
This section does not cite any sources  Please help improve this section by adding citations to reliable sources  Unsourced material may be challenged and removed    April        Learn how and when to remove this message 
Pseudorandom number generators are widely available in computer language libraries and application programs  They are  almost universally  unsuited to cryptographic use as they do not evade the deterministic nature of modern computer equipment and software  A class of improved random number generators is termed cryptographically secure pseudorandom number generators  but even they require random seeds external to the software to work as intended  These can be obtained via extractors  if done carefully  The measure of sufficient randomness in extractors is min entropy  a value related to Shannon entropy through R nyi entropy  R nyi entropy is also used in evaluating randomness in cryptographic systems  Although related  the distinctions among these measures mean that a random variable with high Shannon entropy is not necessarily satisfactory for use in an extractor and so for cryptography uses 

Seismic exploration edit 
One early commercial application of information theory was in the field of seismic oil exploration  Work in this field made it possible to strip off and separate the unwanted noise from the desired seismic signal  Information theory and digital signal processing offer a major improvement of resolution and image clarity over previous analog methods             

Semiotics edit 
Semioticians Doede Nauta           nl      and Winfried N th both considered Charles Sanders Peirce as having created a theory of information in his works on semiotics                                                                      Nauta defined semiotic information theory as the study of  the internal processes of coding  filtering  and information processing                                   
Concepts from information theory such as redundancy and code control have been used by semioticians such as Umberto Eco and Ferruccio Rossi Landi           it      to explain ideology as a form of message transmission whereby a dominant social class emits its message by using signs that exhibit a high degree of redundancy such that only one message is decoded among a selection of competing ones             

Integrated process organization of neural information edit 
Quantitative information theoretic methods have been applied in cognitive science to analyze the integrated process organization of neural information in the context of the binding problem in cognitive neuroscience              In this context  either an information theoretical measure  such as functional clusters  Gerald Edelman and Giulio Tononi s functional clustering model and dynamic core hypothesis  DCH               or effective information  Tononi s integrated information theory  IIT  of consciousness                                       is defined  on the basis of a reentrant process organization  i e  the synchronization of neurophysiological activity between groups of neuronal populations   or the measure of the minimization of free energy on the basis of statistical methods  Karl J  Friston s free energy principle  FEP   an information theoretical measure which states that every adaptive change in a self organized system leads to a minimization of free energy  and the Bayesian brain hypothesis                                                              

Miscellaneous applications edit 
Information theory also has applications in the search for extraterrestrial intelligence              black holes              bioinformatics              and gambling                         

See also edit 

Mathematics portal

Algorithmic probability
Bayesian inference
Communication theory
Constructor theory   a generalization of information theory that includes quantum information
Formal science
Inductive probability
Info metrics
Minimum message length
Minimum description length
Philosophy of information

Applications edit 

Active networking
Cryptanalysis
Cryptography
Cybernetics
Entropy in thermodynamics and information theory
Gambling
Intelligence  information gathering 
Seismic exploration

History edit 
Hartley  R V L 
History of information theory
Shannon  C E 
Timeline of information theory
Yockey  H P 
Andrey Kolmogorov
Theory edit 

Coding theory
Detection theory
Estimation theory
Fisher information
Information algebra
Information asymmetry
Information field theory
Information geometry
Information theory and measure theory
Kolmogorov complexity
List of unsolved problems in information theory
Logic of information
Network coding
Philosophy of information
Quantum information science
Source coding

Concepts edit 

Ban  unit 
Channel capacity
Communication channel
Communication source
Conditional entropy
Covert channel
Data compression
Decoder
Differential entropy
Fungible information
Information fluctuation complexity
Information entropy
Joint entropy
Kullback Leibler divergence
Mutual information
Pointwise mutual information  PMI 
Receiver  information theory 
Redundancy
R nyi entropy
Self information
Unicity distance
Variety
Hamming distance
Perplexity

References edit 


  Schneider  Thomas D           Claude Shannon  Biologist   IEEE Engineering in Medicine and Biology Magazine  The Quarterly Magazine of the Engineering in Medicine  amp  Biology Society                 doi         memb               ISSN                 PMC               PMID               

  a b Cruces  Sergio  Mart n Clemente  Rub n  Samek  Wojciech                Information Theory Applications in Signal Processing   Entropy               Bibcode     Entrp         C  doi         e          ISSN                 PMC               PMID               

  a b Baleanu  D   Balas  Valentina Emilia  Agarwal  Praveen  eds          Fractional Order Systems and Applications in Engineering  Advanced Studies in Complex Systems  London  United Kingdom  Academic Press  p           ISBN                         OCLC                 

  Horgan  John                Claude Shannon  Tinkerer  Prankster  and Father of Information Theory   IEEE  Retrieved            

  Shi  Zhongzhi         Advanced Artificial Intelligence  World Scientific Publishing  p          doi               ISBN                        

  Sinha  Sudhi  Al Huraimel  Khaled               Reimagining Businesses with AI         ed    Wiley  p          doi                        ISBN                        

  Burnham  K  P   Anderson  D  R          Model Selection and Multimodel Inference  A Practical Information Theoretic Approach  Second      ed    New York  Springer Science  ISBN                        

  a b F  Rieke  D  Warland  R Ruyter van Steveninck  W Bialek         Spikes  Exploring the Neural Code  The MIT press  ISBN                     

  Delgado Bonal  Alfonso  Mart n Torres  Javier                Human vision is determined based on information theory   Scientific Reports                Bibcode     NatSR         D  doi         srep       ISSN                 PMC               PMID               

  cf  Huelsenbeck  J  P   Ronquist  F   Nielsen  R   Bollback  J  P           Bayesian inference of phylogeny and its impact on evolutionary biology   Science                         Bibcode     Sci           H  doi         science          PMID                S CID              

  Allikmets  Rando  Wasserman  Wyeth W   Hutchinson  Amy  Smallwood  Philip  Nathans  Jeremy  Rogan  Peter K           Thomas D  Schneider       Michael Dean        Organization of the ABCR gene  analysis of promoter and splice junction sequences   Gene                    doi         s                      PMID              

  Jaynes  E  T           Information Theory and Statistical Mechanics   Phys  Rev                Bibcode     PhRv          J  doi         physrev          S CID               

  Talaat  Khaled  Cowen  Benjamin  Anderoglu  Osman                Method of information entropy for convergence assessment of molecular dynamics simulations   Journal of Applied Physics                    Bibcode     JAP      m    T  doi                    OSTI               S CID                

  Bennett  Charles H   Li  Ming  Ma  Bin          Chain Letters and Evolutionary Histories   Scientific American                  Bibcode     SciAm    f    B  doi         scientificamerican         PMID                Archived from the original on             Retrieved            

  David R  Anderson  November            Some background on why people in the empirical sciences may want to better understand the information theoretic methods   PDF   Archived from the original  PDF  on July           Retrieved            

  Loy  D  Gareth         Pareyon  Gabriel  Pina Romero  Silvia  Agust n Aquino  Octavio A   Lluis Puebla  Emilio  eds     Music  Expectation  and Information Theory   The Musical Mathematical Mind  Patterns and Transformations  Computational Music Science  Cham  Springer International Publishing  pp                doi                               ISBN                         retrieved           

  Rocamora  Mart n  Cancela  Pablo  Biscainho  Luiz                Information Theory Concepts Applied to the Analysis of Rhythm in Recorded Music with Recurrent Rhythmic Patterns   Journal of the Audio Engineering Society                   doi          jaes           

  Marsden  Alan          New Prospects for Information Theory in Arts Research   Leonardo                   doi         leon a        ISSN              X 

  Pinkard  Henry  Kabuli  Leyla  Markley  Eric  Chien  Tiffany  Jiao  Jiantao  Waller  Laura          Universal evaluation and design of imaging systems using information estimation   arXiv             physics optics  

  Wing  Simon  Johnson  Jay R                 Applications of Information Theory in Solar and Space Physics   Entropy               Bibcode     Entrp         W  doi         e          ISSN                 PMC               PMID               

  Kak  Subhash                Information theory and dimensionality of space   Scientific Reports                 doi         s                   ISSN                 PMC               PMID               

  Harms  William F           The Use of Information Theory in Epistemology   Philosophy of Science                   doi                 ISSN                 JSTOR             

  Gleick       pp           

  Horgan  John                Claude Shannon  Tinkerer  Prankster  and Father of Information Theory   IEEE  Retrieved            

  Roberts  Siobhan                The Forgotten Father of the Information Age   The New Yorker  ISSN              X  Retrieved            

  a b Tse  David                How Claude Shannon Invented the Future   Quanta Magazine  Retrieved            

  Braverman  Mark  September             Information Theory in Computer Science   PDF  

  Reza      

  Ash      

  a b Massey  James          Causality  Feedback And Directed Information   Proc       Intl  Symp  on Info  Th  and its Applications  CiteSeerX                    

  Permuter  Haim Henry  Weissman  Tsachy  Goldsmith  Andrea J   February         Finite State Channels With Time Invariant Deterministic Feedback   IEEE Transactions on Information Theory                   arXiv cs          doi         TIT               S CID            

  Kramer  G   January         Capacity results for the discrete memoryless network   IEEE Transactions on Information Theory                doi         TIT             

  Permuter  Haim H   Kim  Young Han  Weissman  Tsachy  June         Interpretations of Directed Information in Portfolio Theory  Data Compression  and Hypothesis Testing   IEEE Transactions on Information Theory                     arXiv            doi         TIT               S CID               

  Simeone  Osvaldo  Permuter  Haim Henri  June         Source Coding When the Side Information May Be Delayed   IEEE Transactions on Information Theory                     arXiv            doi         TIT               S CID              

  Charalambous  Charalambos D   Stavrou  Photios A   August         Directed Information on Abstract Spaces  Properties and Variational Equalities   IEEE Transactions on Information Theory                      arXiv            doi         TIT               S CID              

  Tanaka  Takashi  Esfahani  Peyman Mohajerin  Mitter  Sanjoy K   January         LQG Control With Minimum Directed Information  Semidefinite Programming Approach   IEEE Transactions on Automatic Control                 arXiv             doi         TAC               S CID               Archived from the original on Apr                  via TU Delft Repositories 

  Vinkler  Dror A  Permuter  Haim H  Merhav  Neri     April         Analogy between gambling and measurement based work extraction   Journal of Statistical Mechanics  Theory and Experiment                    arXiv            Bibcode     JSMTE         V  doi                                   S CID                

  Jerry D  Gibson         Digital Compression for Multimedia  Principles and Standards  Morgan Kaufmann  ISBN                    

  Permuter  Haim Henry  Weissman  Tsachy  Goldsmith  Andrea J   February         Finite State Channels With Time Invariant Deterministic Feedback   IEEE Transactions on Information Theory                   arXiv cs          doi         TIT               S CID            

  Bartlett  Stephen D   Rudolph  Terry  Spekkens  Robert W   April June         Reference frames  superselection rules  and quantum information   Reviews of Modern Physics                   arXiv quant ph          Bibcode     RvMP          B  doi         RevModPhys        

  Peres  A   P  F  Scudo      b   A  Khrennikov  ed    Quantum Theory  Reconsideration of Foundations  V xj  University Press  V xj   Sweden  p           

  Haggerty  Patrick E           The corporation and innovation   Strategic Management Journal                 doi         smj            

  a b Nauta  Doede         The Meaning of Information  The Hague  Mouton  ISBN                    

  N th  Winfried  January         Charles S  Peirce s theory of information  a theory of the growth of symbols and of knowledge   Cybernetics and Human Knowing                    

  N th  Winfried          Semiotics of ideology   Semiotica  Issue     

  Maurer  H           Chapter     Systematic Class of Information Based Architecture Types   Cognitive Science  Integrative Synchronization Mechanisms in Cognitive Neuroarchitectures of the Modern Connectionism  Boca Raton FL  CRC Press  doi                        ISBN                        

  Edelman  G M   Tononi  G          A Universe of Consciousness  How Matter Becomes Imagination  New York  Basic Books  ISBN                     

  Tononi  G   Sporns  O           Measuring information integration   BMC Neuroscience           doi                         PMC              PMID               

  Tononi  G       a    An information integration theory of consciousness   BMC Neuroscience           doi                         PMC              PMID               

  Tononi  G       b    Consciousness and the brain  theoretical aspects   In Adelman  G   Smith  B   eds    Encyclopedia of Neuroscience   rd      ed    Amsterdam  Oxford  Elsevier  ISBN                     Archived  PDF  from the original on            

  Friston  K   Stephan  K E           Free energy and the brain   Synthese                    doi         s               y  PMC               PMID               

  Friston  K           The free energy principle  a unified brain theory   Nature Reviews Neuroscience                   doi         nrn      PMID               

  Friston  K   Breakstear  M   Deco  G           Perception and self organized instability   Frontiers in Computational Neuroscience           doi         fncom             PMC               PMID               

  Friston  K           Life as we know it   Journal of the Royal Society Interface                     doi         rsif            PMC               PMID               

  Kirchhoff  M   Parr  T   Palacios  E   Friston  K   Kiverstein  J           The Markov blankets of life  autonomy  active inference and the free energy principle   Journal of the Royal Society Interface                      doi         rsif            PMC               PMID               

  Doyle  Laurance R   McCowan  Brenda  Johnston  Simon  Hanser  Sean F   February         Information theory  animal communication  and the search for extraterrestrial intelligence   Acta Astronautica                     Bibcode     AcAau         D  doi         j actaastro             

  Bekenstein  Jacob D          Black holes and information theory   Contemporary Physics                 arXiv quant ph          Bibcode     ConPh         B  doi                               ISSN                

  Vinga  Susana                Information theory applications for biological sequence analysis   Briefings in Bioinformatics                   doi         bib bbt     ISSN                 PMC               PMID               

  Thorp  Edward O                Zenios  S  A   Ziemba  W  T   eds     The kelly criterion in blackjack sports betting  and the stock market    Handbook of Asset and Liability Management  San Diego  North Holland  pp                doi         b                         ISBN                         retrieved           

  Haigh  John          The Kelly Criterion and Bet Comparisons in Spread Betting   Journal of the Royal Statistical Society  Series D  The Statistician                    doi                          ISSN                


Further reading edit 
The classic work edit 

Shannon  C E           A Mathematical Theory of Communication   Bell System Technical Journal      pp                amp           July  amp  October        PDF  Notes and other formats 
R V L  Hartley   Transmission of Information   Bell System Technical Journal  July     
Andrey Kolmogorov          Three approaches to the quantitative definition of information  in International Journal of Computer Mathematics     pp               

Other journal articles edit 

J  L  Kelly Jr   Princeton   A New Interpretation of Information Rate  Bell System Technical Journal  Vol      July       pp              
R  Landauer  IEEE org   Information is Physical  Proc  Workshop on Physics and Computation PhysComp     IEEE Comp  Sci Press  Los Alamitos        pp           
Landauer  R           Irreversibility and Heat Generation in the Computing Process   PDF   IBM J  Res  Dev                  doi         rd         
Timme  Nicholas  Alford  Wesley  Flecker  Benjamin  Beggs  John M           Multivariate information measures  an experimentalist s perspective   arXiv            cs IT  

Textbooks on information theory edit 

Alajaji  F  and Chen  P N  An Introduction to Single User Information Theory  Singapore  Springer        ISBN                       
Arndt  C   Information Measures  Information and its Description in Science and Engineering  Springer Series  Signals and Communication Technology         ISBN                       
Ash  Robert B                 Information Theory  New York  Dover Publications  Inc  ISBN                    
Gallager  R  Information Theory and Reliable Communication  New York  John Wiley and Sons        ISBN                   
Goldman  S  Information Theory  New York  Prentice Hall        New York  Dover      ISBN                          ISBN                   
Cover  Thomas  Thomas  Joy A          Elements of information theory   nd      ed    New York  Wiley Interscience  ISBN                    
Csiszar  I  Korner  J  Information Theory  Coding Theorems for Discrete Memoryless Systems  Akademiai Kiado   nd edition        ISBN                   
MacKay  David J  C  Information Theory  Inference  and Learning Algorithms Cambridge  Cambridge University Press        ISBN                   
Mansuripur  M  Introduction to Information Theory  New York  Prentice Hall        ISBN                   
McEliece  R  The Theory of Information and Coding  Cambridge         ISBN                    
Pierce  JR    An introduction to information theory  symbols  signals and noise    Dover   nd Edition         reprinted by Dover       
Reza  Fazlollah M                 An Introduction to Information Theory  New York  Dover Publications  Inc  ISBN                    
Shannon  Claude  Weaver  Warren         The Mathematical Theory of Communication  PDF   Urbana  Illinois  University of Illinois Press  ISBN                     LCCN                  cite book    ISBN   Date incompatibility  help 
Stone  JV   Chapter   of book  Information Theory  A Tutorial Introduction   University of Sheffield  England        ISBN                     
Yeung  RW   A First Course in Information Theory Kluwer Academic Plenum Publishers         ISBN                    
Yeung  RW   Information Theory and Network Coding Springer              ISBN                       

Other books edit 

Leon Brillouin  Science and Information Theory  Mineola  N Y   Dover                     ISBN                   
Gleick  James         The Information  A History  a Theory  a Flood   st      ed    New York  Pantheon Books  ISBN                         OCLC                
A  I  Khinchin  Mathematical Foundations of Information Theory  New York  Dover        ISBN                   
H  S  Leff and A  F  Rex  Editors  Maxwell s Demon  Entropy  Information  Computing  Princeton University Press  Princeton  New Jersey         ISBN                  X
Robert K  Logan  What is Information    Propagating Organization in the Biosphere  the Symbolosphere  the Technosphere and the Econosphere  Toronto  DEMO Publishing 
Tom Siegfried  The Bit and the Pendulum  Wiley        ISBN                   
Charles Seife  Decoding the Universe  Viking        ISBN                  X
Jeremy Campbell  Grammatical Man  Touchstone Simon  amp  Schuster        ISBN                   
Henri Theil  Economics and Information Theory  Rand McNally  amp  Company   Chicago       
Escolano  Suau  Bonev  Information Theory in Computer Vision and Pattern Recognition  Springer        ISBN                       
Vlatko Vedral  Decoding Reality  The Universe as Quantum Information  Oxford University Press       ISBN                   

External links edit 



Wikiquote has quotations related to Information theory 



Library resources about  Information theory 

Resources in your library
Resources in other libraries


 Information   Encyclopedia of Mathematics  EMS Press             
Lambert F  L           Shuffled Cards  Messy Desks  and Disorderly Dorm Rooms   Examples of Entropy Increase  Nonsense    Journal of Chemical Education
IEEE Information Theory Society and ITSOC Monographs  Surveys  and Reviews Archived            at the Wayback Machine
vteSubfields of and cyberneticians involved in cyberneticsSubfields
Artificial intelligence
Biological cybernetics
Biomedical cybernetics
Biorobotics
Biosemiotics
Neurocybernetics
Catastrophe theory
Computational neuroscience
Connectionism
Control theory
Conversation theory
Cybernetics in the Soviet Union
Decision theory
Emergence
Engineering cybernetics
Homeostasis
Information theory
Management cybernetics
Medical cybernetics
Second order cybernetics
Cybersemiotics
Sociocybernetics
Synergetics
Cyberneticians
Alexander Lerner
Alexey Lyapunov
Alfred Radcliffe Brown
Allenna Leonard
Anthony Wilden
Buckminster Fuller
Charles Fran ois
Genevieve Bell
Margaret Boden
Claude Bernard
Cliff Joslyn
Erich von Holst
Ernst von Glasersfeld
Francis Heylighen
Francisco Varela
Frederic Vester
Charles Geoffrey Vickers
Gordon Pask
Gordon S  Brown
Gregory Bateson
Heinz von Foerster
Humberto Maturana
I  A  Richards
Igor Aleksander
Jacque Fresco
Jakob von Uexk ll
Jason Jixuan Hu
Jay Wright Forrester
Jennifer Wilby
John N  Warfield
Kevin Warwick
Ludwig von Bertalanffy
Maleyka Abbaszadeh
Manfred Clynes
Margaret Mead
Marian Mazur
N  Katherine Hayles
Natalia Bekhtereva
Niklas Luhmann
Norbert Wiener
Pyotr Grigorenko
Qian Xuesen
Ranulph Glanville
Robert Trappl
Sergei P  Kurdyumov
Anthony Stafford Beer
Stuart Kauffman
Stuart Umpleby
Talcott Parsons
Ulla Mitzdorf
Valentin Turchin
Valentin Braitenberg
William Ross Ashby
Walter Bradford Cannon
Walter Pitts
Warren McCulloch
William Grey Walter

vteInformation processingInformation processesinformation processes by function
perception
attention
influence
operating
communication
reasoning
learning
storing
decision making
information processing abstractions
event processing
sign processesing
signal processing
data processing
stream processing
agent processing
state processing
Information processorsnatural
nature as information processing
humans as information processing systems
society as information processing system
mixed
mixed reality
brain computer interface
physical computing
human computer interaction
artificial
processors and processes
bio inspired computing
ubiquitous computing
artificial brain and mind uploading
virtual reality
virtual world
Information processing theories and conceptsin biology
computational and systems biology
genetic informatics and cellular computing
computational neuroscience and neurocomputing
in cognitive psychology
information processing theory
mind and intelligence
cognitive informatics and neuroinformatics
behavior informatics
in computer science
neural computation
computation theory
algorithms and information structures
computational circuits
artificial intelligence
in philosophy
computational theory of mind
philosophy of information
philosophy of artificial intelligence
interdisciplinary
information theory
decision theory
systems theory
other
infosphere
inforg
Decoding the Universe
information overload

vteData compression methodsLosslessEntropy type
Adaptive coding
Arithmetic
Asymmetric numeral systems
Golomb
Huffman
Adaptive
Canonical
Modified
Range
Shannon
Shannon Fano
Shannon Fano Elias
Tunstall
Unary
Universal
Exp Golomb
Fibonacci
Gamma
Levenshtein
Dictionary type
Byte pair encoding
Lempel Ziv
   
LZ 
LZJB
LZO
LZRW
LZSS
LZW
LZWL
Snappy
Other types
BWT
CTW
CM
Delta
Incremental
DMC
DPCM
Grammar
Re Pair
Sequitur
LDCT
MTF
PAQ
PPM
RLE
Hybrid
LZ     Huffman
Deflate
LZX
LZS
LZ     ANS
LZFSE
LZ     Huffman   ANS
Zstandard
LZ     Huffman   context
Brotli
LZSS   Huffman
LHA LZH
LZ     Range
LZMA
LZHAM
RLE   BWT   MTF   Huffman
bzip 
LossyTransform type
Discrete cosine transform
DCT
MDCT
DST
FFT
Wavelet
Daubechies
DWT
SPIHT
Predictive type
DPCM
ADPCM
LPC
ACELP
CELP
LAR
LSP
WLPC
Motion
Compensation
Estimation
Vector
Psychoacoustic
AudioConcepts
Bit rate
ABR
CBR
VBR
Companding
Convolution
Dynamic range
Latency
Nyquist Shannon theorem
Sampling
Silence compression
Sound quality
Speech coding
Sub band coding
Codec parts
A law
  law
DPCM
ADPCM
DM
FT
FFT
LPC
ACELP
CELP
LAR
LSP
WLPC
MDCT
Psychoacoustic model
ImageConcepts
Chroma subsampling
Coding tree unit
Color space
Compression artifact
Image resolution
Macroblock
Pixel
PSNR
Quantization
Standard test image
Texture compression
Methods
Chain code
DCT
Deflate
Fractal
KLT
LP
RLE
Wavelet
Daubechies
DWT
EZW
SPIHT
VideoConcepts
Bit rate
ABR
CBR
VBR
Display resolution
Frame
Frame rate
Frame types
Interlace
Video characteristics
Video quality
Codec parts
DCT
DPCM
Deblocking filter
Lapped transform
Motion
Compensation
Estimation
Vector
Wavelet
Daubechies
DWT
Theory
Compressed data structures
Compressed suffix array
FM index
Entropy
Information theory
Timeline
Kolmogorov complexity
Prefix code
Quantization
Rate distortion
Redundancy
Symmetry
Smallest grammar problem
Community
Hutter Prize
People
Mark Adler

vteMajor mathematics areas
History
Timeline
Future
Lists
Glossary
Foundations
Category theory
Information theory
Mathematical logic
Philosophy of mathematics
Set theory
Type theory
Algebra
Abstract
Commutative
Elementary
Group theory
Linear
Multilinear
Universal
Homological
Analysis
Calculus
Real analysis
Complex analysis
Hypercomplex analysis
Differential equations
Functional analysis
Harmonic analysis
Measure theory
Discrete
Combinatorics
Graph theory
Order theory
Geometry
Algebraic
Analytic
Arithmetic
Differential
Discrete
Euclidean
Finite
Number theory
Arithmetic
Algebraic number theory
Analytic number theory
Diophantine geometry
Topology
General
Algebraic
Differential
Geometric
Homotopy theory
Applied
Engineering mathematics
Mathematical biology
Mathematical chemistry
Mathematical economics
Mathematical finance
Mathematical physics
Mathematical psychology
Mathematical sociology
Mathematical statistics
Probability
Statistics
Systems science
Control theory
Game theory
Operations research
Computational
Computer science
Theory of computation
Computational complexity theory
Numerical analysis
Optimization
Computer algebra
Related topics
Mathematicians
lists
Informal mathematics
Films about mathematicians
Recreational mathematics
Mathematics and art
Mathematics education

 Mathematics     portal
 Category
 Commons
 WikiProject

vteComputer scienceNote  This template roughly follows the      ACM Computing Classification System Hardware
Printed circuit board
Peripheral
Integrated circuit
Very Large Scale Integration
Systems on Chip  SoCs 
Energy consumption  Green computing 
Electronic design automation
Hardware acceleration
Processor
Size   Form
Computer systems organization
Computer architecture
Computational complexity
Dependability
Embedded system
Real time computing
Networks
Network architecture
Network protocol
Network components
Network scheduler
Network performance evaluation
Network service
Software organization
Interpreter
Middleware
Virtual machine
Operating system
Software quality
Software notations and tools
Programming paradigm
Programming language
Compiler
Domain specific language
Modeling language
Software framework
Integrated development environment
Software configuration management
Software library
Software repository
Software development
Control variable
Software development process
Requirements analysis
Software design
Software construction
Software deployment
Software engineering
Software maintenance
Programming team
Open source model
Theory of computation
Model of computation
Stochastic
Formal language
Automata theory
Computability theory
Computational complexity theory
Logic
Semantics
Algorithms
Algorithm design
Analysis of algorithms
Algorithmic efficiency
Randomized algorithm
Computational geometry
Mathematics of computing
Discrete mathematics
Probability
Statistics
Mathematical software
Information theory
Mathematical analysis
Numerical analysis
Theoretical computer science
Information systems
Database management system
Information storage systems
Enterprise information system
Social information systems
Geographic information system
Decision support system
Process control system
Multimedia information system
Data mining
Digital library
Computing platform
Digital marketing
World Wide Web
Information retrieval
Security
Cryptography
Formal methods
Security hacker
Security services
Intrusion detection system
Hardware security
Network security
Information security
Application security
Human computer interaction
Interaction design
Augmented reality
Virtual reality
Social computing
Ubiquitous computing
Visualization
Accessibility
Concurrency
Concurrent computing
Parallel computing
Distributed computing
Multithreading
Multiprocessing
Artificial intelligence
Natural language processing
Knowledge representation and reasoning
Computer vision
Automated planning and scheduling
Search methodology
Control method
Philosophy of artificial intelligence
Distributed artificial intelligence
Machine learning
Supervised learning
Unsupervised learning
Reinforcement learning
Multi task learning
Cross validation
Graphics
Animation
Rendering
Photograph manipulation
Graphics processing unit
Image compression
Solid modeling
Applied computing
Quantum Computing
E commerce
Enterprise software
Computational mathematics
Computational physics
Computational chemistry
Computational biology
Computational social science
Computational engineering
Differentiable computing
Computational healthcare
Digital art
Electronic publishing
Cyberwarfare
Electronic voting
Video games
Word processing
Operations research
Educational technology
Document management

 Category
 Outline
 Glossaries

Authority control databases  National GermanyUnited StatesFranceBnF dataJapanCzech RepublicSpainLatviaIsrael





Retrieved from  https   en wikipedia org w index php title Information theory amp oldid