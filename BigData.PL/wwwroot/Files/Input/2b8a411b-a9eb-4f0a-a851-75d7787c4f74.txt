AI whose outputs can be understood by humans
For other uses of  XAI   see XAI  disambiguation  
Part of a series onArtificial intelligence  AI 
Major goals
Artificial general intelligence
Intelligent agent
Recursive self improvement
Planning
Computer vision
General game playing
Knowledge reasoning
Natural language processing
Robotics
AI safety

Approaches
Machine learning
Symbolic
Deep learning
Bayesian networks
Evolutionary algorithms
Hybrid intelligent systems
Systems integration

Applications
Bioinformatics
Deepfake
Earth sciences
 Finance 
Generative AI
Art
Audio
Music
Government
Healthcare
Mental health
Industry
Translation
 Military 
Physics
Projects

Philosophy
Artificial consciousness
Chinese room
Friendly AI
Control problem Takeover
Ethics
Existential risk
Turing test
Uncanny valley

History
Timeline
Progress
AI winter
AI boom

Glossary
Glossary
vte
Explainable AI  XAI   often overlapping with interpretable AI  or explainable machine learning  XML   is a field of research within artificial intelligence  AI  that explores methods that provide humans with the ability of intellectual oversight over AI algorithms                        The main focus is on the reasoning behind the decisions or predictions made by the AI algorithms             to make them more understandable and transparent             This addresses users  requirement to assess safety and scrutinize the automated decision making in applications             XAI counters the  black box  tendency of machine learning  where even the AI s designers cannot explain why it arrived at a specific decision                       
XAI hopes to help users of AI powered systems perform more effectively by improving their understanding of how those systems reason             XAI may be an implementation of the social right to explanation             Even if there is no such legal right or regulatory requirement  XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions              XAI aims to explain what has been done  what is being done  and what will be done next  and to unveil which information these actions are based on              This makes it possible to confirm existing knowledge  challenge existing knowledge  and generate new assumptions             
Machine learning  ML  algorithms used in AI can be categorized as white box or black box              White box models provide results that are understandable to experts in the domain  Black box models  on the other hand  are extremely hard to explain and may not be understood even by domain experts              XAI algorithms follow the three principles of transparency  interpretability  and explainability 

A model is transparent  if the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer              
Interpretability describes the possibility of comprehending the ML model and presenting the underlying basis for decision making in a way that is understandable to humans                                     
Explainability is a concept that is recognized as important  but a consensus definition is not yet available              one possibility is  the collection of features of the interpretable domain that have contributed  for a given example  to producing a decision  e g   classification or regression               
In summary  Interpretability refers to the user s ability to understand model outputs  while Model Transparency includes Simulatability  reproducibility of predictions   Decomposability  intuitive explanations for parameters   and Algorithmic Transparency  explaining how algorithms work   Model Functionality focuses on textual descriptions  visualization  and local explanations  which clarify specific outputs or instances rather than entire models  All these concepts aim to enhance the comprehensibility and usability of AI systems             
If algorithms fulfill these principles  they provide a basis for justifying decisions  tracking them and thereby verifying them  improving the algorithms  and exploring new facts             
Sometimes it is also possible to achieve a high accuracy result with white box ML algorithms  These algorithms have an interpretable structure that can be used to explain predictions              Concept Bottleneck Models  which use concept level abstractions to explain model reasoning  are examples of this and can be applied in both image             and text             prediction tasks  This is especially important in domains like medicine  defense  finance  and law  where it is crucial to understand decisions and build trust in the algorithms              Many researchers argue that  at least for supervised machine learning  the way forward is symbolic regression  where the algorithm searches the space of mathematical expressions to find the model that best fits a given dataset                                     
AI systems optimize behavior to satisfy a mathematically specified goal system chosen by the system designers  such as the command  maximize the accuracy of assessing how positive film reviews are in the test dataset   The AI may learn useful general rules from the test set  such as  reviews containing the word  horrible  are likely to be negative   However  it may also learn inappropriate rules  such as  reviews containing  Daniel Day Lewis  are usually positive   such rules may be undesirable if they are likely to fail to generalize outside the training set  or if people consider the rule to be  cheating  or  unfair   A human can audit rules in an XAI to get an idea of how likely the system is to generalize to future real world data outside the test set             


Goals edit 
Cooperation between agents   in this case  algorithms and humans   depends on trust  If humans are to accept algorithmic prescriptions  they need to trust them  Incompleteness in formal trust criteria is a barrier to optimization  Transparency  interpretability  and explainability are intermediate goals on the road to these more comprehensive trust criteria              This is particularly relevant in medicine              especially with clinical decision support systems  CDSS   in which medical professionals should be able to understand how and why a machine based decision was made in order to trust the decision and augment their decision making process             
AI systems sometimes learn undesirable tricks that do an optimal job of satisfying explicit pre programmed goals on the training data but do not reflect the more nuanced implicit desires of the human system designers or the full complexity of the domain data  For example  a      system tasked with image recognition learned to  cheat  by looking for a copyright tag that happened to be associated with horse pictures rather than learning how to tell if a horse was actually pictured             In another      system  a supervised learning AI tasked with grasping items in a virtual world learned to cheat by placing its manipulator between the object and the viewer in a way such that it falsely appeared to be grasping the object                         
One transparency project  the DARPA XAI program  aims to produce  glass box  models that are explainable to a  human in the loop  without greatly sacrificing AI performance  Human users of such a system can understand the AI s cognition  both in real time and after the fact  and can determine whether to trust the AI              Other applications of XAI are knowledge extraction from black box models and model comparisons              In the context of monitoring systems for ethical and socio legal compliance  the term  glass box  is commonly used to refer to tools that track the inputs and outputs of the system in question  and provide value based explanations for their behavior  These tools aim to ensure that the system operates in accordance with ethical and legal standards  and that its decision making processes are transparent and accountable  The term  glass box  is often used in contrast to  black box  systems  which lack transparency and can be more difficult to monitor and regulate             
The term is also used to name a voice assistant that produces counterfactual statements as explanations             

Explainability and interpretability techniques edit 
There is a subtle difference between the terms explainability and interpretability in the context of AI             




Term
Definition
Source


Interpretability
 level of understanding how the underlying  AI  technology works 
ISO IEC TR               en                     


Explainability
 level of understanding how the AI based system     came up with a given result 
ISO IEC TR               en                     

Some explainability techniques don t involve understanding how the model works  and may work across various AI systems  Treating the model as a black box and analyzing how marginal changes to the inputs affect the result sometimes provides a sufficient explanation 

Explainability edit 
Explainability is useful for ensuring that AI models are not making decisions based on irrelevant or otherwise unfair criteria  For classification and regression models  several popular techniques exist 

Partial dependency plots show the marginal effect of an input feature on the predicted outcome 
SHAP  SHapley Additive exPlanations  enables visualization of the contribution of each input feature to the output  It works by calculating Shapley values  which measure the average marginal contribution of a feature across all possible combinations of features             
Feature importance estimates how important a feature is for the model  It is usually done using permutation importance  which measures the performance decrease when it the feature value randomly shuffled across all samples 
LIME approximates locally a model s outputs with a simpler  interpretable model             
Multitask learning provides a large number of outputs in addition to the target classification  These other outputs can help developers deduce what the network has learned             
For images  saliency maps highlight the parts of an image that most influenced the result             
Systems that are expert or knowledge based are software systems that are made by experts  This system consists of a knowledge based encoding for the domain knowledge  This system is usually modeled as production rules  and someone uses this knowledge base which the user can question the system for knowledge  In expert systems  the language and explanations are understood with an explanation for the reasoning or a problem solving activity            
However  these techniques are not very suitable for language models like generative pretrained transformers  Since these models generate language  they can provide an explanation  but which may not be reliable  Other techniques include attention analysis  examining how the model focuses on different parts of the input   probing methods  testing what information is captured in the model s representations   causal tracing  tracing the flow of information through the model  and circuit discovery  identifying specific subnetworks responsible for certain behaviors   Explainability research in this area overlaps significantly with interpretability and alignment research             

Interpretability edit 
Grokking is an example of phenomenon studied in interpretability  It involves a model that initially memorizes all the answers  overfitting   but later adopts an algorithm that generalizes to unseen data             
Scholars sometimes use the term  mechanistic interpretability  to refer to the process of reverse engineering artificial neural networks to understand their internal decision making mechanisms and components  similar to how one might analyze a complex machine or computer program             
Interpretability research often focuses on generative pretrained transformers  It is particularly relevant for AI safety and alignment  as it may enable to identify signs of undesired behaviors such as sycophancy  deceptiveness or bias  and to better steer AI models             
Studying the interpretability of the most advanced foundation models often involves searching for an automated way to identify  features  in generative pretrained transformers  In a neural network  a feature is a pattern of neuron activations that corresponds to a concept  A compute intensive technique called  dictionary learning  makes it possible to identify features to some degree  Enhancing the ability to identify and edit features is expected to significantly improve the safety of frontier AI models                         
For convolutional neural networks  DeepDream can generate images that strongly activate a particular neuron  providing a visual hint about what the neuron is trained to identify             

History and methods edit 
During the     s to     s  symbolic reasoning systems  such as MYCIN              GUIDON              SOPHIE              and PROTOS                         could represent  reason about  and explain their reasoning for diagnostic  instructional  or machine learning  explanation based learning  purposes  MYCIN  developed in the early     s as a research prototype for diagnosing bacteremia infections of the bloodstream  could explain             which of its hand coded rules contributed to a diagnosis in a specific case  Research in intelligent tutoring systems resulted in developing systems such as SOPHIE that could act as an  articulate expert   explaining problem solving strategy at a level the student could understand  so they would know what action to take next  For instance  SOPHIE could explain the qualitative reasoning behind its electronics troubleshooting  even though it ultimately relied on the SPICE circuit simulator  Similarly  GUIDON added tutorial rules to supplement MYCIN s domain level rules so it could explain the strategy for medical diagnosis  Symbolic approaches to machine learning relying on explanation based learning  such as PROTOS  made use of explicit representations of explanations expressed in a dedicated explanation language  both to explain their actions and to acquire new knowledge             
In the     s through the early     s  truth maintenance systems  TMS  extended the capabilities of causal reasoning  rule based  and logic based inference systems                                        A TMS explicitly tracks alternate lines of reasoning  justifications for conclusions  and lines of reasoning that lead to contradictions  allowing future reasoning to avoid these dead ends  To provide an explanation  they trace reasoning from conclusions to assumptions through rule operations or logical inferences  allowing explanations to be generated from the reasoning traces  As an example  consider a rule based problem solver with just a few rules about Socrates that concludes he has died from poison 

By just tracing through the dependency structure the problem solver can construct the following explanation   Socrates died because he was mortal and drank poison  and all mortals die when they drink poison  Socrates was mortal because he was a man and all men are mortal  Socrates drank poison because he held dissident beliefs  the government was conservative  and those holding conservative dissident beliefs under conservative governments must drink poison                                        
By the     s researchers began studying whether it is possible to meaningfully extract the non hand coded rules being generated by opaque trained neural networks              Researchers in clinical expert systems creating     clarification needed      neural network powered decision support for clinicians sought to develop dynamic explanations that allow these technologies to be more trusted and trustworthy in practice             In the     s public concerns about racial and other bias in the use of AI for criminal sentencing decisions and findings of creditworthiness may have led to increased demand for transparent artificial intelligence             As a result  many academics and organizations are developing tools to help detect bias in their systems             
Marvin Minsky et al  raised the issue that AI can function as a form of surveillance  with the biases inherent in surveillance  suggesting HI  Humanistic Intelligence  as a way to create a more fair and balanced  human in the loop  AI             
Explainable AI has been recently a new topic researched amongst the context of modern deep learning  Modern complex AI techniques  such as deep learning  are naturally opaque              To address this issue  methods have been developed to make new models more explainable and interpretable                                                                          This includes layerwise relevance propagation  LRP   a technique for determining which features in a particular input vector contribute most strongly to a neural network s output                          Other techniques explain some particular prediction made by a  nonlinear  black box model  a goal referred to as  local interpretability                                                                           We still today cannot explain the output of today s DNNs without the new explanatory mechanisms  we also can t by the neural network  or external explanatory components              There is also research on whether the concepts of local interpretability can be applied to a remote context  where a model is operated by a third party                         
There has been work on making glass box models which are more transparent to inspection                          This includes decision trees              Bayesian networks  sparse linear models              and more              The Association for Computing Machinery Conference on Fairness  Accountability  and Transparency  ACM FAccT  was established in      to study transparency and explainability in the context of socio technical systems  many of which include artificial intelligence                         
Some techniques allow visualisations of the inputs to which individual software neurons respond to most strongly  Several groups found that neurons can be aggregated into circuits that perform human comprehensible functions  some of which reliably arise across different networks trained independently                         
There are various techniques to extract compressed representations of the features of given inputs  which can then be analysed by standard clustering techniques  Alternatively  networks can be trained to output linguistic explanations of their behaviour  which are then directly human interpretable              Model behaviour can also be explained with reference to training data for example  by evaluating which training inputs influenced a given behaviour the most             
The use of explainable artificial intelligence  XAI  in pain research  specifically in understanding the role of electrodermal activity for automated pain recognition  hand crafted features and deep learning models in pain recognition  highlighting the insights that simple hand crafted features can yield comparative performances to deep learning models and that both traditional feature engineering and deep feature learning approaches rely on simple characteristics of the input time series data             

Regulation edit 
As regulators  official bodies  and general users come to depend on AI based dynamic systems  clearer accountability will be required for automated decision making processes to ensure trust and transparency  The first global conference exclusively dedicated to this emerging discipline was the      International Joint Conference on Artificial Intelligence  Workshop on Explainable Artificial Intelligence  XAI               It has evolved over the years  with various workshops organised and co located to many other international conferences  and it has now a dedicated global event   The world conference on eXplainable Artificial Intelligence   with its own proceedings                         
The European Union introduced a right to explanation in the General Data Protection Regulation  GDPR  to address potential problems stemming from the rising importance of algorithms  The implementation of the regulation began in       However  the right to explanation in GDPR covers only the local aspect of interpretability  In the United States  insurance companies are required to be able to explain their rate and coverage decisions              In France the Loi pour une R publique num rique  Digital Republic Act  grants subjects the right to request and receive information pertaining to the implementation of algorithms that process data about them 

Limitations edit 
Despite ongoing endeavors to enhance the explainability of AI models  they persist with several inherent limitations 

Adversarial parties edit 
By making an AI system more explainable  we also reveal more of its inner workings  For example  the explainability method of feature importance identifies features or variables that are most important in determining the model s output  while the influential samples method identifies the training samples that are most influential in determining the output  given a particular input              Adversarial parties could take advantage of this knowledge 
For example  competitor firms could replicate aspects of the original AI system in their own product  thus reducing competitive advantage              An explainable AI system is also susceptible to being  gamed  influenced in a way that undermines its intended purpose  One study gives the example of a predictive policing system  in this case  those who could potentially  game  the system are the criminals subject to the system s decisions  In this study  developers of the system discussed the issue of criminal gangs looking to illegally obtain passports  and they expressed concerns that  if given an idea of what factors might trigger an alert in the passport application process  those gangs would be able to  send guinea pigs  to test those triggers  eventually finding a loophole that would allow them to  reliably get passports from under the noses of the authorities              

Adaptive integration and explanation edit 
Many approaches that it uses provides explanation in general  it doesn t take account for the diverse backgrounds and knowledge level of the users  This leads to challenges with accurate comprehension for all users  Expert users can find the explanations lacking in depth  and are oversimplified  while a beginner user may struggle understanding the explanations as they are complex  This limitation downplays the ability of the XAI techniques to appeal to their users with different levels of knowledge  which can impact the trust from users and who uses it  The quality of explanations can be different amongst their users as they all have different expertise levels  including different situation and conditions             

Technical complexity edit 
A fundamental barrier to making AI systems explainable is the technical complexity of such systems  End users often lack the coding knowledge required to understand software of any kind  Current methods used to explain AI are mainly technical ones  geared toward machine learning engineers for debugging purposes  rather than toward the end users who are ultimately affected by the system  causing  a gap between explainability in practice and the goal of transparency               Proposed solutions to address the issue of technical complexity include either promoting the coding education of the general public so technical explanations would be more accessible to end users  or providing explanations in layperson terms             
The solution must avoid oversimplification  It is important to strike a balance between accuracy   how faithfully the explanation reflects the process of the AI system   and explainability   how well end users understand the process  This is a difficult balance to strike  since the complexity of machine learning makes it difficult for even ML engineers to fully understand  let alone non experts             

Understanding versus trust edit 
The goal of explainability to end users of AI systems is to increase trust in the systems  even  address concerns about lack of  fairness  and discriminatory effects               However  even with a good understanding of an AI system  end users may not necessarily trust the system              In one study  participants were presented with combinations of white box and black box explanations  and static and interactive explanations of AI systems  While these explanations served to increase both their self reported and objective understanding  it had no impact on their level of trust  which remained skeptical             
This outcome was especially true for decisions that impacted the end user in a significant way  such as graduate school admissions  Participants judged algorithms to be too inflexible and unforgiving in comparison to human decision makers  instead of rigidly adhering to a set of rules  humans are able to consider exceptional cases as well as appeals to their initial decision              For such decisions  explainability will not necessarily cause end users to accept the use of decision making algorithms  We will need to either turn to another method to increase trust and acceptance of decision making algorithms  or question the need to rely solely on AI for such impactful decisions in the first place 
However  some emphasize that the purpose of explainability of artificial intelligence is not to merely increase users  trust in the system s decisions  but to calibrate the users  level of trust to the correct level              According to this principle  too much or too little user trust in the AI system will harm the overall performance of the human system unit  When the trust is excessive  the users are not critical of possible mistakes of the system and when the users do not have enough trust in the system  they will not exhaust the benefits inherent in it 

Criticism edit 
Some scholars have suggested that explainability in AI should be considered a goal secondary to AI effectiveness  and that encouraging the exclusive development of XAI may limit the functionality of AI more broadly                            Critiques of XAI rely on developed concepts of mechanistic and empiric reasoning from evidence based medicine to suggest that AI technologies can be clinically validated even when their function cannot be understood by their operators               
Some researchers advocate the use of inherently interpretable machine learning models  rather than using post hoc explanations in which a second model is created to explain the first  This is partly because post hoc models increase the complexity in a decision pathway and partly because it is often unclear how faithfully a post hoc explanation can mimic the computations of an entirely separate model              However  another view is that what is important is that the explanation accomplishes the given task at hand  and whether it is pre or post hoc doesn t matter  If a post hoc explanation method helps a doctor diagnose cancer better  it is of secondary importance whether it is a correct incorrect explanation 
The goals of XAI amount to a form of lossy compression that will become less effective as AI models grow in their number of parameters  Along with other factors this leads to a theoretical limit for explainability              

Explainability in social choice edit 
Explainability was studied also in social choice theory  Social choice theory aims at finding solutions to social decision problems  that are based on well established axioms  Ariel D  Procaccia              explains that these axioms can be used to construct convincing explanations to the solutions  This principle has been used to construct explanations in various subfields of social choice 

Voting edit 
Cailloux and Endriss              present a method for explaining voting rules using the axioms that characterize them  They exemplify their method on the Borda voting rule  
Peters  Procaccia  Psomas and Zhou              present an algorithm for explaining the outcomes of the Borda rule using O m   explanations  and prove that this is tight in the worst case 

Participatory budgeting edit 
Yang  Hausladen  Peters  Pournaras  Fricker and Helbing              present an empirical study of explainability in participatory budgeting  They compared the greedy and the equal shares rules  and three types of explanations  mechanism explanation  a general explanation of how the aggregation rule works given the voting input   individual explanation  explaining how many voters had at least one approved project  at least       CHF in approved projects   and group explanation  explaining how the budget is distributed among the districts and topics   They compared the perceived trustworthiness and fairness of greedy and equal shares  before and after the explanations  They found out that  for MES  mechanism explanation yields the highest increase in perceived fairness and trustworthiness  the second highest was Group explanation  For Greedy  Mechanism explanation increases perceived trustworthiness but not fairness  whereas Individual explanation increases both perceived fairness and trustworthiness  Group explanation decreases the perceived fairness and trustworthiness 

Payoff allocation edit 
Nizri  Azaria and Hazon              present an algorithm for computing explanations for the Shapley value  Given a coalitional game  their algorithm decomposes it to sub games  for which it is easy to generate verbal explanations based on the axioms characterizing the Shapley value  The payoff allocation for each sub game is perceived as fair  so the Shapley based payoff allocation for the given game should seem fair as well  An experiment with     human subjects shows that  with their automatically generated explanations  subjects perceive Shapley based payoff allocation as significantly fairer than with a general standard explanation 

See also edit 
Algorithmic transparency        study on the transparency of algorithmsPages displaying wikidata descriptions as a fallback
Right to explanation        Right to have an algorithm explained
Accumulated local effects        Machine learning method
References edit 


  Longo  Luca  et      al           Explainable Artificial Intelligence  XAI       A manifesto of open challenges and interdisciplinary research directions   Information Fusion       doi         j inffus             

  Mih ly  H der          Explainable AI  A Brief History of the Concept   PDF   ERCIM News             

  Phillips  P  Jonathon  Hahn  Carina A   Fontana  Peter C   Yates  Amy N   Greene  Kristen  Broniatowski  David A   Przybocki  Mark A                 Four Principles of Explainable Artificial Intelligence   NIST  doi         nist ir      

  Vilone  Giulia  Longo  Luca          Notions of explainability and evaluation approaches for explainable artificial intelligence   Information Fusion  December        Volume             doi         j inffus             

  a b Confalonieri  Roberto  Coba  Ludovik  Wagner  Benedikt  Besold  Tarek R   January         A historical perspective of explainable Artificial Intelligence   WIREs Data Mining and Knowledge Discovery          doi         widm       ISSN                

  Castelvecchi  Davide                Can we open the black box of AI    Nature                     Bibcode     Natur         C  doi               a  ISSN                 PMID                S CID              

  a b c Sample  Ian    November         Computer says no  why making AIs fair  accountable and transparent is crucial   The Guardian  Retrieved    January      

  Alizadeh  Fatemeh          I Don t Know  Is AI Also Used in Airbags   An Empirical Study of Folk Concepts and People s Expectations of Current and Future Artificial Intelligence   Icom                doi         icom            S CID                

  a b Edwards  Lilian  Veale  Michael          Slave to the Algorithm  Why a  Right to an Explanation  Is Probably Not the Remedy You Are Looking For   Duke Law and Technology Review          SSRN              

  Do Couto  Mark  February             Entering the Age of Explainable AI   TDWI  Retrieved            

  a b Gunning  D   Stefik  M   Choi  J   Miller  T   Stumpf  S   Yang  G  Z                 XAI Explainable artificial intelligence   Science Robotics          eaay      doi         scirobotics aay      ISSN                 PMID               

  Rieg  Thilo  Frick  Janek  Baumgartl  Hermann  Buettner  Ricardo                Demonstration of the potential of white box machine learning approaches to gain insights from cardiovascular disease electrocardiograms   PLOS ONE           e         Bibcode     PLoSO         R  doi         journal pone          ISSN                 PMC               PMID               

  Vilone  Giulia  Longo  Luca          Classification of Explainable Artificial Intelligence Methods through Their Output Formats   Machine Learning and Knowledge Extraction                  doi         make        

  Loyola Gonz lez  O           Black Box vs  White Box  Understanding Their Advantages and Weaknesses From a Practical Point of View   IEEE Access                    Bibcode     IEEEA    o    L  doi         ACCESS               ISSN                

  a b Roscher  R   Bohn  B   Duarte  M  F   Garcke  J           Explainable Machine Learning for Scientific Insights and Discoveries   IEEE Access                  arXiv             Bibcode     IEEEA         R  doi         ACCESS               ISSN                

  a b Murdoch  W  James  Singh  Chandan  Kumbier  Karl  Abbasi Asl  Reza  Yu  Bin                Interpretable machine learning  definitions  methods  and applications   Proceedings of the National Academy of Sciences of the United States of America                         arXiv             doi         pnas             PMC               PMID               

  a b Lipton  Zachary C   June         The Mythos of Model Interpretability  In machine learning  the concept of interpretability is both important and slippery   Queue                 doi                          ISSN                

   Explainable Artificial Intelligence  XAI   Concepts  Taxonomies  Opportunities and Challenges toward Responsible AI   DeepAI              Retrieved            

  Montavon  Gr goire  Samek  Wojciech  M ller  Klaus Robert                Methods for interpreting and understanding deep neural networks   Digital Signal Processing            arXiv             Bibcode     DSP           M  doi         j dsp              ISSN                

  Notovich  Aviv   Chalutz Ben Gal  Hila   amp  Ben Gal  Irad          Explainable Artificial Intelligence  XAI   Motivation  Terminology  and Taxonomy   PDF   In Machine Learning for Data Science Handbook  Data Mining and Knowledge Discovery Handbook  pp            Cham  Springer International Publishing   cite web     CS  maint  multiple names  authors list  link  CS  maint  numeric names  authors list  link 

  Adadi  A   Berrada  M           Peeking Inside the Black Box  A Survey on Explainable Artificial Intelligence  XAI    IEEE Access                  Bibcode     IEEEA         A  doi         ACCESS               ISSN                

  a b c Rudin  Cynthia          Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead   Nature Machine Intelligence                  arXiv             doi         s               x  ISSN                 PMC               PMID               

  Koh  P  W   Nguyen  T   Tang  Y  S   Mussmann  S   Pierson  E   Kim  B   Liang  P   November         Concept bottleneck models   International Conference on Machine Learning  PMLR  pp                 

  Ludan  J  M   Lyu  Q   Yang  Y   Dugan  L   Yatskar  M   Callison Burch  C           Interpretable by Design Text Classification with Iteratively Generated Concept Bottleneck   arXiv             cs CL  

  Wenninger  Simon  Kaymakci  Can  Wiethe  Christian          Explainable long term building energy consumption prediction using QLattice   Applied Energy       Elsevier BV          Bibcode     ApEn          W  doi         j apenergy              ISSN                 S CID                

  Christiansen  Michael  Wilstrup  Casper  Hedley  Paula L           Explainable  white box  machine learning is the way forward in preeclampsia screening   American Journal of Obstetrics and Gynecology           Elsevier BV       doi         j ajog              ISSN                 PMID                S CID                

  Wilstup  Casper  Cave  Chris               Combining symbolic regression with the Cox proportional hazards model improves prediction of heart failure deaths  Cold Spring Harbor Laboratory  doi                              S CID               

   How AI detectives are cracking open the black box of deep learning   Science    July       Retrieved    January       

  Dosilovic  Filip  Brcic  Mario  Hlupic  Nikica                Explainable Artificial Intelligence  A Survey   PDF   MIPRO          st International Convention Proceedings  MIPRO       Opatija  Croatia  pp                doi          MIPRO               ISBN                         Archived from the original  PDF  on             Retrieved            

  Bernal  Jose  Mazo  Claudia                Transparency of Artificial Intelligence in Healthcare  Insights from Professionals in Computing and Healthcare Worldwide   Applied Sciences                  doi         app           ISSN                

  Antoniadi  Anna Markella  Du  Yuhan  Guendouz  Yasmine  Wei  Lan  Mazo  Claudia  Becker  Brett A   Mooney  Catherine  January         Current Challenges and Future Opportunities for XAI in Machine Learning Based Clinical Decision Support Systems  A Systematic Review   Applied Sciences                 doi         app          ISSN                

   DeepMind Has Simple Tests That Might Prevent Elon Musk s AI Apocalypse   Bloomberg com     December       Retrieved    January      

   Learning from Human Preferences   OpenAI Blog     June       Retrieved    January      

   Explainable Artificial Intelligence  XAI    DARPA  Retrieved    July      

  Biecek  Przemyslaw     June         DALEX  explainers for complex predictive models   Journal of Machine Learning Research           arXiv            

  Rai  Arun   Explainable AI  From black box to glass box   Journal of the Academy of Marketing Science                    

  Sokol  Kacper  Flach  Peter          Glass Box  Explaining AI Decisions With Counterfactual Statements Through Conversation With a Voice enabled Virtual Assistant   Proceedings of the Twenty Seventh International Joint Conference on Artificial Intelligence  pp                  doi          ijcai           ISBN                     S CID               

  Broniatowski  David A           Psychological Foundations of Explainability and Interpretability in Artificial Intelligence   NIST Pubs 

  a b ISO IEC TR                Software and systems engineering  Software testing  Part     Guidelines on the testing of AI based systems  ISO        Retrieved    November      

  Verma  Yugesh                Complete Guide to SHAP   SHAPley Additive exPlanations for Practitioners   Analytics India Magazine  Retrieved            

  Rothman  Denis                Exploring LIME Explanations and the Mathematics Behind It   Codemotion Magazine  Retrieved            

  Christian  Brian          TELL ME EVERYTHING  MULTITASK NETS   The Alignment Problem  Machine learning and human values  W  W  Norton  amp  Company  ISBN                        

  Sharma  Abhishek                What Are Saliency Maps In Deep Learning    Analytics India Magazine  Retrieved            

  Luo  Haoyan  Specia  Lucia                From Understanding to Utilization  A Survey on Explainability for Large Language Models   arXiv             cs CL  

  Ananthaswamy  Anil                How Do Machines  Grok  Data    Quanta Magazine  Retrieved            

  Olah  Chris  June             Mechanistic Interpretability  Variables  and the Importance of Interpretable Bases   www transformer circuits pub  Retrieved            

  Mittal  Aayush                Understanding Sparse Autoencoders  GPT    amp  Claude          An In Depth Technical Exploration   Unite AI  Retrieved            

  Ropek  Lucas                New Anthropic Research Sheds Light on AI s  Black Box    Gizmodo  Retrieved            

  Perrigo  Billy                Artificial Intelligence Is a  Black Box   Maybe Not For Long   Time  Retrieved            

  Barber  Gregory   Inside the  Black Box  of a Neural Network   Wired  ISSN                 Retrieved            

  Fagan  L  M   Shortliffe  E  H   Buchanan  B  G           Computer based medical decision making  from MYCIN to VM   Automedica                

  Clancey  William         Knowledge Based Tutoring  The GUIDON Program  Cambridge  Massachusetts  The MIT Press 

  Brown  John S   Burton  R  R   De Kleer  Johan          Pedagogical  natural language  and knowledge engineering techniques in SOPHIE I  II  and III   Intelligent Tutoring Systems  Academic Press  ISBN                    

  Bareiss  Ray  Porter  Bruce  Weir  Craig  Holte  Robert          Protos  An Exemplar Based Learning Apprentice   Machine Learning  Vol          Morgan Kaufmann Publishers Inc  pp                ISBN                    

  a b Bareiss  Ray         Exemplar Based Knowledge Acquisition  A Unified Approach to Concept Representation  Classification  and Learning  Perspectives in Artificial Intelligence  Academic Press 

  Van Lent  M   Fisher  W   Mancuso  M   July         An explainable artificial intelligence system for small unit tactical behavior   Proceedings of the National Conference on Artificial Intelligence  San Jose  CA  AAAI Press  pp                ISBN                 

  Russell  Stuart  Norvig  Peter         Artificial Intelligence  A Modern Approach  Prentice Hall Series in Artificial Intelligence  Second      ed    Upper Saddle River  New Jersey  Prentice Hall  Pearson Education  ISBN                    

  Forbus  Kenneth  De Kleer  Johan         Building Problem Solvers  Cambridge  Massachusetts  The MIT Press  ISBN                    

  Tickle  A  B   Andrews  R   Golea  M   Diederich  J   November         The truth will come to light  directions and challenges in extracting the knowledge embedded within trained artificial neural networks   IEEE Transactions on Neural Networks                    doi                    ISSN                 PMID                S CID               

   Accenture Unveils Tool to Help Companies Insure Their AI Is Fair   Bloomberg com  June       Retrieved   August      

  Minsky  et al    The Society of Intelligent Veillance  IEEE ISTAS      pages       

  Mukherjee  Siddhartha     March         A I  Versus M D   The New Yorker  Retrieved    January      

  Csisz r  Orsolya  Csisz r  G bor  Dombi  J zsef                Interpretable neural networks based on continuous valued logic and multicriteria decision operators   Knowledge Based Systems               arXiv             doi         j knosys              ISSN                

  Doshi Velez  Finale  Kim  Been                Towards A Rigorous Science of Interpretable Machine Learning   arXiv             stat ML  

  Abdollahi  Behnoush  and Olfa Nasraoui           Explainable Restricted Boltzmann Machines for Collaborative Filtering   arXiv             stat ML    cite arXiv     CS  maint  multiple names  authors list  link 

  Dombi  J zsef  Csisz r  Orsolya         Explainable Neural Networks Based on Fuzzy Logic and Multi criteria Decision Tools  Studies in Fuzziness and Soft Computing  Vol            doi                            ISBN                         ISSN                 S CID                

  Bach  Sebastian  Binder  Alexander  Montavon  Gr goire  Klauschen  Frederick  M ller  Klaus Robert  Samek  Wojciech               Suarez  Oscar Deniz  ed     On Pixel Wise Explanations for Non Linear Classifier Decisions by Layer Wise Relevance Propagation   PLOS ONE          e         Bibcode     PLoSO         B  doi         journal pone          ISSN                 PMC               PMID               

  Sample  Ian    November         Computer says no  why making AIs fair  accountable and transparent is crucial   The Guardian  Retrieved   August      

  Martens  David  Provost  Foster          Explaining data driven document classifications   PDF   MIS Quarterly             doi          MISQ               S CID               

    Why Should I Trust You     Proceedings of the   nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining   doi                          S CID                  cite journal    Cite journal requires       journal   help 

  Lundberg  Scott M  Lee  Su In         Guyon  I   Luxburg  U  V   Bengio  S   Wallach  H   eds     A Unified Approach to Interpreting Model Predictions   PDF   Advances in Neural Information Processing Systems     Curran Associates  Inc   pp                  arXiv             retrieved           

  Carter  Brandon  Mueller  Jonas  Jain  Siddhartha  Gifford  David                What made you do this  Understanding black box decisions with sufficient input subsets   The   nd International Conference on Artificial Intelligence and Statistics          

  Shrikumar  Avanti  Greenside  Peyton  Kundaje  Anshul                Learning Important Features Through Propagating Activation Differences   International Conference on Machine Learning            

   Axiomatic attribution for deep networks   Proceedings of the   th International Conference on Machine Learning   Volume      dl acm org  Icml                  August       Retrieved            

  Xu  Feiyu  Uszkoreit  Hans  Du  Yangzhou  Fan  Wei  Zhao  Dongyan  Zhu  Jun         Tang  Jie  Kan  Min Yen  Zhao  Dongyan  Li  Sujian  eds     Explainable AI  A Brief Survey on History  Research Areas  Approaches and Challenges   Natural Language Processing and Chinese Computing  vol              Cham  Springer International Publishing  pp                doi                               ISBN                         retrieved           

  Le Merrer  Erwan  Tr dan  Gilles  September         Remote explainability faces the bouncer problem   Nature Machine Intelligence                  arXiv             doi         s               z  ISSN                 S CID                

  Aivodji  Ulrich  Arai  Hiromi  Fortineau  Olivier  Gambs  S bastien  Hara  Satoshi  Tapp  Alain                Fairwashing  the risk of rationalization   International Conference on Machine Learning  PMLR           arXiv            

  Singh  Chandan  Nasseri  Keyan  Tan  Yan Shuo  Tang  Tiffany  Yu  Bin    May         imodels  a python package for fitting interpretable models   Journal of Open Source Software                Bibcode     JOSS          S  doi          joss        ISSN                 S CID                

  Vidal  Thibaut  Schiffer  Maximilian          Born Again Tree Ensembles   International Conference on Machine Learning       PMLR             arXiv            

  Ustun  Berk  Rudin  Cynthia    March         Supersparse linear integer models for optimized medical scoring systems   Machine Learning                    doi         s                  ISSN                 S CID                

  Bostrom  N    amp  Yudkowsky  E          The ethics of artificial intelligence  The Cambridge Handbook of Artificial Intelligence          

   FAT  Conference  

   Computer programs recognise white men better than black women   The Economist        Retrieved   August      

  Olah  Chris  Cammarata  Nick  Schubert  Ludwig  Goh  Gabriel  Petrov  Michael  Carter  Shan     March         Zoom In  An Introduction to Circuits   Distill         e           doi          distill            ISSN                

  Li  Yixuan  Yosinski  Jason  Clune  Jeff  Lipson  Hod  Hopcroft  John    December         Convergent Learning  Do different neural networks learn the same representations    Feature Extraction  Modern Questions and Challenges  PMLR          

  Hendricks  Lisa Anne  Akata  Zeynep  Rohrbach  Marcus  Donahue  Jeff  Schiele  Bernt  Darrell  Trevor          Generating Visual Explanations   Computer Vision   ECCV       Lecture Notes in Computer Science  Vol             Springer International Publishing  pp             arXiv             doi                              ISBN                         S CID               

  Koh  Pang Wei  Liang  Percy     July         Understanding Black box Predictions via Influence Functions   International Conference on Machine Learning  PMLR             arXiv            

  Gouverneur  Philip  Li  Fr d ric  Shirahama  Kimiaki  Luebke  Luisa  Adamczyk  Wac aw M   Szikszay  Tibor M   Luedtke  Kerstin  Grzegorzek  Marcin                Explainable Artificial Intelligence  XAI  in Pain Research  Understanding the Role of Electrodermal Activity for Automated Pain Recognition   Sensors                Bibcode     Senso         G  doi         s          ISSN                 PMC               PMID               

   IJCAI      Workshop on Explainable Artificial Intelligence  XAI    PDF   Earthlink  IJCAI  Archived from the original  PDF  on   April       Retrieved    July      

  Explainable Artificial Intelligence  First World Conference  xAI       Lisbon  Portugal  July              Proceedings  Parts I II III  Communications in Computer and Information Science  Vol             springer        doi                            ISBN                        

  Explainable Artificial Intelligence  Second World Conference  xAI       Valletta  Malta  July              Proceedings  Part I II III IV  Communications in Computer and Information Science  Vol             springer        doi                            ISBN                        

  Kahn  Jeremy     December         Artificial Intelligence Has Some Explaining to Do   Bloomberg Businessweek  Retrieved    December      

  a b c Bhatt  Umang  Xiang  Alice  Sharma  Shubham  Weller  Adrian  Taly  Ankur  Jia  Yunhan  Ghosh  Joydeep  Puri  Richir  M F  Moura  Jos   Eckersley  Peter          Explainable Machine Learning in Deployment   Proceedings of the      Conference on Fairness  Accountability  and Transparency  pp                doi                          ISBN                     S CID                

  a b c Burrel  Jenna          How the machine  thinks   Understanding opacity in machine learning algorithms   Big Data  amp  Society         doi                           S CID               

  Veale  Michael  Van Kleek  Max  Binns  Reuben          Fairness and Accountability Design Needs for Algorithmic Support in High Stakes Public Sector Decision Making   Proceedings of the      CHI Conference on Human Factors in Computing Systems  Vol           pp             doi                          ISBN                     S CID              

  Yang  Wenli  Wei  Yuchen  Wei  Hanyu  Chen  Yanyu  Huang  Guan  Li  Xiang  Li  Renjie  Yao  Naimeng  Wang  Xinyi  Gu  Xiaotong  Amin  Muhammad Bilal  Kang  Byeong                Survey on Explainable AI  From Approaches  Limitations and Applications Aspects   Human Centric Intelligent Systems                  doi         s                y  ISSN                

  Hu  Tongxi  Zhang  Xuesong  Bohrer  Gil  Liu  Yanlan  Zhou  Yuyu  Martin  Jay  LI  Yang  Zhao  Kaiguang          Crop yield prediction via explainable AI and interpretable machine learning  Dangers of black box models for evaluating climate change impacts on crop yield   Agricultural and Forest Meteorology               doi         j agrformet              S CID                

  a b Cheng  Hao Fei  Wang  Ruotang  Zhang  Zheng  O Connell  Fiona  Gray  Terrance  Harper  F  Maxwell  Zhu  Haiyi         Explaining Decision Making Algorithms through UI  Strategies to Help Non Expert Stakeholders  Proceedings of the      CHI Conference on Human Factors in Computing Systems  Vol            pp             doi                          ISBN                     S CID                

  Liel  Yotam  Zalmanson  Lior  August         Turning Off Your Better Judgment   Conformity to Algorithmic Recommendations   Academy of Management Proceedings            doi         AMPROC         bp  ISSN                 S CID                

  a b McCoy  Liam G   Brenna  Connor T  A   Chen  Stacy S   Vold  Karina  Das  Sunit                Believing in black boxes  machine learning for healthcare does not need explainability to be evidence based   Journal of Clinical Epidemiology       Online ahead of print            doi         j jclinepi              ISSN                 PMID                S CID                

  Ghassemi  Marzyeh  Oakden Rayner  Luke  Beam  Andrew L                 The false hope of current approaches to explainable artificial intelligence in health care   The Lancet Digital Health          e      e     doi         S                      ISSN                 PMID                S CID                

  Sarkar  Advait          Is explainable AI a race against model complexity    PDF   Workshop on Transparency and Explanations in Smart Systems  TeXSS   in Conjunction with ACM Intelligent User Interfaces  IUI                 arXiv                    via CEUR Workshop Proceedings 

  Procaccia  Ariel D          Laslier  Jean Fran ois  Moulin  Herv   Sanver  M  Remzi  Zwicker  William S   eds     Axioms Should Explain Solutions   The Future of Economic Design  The Continuing Development of a Field as Envisioned by Its Researchers  Studies in Economic Design  Cham  Springer International Publishing  pp                doi                               ISBN                         retrieved           

  Cailloux  Olivier  Endriss  Ulle                Arguing about Voting Rules   Proceedings of the      International Conference on Autonomous Agents  amp  Multiagent Systems  AAMAS      Richland  SC  International Foundation for Autonomous Agents and Multiagent Systems           ISBN                        

  Peters  Dominik  Procaccia  Ariel D  Psomas  Alexandros  Zhou  Zixin          Explainable Voting   Advances in Neural Information Processing Systems      Curran Associates  Inc             

  Yang  Joshua C   Hausladen  Carina I   Peters  Dominik  Pournaras  Evangelos  Regula H enggli Fricker  Helbing  Dirk          Designing Digital Voting Systems for Citizens  Achieving Fairness and Legitimacy in Participatory Budgeting   Digital Government  Research and Practice               arXiv             doi                 

  Nizri  Meir  Hazon  Noam  Azaria  Amos                Explainable Shapley Based Allocation  Student Abstract    Proceedings of the AAAI Conference on Artificial Intelligence                        doi         aaai v  i          ISSN                 S CID                


External links edit 
 the World Conference on eXplainable Artificial Intelligence  
 ACM Conference on Fairness  Accountability  and Transparency  FAccT   
Mazumdar  Dipankar  Neto  M rio Popolin  Paulovich  Fernando V           Random Forest similarity maps  A Scalable Visual Representation for Global and Local Interpretation   Electronics                 doi         electronics         
Park  Dong Huk  Hendricks  Lisa Anne  Akata  Zeynep  Schiele  Bernt  Darrell  Trevor  Rohrbach  Marcus                Attentive Explanations  Justifying Decisions and Pointing to the Evidence   arXiv             cs CV  
 Explainable AI  Making machines understandable for humans   Explainable AI  Making machines understandable for humans  Retrieved            
 Explaining How End to End Deep Learning Steers a Self Driving Car   Parallel Forall              Retrieved            
Knight  Will                DARPA is funding projects that will try to open up AI s black boxes   MIT Technology Review  Retrieved            
Alvarez Melis  David  Jaakkola  Tommi S                 A causal framework for explaining the predictions of black box sequence to sequence models   arXiv             cs LG  
 Similarity Cracks the Code Of Explainable AI   simMachines              Retrieved            
vteDifferentiable computingGeneral
Differentiable programming
Information geometry
Statistical manifold
Automatic differentiation
Neuromorphic computing
Pattern recognition
Ricci calculus
Computational learning theory
Inductive bias
Hardware
IPU
TPU
VPU
Memristor
SpiNNaker
Software libraries
TensorFlow
PyTorch
Keras
scikit learn
Theano
JAX
Flux jl
MindSpore

 Portals
Computer programming
Technology






Retrieved from  https   en wikipedia org w index php title Explainable artificial intelligence amp oldid