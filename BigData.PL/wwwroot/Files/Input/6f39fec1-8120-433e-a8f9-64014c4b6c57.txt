text generating language model
See also  Generative pre trained transformer        Foundation models



Generative Pre trained Transformer    GPT   Original author s OpenAI           Initial releaseMay           publication   June           OA API beta Repositorygithub com openai gpt   
PredecessorGPT  SuccessorGPT    GPT  Type
Large language model
Generative pre trained transformer
Foundation model
LicenseproprietaryWebsiteopenai com blog openai api
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
Generative Pre trained Transformer    GPT    is a large language model released by OpenAI in      
Like its predecessor  GPT    it is a decoder only            transformer model of deep neural network  which supersedes recurrence and convolution based architectures with a technique known as  attention              This attention mechanism allows the model to focus selectively on segments of input text it predicts to be most relevant             GPT   has     billion parameters  each with    bit precision  requiring    GB of storage since each parameter occupies   bytes  It has a context window size of      tokens  and has demonstrated strong  zero shot  and  few shot  learning abilities on many tasks            
On September           Microsoft announced that it had licensed GPT   exclusively  Others can still receive output from its public API  but only Microsoft has access to the underlying model            


Background edit 
According to The Economist  improved algorithms  more powerful computers  and a recent increase in the amount of digitized material have fueled a revolution in machine learning  New techniques in the     s resulted in  rapid improvements in tasks   including manipulating language            
Software models are trained to learn by using thousands or millions of examples in a  structure          loosely based on the neural architecture of the brain              One architecture used in natural language processing  NLP  is a neural network based on a deep learning model that was introduced in      the transformer architecture             There are a number of NLP systems capable of processing  mining  organizing  connecting and contrasting textual input  as well as correctly answering questions            
On June           OpenAI researchers and engineers published a paper introducing the first generative pre trained transformer  GPT  a type of generative large language model that is pre trained with an enormous and diverse text corpus in datasets  followed by discriminative fine tuning to focus on a specific task  GPT models are transformer based deep learning neural network architectures  Previously  the best performing neural NLP models commonly employed supervised learning from large amounts of manually labeled data  which made it prohibitively expensive and time consuming to train extremely large language models             The first GPT model was known as  GPT     and it was followed by  GPT    in February       Created as a direct scale up of its predecessor  GPT   had both its parameter count and dataset size increased by a factor of     It had     billion parameters  and was trained on a dataset of   million web pages             
In February       Microsoft introduced its Turing Natural Language Generation  T NLG   which they claimed was  largest language model ever published at    billion parameters               It performed better than any other language model at a variety of tasks  including summarizing texts and answering questions 

Training and capabilities edit 

A sample student essay about pedagogy written by GPT  

The construct of  learning styles  is problematic because it fails to account for the processes through which learning styles are shaped  Some students might develop a particular learning style because they have had particular experiences  Others might develop a particular learning style by trying to accommodate to a learning environment that was not well suited to their learning needs  Ultimately  we need to understand the interactions among learning styles and environmental and personal factors  and how these shape how we learn and the kinds of learning we experience 


  Text generated by Mike Sharples            

On May           an arXiv preprint by a group of    engineers and researchers at OpenAI described the achievement and development of GPT    a third generation  state of the art language model                          The team increased the capacity of GPT   by over two orders of magnitude from that of its predecessor  GPT                making GPT   the largest non sparse language model to date                                              Because GPT   is structurally similar to its predecessors             its greater accuracy is attributed to its increased capacity and greater number of parameters              GPT   s capacity is ten times larger than that of Microsoft s Turing NLG  the next largest NLP model known at the time             
Lambdalabs estimated a hypothetical cost of around      million US dollars and     years to train GPT   on a single GPU in                   with lower actual training time by using more GPUs in parallel 
Sixty percent of the weighted pre training dataset for GPT   comes from a filtered version of Common Crawl consisting of     billion byte pair encoded tokens  Fuzzy deduplication used Apache Spark s MinHashLSH                                 Other sources are    billion tokens from WebText  representing     of the weighted total     billion tokens from Books  representing        billion tokens from Books  representing     and   billion tokens from Wikipedia representing                                    GPT   was trained on hundreds of billions of words and is also capable of coding in CSS  JSX  and Python  among others      citation needed     


GPT   training data                               


Dataset

  tokens

Proportion within training


Common Crawl

    billion

   


WebText 

   billion

   


Books 

   billion

  


Books 

   billion

  


Wikipedia

  billion

  

Since GPT   s training data was all encompassing  it does not require further training for distinct language tasks      citation needed      The training data contains occasional toxic language and GPT   occasionally generates toxic language as a result of mimicking its training data  A study from the University of Washington found that GPT   produced toxic language at a toxicity level comparable to the similar natural language processing models of GPT   and CTRL  OpenAI has implemented several strategies to limit the amount of toxic language generated by GPT    As a result  GPT   produced less toxic language compared to its predecessor model  GPT    although it produced both more generations and a higher toxicity of toxic language compared to CTRL Wiki  a language model trained entirely on Wikipedia data             
On June           OpenAI announced that users could request access to its user friendly GPT   API a  machine learning toolset  to help OpenAI  explore the strengths and limits  of this new technology                          The invitation described how this API had a general purpose  text in  text out  interface that can complete almost  any English language task   instead of the usual single use case              According to one user  who had access to a private early release of the OpenAI GPT   API  GPT   was  eerily good  at writing  amazingly coherent text  with only a few simple prompts              In an initial experiment    US subjects were asked to judge if short      word articles were written by humans or GPT    The participants judged correctly     of the time  doing only slightly better than random guessing            
On November           OpenAI announced that enough safeguards had been implemented that access to its API would be unrestricted              OpenAI provided developers with a content moderation tool that helps them abide by OpenAI s content policy              On January           OpenAI announced that its newest GPT   language models  collectively referred to as InstructGPT  were now the default language model used on their API  According to OpenAI  InstructGPT produced content that was better aligned to user intentions by following instructions better  generating fewer made up facts  and producing somewhat less toxic content             
Because GPT   can  generate news articles which human evaluators have difficulty distinguishing from articles written by humans               GPT   has the  potential to advance both the beneficial and harmful applications of language models                                   In their May          paper  the researchers described in detail the potential  harmful effects of GPT                which include  misinformation  spam  phishing  abuse of legal and governmental processes  fraudulent academic essay writing and social engineering pretexting              The authors draw attention to these dangers to call for research on risk mitigation                                 
GPT   is capable of performing zero shot and few shot learning  including one shot             
In June       Almira Osmanovic Thunstr m wrote that GPT   was the primary author on an article on itself  that they had submitted it for publication              and that it had been pre published while waiting for completion of its review             

GPT   models edit 
There are many models in the GPT   family  some serving different purposes than others  In the initial research paper published by OpenAI  they mentioned   different sizes of the main GPT   model 



Model name

Parameters

API name


GPT   Small

    M

n a


GPT   Medium

    M

ada


GPT   Large

    M

n a


GPT   XL

    B

babbage


GPT      B

    B

n a


GPT      B

    B

curie


GPT     B

  B

n a


GPT      B

   B

davinci

Half of the models are accessible through the API  namely GPT   medium  GPT   xl  GPT      B and GPT      b  which are referred to as ada  babbage  curie and davinci respectively  While the size of the API models was not originally disclosed by OpenAI  EleutherAI announced the mapping between model sizes and API names in May                   These model sizes were later confirmed by OpenAI              but the sizes of subsequent models have not been disclosed 



Model

Parameters

Description

Series


ada

    M

Capable of very simple tasks  usually the fastest model in the GPT   series  and lowest cost 

Base GPT  


babbage
babbage    


    B

Capable of straightforward tasks  very fast  and lower cost 

Base GPT  


curie

   B

Very capable  but faster and lower cost than Davinci 

Base GPT  


davinci
davinci    


    B

Most capable GPT   model  Can do any task the other models can do  often with higher quality 

Base GPT  


text ada    

    M

Capable of very simple tasks  usually the fastest model in the GPT   series  and lowest cost 

InstructGPT


text babbage    

   B

Capable of straightforward tasks  very fast  and lower cost 

InstructGPT


text curie    

   B

Very capable  faster and lower cost than Davinci 

InstructGPT


text davinci    

   B

Older version of the most capable model in the GPT   series  Can perform any task the other GPT   models can  often with less context 

InstructGPT


text davinci    
code davinci    


Undisclosed

Similar capabilities to text davinci     but trained with supervised fine tuning instead of reinforcement learning

GPT    


text davinci    

Undisclosed

Can do any language task with better quality  longer output  and consistent instruction following than the curie  babbage  or ada models  Also supports inserting completions within text 

GPT    


gpt     turbo
gpt     turbo instruct
gpt     turbo   k


Undisclosed

Most capable and cost effective  fastest  GPT     model and optimized for chat at     th the cost of text davinci     

GPT    

 GPT     edit 
Generative Pre trained Transformer      GPT     Original author s OpenAI           Initial releaseMarch                      years ago                   Preview releasegpt     turbo     
     January                       months ago                   
RepositoryN APredecessorGPT  SuccessorGPT  GPT  o miniType
Large language model
Generative pre trained transformer
Foundation model
LicensePrivativeWebsiteN A
Generative Pre trained Transformer      GPT      is a sub class of GPT   Models created by OpenAI in      
On March           OpenAI made available new versions of GPT   and Codex in its API with edit and insert capabilities under the names  text davinci      and  code davinci                   These models were described as more capable than previous versions and were trained on data up to June                   On November           OpenAI introduced text davinci                  On November           OpenAI began referring to these models as belonging to the  GPT      series              and released ChatGPT  which was fine tuned from a model in the GPT     series              OpenAI does not include GPT     in GPT               

Models edit 
There are three models             

Chat
gpt     turbo
Text completion
text davinci    
text davinci    
GPT     with browsing edit 
On April           OpenAI introduced a new variant of its GPT     series model  known as GPT     with Browsing  ALPHA               This updated model was described to build upon the capabilities of its predecessors  text davinci      and  code davinci                   The GPT     with Browsing  ALPHA  model incorporated the ability to access and browse online information  This has led to more accurate and up to date responses to user queries             
The GPT     with Browsing  ALPHA  model has been trained on data up to September       giving it more information compared to previous GPT     models  which were trained on data up until June       The model attempted to provide developers and users with an advanced natural language processing tool that can effectively retrieve and synthesize online information             
To enable browsing capabilities  OpenAI implemented a new API that allows the GPT     with Browsing  ALPHA  model to access selected online resources during operation              This feature allows users to ask questions or request information with the expectation that the model will deliver updated  accurate  and relevant answers based on the latest online sources available to it 
On April           OpenAI made the GPT     with Browsing  ALPHA  model publicly available to GPT Plus users  This allowed more people to access to its new features             

InstructGPT edit 
InstructGPT is a fine tuned version of GPT     trained on a dataset of human written instructions             

Reception edit 
Applications edit 
GPT    specifically the Codex model  was the basis for GitHub Copilot  a code completion and generation software that can be used in various code editors and IDEs                         
GPT   is used in certain Microsoft products to translate conventional language into formal computer code                         
GPT   has been used in CodexDB             to generate query specific code for SQL processing 
GPT   has been used by Jason Rohrer in a retro themed chatbot project named  Project December   which is accessible online and allows users to converse with several AIs using GPT   technology             
GPT   was used by The Guardian to write an article about AI being harmless to human beings  It was fed some ideas and produced eight different essays  which were ultimately merged into one article             
GPT   was used in AI Dungeon  which generates text based adventure games  Later it was replaced by a competing model after OpenAI changed their policy regarding generated content                         
GPT   is used to aid in writing copy and other marketing materials             
A      study from Drexel University suggested that GPT   based systems could be used to screen for early signs of Alzheimer s disease                         
Reviews edit 
In a July      review in The New York Times  Farhad Manjoo said that GPT   s ability to generate computer code  poetry  and prose is not just  amazing    spooky   and  humbling   but also  more than a little terrifying              
Daily Nous presented a series of articles by nine philosophers on GPT                Australian philosopher David Chalmers described GPT   as  one of the most interesting and important AI systems ever produced              
A review in Wired said that GPT   was  provoking chills across Silicon Valley              
The National Law Review said that GPT   is an  impressive step in the larger process   with OpenAI and others finding  useful applications for all of this power  while continuing to  work toward a more general intelligence              
An article in the MIT Technology Review  co written by Deep Learning critic Gary Marcus              stated that GPT   s  comprehension of the world is often seriously off  which means you can never really trust what it says               According to the authors  GPT   models relationships between words without having an understanding of the meaning behind each word 
Jerome Pesenti  head of the Facebook AI lab  said GPT   is  unsafe   pointing to the sexist  racist and other biased and negative language generated by the system when it was asked to discuss Jews  women  black people  and the Holocaust             
Nabla  a French start up specializing in healthcare technology  tested GPT   as a medical chatbot  though OpenAI itself warned against such use  As expected  GPT   showed several limitations  For example  while testing GPT   responses about mental health issues  the AI advised a simulated patient to commit suicide             
Noam Chomsky expressed his skepticism about GPT   s scientific value   It s not a language model  It works just as well for impossible languages as for actual languages  It is therefore refuted  if intended as a language model  by normal scientific criteria        Perhaps it s useful for some purpose  but it seems to tell us nothing about language or cognition generally              
Luciano Floridi and Massimo Chiriatti highlighted the risk of  cheap production of good  semantic artefacts              
OpenAI s Sam Altman himself criticized what he called  GPT   hype   acknowledging GPT    has serious weakness and sometimes makes very silly mistakes    AI is going to change the world  but GPT   is just a very early glimpse              
Criticism edit 
GPT   s builder  OpenAI  was initially founded as a non profit in                   In       OpenAI broke from its usual open source standards by not publicly releasing GPT   s predecessor model  citing concerns that the model could facilitate the propagation of fake news  OpenAI eventually released a version of GPT   that was    of the original model s size              In the same year  OpenAI restructured to be a for profit company              In       Microsoft announced the company had exclusive licensing of GPT   for Microsoft s products and services following a multi billion dollar investment in OpenAI  The agreement permits OpenAI to offer a public facing API such that users can send text to GPT   to receive the model s output  but only Microsoft will have access to GPT   s source code            
Large language models  such as GPT    have come under criticism from a few of Google s AI ethics researchers for the environmental impact of training and storing the models  detailed in a paper co authored by Timnit Gebru and Emily M  Bender in                  
The growing     when       use of automated writing technologies based on GPT   and other language generators  has raised concerns regarding academic integrity             and raised the stakes of how universities and schools will gauge what constitutes academic misconduct such as plagiarism             
OpenAI s GPT series was built with data from the Common Crawl dataset              a conglomerate of copyrighted articles  internet posts  web pages  and books scraped from    million domains over a period of    years  TechCrunch reports this training data includes copyrighted material from the BBC  The New York Times  Reddit  the full text of online books  and more              In its response to a      Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation from the United States Patent and Trademark Office  USPTO   OpenAI argued that  Under current law  training AI systems  such as its GPT models  constitutes fair use   but that  given the lack of case law on point  OpenAI and other AI developers like us face substantial legal uncertainty and compliance costs              

See also edit 
BERT  language model 
Hallucination  artificial intelligence 
LaMDA
Gemini  language model 
Wu Dao
GPTZero
References edit 


  a b c d e f g h i j k l m Brown  Tom B   Mann  Benjamin  Ryder  Nick  Subbiah  Melanie  Kaplan  Jared  Dhariwal  Prafulla  Neelakantan  Arvind  Shyam  Pranav  Sastry  Girish  Askell  Amanda  Agarwal  Sandhini  Herbert Voss  Ariel  Krueger  Gretchen  Henighan  Tom  Child  Rewon  Ramesh  Aditya  Ziegler  Daniel M   Wu  Jeffrey  Winter  Clemens  Hesse  Christopher  Chen  Mark  Sigler  Eric  Litwin  Mateusz  Gray  Scott  Chess  Benjamin  Clark  Jack  Berner  Christopher  McCandlish  Sam  Radford  Alec  Sutskever  Ilya  Amodei  Dario  May             Language Models are Few Shot Learners   arXiv             cs CL  

  a b c Radford  Alec  Narasimhan  Karthik  Salimans  Tim  Sutskever  Ilya  June             Improving Language Understanding by Generative Pre Training   PDF   p           Archived  PDF  from the original on January           Retrieved July          

  Vaswani  Ashish  Shazeer  Noam  Parmar  Niki  Uszkoreit  Jakob  Jones  Llion  Gomez  Aidan N  Kaiser   ukasz  Polosukhin  Illia          Attention is All you Need   PDF   Advances in Neural Information Processing Systems      Curran Associates  Inc 

  Bahdanau  Dzmitry  Cho  Kyunghyun  Bengio  Yoshua  September            Neural Machine Translation by Jointly Learning to Align and Translate   arXiv            cs CL  

  a b Hao  Karen  September             OpenAI is giving Microsoft exclusive access to its GPT   language model   MIT Technology Review  Archived from the original on February          Retrieved September           The companies say OpenAI will continue to offer its public facing API  which allows chosen users to send text to GPT   or OpenAI s other models and receive its output  Only Microsoft  however  will have access to GPT   s underlying code  allowing it to embed  repurpose  and modify the model as it pleases 

  a b  An understanding of AI s limitations is starting to sink in   The Economist  June           ISSN                 Archived from the original on July           Retrieved July          

  Polosukhin  Illia  Kaiser  Lukasz  Gomez  Aidan N   Jones  Llion  Uszkoreit  Jakob  Parmar  Niki  Shazeer  Noam  Vaswani  Ashish  June             Attention Is All You Need   arXiv             cs CL  

   Natural Language Processing   Archived from the original on August           Retrieved July          

   Archived copy   PDF   Archived  PDF  from the original on February          Retrieved April            cite web     CS  maint  archived copy as title  link 

  Sterling  Bruce  February             Web Semantics  Microsoft Project Turing introduces Turing Natural Language Generation  T NLG    Wired  ISSN                 Archived from the original on November          Retrieved July          

  Marche  Stephen  December            The College Essay Is Dead   The Atlantic  Archived from the original on January           Retrieved December         

  a b c d Sagar  Ram  June            OpenAI Releases GPT    The Largest Model So Far   Analytics India Magazine  Archived from the original on August          Retrieved July          

   Language Models are Unsupervised Multitask Learners   PDF   openai com  Archived  PDF  from the original on December           Retrieved December          GPT    is a    B parameter Transformer

  Shead  Sam  July             Why everyone is talking about the A I  text generator released by an Elon Musk backed lab   CNBC  Archived from the original on July           Retrieved July           Four preprints were released between May    and July          

  Ray  Tiernan  June            OpenAI s gigantic GPT   hints at the limits of language models for AI   ZDNet  Archived from the original on June          Retrieved July          

  Li  Chuan  June           OpenAI s GPT   Language Model  A Technical Overview  archived from the original on March           retrieved March         

  Gehman  Samuel  Gururangan  Suchin  Sap  Maarten  Choi  Yejin  Smith  Noah A   November               REALTOXICITYPROMPTS  Evaluating Neural Toxic Degeneration in Language Models  Association for Computational Linguistics  pp                  arXiv           

  a b  OpenAI API   OpenAI  June           Archived from the original on June           Retrieved July          

  Coldewey  Devin  June             OpenAI makes an all purpose API for its text based AI capabilities   TechCrunch  Archived from the original on October           Retrieved July           If you ve ever wanted to try out OpenAI s vaunted machine learning toolset  it just got a lot easier  The company has released an API that lets developers call its AI tools in on  virtually any English language task  

  Arram  July            GPT    An AI that s eerily good at writing almost anything   Arram Sabeti  Archived from the original on July           Retrieved July          

   OpenAI s API Now Available with No Waitlist   OpenAI  November           Archived from the original on November          Retrieved November         

   OpenAI API   beta openai com  Archived from the original on December           Retrieved November         

   Aligning Language Models to Follow Instructions   OpenAI  January           Archived from the original on November          Retrieved November         

  Thunstr m  Almira Osmanovic  June             We Asked GPT   to Write an Academic Paper about Itself   Then We Tried to Get It Published   Scientific American  Archived from the original on June           Retrieved June          

  Transformer  Gpt Generative Pretrained  Thunstr m  Almira Osmanovic  Steingrimsson  Steinn  June             Can GPT   write an academic paper on itself  with minimal human input    Archive ouverte HAL  in French   Archived from the original on June           Retrieved June          

  Gao  Leo  May             On the Sizes of OpenAI API Models   EleutherAI Blog  EleutherAI  Retrieved November          

   Model index for researchers   OpenAI  Retrieved November          

   New GPT   Capabilities  Edit  amp  Insert   OpenAI  March           Archived from the original on January           Retrieved January          

  a b  OpenAI API   platform openai com  Archived from the original on March           Retrieved March          

   Check out OpenAI s new text davinci      Same underlying model as text davinci     but more aligned  Would love to hear feedback about it    Twitter   Archived from the original on March           Retrieved May         

   ChatGPT  Optimizing Language Models for Dialogue   OpenAI  November           Archived from the original on November           Retrieved January          

   OpenAI API   Archived from the original on March           Retrieved May         

   OpenAI API   Archived from the original on May          Retrieved May         

  a b c tingetici  April             Default  GPT      with browsing ALPHA    NEW Model showed up just now   r OpenAI  Archived from the original on April           Retrieved April          

   Introducing GPT     Series  text davinci     and code davinci     Models   OPEN AI  March           Archived from the original on March           Retrieved April          

  a b  GPT     with Browsing  ALPHA  Now Available for GPT Plus Users   OPEN AI  April           Archived from the original on March           Retrieved April          

  Gilson A  Safranek CW  Huang T  Socrates V  Chi L  Taylor RA  Chartash D  February         How Does ChatGPT Perform on the United States Medical Licensing Examination  USMLE   The Implications of Large Language Models for Medical Education and Knowledge Assessment   JMIR Med Educ     e       doi                PMC               PMID               

   OpenAI Codex   OpenAI  August           Archived from the original on February          Retrieved December          

  Thompson  Clive  March             How an AI Became My Code Writing Genie   Wired  Archived from the original on December           Retrieved December          

   Microsoft announced its first customer product features powered by GPT   and  Azure   The AI Blog  May           Archived from the original on May           Retrieved May          

  Vincent  James  May             Microsoft has built an AI powered autocomplete for code using GPT     The Verge  Archived from the original on December           Retrieved December          

   CodexDB   SQL Processing Powered by GPT     CodexDB   SQL Processing Powered by GPT    Archived from the original on December          Retrieved December         

  Fagone  Jason  July             The Jessica Simulation  Love and loss in the age of A I   San Francisco Chronicle  Archived from the original on July           Retrieved July          

  GPT    September            A robot wrote this entire article  Are you scared yet  human    GPT     The Guardian  ISSN                 Archived from the original on September          Retrieved September            cite news     CS  maint  numeric names  authors list  link 

   Update  Language Models and Dragon   Latitude blog  December          Archived from the original on April           Retrieved March          

   This Mystical Book Was Co Authored by a Disturbingly Realistic AI   www vice com        Archived from the original on December           Retrieved December          

  GPT    February                Prompt Examples in    Different Categories   GPT     GiPiTi Chat  Archived from the original on April          Retrieved February            cite news     CS  maint  numeric names  authors list  link 

   Can ChatGPT AI chatbot spot early stages of Alzheimer s    study   The Jerusalem Post        Archived from the original on February           Retrieved February          

  Agbavor  Felix  Liang  Hualou  December             Predicting dementia from spontaneous speech using large language models   PLOS Digital Health          e         doi         journal pdig          PMC               PMID                S CID                

  Manjoo  Farhad  July             How Do You Know a Human Wrote This    The New York Times  ISSN                 Archived from the original on October           Retrieved August         

  Weinberg  Justin  ed   July             Philosophers On GPT    updated with replies by GPT      Daily Nous  Archived from the original on October           Retrieved July          

  Chalmers  David  July            Weinberg  Justin  ed     GPT   and General Intelligence   Daily Nous  Philosophers On GPT    updated with replies by GPT     Archived from the original on August          Retrieved August         

  Simonite  Tom  July             Did a Person Write This Headline  or a Machine    Wired  ISSN                 Archived from the original on November          Retrieved July          

  Claypoole  Theodore  July             New AI Tool GPT   Ascends to New Peaks  But Proves How Far We Still Need to Travel   The National Law Review  Archived from the original on October           Retrieved August         

  Marcus  Gary  December            The deepest problem with deep learning   Medium  Archived from the original on August          Retrieved September          

  Marcus  Gary  Davis  Ernest  August             GPT    Bloviator  OpenAI s language generator has no idea what it s talking about   MIT Technology Review  Archived from the original on August           Retrieved August          

  Metz  Cade  November             Meet GPT    It Has Learned to Code  and Blog and Argue    The New York Times  ISSN                 Archived from the original on December          Retrieved November          

   Medical chatbot using OpenAI s GPT   told a fake patient to kill themselves   AI News  October           Archived from the original on January           Retrieved January         

  Chomsky on Terence McKenna  Sam Harris  GPT   Cryptocurrencies  Kierkegaard  Neuralink   amp  Hofstadter  March            Event occurs at          Archived from the original on April           Retrieved April          

  Floridi  Luciano  Chiriatti  Massimo  November            GPT    Its Nature  Scope  Limits  and Consequences   Minds and Machines                   doi         s                   S CID                

  Vincent  James  July             OpenAI s latest breakthrough is astonishingly powerful  but still fighting its flaws   The Verge  Archived from the original on July           Retrieved November         

  Olanoff  Drew  December             Artificial Intelligence Nonprofit OpenAI Launches With Backing From Elon Musk And Sam Altman   Tech Crunch  Archived from the original on October           Retrieved May          

  Hao  Karen  August             OpenAI has released the largest version yet of its fake news spewing AI   MIT Technology Review  Archived from the original on May          Retrieved May          

  Coldewey  Devin  March             OpenAI shifts from nonprofit to  capped profit  to attract capital   Tech Crunch  Archived from the original on January          Retrieved May          

  Bender  Emily M   Gebru  Timnit  McMillan Major  Angelina  Shmitchell  Shmargaret  March           On the Dangers of Stochastic Parrots  Can Language Models Be Too Big   FAccT      Proceedings of the      ACM Conference on Fairness  Accountability  and Transparency  pp                doi                         

  Mindzak  Michael  Eaton  Sarah Elaine   Artificial intelligence is getting better at writing  and universities should worry about plagiarism   The Conversation  Archived from the original on November          Retrieved November         

  Rogerson  Ann M   McCarthy  Grace  December         Using Internet based paraphrasing tools  Original work  patchwriting or facilitated plagiarism    International Journal for Educational Integrity                doi         s               y  ISSN                 S CID              

  Ver Meer  Dave   ChatGPT Statistics   NamePepper  Archived from the original on June          Retrieved June          

  Here are a few ways GPT   can go wrong  TechCrunch  Archived from the original on November           Retrieved November          

  Comment Regarding Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation  PDF   USPTO  Archived  PDF  from the original on October           Retrieved November          


vteOpenAIProductsChatbots
ChatGPT
in education
GPT Store
DALL E
ChatGPT Search
Sora
Whisper
GitHub Copilot
Foundationmodels
OpenAI Codex
Generative pre trained transformer
GPT  
GPT  
GPT  
GPT  
GPT  o
o 
o 
GPT    
GPT    
o 
Intelligentagents
ChatGPT Deep Research
Operator
PeopleSeniormanagementCurrent
Sam Altman
removal
Greg Brockman
Sarah Friar
Scott Schools
Former
Mira Murati
Emmett Shear
Board ofdirectorsCurrent
Sam Altman
Adam D Angelo
Sue Desmond Hellmann
Paul Nakasone
Adebayo Ogunlesi
Nicole Seligman
Fidji Simo
Lawrence Summers
Bret Taylor  chair 
Jakub Pachocki  chief scientist 
Former
Greg Brockman            
Reid Hoffman            
Will Hurd            
Holden Karnofsky            
Elon Musk            
Ilya Sutskever            
Helen Toner            
Shivon Zilis            
Joint ventures
Stargate LLC
Related
Apple Intelligence
AI Dungeon
AutoGPT
 Deep Learning 
LangChain
Microsoft Copilot
OpenAI Five
Transformer

 Category

vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects

vteGenerative AIConcepts
Autoencoder
Deep learning
Generative adversarial network
Generative pre trained transformer
Large language model
Neural network
Prompt engineering
Retrieval augmented generation
Reinforcement learning from human feedback
Self supervised learning
Transformer
Variational autoencoder
Vision transformer
Word embedding
ModelsText
Claude
DBRX
DeepSeek
ERNIE
Gemini
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Granite
Grok
Llama
Manus
Mistral Large
PanGu  
Qwen
Image
Aurora
DALL E
Firefly
Flux
GPT Image  
Ideogram
Imagen
Midjourney
Stable Diffusion
Speech
   ai
WaveNet
Video
Dream Machine
Gen  
Hailuo AI
Kling
Sora
Veo
VideoPoet
Music
Endel
Suno AI
Udio
Companies
   AI
Alibaba
Anthropic
Baichuan
Baidu
DeepSeek
ElevenLabs
Google DeepMind
Hugging Face
Kuaishou
Meta AI
MiniMax
Mistral AI
Moonshot AI
OpenAI
Runway
Stability AI
Synthesia
xAI
Zhipu AI

 Category
 Commons

vteExistential risk from artificial intelligenceConcepts
AGI
AI alignment
AI capability control
AI safety
AI takeover
Consequentialism
Effective accelerationism
Ethics of artificial intelligence
Existential risk from artificial intelligence
Friendly artificial intelligence
Instrumental convergence
Vulnerable world hypothesis
Intelligence explosion
Longtermism
Machine ethics
Suffering risks
Superintelligence
Technological singularity
Organizations
Alignment Research Center
Center for AI Safety
Center for Applied Rationality
Center for Human Compatible Artificial Intelligence
Centre for the Study of Existential Risk
EleutherAI
Future of Humanity Institute
Future of Life Institute
Google DeepMind
Humanity 
Institute for Ethics and Emerging Technologies
Leverhulme Centre for the Future of Intelligence
Machine Intelligence Research Institute
OpenAI
People
Scott Alexander
Sam Altman
Yoshua Bengio
Nick Bostrom
Paul Christiano
Eric Drexler
Sam Harris
Stephen Hawking
Dan Hendrycks
Geoffrey Hinton
Bill Joy
Shane Legg
Elon Musk
Steve Omohundro
Huw Price
Martin Rees
Stuart J  Russell
Jaan Tallinn
Max Tegmark
Frank Wilczek
Roman Yampolskiy
Eliezer Yudkowsky
Other
Statement on AI risk of extinction
Human Compatible
Open letter on artificial intelligence       
Our Final Invention
The Precipice
Superintelligence  Paths  Dangers  Strategies
Do You Trust This Computer 
Artificial Intelligence Act
 Category





Retrieved from  https   en wikipedia org w index php title GPT   amp oldid