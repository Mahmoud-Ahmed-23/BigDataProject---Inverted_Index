Machine learning methods using multiple input modalities
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
Multimodal learning is a type of deep learning that integrates and processes multiple types of data  referred to as  modalities  such as text  audio  images  or video  This integration allows for a more holistic understanding of complex data  improving model performance in tasks like visual question answering  cross modal retrieval             text to image generation             aesthetic ranking             and image captioning            
Large multimodal models  such as Google Gemini and GPT  o  have become increasingly popular since       enabling increased versatility and a broader understanding of real world phenomena            


Motivation edit 
Data usually comes with different modalities which carry different information  For example  it is very common to caption an image to convey the information not presented in the image itself  Similarly  sometimes it is more straightforward to use an image to describe information which may not be obvious from text  As a result  if different words appear in similar images  then these words likely describe the same thing  Conversely  if a word is used to describe seemingly dissimilar images  then these images may represent the same object  Thus  in cases dealing with multi modal data  it is important to use a model which is able to jointly represent the information such that the model can capture the combined information from different modalities 

Multimodal transformers edit 
This section is an excerpt from Transformer  deep learning architecture    Multimodality  edit 
Transformers can also be used adapted for modalities  input or output  beyond just text  usually by finding a way to  tokenize  the modality 
Multimodal models can either be trained from scratch  or by finetuning  A      study found that Transformers pretrained only on natural language can be finetuned on only       of parameters and become competitive with LSTMs on a variety of logical and visual tasks  demonstrating transfer learning             The LLaVA was a vision language model composed of a language model  Vicuna   B             and a vision model  ViT L      connected by a linear layer  Only the linear layer is finetuned            
Vision transformers            adapt the transformer to computer vision by breaking down input images as a series of patches  turning them into vectors  and treating them like tokens in a standard transformer 
Conformer             and later Whisper             follow the same pattern for speech recognition  first turning the speech signal into a spectrogram  which is then treated like an image  i e  broken down into a series of patches  turned into vectors and treated like tokens in a standard transformer 
Perceivers                         are a variant of Transformers designed for multimodality 

For image generation  notable architectures are DALL E           Parti                     Phenaki                     and Muse                     Unlike later models  DALL E is not a diffusion model  Instead  it uses a decoder only Transformer that autoregressively generates a text  followed by the token representation of an image  which is then converted by a variational autoencoder to an image              Parti is an encoder decoder Transformer  where the encoder processes a text prompt  and the decoder generates a token representation of an image              Muse is an encoder only Transformer that is trained to predict masked image tokens from unmasked image tokens  During generation  all input tokens are masked  and the highest confidence predictions are included for the next iteration  until all tokens are predicted              Phenaki is a text to video model  It is a bidirectional masked transformer conditioned on pre computed text tokens  The generated tokens are then decoded to a video             
Multimodal large language models edit 
This section is an excerpt from Large language model   Multimodality  edit 
Multimodality means  having several modalities   and a  modality  refers to a type of input or output  such as video  image  audio  text  proprioception  etc              There have been many AI models trained specifically to ingest one modality and output another modality  such as AlexNet for image to label              visual question answering for image text to text              and speech recognition for speech to text 
A common method to create multimodal models out of an LLM is to  tokenize  the output of a trained encoder  Concretely  one can construct an LLM that can understand images as follows  take a trained LLM  and take a trained image encoder 
  
    
      
        E
      
    
      displaystyle E 
  
  Make a small multilayered perceptron 
  
    
      
        f
      
    
      displaystyle f 
  
  so that for any image 
  
    
      
        y
      
    
      displaystyle y 
  
  the post processed vector 
  
    
      
        f
         
        E
         
        y
         
         
      
    
      displaystyle f E y   
  
 has the same dimensions as an encoded token  That is an  image token   Then  one can interleave text tokens and image tokens  The compound model is then fine tuned on an image text dataset  This basic construction can be applied with more sophistication to improve the model  The image encoder may be frozen to improve stability             
Flamingo demonstrated the effectiveness of the tokenization method  finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch              Google PaLM model was fine tuned into a multimodal model PaLM E using the tokenization method  and applied to robotic control              LLaMA models have also been turned multimodal using the tokenization method  to allow image inputs              and video inputs             

GPT   can use both text and image as inputs              although the vision component was not released to the public until GPT  V               Google DeepMind s Gemini is also multimodal               Mistral introduced its own multimodel Pixtral   B model in September                  
Multimodal deep Boltzmann machines edit 
A Boltzmann machine is a type of stochastic neural network invented by Geoffrey Hinton and Terry Sejnowski in       Boltzmann machines can be seen as the stochastic  generative counterpart of Hopfield nets  They are named after the Boltzmann distribution in statistical mechanics  The units in Boltzmann machines are divided into two groups  visible units and hidden units  Each unit is like a neuron with a binary output that represents whether it is activated or not              General Boltzmann machines allow connection between any units  However  learning is impractical using general Boltzmann Machines because the computational time is exponential to the size of the machine     citation needed       A more efficient architecture is called restricted Boltzmann machine where connection is only allowed between hidden unit and visible unit  which is described in the next section 
Multimodal deep Boltzmann machines can process and learn from different types of information  such as images and text  simultaneously  This can notably be done by having a separate deep Boltzmann machine for each modality  for example one for images and one for text  joined at an additional top hidden layer             

Applications edit 
Multimodal machine learning has numerous applications across various domains 

Cross Modal Retrieval edit 
Cross modal retrieval allows users to search for data across different modalities  e g   retrieving images based on text descriptions   improving multimedia search engines and content recommendation systems  Models like CLIP facilitate efficient  accurate retrieval by embedding data in a shared space  demonstrating strong performance even in zero shot settings             

Classification and Missing Data Retrieval edit 
Multimodal Deep Boltzmann Machines outperform traditional models like support vector machines and latent Dirichlet allocation in classification tasks and can predict missing data in multimodal datasets  such as images and text 

Healthcare Diagnostics edit 
Multimodal models integrate medical imaging  genomic data  and patient records to improve diagnostic accuracy and early disease detection  especially in cancer screening                                     

Content Generation edit 
Models like DALL E generate images from textual descriptions  benefiting creative industries  while cross modal retrieval enables dynamic multimedia searches             

Robotics and HCI edit 
Multimodal learning improves interaction in robotics and AI by integrating sensory inputs like speech  vision  and touch  aiding autonomous systems and human computer interaction 

Emotion Recognition edit 
Combining visual  audio  and text data  multimodal systems enhance sentiment analysis and emotion recognition  applied in customer service  social media  and marketing 

See also edit 
Hopfield network
Markov random field
Markov chain Monte Carlo
References edit 


  Hendriksen  Mariya  Bleeker  Maurits  Vakulenko  Svitlana  van Noord  Nanne  Kuiper  Ernst  de Rijke  Maarten          Extending CLIP for Category to image Retrieval in E commerce   arXiv             cs CV  

   Stable Diffusion Repository on GitHub   CompVis   Machine Vision and Learning Research Group  LMU Munich     September       Archived from the original on January           Retrieved    September      

  LAION AI aesthetic predictor  LAION AI              retrieved           

  Mokady  Ron  Hertz  Amir  Bermano  Amit H           ClipCap  CLIP Prefix for Image Captioning   arXiv             cs CV  

  Zia  Tehseen  January            Unveiling of Large Multimodal Models  Shaping the Landscape of Language Models in        Unite ai  Retrieved            

  Lu  Kevin  Grover  Aditya  Abbeel  Pieter  Mordatch  Igor                Frozen Pretrained Transformers as Universal Computation Engines   Proceedings of the AAAI Conference on Artificial Intelligence                     doi         aaai v  i         ISSN                

   Vicuna  An Open Source Chatbot Impressing GPT   with      ChatGPT Quality   LMSYS Org   lmsys org  Retrieved            

  Liu  Haotian  Li  Chunyuan  Wu  Qingyang  Lee  Yong Jae                Visual Instruction Tuning   Advances in Neural Information Processing Systems                  

  Dosovitskiy  Alexey  Beyer  Lucas  Kolesnikov  Alexander  Weissenborn  Dirk  Zhai  Xiaohua  Unterthiner  Thomas  Dehghani  Mostafa  Minderer  Matthias  Heigold  Georg  Gelly  Sylvain  Uszkoreit  Jakob                An Image is Worth   x   Words  Transformers for Image Recognition at Scale   arXiv             cs CV  

  Gulati  Anmol  Qin  James  Chiu  Chung Cheng  Parmar  Niki  Zhang  Yu  Yu  Jiahui  Han  Wei  Wang  Shibo  Zhang  Zhengdong  Wu  Yonghui  Pang  Ruoming          Conformer  Convolution augmented Transformer for Speech Recognition   arXiv             eess AS  

  Radford  Alec  Kim  Jong Wook  Xu  Tao  Brockman  Greg  McLeavey  Christine  Sutskever  Ilya          Robust Speech Recognition via Large Scale Weak Supervision   arXiv             eess AS  

  Jaegle  Andrew  Gimeno  Felix  Brock  Andrew  Zisserman  Andrew  Vinyals  Oriol  Carreira  Joao                Perceiver  General Perception with Iterative Attention   arXiv             cs CV  

  Jaegle  Andrew  Borgeaud  Sebastian  Alayrac  Jean Baptiste  Doersch  Carl  Ionescu  Catalin  Ding  David  Koppula  Skanda  Zoran  Daniel  Brock  Andrew  Shelhamer  Evan  H naff  Olivier                Perceiver IO  A General Architecture for Structured Inputs  amp  Outputs   arXiv             cs LG  

   Parti  Pathways Autoregressive Text to Image Model   sites research google  Retrieved            

  a b Villegas  Ruben  Babaeizadeh  Mohammad  Kindermans  Pieter Jan  Moraldo  Hernan  Zhang  Han  Saffar  Mohammad Taghi  Castro  Santiago  Kunze  Julius  Erhan  Dumitru                Phenaki  Variable Length Video Generation from Open Domain Textual Descriptions     cite journal    Cite journal requires       journal   help 

  a b Chang  Huiwen  Zhang  Han  Barber  Jarred  Maschinot  A  J   Lezama  Jose  Jiang  Lu  Yang  Ming Hsuan  Murphy  Kevin  Freeman  William T                 Muse  Text To Image Generation via Masked Generative Transformers   arXiv             cs CV  

  Ramesh  Aditya  Pavlov  Mikhail  Goh  Gabriel  Gray  Scott  Voss  Chelsea  Radford  Alec  Chen  Mark  Sutskever  Ilya               Zero Shot Text to Image Generation  arXiv           

  Yu  Jiahui  Xu  Yuanzhong  Koh  Jing Yu  Luong  Thang  Baid  Gunjan  Wang  Zirui  Vasudevan  Vijay  Ku  Alexander  Yang  Yinfei               Scaling Autoregressive Models for Content Rich Text to Image Generation  arXiv           

  Kiros  Ryan  Salakhutdinov  Ruslan  Zemel  Rich                Multimodal Neural Language Models   Proceedings of the   st International Conference on Machine Learning  PMLR           Archived from the original on             Retrieved            

  Krizhevsky  Alex  Sutskever  Ilya  Hinton  Geoffrey E          ImageNet Classification with Deep Convolutional Neural Networks   Advances in Neural Information Processing Systems      Curran Associates  Inc  Archived from the original on             Retrieved            

  Antol  Stanislaw  Agrawal  Aishwarya  Lu  Jiasen  Mitchell  Margaret  Batra  Dhruv  Zitnick  C  Lawrence  Parikh  Devi          VQA  Visual Question Answering   ICCV             Archived from the original on             Retrieved            

  Li  Junnan  Li  Dongxu  Savarese  Silvio  Hoi  Steven                BLIP    Bootstrapping Language Image Pre training with Frozen Image Encoders and Large Language Models   arXiv             cs CV  

  Alayrac  Jean Baptiste  Donahue  Jeff  Luc  Pauline  Miech  Antoine  Barr  Iain  Hasson  Yana  Lenc  Karel  Mensch  Arthur  Millican  Katherine  Reynolds  Malcolm  Ring  Roman  Rutherford  Eliza  Cabi  Serkan  Han  Tengda  Gong  Zhitao                Flamingo  a Visual Language Model for Few Shot Learning   Advances in Neural Information Processing Systems                   arXiv             Archived from the original on             Retrieved            

  Driess  Danny  Xia  Fei  Sajjadi  Mehdi S  M   Lynch  Corey  Chowdhery  Aakanksha  Ichter  Brian  Wahid  Ayzaan  Tompson  Jonathan  Vuong  Quan  Yu  Tianhe  Huang  Wenlong  Chebotar  Yevgen  Sermanet  Pierre  Duckworth  Daniel  Levine  Sergey                PaLM E  An Embodied Multimodal Language Model   arXiv             cs LG  

  Liu  Haotian  Li  Chunyuan  Wu  Qingyang  Lee  Yong Jae                Visual Instruction Tuning   arXiv             cs CV  

  Zhang  Hang  Li  Xin  Bing  Lidong                Video LLaMA  An Instruction tuned Audio Visual Language Model for Video Understanding   arXiv             cs CL  

  OpenAI                GPT   Technical Report   arXiv             cs CL  

  OpenAI  September             GPT  V ision  System Card   PDF  

  Pichai  Sundar     May        Google Keynote  Google I O       timestamp        retrieved           

  Wiggers  Kyle     September         Mistral releases Pixtral   B  its first multimodal model   TechCrunch  Retrieved    September      

  Dey  Victor                Beginners Guide to Boltzmann Machine   Analytics India Magazine  Retrieved            

   Multimodal Learning with Deep Boltzmann Machine   PDF         Archived  PDF  from the original on             Retrieved            

  Hendriksen  Mariya  Vakulenko  Svitlana  Kuiper  Ernst  de Rijke  Maarten          Scene centric vs  Object centric Image Text Cross modal Retrieval  A Reproducibility Study   arXiv             cs CV  

  Quach  Katyanna   Harvard boffins build multimodal AI system to predict cancer   The Register  Archived from the original on    September       Retrieved    September      

  Chen  Richard J   Lu  Ming Y   Williamson  Drew F  K   Chen  Tiffany Y   Lipkova  Jana  Noor  Zahra  Shaban  Muhammad  Shady  Maha  Williams  Mane  Joo  Bumjin  Mahmood  Faisal    August         Pan cancer integrative histology genomic analysis via multimodal deep learning   Cancer Cell                  e   doi         j ccell              ISSN                 PMC                PMID                S CID                
Teaching hospital press release   New AI technology integrates multiple data types to predict cancer outcomes   Brigham and Women s Hospital via medicalxpress com  Archived from the original on    September       Retrieved    September      

  Shi  Yuge  Siddharth  N   Paige  Brooks  Torr  Philip HS          Variational Mixture of Experts Autoencoders for Multi Modal Deep Generative Models   arXiv             cs LG  

  Shi  Yuge  Siddharth  N   Paige  Brooks  Torr  Philip HS          Variational Mixture of Experts Autoencoders for Multi Modal Deep Generative Models   arXiv             cs LG  







Retrieved from  https   en wikipedia org w index php title Multimodal learning amp oldid