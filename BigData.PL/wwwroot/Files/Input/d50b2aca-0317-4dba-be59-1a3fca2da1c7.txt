Grouping a set of objects by similarity
The result of a cluster analysis shown as the coloring of the squares into three clusters
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
Cluster analysis or clustering is the data analyzing technique in which task of grouping a set of objects in such a way that objects in the same group  called a cluster  are more similar  in some specific sense defined by the analyst  to each other than to those in other groups  clusters   It is a main task of exploratory data analysis  and a common technique for statistical data analysis  used in many fields  including pattern recognition  image analysis  information retrieval  bioinformatics  data compression  computer graphics and machine learning 
Cluster analysis refers to a family of algorithms and tasks rather than one specific algorithm  It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them  Popular notions of clusters include groups with small distances between cluster members  dense areas of the data space  intervals or particular statistical distributions  Clustering can therefore be formulated as a multi objective optimization problem  The appropriate clustering algorithm and parameter settings  including parameters such as the distance function to use  a density threshold or the number of expected clusters  depend on the individual data set and intended use of the results  Cluster analysis as such is not an automatic task  but an iterative process of knowledge discovery or interactive multi objective optimization that involves trial and failure  It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties 
Besides the term clustering  there are a number of terms with similar meanings  including automatic classification  numerical taxonomy  botryology  from Greek          grape    typological analysis  and community detection  The subtle differences are often in the use of the results  while in data mining  the resulting groups are the matter of interest  in automatic classification the resulting discriminative power is of interest 
Cluster analysis originated in anthropology by Driver and Kroeber in                 and introduced to psychology by Joseph Zubin in                 and Robert Tryon in                 and famously used by Cattell beginning in                 for trait theory classification in personality psychology 


Definition edit 
The notion of a  cluster  cannot be precisely defined  which is one of the reasons why there are so many clustering algorithms             There is a common denominator  a group of data objects  However  different researchers employ different cluster models  and for each of these cluster models again different algorithms can be given  The notion of a cluster  as found by different algorithms  varies significantly in its properties  Understanding these  cluster models  is key to understanding the differences between the various algorithms  Typical cluster models include 

Connectivity models  for example  hierarchical clustering builds models based on distance connectivity 
Centroid models  for example  the k means algorithm represents each cluster by a single mean vector 
Distribution models  clusters are modeled using statistical distributions  such as multivariate normal distributions used by the expectation maximization algorithm 
Density models  for example  DBSCAN and OPTICS defines clusters as connected dense regions in the data space 
Subspace models  in biclustering  also known as co clustering or two mode clustering   clusters are modeled with both cluster members and relevant attributes 
Group models  some algorithms do not provide a refined model for their results and just provide the grouping information 
Graph based models  a clique  that is  a subset of nodes in a graph such that every two nodes in the subset are connected by an edge can be considered as a prototypical form of cluster  Relaxations of the complete connectivity requirement  a fraction of the edges can be missing  are known as quasi cliques  as in the HCS clustering algorithm 
Signed graph models  Every path in a signed graph has a sign from the product of the signs on the edges  Under the assumptions of balance theory  edges may change sign and result in a bifurcated graph  The weaker  clusterability axiom   no cycle has exactly one negative edge  yields results with more than two clusters  or subgraphs with only positive edges            
Neural models  the most well known unsupervised neural network is the self organizing map and these models can usually be characterized as similar to one or more of the above models  and including subspace models when neural networks implement a form of Principal Component Analysis or Independent Component Analysis 
A  clustering  is essentially a set of such clusters  usually containing all objects in the data set  Additionally  it may specify the relationship of the clusters to each other  for example  a hierarchy of clusters embedded in each other  Clusterings can be roughly distinguished as 

Hard clustering  each object belongs to a cluster or not
Soft clustering  also  fuzzy clustering   each object belongs to each cluster to a certain degree  for example  a likelihood of belonging to the cluster 
There are also finer distinctions possible  for example 

Strict partitioning clustering  each object belongs to exactly one cluster
Strict partitioning clustering with outliers  objects can also belong to no cluster  in which case they are considered outliers
Overlapping clustering  also  alternative clustering  multi view clustering   objects may belong to more than one cluster  usually involving hard clusters
Hierarchical clustering  objects that belong to a child cluster also belong to the parent cluster
Subspace clustering  while an overlapping clustering  within a uniquely defined subspace  clusters are not expected to overlap
Algorithms edit 
Main category  Cluster analysis algorithms
As listed above  clustering algorithms can be categorized based on their cluster model  The following overview will only list the most prominent examples of clustering algorithms  as there are possibly over     published clustering algorithms  Not all provide models for their clusters and can thus not easily be categorized  An overview of algorithms explained in Wikipedia can be found in the list of statistics algorithms 
There is no objectively  correct  clustering algorithm  but as it was noted   clustering is in the eye of the beholder              In fact  an axiomatic approach to clustering demonstrates that it is impossible for any clustering method to meet three fundamental properties simultaneously  scale invariance  results remain unchanged under proportional scaling of distances   richness  all possible partitions of the data can be achieved   and consistency between distances and the clustering structure             The most appropriate clustering algorithm for a particular problem often needs to be chosen experimentally  unless there is a mathematical reason to prefer one cluster model over another  An algorithm that is designed for one kind of model will generally fail on a data set that contains a radically different kind of model             For example  k means cannot find non convex clusters             Most traditional clustering methods assume the clusters exhibit a spherical  elliptical or convex shape             

Connectivity based clustering  hierarchical clustering  edit 
Main article  Hierarchical clustering
Connectivity based clustering  also known as hierarchical clustering  is based on the core idea of objects being more related to nearby objects than to objects farther away  These algorithms connect  objects  to form  clusters  based on their distance  A cluster can be described largely by the maximum distance needed to connect parts of the cluster  At different distances  different clusters will form  which can be represented using a dendrogram  which explains where the common name  hierarchical clustering  comes from  these algorithms do not provide a single partitioning of the data set  but instead provide an extensive hierarchy of clusters that merge with each other at certain distances  In a dendrogram  the y axis marks the distance at which the clusters merge  while the objects are placed along the x axis such that the clusters don t mix 
Connectivity based clustering is a whole family of methods that differ by the way distances are computed  Apart from the usual choice of distance functions  the user also needs to decide on the linkage criterion  since a cluster consists of multiple objects  there are multiple candidates to compute the distance  to use  Popular choices are known as single linkage clustering  the minimum of object distances   complete linkage clustering  the maximum of object distances   and UPGMA or WPGMA   Unweighted or Weighted Pair Group Method with Arithmetic Mean   also known as average linkage clustering   Furthermore  hierarchical clustering can be agglomerative  starting with single elements and aggregating them into clusters  or divisive  starting with the complete data set and dividing it into partitions  
These methods will not produce a unique partitioning of the data set  but a hierarchy from which the user still needs to choose appropriate clusters  They are not very robust towards outliers  which will either show up as additional clusters or even cause other clusters to merge  known as  chaining phenomenon   in particular with single linkage clustering   In the general case  the complexity is 
  
    
      
        
          
            O
          
        
         
        
          n
          
             
          
        
         
      
    
      displaystyle   mathcal  O   n      
  
 for agglomerative clustering and 
  
    
      
        
          
            O
          
        
         
        
           
          
            n
              x     
             
          
        
         
      
    
      displaystyle   mathcal  O      n     
  
 for divisive clustering             which makes them too slow for large data sets  For some special cases  optimal efficient methods  of complexity 
  
    
      
        
          
            O
          
        
         
        
          n
          
             
          
        
         
      
    
      displaystyle   mathcal  O   n      
  
  are known  SLINK             for single linkage and CLINK             for complete linkage clustering 


	Linkage clustering examples
		
			
			Single linkage on Gaussian data  At    clusters  the biggest cluster starts fragmenting into smaller parts  while before it was still connected to the second largest due to the single link effect 
		
		
			
			Single linkage on density based clusters     clusters extracted  most of which contain single elements  since linkage clustering does not have a notion of  noise  
		

Centroid based clustering edit 
Main article  k means clustering
In centroid based clustering  each cluster is represented by a central vector  which is not necessarily a member of the data set  When the number of clusters is fixed to k  k means clustering gives a formal definition as an optimization problem  find the k cluster centers and assign the objects to the nearest cluster center  such that the squared distances from the cluster are minimized 
The optimization problem itself is known to be NP hard  and thus the common approach is to search only for approximate solutions  A particularly well known approximate method is Lloyd s algorithm              often just referred to as  k means algorithm   although another algorithm introduced this name   It does however only find a local optimum  and is commonly run multiple times with different random initializations  Variations of k means often include such optimizations as choosing the best of multiple runs  but also restricting the centroids to members of the data set  k medoids   choosing medians  k medians clustering   choosing the initial centers less randomly  k means    or allowing a fuzzy cluster assignment  fuzzy c means  
Most k means type algorithms require the number of clusters   k   to be specified in advance  which is considered to be one of the biggest drawbacks of these algorithms  Furthermore  the algorithms prefer clusters of approximately similar size  as they will always assign an object to the nearest centroid  often yielding improperly cut borders of clusters  This happens primarily because the algorithm optimizes cluster centers  not cluster borders  Steps involved in the centroid based clustering algorithm are 

Choose  k distinct clusters at random  These are the initial centroids to be improved upon 
Suppose a set of observations   x   x        xn   Assign each observation to the centroid to which it has the smallest squared Euclidean distance  This results in k distinct groups  each containing unique observations 
Recalculate centroids  see k means clustering  
Exit iff the new centroids are equivalent to the previous iteration s centroids  Else  repeat the algorithm  the centroids have yet to converge 
K means has a number of interesting theoretical properties  First  it partitions the data space into a structure known as a Voronoi diagram  Second  it is conceptually close to nearest neighbor classification  and as such is popular in machine learning  Third  it can be seen as a variation of model based clustering  and Lloyd s algorithm as a variation of the Expectation maximization algorithm for this model discussed below 


	k means clustering examples
		
			
			k means separates data into Voronoi cells  which assumes equal sized clusters  not adequate here  
		
		
			
			k means cannot represent density based clusters 
		

Centroid based clustering problems such as k means and k medoids are special cases of the uncapacitated  metric facility location problem  a canonical problem in the operations research and computational geometry communities  In a basic facility location problem  of which there are numerous variants that model more elaborate settings   the task is to find the best warehouse locations to optimally service a given set of consumers  One may view  warehouses  as cluster centroids and  consumer locations  as the data to be clustered  This makes it possible to apply the well developed algorithmic solutions from the facility location literature to the presently considered centroid based clustering problem 

Model based clustering edit 
The clustering framework most closely related to statistics is model based clustering  which is based on distribution models  This approach models the data as arising from a mixture of probability distributions  It has the advantages of providing principled statistical answers to questions such as how many clusters there are  what clustering method or model to use  and how to detect and deal with outliers 
While the theoretical foundation of these methods is excellent  they suffer from overfitting unless constraints are put on the model complexity  A more complex model will usually be able to explain the data better  which makes choosing the appropriate model complexity inherently difficult  Standard model based clustering methods include more parsimonious models based on the eigenvalue decomposition of the covariance matrices  that provide a balance between overfitting and fidelity to the data 
One prominent method is known as Gaussian mixture models  using the expectation maximization algorithm   Here  the data set is usually modeled with a fixed  to avoid overfitting  number of Gaussian distributions that are initialized randomly and whose parameters are iteratively optimized to better fit the data set  This will converge to a local optimum  so multiple runs may produce different results  In order to obtain a hard clustering  objects are often then assigned to the Gaussian distribution they most likely belong to  for soft clusterings  this is not necessary 
Distribution based clustering produces complex models for clusters that can capture correlation and dependence between attributes  However  these algorithms put an extra burden on the user  for many real data sets  there may be no concisely defined mathematical model  e g  assuming Gaussian distributions is a rather strong assumption on the data  


	Gaussian mixture model clustering examples
		
			
			On Gaussian distributed data  EM works well  since it uses Gaussians for modelling clusters 
		
		
			
			Density based clusters cannot be modeled using Gaussian distributions 
		

Density based clustering edit 
In density based clustering              clusters are defined as areas of higher density than the remainder of the data set  Objects in sparse areas   that are required to separate clusters   are usually considered to be noise and border points 
The most popular             density based clustering method is DBSCAN              In contrast to many newer methods  it features a well defined cluster model called  density reachability   Similar to linkage based clustering  it is based on connecting points within certain distance thresholds  However  it only connects points that satisfy a density criterion  in the original variant defined as a minimum number of other objects within this radius  A cluster consists of all density connected objects  which can form a cluster of an arbitrary shape  in contrast to many other methods  plus all objects that are within these objects  range  Another interesting property of DBSCAN is that its complexity is fairly low   it requires a linear number of range queries on the database   and that it will discover essentially the same results  it is deterministic for core and noise points  but not for border points  in each run  therefore there is no need to run it multiple times  OPTICS             is a generalization of DBSCAN that removes the need to choose an appropriate value for the range parameter 
  
    
      
          x b  
      
    
      displaystyle  varepsilon  
  
  and produces a hierarchical result related to that of linkage clustering  DeLi Clu              Density Link Clustering combines ideas from single linkage clustering and OPTICS  eliminating the 
  
    
      
          x b  
      
    
      displaystyle  varepsilon  
  
 parameter entirely and offering performance improvements over OPTICS by using an R tree index 
The key drawback of DBSCAN and OPTICS is that they expect some kind of density drop to detect cluster borders  On data sets with  for example  overlapping Gaussian distributions   a common use case in artificial data   the cluster borders produced by these algorithms will often look arbitrary  because the cluster density decreases continuously  On a data set consisting of mixtures of Gaussians  these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data 
Mean shift is a clustering approach where each object is moved to the densest area in its vicinity  based on kernel density estimation  Eventually  objects converge to local maxima of density  Similar to k means clustering  these  density attractors  can serve as representatives for the data set  but mean shift can detect arbitrary shaped clusters similar to DBSCAN  Due to the expensive iterative procedure and density estimation  mean shift is usually slower than DBSCAN or k Means  Besides that  the applicability of the mean shift algorithm to multidimensional data is hindered by the unsmooth behaviour of the kernel density estimate  which results in over fragmentation of cluster tails             


	Density based clustering examples
		
			
			Density based clustering with DBSCAN
		
		
			
			DBSCAN assumes clusters of similar density  and may have problems separating nearby clusters 
		
		
			
			OPTICS is a DBSCAN variant  improving handling of different densities clusters 
		

Grid based clustering edit 
The grid based technique is used for a multi dimensional data set              In this technique  we create a grid structure  and the comparison is performed on grids  also known as cells   The grid based technique is fast and has low computational complexity  There are two types of grid based clustering methods  STING and CLIQUE  Steps involved in the grid based clustering algorithm are 

Divide data space into a finite number of cells 
Randomly select a cell  c   where c should not be traversed beforehand 
Calculate the density of  c 
If the density of  c  greater than threshold density
Mark cell  c  as a new cluster
Calculate the density of all the neighbors of  c 
If the density of a neighboring cell is greater than threshold density then  add the cell in the cluster and repeat steps     and     till there is no neighbor with a density greater than threshold density 
Repeat steps     and   till all the cells are traversed 
Stop 
Recent developments edit 
In recent years  considerable effort has been put into improving the performance of existing algorithms                          Among them are CLARANS              and BIRCH              With the recent need to process larger and larger data sets  also known as big data   the willingness to trade semantic meaning of the generated clusters for performance has been increasing  This led to the development of pre clustering methods such as canopy clustering  which can process huge data sets efficiently  but the resulting  clusters  are merely a rough pre partitioning of the data set to then analyze the partitions with existing slower methods such as k means clustering 
For high dimensional data  many of the existing methods fail due to the curse of dimensionality  which renders particular distance functions problematic in high dimensional spaces  This led to new clustering algorithms for high dimensional data that focus on subspace clustering  where only some attributes are used  and cluster models include the relevant attributes for the cluster  and correlation clustering that also looks for arbitrary rotated   correlated   subspace clusters that can be modeled by giving a correlation of their attributes              Examples for such clustering algorithms are CLIQUE             and SUBCLU             
Ideas from density based clustering methods  in particular the DBSCAN OPTICS family of algorithms  have been adapted to subspace clustering  HiSC              hierarchical subspace clustering and DiSH              and correlation clustering  HiCO              hierarchical correlation clustering   C             using  correlation connectivity  and ERiC             exploring hierarchical density based correlation clusters  
Several different clustering systems based on mutual information have been proposed  One is Marina Meil  s variation of information metric              another provides hierarchical clustering              Using genetic algorithms  a wide range of different fit functions can be optimized  including mutual information              Also belief propagation  a recent development in computer science and statistical physics  has led to the creation of new types of clustering algorithms             

Evaluation and assessment edit 
Evaluation  or  validation   of clustering results is as difficult as the clustering itself              Popular approaches involve  internal  evaluation  where the clustering is summarized to a single quality score   external  evaluation  where the clustering is compared to an existing  ground truth  classification   manual  evaluation by a human expert  and  indirect  evaluation by evaluating the utility of the clustering in its intended application             
Internal evaluation measures suffer from the problem that they represent functions that themselves can be seen as a clustering objective  For example  one could cluster the data set by the Silhouette coefficient  except that there is no known efficient algorithm for this  By using such an internal measure for evaluation  one rather compares the similarity of the optimization problems              and not necessarily how useful the clustering is 
External evaluation has similar problems  if we have such  ground truth  labels  then we would not need to cluster  and in practical applications we usually do not have such labels  On the other hand  the labels only reflect one possible partitioning of the data set  which does not imply that there does not exist a different  and maybe even better  clustering 
Neither of these approaches can therefore ultimately judge the actual quality of a clustering  but this needs human evaluation              which is highly subjective  Nevertheless  such statistics can be quite informative in identifying bad clusterings              but one should not dismiss subjective human evaluation             

Internal evaluation edit 
See also  Determining the number of clusters in a data set
When a clustering result is evaluated based on the data that was clustered itself  this is called internal evaluation  These methods usually assign the best score to the algorithm that produces clusters with high similarity within a cluster and low similarity between clusters  One drawback of using internal criteria in cluster evaluation is that high scores on an internal measure do not necessarily result in effective information retrieval applications              Additionally  this evaluation is biased towards algorithms that use the same cluster model  For example  k means clustering naturally optimizes object distances  and a distance based internal criterion will likely overrate the resulting clustering 
Therefore  the internal evaluation measures are best suited to get some insight into situations where one algorithm performs better than another  but this shall not imply that one algorithm produces more valid results than another             Validity as measured by such an index depends on the claim that this kind of structure exists in the data set  An algorithm designed for some kind of models has no chance if the data set contains a radically different set of models  or if the evaluation measures a radically different criterion             For example  k means clustering can only find convex clusters  and many evaluation indexes assume convex clusters  On a data set with non convex clusters neither the use of k means  nor of an evaluation criterion that assumes convexity  is sound 
More than a dozen of internal evaluation measures exist  usually based on the intuition that items in the same cluster should be more similar than items in different clusters                                        For example  the following methods can be used to assess the quality of clustering algorithms based on internal criterion 

Davies Bouldin index edit 
The Davies Bouldin index can be calculated by the following formula 

  
    
      
        D
        B
         
        
          
             
            n
          
        
        
            x     
          
            i
             
             
          
          
            n
          
        
        
          max
          
            j
              x     
            i
          
        
        
           
          
            
              
                
                    x c  
                  
                    i
                  
                
                 
                
                    x c  
                  
                    j
                  
                
              
              
                d
                 
                
                  c
                  
                    i
                  
                
                 
                
                  c
                  
                    j
                  
                
                 
              
            
          
           
        
      
    
      displaystyle DB   frac     n   sum   i     n  max   j neq i  left   frac   sigma   i   sigma   j   d c  i  c  j     right  
  

where n is the number of clusters  
  
    
      
        
          c
          
            i
          
        
      
    
      displaystyle c  i  
  
 is the centroid of cluster 
  
    
      
        i
      
    
      displaystyle i 
  
  
  
    
      
        
            x c  
          
            i
          
        
      
    
      displaystyle  sigma   i  
  
 is the average distance of all elements in cluster 
  
    
      
        i
      
    
      displaystyle i 
  
 to centroid 
  
    
      
        
          c
          
            i
          
        
      
    
      displaystyle c  i  
  
  and 
  
    
      
        d
         
        
          c
          
            i
          
        
         
        
          c
          
            j
          
        
         
      
    
      displaystyle d c  i  c  j   
  
 is the distance between centroids 
  
    
      
        
          c
          
            i
          
        
      
    
      displaystyle c  i  
  
 and 
  
    
      
        
          c
          
            j
          
        
      
    
      displaystyle c  j  
  
  Since algorithms that produce clusters with low intra cluster distances  high intra cluster similarity  and high inter cluster distances  low inter cluster similarity  will have a low Davies Bouldin index  the clustering algorithm that produces a collection of clusters with the smallest Davies Bouldin index is considered the best algorithm based on this criterion 

Dunn index edit 
The Dunn index aims to identify dense and well separated clusters  It is defined as the ratio between the minimal inter cluster distance to maximal intra cluster distance  For each cluster partition  the Dunn index can be calculated by the following formula             


  
    
      
        D
         
        
          
            
              
                min
                
                   
                    x     
                  i
                   lt 
                  j
                    x     
                  n
                
              
              d
               
              i
               
              j
               
            
            
              
                max
                
                   
                    x     
                  k
                    x     
                  n
                
              
              
                d
                
                    x     
                
              
               
              k
               
            
          
        
        
         
      
    
      displaystyle D   frac   min     leq i lt j leq n d i j    max     leq k leq n d   prime   k       
  

where d i j  represents the distance between clusters i and j  and d   k  measures the intra cluster distance of cluster k  The inter cluster distance d i j  between two clusters may be any number of distance measures  such as the distance between the centroids of the clusters  Similarly  the intra cluster distance d   k  may be measured in a variety of ways  such as the maximal distance between any pair of elements in cluster      k  Since internal criterion seek clusters with high intra cluster similarity and low inter cluster similarity  algorithms that produce clusters with high Dunn index are more desirable 

Silhouette coefficient edit 
The silhouette coefficient contrasts the average distance to elements in the same cluster with the average distance to elements in other clusters  Objects with a high silhouette value are considered well clustered  objects with a low value may be outliers  This index works well with k means clustering  and is also used to determine the optimal number of clusters             

External evaluation edit 
In external evaluation  clustering results are evaluated based on data that was not used for clustering  such as known class labels and external benchmarks  Such benchmarks consist of a set of pre classified items  and these sets are often created by  expert  humans  Thus  the benchmark sets can be thought of as a gold standard for evaluation              These types of evaluation methods measure how close the clustering is to the predetermined benchmark classes  However  it has recently been discussed whether this is adequate for real data  or only on synthetic data sets with a factual ground truth  since classes can contain internal structure  the attributes present may not allow separation of clusters or the classes may contain anomalies              Additionally  from a knowledge discovery point of view  the reproduction of known knowledge may not necessarily be the intended result              In the special scenario of constrained clustering  where meta information  such as class labels  is used already in the clustering process  the hold out of information for evaluation purposes is non trivial             
A number of measures are adapted from variants used to evaluate classification tasks  In place of counting the number of times a class was correctly assigned to a single data point  known as true positives   such pair counting metrics assess whether each pair of data points that is truly in the same cluster is predicted to be in the same cluster             
As with internal evaluation  several external evaluation measures exist                                        for example 

Purity edit 
Purity is a measure of the extent to which clusters contain a single class              Its calculation can be thought of as follows  For each cluster  count the number of data points from the most common class in said cluster  Now take the sum over all clusters and divide by the total number of data points  Formally  given some set of clusters 
  
    
      
        M
      
    
      displaystyle M 
  
 and some set of classes 
  
    
      
        D
      
    
      displaystyle D 
  
  both partitioning 
  
    
      
        N
      
    
      displaystyle N 
  
 data points  purity can be defined as 


  
    
      
        
          
             
            N
          
        
        
            x     
          
            m
              x     
            M
          
        
        
          max
          
            d
              x     
            D
          
        
        
          
             
          
          m
            x     
          d
          
             
          
        
      
    
      displaystyle   frac     N   sum   m in M  max   d in D   m cap d   
  

This measure doesn t penalize having many clusters  and more clusters will make it easier to produce a high purity  A purity score of   is always possible by putting each data point in its own cluster  Also  purity doesn t work well for imbalanced data  where even poorly performing clustering algorithms will give a high purity value  For example  if a size      dataset consists of two classes  one containing     points and the other containing   point  then every possible partition will have a purity of at least        

Rand index edit 
The Rand index             computes how similar the clusters  returned by the clustering algorithm  are to the benchmark classifications  It can be computed using the following formula 


  
    
      
        R
        I
         
        
          
            
              T
              P
               
              T
              N
            
            
              T
              P
               
              F
              P
               
              F
              N
               
              T
              N
            
          
        
      
    
      displaystyle RI   frac  TP TN  TP FP FN TN   
  

where  
  
    
      
        T
        P
      
    
      displaystyle TP 
  
 is the number of true positives  
  
    
      
        T
        N
      
    
      displaystyle TN 
  
 is the number of true negatives  
  
    
      
        F
        P
      
    
      displaystyle FP 
  
 is the number of false positives  and 
  
    
      
        F
        N
      
    
      displaystyle FN 
  
 is the number of false negatives  The instances being counted here are the number of correct pairwise assignments  That is  
  
    
      
        T
        P
      
    
      displaystyle TP 
  
 is the number of pairs of points that are clustered together in the predicted partition and in the ground truth partition  
  
    
      
        F
        P
      
    
      displaystyle FP 
  
 is the number of pairs of points that are clustered together in the predicted partition but not in the ground truth partition etc  If the dataset is of size N  then 
  
    
      
        T
        P
         
        T
        N
         
        F
        P
         
        F
        N
         
        
          
            
               
            
            
              N
               
            
            
               
            
          
        
      
    
      displaystyle TP TN FP FN   binom  N      
  
  
One issue with the Rand index is that false positives and false negatives are equally weighted  This may be an undesirable characteristic for some clustering applications  The F measure addresses this concern      citation needed      as does the chance corrected adjusted Rand index 

F measure edit 
The F measure can be used to balance the contribution of false negatives by weighting recall through a parameter 
  
    
      
          x b  
          x     
         
      
    
      displaystyle  beta  geq   
  
  Let precision and recall  both external evaluation measures in themselves  be defined as follows 

  
    
      
        P
         
        
          
            
              T
              P
            
            
              T
              P
               
              F
              P
            
          
        
      
    
      displaystyle P   frac  TP  TP FP   
  


  
    
      
        R
         
        
          
            
              T
              P
            
            
              T
              P
               
              F
              N
            
          
        
      
    
      displaystyle R   frac  TP  TP FN   
  

where 
  
    
      
        P
      
    
      displaystyle P 
  
 is the precision rate and 
  
    
      
        R
      
    
      displaystyle R 
  
 is the recall rate  We can calculate the F measure by using the following formula             

  
    
      
        
          F
          
              x b  
          
        
         
        
          
            
               
              
                  x b  
                
                   
                
              
               
               
               
                x  c  
              P
                x  c  
              R
            
            
              
                  x b  
                
                   
                
              
                x  c  
              P
               
              R
            
          
        
      
    
      displaystyle F   beta     frac    beta         cdot P cdot R   beta      cdot P R   
  

When 
  
    
      
          x b  
         
         
      
    
      displaystyle  beta    
  
  
  
    
      
        
          F
          
             
          
        
         
        P
      
    
      displaystyle F     P 
  
  In other words  recall has no impact on the F measure when 
  
    
      
          x b  
         
         
      
    
      displaystyle  beta    
  
  and increasing 
  
    
      
          x b  
      
    
      displaystyle  beta  
  
 allocates an increasing amount of weight to recall in the final F measure 
Also 
  
    
      
        T
        N
      
    
      displaystyle TN 
  
 is not taken into account and can vary from   upward without bound 

Jaccard index edit 
The Jaccard index is used to quantify the similarity between two datasets  The Jaccard index takes on a value between   and    An index of   means that the two dataset are identical  and an index of   indicates that the datasets have no common elements  The Jaccard index is defined by the following formula 

  
    
      
        J
         
        A
         
        B
         
         
        
          
            
              
                 
              
              A
                x     
              B
              
                 
              
            
            
              
                 
              
              A
                x   a 
              B
              
                 
              
            
          
        
         
        
          
            
              T
              P
            
            
              T
              P
               
              F
              P
               
              F
              N
            
          
        
      
    
      displaystyle J A B    frac   A cap B    A cup B      frac  TP  TP FP FN   
  

This is simply the number of unique elements common to both sets divided by the total number of unique elements in both sets 
Note that 
  
    
      
        T
        N
      
    
      displaystyle TN 
  
 is not taken into account 

Dice index edit 
The Dice symmetric measure doubles the weight on 
  
    
      
        T
        P
      
    
      displaystyle TP 
  
 while still ignoring 
  
    
      
        T
        N
      
    
      displaystyle TN 
  
 

  
    
      
        D
        S
        C
         
        
          
            
               
              T
              P
            
            
               
              T
              P
               
              F
              P
               
              F
              N
            
          
        
      
    
      displaystyle DSC   frac   TP   TP FP FN   
  


Fowlkes Mallows index edit 
The Fowlkes Mallows index             computes the similarity between the clusters returned by the clustering algorithm and the benchmark classifications  The higher the value of the Fowlkes Mallows index the more similar the clusters and the benchmark classifications are  It can be computed using the following formula 

  
    
      
        F
        M
         
        
          
            
              
                
                  T
                  P
                
                
                  T
                  P
                   
                  F
                  P
                
              
            
              x  c  
            
              
                
                  T
                  P
                
                
                  T
                  P
                   
                  F
                  N
                
              
            
          
        
      
    
      displaystyle FM   sqrt    frac  TP  TP FP   cdot   frac  TP  TP FN     
  

where  
  
    
      
        T
        P
      
    
      displaystyle TP 
  
 is the number of true positives  
  
    
      
        F
        P
      
    
      displaystyle FP 
  
 is the number of false positives  and 
  
    
      
        F
        N
      
    
      displaystyle FN 
  
 is the number of false negatives  The 
  
    
      
        F
        M
      
    
      displaystyle FM 
  
 index is the geometric mean of the precision and recall 
  
    
      
        P
      
    
      displaystyle P 
  
 and 
  
    
      
        R
      
    
      displaystyle R 
  
  and is thus also known as the G measure  while the F measure is their harmonic mean                          Moreover  precision and recall are also known as Wallace s indices 
  
    
      
        
          B
          
            I
          
        
      
    
      displaystyle B  I  
  
 and 
  
    
      
        
          B
          
            I
            I
          
        
      
    
      displaystyle B  II  
  
              Chance normalized versions of recall  precision and G measure correspond to Informedness  Markedness and Matthews Correlation and relate strongly to Kappa             

Chi Index edit 
The Chi index             is an external validation index that measure the clustering results by applying the chi squared statistic  This index scores positively the fact that the labels are as sparse as possible across the clusters  i e   that each cluster has as few different labels as possible  The higher the value of the Chi Index the greater the relationship between the resulting clusters and the label used 

Mutual Information edit 
The mututal information is an information theoretic measure of how much information is shared between a clustering and a ground truth classification that can detect a non linear similarity between two clusterings  Normalized mutual information is a family of corrected for chance variants of this that has a reduced bias for varying cluster numbers             

Confusion matrix edit 
A confusion matrix can be used to quickly visualize the results of a classification  or clustering  algorithm  It shows how different a cluster is from the gold standard cluster 

Validity Measure edit 
The validity measure  short v measure  is a combined metric for homogeneity and completeness of the clusters            

Cluster tendency edit 
This section needs additional citations for verification  Please help improve this article by adding citations to reliable sources     in this section  Unsourced material may be challenged and removed    April        Learn how and when to remove this message 
To measure cluster tendency is to measure to what degree clusters exist in the data to be clustered  and may be performed as an initial test  before attempting clustering  One way to do this is to compare the data against random data  On average  random data should not have clusters      verification needed      

Hopkins statistic
There are multiple formulations of the Hopkins statistic              A typical one is as follows              Let 
  
    
      
        X
      
    
      displaystyle X 
  
 be the set of 
  
    
      
        n
      
    
      displaystyle n 
  
 data points in 
  
    
      
        d
      
    
      displaystyle d 
  
 dimensional space  Consider a random sample  without replacement  of 
  
    
      
        m
          x   a 
        n
      
    
      displaystyle m ll n 
  
 data points with members 
  
    
      
        
          x
          
            i
          
        
      
    
      displaystyle x  i  
  
  Also generate a set 
  
    
      
        Y
      
    
      displaystyle Y 
  
 of 
  
    
      
        m
      
    
      displaystyle m 
  
 uniformly randomly distributed data points  Now define two distance measures  
  
    
      
        
          u
          
            i
          
        
      
    
      displaystyle u  i  
  
 to be the distance of 
  
    
      
        
          y
          
            i
          
        
          x     
        Y
      
    
      displaystyle y  i  in Y 
  
 from its nearest neighbor in X and 
  
    
      
        
          w
          
            i
          
        
      
    
      displaystyle w  i  
  
 to be the distance of 
  
    
      
        
          x
          
            i
          
        
          x     
        X
      
    
      displaystyle x  i  in X 
  
 from its nearest neighbor in X  We then define the Hopkins statistic as 

  
    
      
        H
         
        
          
            
              
                  x     
                
                  i
                   
                   
                
                
                  m
                
              
              
                
                  u
                  
                    i
                  
                  
                    d
                  
                
              
            
            
              
                  x     
                
                  i
                   
                   
                
                
                  m
                
              
              
                
                  u
                  
                    i
                  
                  
                    d
                  
                
              
               
              
                  x     
                
                  i
                   
                   
                
                
                  m
                
              
              
                
                  w
                  
                    i
                  
                  
                    d
                  
                
              
            
          
        
        
         
      
    
      displaystyle H   frac   sum   i     m  u  i   d     sum   i     m  u  i   d    sum   i     m  w  i   d        
  

With this definition  uniform random data should tend to have values near to      and clustered data should tend to have values nearer to   
However  data containing just a single Gaussian will also score close to    as this statistic measures deviation from a uniform distribution  not multimodality  making this statistic largely useless in application  as real data never is remotely uniform  
Applications edit 
This section needs additional citations for verification  Please help improve this article by adding citations to reliable sources     in this section  Unsourced material may be challenged and removed    November        Learn how and when to remove this message 
Biology  computational biology and bioinformatics edit 
See also  Distance matrices in phylogeny
Plant and animal ecology
Cluster analysis is used to describe and to make spatial and temporal comparisons of communities  assemblages  of organisms in heterogeneous environments  It is also used in plant systematics to generate artificial phylogenies or clusters of organisms  individuals  at the species  genus or higher level that share a number of attributes 
Transcriptomics
Clustering is used to build groups of genes with related expression patterns  also known as coexpressed genes  as in HCS clustering algorithm                          Often such groups contain functionally related proteins  such as enzymes for a specific pathway  or genes that are co regulated  High throughput experiments using expressed sequence tags  ESTs  or DNA microarrays can be a powerful tool for genome annotation                  a general aspect of genomics 
Sequence analysis
Sequence clustering is used to group homologous sequences into gene families              This is a very important concept in bioinformatics  and evolutionary biology in general  See evolution by gene duplication 
High throughput genotyping platforms
Clustering algorithms are used to automatically assign genotypes             
Human genetic clustering
The similarity of genetic data is used in clustering to infer population structures 
Medicine edit 
Medical imaging
On PET scans  cluster analysis can be used to differentiate between different types of tissue in a three dimensional image for many different purposes             
Analysis of antimicrobial activity
Cluster analysis can be used to analyse patterns of antibiotic resistance  to classify antimicrobial compounds according to their mechanism of action  to classify antibiotics according to their antibacterial activity 
IMRT segmentation
Clustering can be used to divide a fluence map into distinct regions for conversion into deliverable fields in MLC based Radiation Therapy 
Business and marketing edit 
Market research
Cluster analysis is widely used in market research when working with multivariate data from surveys and test panels  Market researchers use cluster analysis to partition the general population of consumers into market segments and to better understand the relationships between different groups of consumers potential customers  and for use in market segmentation  product positioning  new product development and selecting test markets 
Grouping of shopping items
Clustering can be used to group all the shopping items available on the web into a set of unique products  For example  all the items on eBay can be grouped into unique products  eBay does not have the concept of a SKU  
World Wide Web edit 
Social network analysis
In the study of social networks  clustering may be used to recognize communities within large groups of people 
Search result grouping
In the process of intelligent grouping of the files and websites  clustering may be used to create a more relevant set of search results compared to normal search engines like Google     citation needed       There are currently a number of web based clustering tools such as Clusty  It also may be used to return a more comprehensive set of results in cases where a search term could refer to vastly different things  Each distinct use of the term corresponds to a unique cluster of results  allowing a ranking algorithm to return comprehensive results by picking the top result from each cluster             
Slippy map optimization
Flickr s map of photos and other map sites use clustering to reduce the number of markers on a map      citation needed      This makes it both faster and reduces the amount of visual clutter 
Computer science edit 
Software evolution
Clustering is useful in software evolution as it helps to reduce legacy properties in code by reforming functionality that has become dispersed  It is a form of restructuring and hence is a way of direct preventative maintenance 
Image segmentation
Image segmentation is the process of dividing a digital image into multiple meaningful regions or segments to simplify and or change the representation of an image  making it easier to analyze  These segments may correspond to different objects  parts of objects  or background areas  The goal is to assign a label to every pixel in the image so that the pixels with similar attributes are grouped together 
This process is used in fields like medical imaging  computer vision  satellite imaging  and in daily applications like face detection and photo editing 
The aurora borealis  or northern lights  above Bear Lake  AlaskaImage after running k means clustering with k      Clustering in Image Segmentation 
Clustering plays a significant role in image segmentation  It groups pixels into clusters based on similarity without needing labeled data  These clusters then define segments within the image 

Here are the most commonly used clustering algorithms for image segmentation 
K means Clustering  One of the most popular and straightforward methods  Pixels are treated as data points in a feature space  usually defined by color or intensity  and grouped into k clusters  Each pixel is assigned to the nearest cluster center  and the centers are updated iteratively 
Mean Shift Clustering  A non parametric method that does not require specifying the number of clusters in advance  It identifies clusters by locating dense areas of data points in the feature space 
Fuzzy C means  Unlike k means  which assigns pixels to exactly one cluster  fuzzy c means allows each pixel to belong to multiple clusters with varying degrees of membership 

Evolutionary algorithms
Clustering may be used to identify different niches within the population of an evolutionary algorithm so that reproductive opportunity can be distributed more evenly amongst the evolving species or subspecies 
Recommender systems
Recommender systems suggest items  products  or other users to an individual based on their past behavior and current preferences  These systems will occasionally use clustering algorithms to predict a user s unknown preferences by analyzing the preferences and activities of other users within the same cluster  Cluster analysis is not the only approach for recommendation systems  for example there are systems that leverage graph theory  Recommendation algorithms that utilize cluster analysis often fall into one of the three main categories  Collaborative filtering  Content Based filtering  and a hybrid of the collaborative and content based 


Collaborative Filtering Recommendation Algorithm
Collaborative filtering works by analyzing large amounts of data on user behavior  preferences  and activities to predict what a user might like based on similarities with others  It detects patterns in how users rate items and groups similar users or items into distinct  neighborhoods   Recommendations are then generated by leveraging the ratings of content from others within the same neighborhood  The algorithm can focus on either user based or item based grouping depending on the context             

Flow diagram that shows a basic and generic approach to recommendation systems and how they utilize clustering 


Content Based Filtering Recommendation Algorithm
Content based filtering uses item descriptions and a user s preference profile to recommend items with similar characteristics to those the user previously liked  It evaluates the distance between feature vectors of item clusters  or  neighborhoods   The user s past interactions are represented as a weighted feature vector  which is compared to these clusters  Recommendations are generated by identifying the cluster evaluated be the closest in distance with the user s preferences             



Hybrid Recommendation Algorithms
Hybrid recommendation algorithms combine collaborative and content based filtering to better meet the requirements of specific use cases  In certain cases this approach leads to more effective recommendations  Common strategies include      running collaborative and content based filtering separately and combining the results      adding onto one approach with specific features of the other  and     integrating both hybrid methods into one model             
Markov chain Monte Carlo methods
Clustering is often utilized to locate and characterize extrema in the target distribution 
Anomaly detection
Anomalies outliers are typically   be it explicitly or implicitly   defined with respect to clustering structure in data 
Natural language processing
Clustering can be used to resolve lexical ambiguity             
DevOps
Clustering has been used to analyse the effectiveness of DevOps teams             
Social science edit 
Sequence analysis in social sciences
Cluster analysis is used to identify patterns of family life trajectories  professional careers  and daily or weekly time use for example 
Crime analysis
Cluster analysis can be used to identify areas where there are greater incidences of particular types of crime  By identifying these distinct areas or  hot spots  where a similar crime has happened over a period of time  it is possible to manage law enforcement resources more effectively 
Educational data mining
Cluster analysis is for example used to identify groups of schools or students with similar properties 
Typologies
From poll data  projects such as those undertaken by the Pew Research Center use cluster analysis to discern typologies of opinions  habits  and demographics that may be useful in politics and marketing 
Others edit 
Field robotics
Clustering algorithms are used for robotic situational awareness to track objects and detect outliers in sensor data             
Mathematical chemistry
To find structural similarity  etc   for example       chemical compounds were clustered in the space of    topological indices             
Climatology
To find weather regimes or preferred sea level pressure atmospheric patterns             
Finance
Cluster analysis has been used to cluster stocks into sectors             
Petroleum geology
Cluster analysis is used to reconstruct missing bottom hole core data or missing log curves in order to evaluate reservoir properties 
Geochemistry
The clustering of chemical properties in different sample locations 
See also edit 



Wikimedia Commons has media related to Cluster analysis 

Specialized types of cluster analysis edit 
Automatic clustering algorithms
Balanced clustering
Clustering high dimensional data
Conceptual clustering
Consensus clustering
Constrained clustering
Community detection
Data stream clustering
HCS clustering
Sequence clustering
Spectral clustering
Techniques used in cluster analysis edit 
Artificial neural network  ANN 
Nearest neighbor search
Neighbourhood components analysis
Latent class analysis
Affinity propagation
Data projection and preprocessing edit 
Dimension reduction
Principal component analysis
Multidimensional scaling
Other edit 
Cluster weighted modeling
Curse of dimensionality
Determining the number of clusters in a data set
Parallel coordinates
Structured data analysis
Linear separability
References edit 


  Driver and Kroeber          Quantitative Expression of Cultural Relationships   University of California Publications in American Archaeology and Ethnology  Quantitative Expression of Cultural Relationships  Berkeley  CA  University of California Press           Archived from the original on             Retrieved            

  Zubin  Joseph          A technique for measuring like mindedness   The Journal of Abnormal and Social Psychology                   doi         h         ISSN              X 

  Tryon  Robert C          Cluster Analysis  Correlation Profile and Orthometric  factor  Analysis for the Isolation of Unities in Mind and Personality  Edwards Brothers 

  Cattell  R  B           The description of personality  Basic traits resolved into clusters   Journal of Abnormal and Social Psychology                   doi         h        

  a b c d e f Estivill Castro  Vladimir     June         Why so many clustering algorithms   A Position Paper   ACM SIGKDD Explorations Newsletter                doi                        S CID              

  James A  Davis  May        Clustering and structural balance in graphs   Human Relations         

  Kleinberg  Jon         An Impossibility Theorem for Clustering  PDF   Advances in Neural Information Processing Systems  Vol           MIT Press 

  Gao  Caroline X   Dwyer  Dominic  Zhu  Ye  Smith  Catherine L   Du  Lan  Filia  Kate M   Bayer  Johanna  Menssink  Jana M   Wang  Teresa  Bergmeir  Christoph  Wood  Stephen  Cotton  Sue M                 An overview of clustering methods with guidelines for application in mental health research   Psychiatry Research               doi         j psychres              hdl              ISSN                 PMID               

  Everitt  Brian         Cluster analysis  Chichester  West Sussex  U K  Wiley  ISBN                    

  Sibson  R           SLINK  an optimally efficient algorithm for the single link cluster method   PDF   The Computer Journal          British Computer Society         doi         comjnl         

  Defays  D           An efficient algorithm for a complete link method   The Computer Journal          British Computer Society           doi         comjnl          

  Lloyd  S           Least squares quantization in PCM   IEEE Transactions on Information Theory                   doi         TIT               S CID               

  Kriegel  Hans Peter  Kr ger  Peer  Sander  J rg  Zimek  Arthur          Density based Clustering   WIREs Data Mining and Knowledge Discovery                  doi         widm     S CID               

  Microsoft academic search  most cited data mining articles Archived            at the Wayback Machine  DBSCAN is on rank     when accessed on           

  Ester  Martin  Kriegel  Hans Peter  Sander  J rg  Xu  Xiaowei          A density based algorithm for discovering clusters in large spatial databases with noise   In Simoudis  Evangelos  Han  Jiawei  Fayyad  Usama M   eds    Proceedings of the Second International Conference on Knowledge Discovery and Data Mining  KDD      AAAI Press  pp                ISBN                    

  Ankerst  Mihael  Breunig  Markus M   Kriegel  Hans Peter  Sander  J rg          OPTICS  Ordering Points To Identify the Clustering Structure   ACM SIGMOD international conference on Management of data  ACM Press  pp              CiteSeerX                      

  a b Achtert  E   B hm  C   Kr ger  P           DeLi Clu  Boosting Robustness  Completeness  Usability  and Efficiency of Hierarchical Clustering by a Closest Pair Ranking   Advances in Knowledge Discovery and Data Mining  Lecture Notes in Computer Science  Vol             pp                CiteSeerX                      doi                      ISBN                        

  Aggarwal  Charu C   Reddy  Chandan K   eds    Data Clustering        Algorithms and Applications  ISBN                         OCLC                 

  Sculley  D          Web scale k means clustering  Proc    th WWW 

  Huang  Z           Extensions to the k means algorithm for clustering large data sets with categorical values   Data Mining and Knowledge Discovery                  doi         A                S CID               

  R  Ng and J  Han   Efficient and effective clustering method for spatial data mining   In  Proceedings of the   th VLDB Conference  pages          Santiago  Chile       

  Tian Zhang  Raghu Ramakrishnan  Miron Livny   An Efficient Data Clustering Method for Very Large Databases   In  Proc  Int l Conf  on Management of Data  ACM SIGMOD  pp          

  Kriegel  Hans Peter  Kr ger  Peer  Zimek  Arthur  July         Subspace clustering   Wiley Interdisciplinary Reviews  Data Mining and Knowledge Discovery                  doi         widm       S CID              

  Agrawal  R   Gehrke  J   Gunopulos  D   Raghavan  P           Automatic Subspace Clustering of High Dimensional Data   Data Mining and Knowledge Discovery            CiteSeerX                       doi         s                  S CID              

  Karin Kailing  Hans Peter Kriegel and Peer Kr ger  Density Connected Subspace Clustering for High Dimensional Data  In  Proc  SIAM Int  Conf  on Data Mining  SDM      pp                

  Achtert  E   B hm  C   Kriegel  H  P   Kr ger  P   M ller Gorman  I   Zimek  A           Finding Hierarchies of Subspace Clusters   Knowledge Discovery in Databases  PKDD       Lecture Notes in Computer Science  Vol             pp                CiteSeerX                       doi                      ISBN                        

  Achtert  E   B hm  C   Kriegel  H  P   Kr ger  P   M ller Gorman  I   Zimek  A           Detection and Visualization of Subspace Cluster Hierarchies   Advances in Databases  Concepts  Systems and Applications  Lecture Notes in Computer Science  Vol             pp                CiteSeerX                      doi                               ISBN                        

  Achtert  E   B hm  C   Kr ger  P   Zimek  A           Mining Hierarchies of Correlation Clusters     th International Conference on Scientific and Statistical Database Management  SSDBM      pp                CiteSeerX                       doi         SSDBM          ISBN                         S CID              

  B hm  C   Kailing  K   Kr ger  P   Zimek  A           Computing Clusters of Correlation Connected objects   Proceedings of the      ACM SIGMOD international conference on Management of data   SIGMOD      p            CiteSeerX                     doi                          ISBN                      S CID              

  Achtert  E   Bohm  C   Kriegel  H  P   Kr ger  P   Zimek  A           On Exploring Complex Relationships of Correlation Clusters     th International Conference on Scientific and Statistical Database Management  SSDBM        p          CiteSeerX                      doi         SSDBM          ISBN                         S CID              

  Meil   Marina          Comparing Clusterings by the Variation of Information   Learning Theory and Kernel Machines  Lecture Notes in Computer Science  Vol             pp                doi                               ISBN                        

  Kraskov  Alexander  St gbauer  Harald  Andrzejak  Ralph G   Grassberger  Peter    December         Hierarchical Clustering Based on Mutual Information   arXiv q bio         

  Auffarth  B   July                Clustering by a Genetic Algorithm with Biased Mutation Operator   Wcci Cec  IEEE 

  Frey  B  J   Dueck  D           Clustering by Passing Messages Between Data Points   Science                       Bibcode     Sci           F  CiteSeerX                       doi         science          PMID                S CID              

  a b c d Pfitzner  Darius  Leibbrandt  Richard  Powers  David          Characterization and evaluation of similarity measures for pairs of clusterings   Knowledge and Information Systems          Springer           doi         s                  S CID              

  a b c Feldman  Ronen  Sanger  James               The Text Mining Handbook  Advanced Approaches in Analyzing Unstructured Data  Cambridge Univ  Press  ISBN                      OCLC                

  a b Weiss  Sholom M   Indurkhya  Nitin  Zhang  Tong  Damerau  Fred J          Text Mining  Predictive Methods for Analyzing Unstructured Information  Springer  ISBN                      OCLC                

  a b c Manning  Christopher D   Raghavan  Prabhakar  Sch tze  Hinrich               Introduction to Information Retrieval  Cambridge University Press  ISBN                        

  a b Knowledge Discovery in Databases   Part III   Clustering  PDF   Heidelberg University        citation     CS  maint  location missing publisher  link 

  Dunn  J           Well separated clusters and optimal fuzzy partitions   Journal of Cybernetics             doi                           

  Peter J  Rousseeuw          Silhouettes  A graphical aid to the interpretation and validation of cluster analysis   Journal of Computational and Applied Mathematics             doi                              

  a b F rber  Ines  G nnemann  Stephan  Kriegel  Hans Peter  Kr ger  Peer  M ller  Emmanuel  Schubert  Erich  Seidl  Thomas  Zimek  Arthur          On Using Class Labels in Evaluation of Clusterings   PDF   In Fern  Xiaoli Z   Davidson  Ian  Dy  Jennifer  eds    MultiClust  Discovering  Summarizing  and Using Multiple Clusterings  ACM SIGKDD 

  Pourrajabi  M   Moulavi  D   Campello  R  J  G  B   Zimek  A   Sander  J   Goebel  R           Model Selection for Semi Supervised Clustering   Proceedings of the   th International Conference on Extending Database Technology  EDBT   pp                doi             edbt         

  Rand  W  M           Objective criteria for the evaluation of clustering methods   Journal of the American Statistical Association            American Statistical Association           arXiv             doi                  JSTOR              

  Fowlkes  E  B   Mallows  C  L           A Method for Comparing Two Hierarchical Clusterings   Journal of the American Statistical Association                     doi                                 JSTOR              

  Powers  David         Recall and Precision versus the Bookmaker  International Conference on Cognitive Science  pp               

  Arabie  P           Comparing partitions   Journal of Classification               doi         BF          S CID                

  Wallace  D  L           Comment   Journal of the American Statistical Association                     doi                                

  Powers  David         The Problem with Kappa  European Chapter of the Association for Computational Linguistics  pp               

  Luna Romera  Jos  Mar a  Mart nez Ballesteros  Mar a  Garc a Guti rrez  Jorge  Riquelme  Jos  C   June         External clustering validity index based on chi squared statistical test   Information Sciences             doi         j ins              hdl               S CID               

  Rosenberg  Andrew  and Julia Hirschberg   V measure  A conditional entropy based external cluster evaluation measure   Proceedings of the      joint conference on empirical methods in natural language processing and computational natural language learning  EMNLP CoNLL         pdf

  Hopkins  Brian  Skellam  John Gordon          A new method for determining the type of distribution of plant individuals   Annals of Botany          Annals Botany Co           doi         oxfordjournals aob a       

  Banerjee  A           Validating clusters using the Hopkins statistic        IEEE International Conference on Fuzzy Systems  IEEE Cat  No   CH        Vol          pp                doi         FUZZY               ISBN                         S CID               

  Johnson  Stephen C                 Hierarchical clustering schemes   Psychometrika                   doi         BF          ISSN                 PMID               S CID             

  Hartuv  Erez  Shamir  Ron                A clustering algorithm based on graph connectivity   Information Processing Letters                   doi         S                      ISSN                

  Remm  Maido  Storm  Christian E  V   Sonnhammer  Erik L  L                 Automatic clustering of orthologs and in paralogs from pairwise species comparisons  Edited by F  Cohen   Journal of Molecular Biology                      doi         jmbi            ISSN                 PMID               

  Botstein  David  Cox  David R   Risch  Neil  Olshen  Richard  Curb  David  Dzau  Victor J   Chen  Yii Der I   Hebert  Joan  Pesich  Robert                High Throughput Genotyping with Single Nucleotide Polymorphisms   Genome Research                     doi         gr         ISSN                 PMC              PMID               

  Filipovych  Roman  Resnick  Susan M   Davatzikos  Christos          Semi supervised Cluster Analysis of Imaging Data   NeuroImage                     doi         j neuroimage              PMC               PMID               

  a b Di Marco  Antonio  Navigli  Roberto          Clustering and Diversifying Web Search Results with Graph Based Word Sense Induction   Computational Linguistics                   doi         COLI a        S CID              

  a b c Beregovskaya  Irina  Koroteev  Mikhail          Review of Clustering Based Recommender Systems   arXiv             cs IR  

       Accelerate State of DevOps Report  PDF   Report   Google Cloud s DevOps Research and Assessment  DORA      September       pp                 

  Bewley  A   et      al   Real time volume estimation of a dragline payload   IEEE International Conference on Robotics and Automation                  

  Basak  S C   Magnuson  V R   Niemi  C J   Regal  R R           Determining Structural Similarity of Chemicals Using Graph Theoretic Indices   Discr  Appl  Math                   doi                 x            

  Huth  R   et      al           Classifications of Atmospheric Circulation Patterns  Recent Advances and Applications   PDF   Ann  N Y  Acad  Sci                     Bibcode     NYASA         H  doi         annals           PMID                S CID               

  Arnott  Robert D                 Cluster Analysis and Stock Price Comovement   Financial Analysts Journal                 doi         faj v   n      ISSN              X 



vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects

vteStatistics
Outline
Index
Descriptive statisticsContinuous dataCenter
Mean
Arithmetic
Arithmetic Geometric
Contraharmonic
Cubic
Generalized power
Geometric
Harmonic
Heronian
Heinz
Lehmer
Median
Mode
Dispersion
Average absolute deviation
Coefficient of variation
Interquartile range
Percentile
Range
Standard deviation
Variance
Shape
Central limit theorem
Moments
Kurtosis
L moments
Skewness
Count data
Index of dispersion
Summary tables
Contingency table
Frequency distribution
Grouped data
Dependence
Partial correlation
Pearson product moment correlation
Rank correlation
Kendall s  
Spearman s  
Scatter plot
Graphics
Bar chart
Biplot
Box plot
Control chart
Correlogram
Fan chart
Forest plot
Histogram
Pie chart
Q Q plot
Radar chart
Run chart
Scatter plot
Stem and leaf display
Violin plot
Data collectionStudy design
Effect size
Missing data
Optimal design
Population
Replication
Sample size determination
Statistic
Statistical power
Survey methodology
Sampling
Cluster
Stratified
Opinion poll
Questionnaire
Standard error
Controlled experiments
Blocking
Factorial experiment
Interaction
Random assignment
Randomized controlled trial
Randomized experiment
Scientific control
Adaptive designs
Adaptive clinical trial
Stochastic approximation
Up and down designs
Observational studies
Cohort study
Cross sectional study
Natural experiment
Quasi experiment
Statistical inferenceStatistical theory
Population
Statistic
Probability distribution
Sampling distribution
Order statistic
Empirical distribution
Density estimation
Statistical model
Model specification
Lp space
Parameter
location
scale
shape
Parametric family
Likelihood       monotone 
Location scale family
Exponential family
Completeness
Sufficiency
Statistical functional
Bootstrap
U
V
Optimal decision
loss function
Efficiency
Statistical distance
divergence
Asymptotics
Robustness
Frequentist inferencePoint estimation
Estimating equations
Maximum likelihood
Method of moments
M estimator
Minimum distance
Unbiased estimators
Mean unbiased minimum variance
Rao Blackwellization
Lehmann Scheff  theorem
Median unbiased
Plug in
Interval estimation
Confidence interval
Pivot
Likelihood interval
Prediction interval
Tolerance interval
Resampling
Bootstrap
Jackknife
Testing hypotheses
    amp    tails
Power
Uniformly most powerful test
Permutation test
Randomization test
Multiple comparisons
Parametric tests
Likelihood ratio
Score Lagrange multiplier
Wald
Specific tests
Z test  normal 
Student s t test
F test
Goodness of fit
Chi squared
G test
Kolmogorov Smirnov
Anderson Darling
Lilliefors
Jarque Bera
Normality  Shapiro Wilk 
Likelihood ratio test
Model selection
Cross validation
AIC
BIC
Rank statistics
Sign
Sample median
Signed rank  Wilcoxon 
Hodges Lehmann estimator
Rank sum  Mann Whitney 
Nonparametric anova
  way  Kruskal Wallis 
  way  Friedman 
Ordered alternative  Jonckheere Terpstra 
Van der Waerden test
Bayesian inference
Bayesian probability
prior
posterior
Credible interval
Bayes factor
Bayesian estimator
Maximum posterior estimator
CorrelationRegression analysisCorrelation
Pearson product moment
Partial correlation
Confounding variable
Coefficient of determination
Regression analysis
Errors and residuals
Regression validation
Mixed effects models
Simultaneous equations models
Multivariate adaptive regression splines  MARS 
Linear regression
Simple linear regression
Ordinary least squares
General linear model
Bayesian regression
Non standard predictors
Nonlinear regression
Nonparametric
Semiparametric
Isotonic
Robust
Homoscedasticity and Heteroscedasticity
Generalized linear model
Exponential families
Logistic  Bernoulli             Binomial            Poisson regressions
Partition of variance
Analysis of variance  ANOVA  anova 
Analysis of covariance
Multivariate ANOVA
Degrees of freedom
Categorical            Multivariate            Time series            Survival analysisCategorical
Cohen s kappa
Contingency table
Graphical model
Log linear model
McNemar s test
Cochran Mantel Haenszel statistics
Multivariate
Regression
Manova
Principal components
Canonical correlation
Discriminant analysis
Cluster analysis
Classification
Structural equation model
Factor analysis
Multivariate distributions
Elliptical distributions
Normal
Time seriesGeneral
Decomposition
Trend
Stationarity
Seasonal adjustment
Exponential smoothing
Cointegration
Structural break
Granger causality
Specific tests
Dickey Fuller
Johansen
Q statistic  Ljung Box 
Durbin Watson
Breusch Godfrey
Time domain
Autocorrelation  ACF 
partial  PACF 
Cross correlation  XCF 
ARMA model
ARIMA model  Box Jenkins 
Autoregressive conditional heteroskedasticity  ARCH 
Vector autoregression  VAR 
Frequency domain
Spectral density estimation
Fourier analysis
Least squares spectral analysis
Wavelet
Whittle likelihood
SurvivalSurvival function
Kaplan Meier estimator  product limit 
Proportional hazards models
Accelerated failure time  AFT  model
First hitting time
Hazard function
Nelson Aalen estimator
Test
Log rank test
ApplicationsBiostatistics
Bioinformatics
Clinical trials            studies
Epidemiology
Medical statistics
Engineering statistics
Chemometrics
Methods engineering
Probabilistic design
Process            quality control
Reliability
System identification
Social statistics
Actuarial science
Census
Crime statistics
Demography
Econometrics
Jurimetrics
National accounts
Official statistics
Population statistics
Psychometrics
Spatial statistics
Cartography
Environmental statistics
Geographic information system
Geostatistics
Kriging

Category
 Mathematics     portal
Commons
 WikiProject






Retrieved from  https   en wikipedia org w index php title Cluster analysis amp oldid