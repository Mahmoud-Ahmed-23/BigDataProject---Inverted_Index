Optimization algorithm for artificial neural networks
This article is about the computer algorithm  For the biological process  see neural backpropagation 
Backpropagation can also refer to the way the result of a playout is propagated up the search tree in Monte Carlo tree search Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
In machine learning  backpropagation is a gradient estimation method commonly used for training a neural network to compute its parameter updates 
It is an efficient application of the chain rule to neural networks  Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input output example  and does so efficiently  computing the gradient one layer at a time  iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule  this can be derived through dynamic programming                                  
Strictly speaking  the term backpropagation refers only to an algorithm for efficiently computing the gradient  not how the gradient is used  but the term is often used loosely to refer to the entire learning algorithm   including how the gradient is used  such as by stochastic gradient descent  or as an intermediate step in a more complicated optimizer  such as Adaptive Moment Estimation             The  local minimum convergence  exploding gradient  vanishing gradient  and weak control of learning rate are main disadvantages of these optimization algorithms  The Hessian and quasi Hessian optimizers solve only local minimum convergence problem  and the backpropagation works longer  These problems caused researchers to develop hybrid            and fractional            optimization algorithms  
Backpropagation had multiple discoveries and partial discoveries  with a tangled history and terminology  See the history section for details  Some other names for the technique include  reverse mode of automatic differentiation  or  reverse accumulation              


Overview edit 
Backpropagation computes the gradient in weight space of a feedforward neural network  with respect to a loss function  Denote 


  
    
      
        x
      
    
      displaystyle x 
  
  input  vector of features 

  
    
      
        y
      
    
      displaystyle y 
  
  target output
For classification  output will be a vector of class probabilities  e g   
  
    
      
         
           
         
           
         
           
         
      
    
      displaystyle               
  
  and target output is a specific class  encoded by the one hot dummy variable  e g   
  
    
      
         
         
         
         
         
         
         
      
    
      displaystyle         
  
  

  
    
      
        C
      
    
      displaystyle C 
  
  loss function or  cost function      a     
For classification  this is usually cross entropy  XC  log loss   while for regression it is usually squared error loss  SEL  

  
    
      
        L
      
    
      displaystyle L 
  
  the number of layers

  
    
      
        
          W
          
            l
          
        
         
         
        
          w
          
            j
            k
          
          
            l
          
        
         
      
    
      displaystyle W  l   w  jk   l   
  
  the weights between layer 
  
    
      
        l
          x     
         
      
    
      displaystyle l   
  
 and 
  
    
      
        l
      
    
      displaystyle l 
  
  where 
  
    
      
        
          w
          
            j
            k
          
          
            l
          
        
      
    
      displaystyle w  jk   l  
  
 is the weight between the 
  
    
      
        k
      
    
      displaystyle k 
  
 th node in layer 
  
    
      
        l
          x     
         
      
    
      displaystyle l   
  
 and the 
  
    
      
        j
      
    
      displaystyle j 
  
 th node in layer 
  
    
      
        l
      
    
      displaystyle l 
  
     b     

  
    
      
        
          f
          
            l
          
        
      
    
      displaystyle f  l  
  
  activation functions at layer 
  
    
      
        l
      
    
      displaystyle l 
  

For classification the last layer is usually the logistic function for binary classification  and softmax  softargmax  for multi class classification  while for the hidden layers this was traditionally a sigmoid function  logistic function or others  on each node  coordinate   but today is more varied  with rectifier  ramp  ReLU  being common 

  
    
      
        
          a
          
            j
          
          
            l
          
        
      
    
      displaystyle a  j   l  
  
  activation of the 
  
    
      
        j
      
    
      displaystyle j 
  
 th node in layer 
  
    
      
        l
      
    
      displaystyle l 
  
 
In the derivation of backpropagation  other intermediate quantities are used by introducing them as needed below  Bias terms are not treated specially since they correspond to a weight with a fixed input of    For backpropagation the specific loss function and activation functions do not matter as long as they and their derivatives can be evaluated efficiently  Traditional activation functions include sigmoid  tanh  and ReLU  Swish             mish             and other activation functions have since been proposed as well 
The overall network is a combination of function composition and matrix multiplication 


  
    
      
        g
         
        x
         
          
        
          f
          
            L
          
        
         
        
          W
          
            L
          
        
        
          f
          
            L
              x     
             
          
        
         
        
          W
          
            L
              x     
             
          
        
          x  ef 
        
          f
          
             
          
        
         
        
          W
          
             
          
        
        x
         
          x  ef 
         
         
      
    
      displaystyle g x   f  L  W  L f  L    W  L    cdots f     W    x  cdots    
  

For a training set there will be a set of input output pairs  
  
    
      
        
           
          
             
            
              x
              
                i
              
            
             
            
              y
              
                i
              
            
             
          
           
        
      
    
      displaystyle  left   x  i  y  i   right   
  
  For each input output pair 
  
    
      
         
        
          x
          
            i
          
        
         
        
          y
          
            i
          
        
         
      
    
      displaystyle  x  i  y  i   
  
 in the training set  the loss of the model on that pair is the cost of the difference between the predicted output 
  
    
      
        g
         
        
          x
          
            i
          
        
         
      
    
      displaystyle g x  i   
  
 and the target output 
  
    
      
        
          y
          
            i
          
        
      
    
      displaystyle y  i  
  
 


  
    
      
        C
         
        
          y
          
            i
          
        
         
        g
         
        
          x
          
            i
          
        
         
         
      
    
      displaystyle C y  i  g x  i    
  

Note the distinction  during model evaluation the weights are fixed while the inputs vary  and the target output may be unknown   and the network ends with the output layer  it does not include the loss function   During model training the input output pair is fixed while the weights vary  and the network ends with the loss function 
Backpropagation computes the gradient for a fixed input output pair 
  
    
      
         
        
          x
          
            i
          
        
         
        
          y
          
            i
          
        
         
      
    
      displaystyle  x  i  y  i   
  
  where the weights 
  
    
      
        
          w
          
            j
            k
          
          
            l
          
        
      
    
      displaystyle w  jk   l  
  
 can vary  Each individual component of the gradient  
  
    
      
          x     
        C
        
           
        
          x     
        
          w
          
            j
            k
          
          
            l
          
        
         
      
    
      displaystyle  partial C  partial w  jk   l   
  
 can be computed by the chain rule  but doing this separately for each weight is inefficient  Backpropagation efficiently computes the gradient by avoiding duplicate calculations and not computing unnecessary intermediate values  by computing the gradient of each layer   specifically the gradient of the weighted input of each layer  denoted by 
  
    
      
        
            x b  
          
            l
          
        
      
    
      displaystyle  delta   l  
  
   from back to front 
Informally  the key point is that since the only way a weight in 
  
    
      
        
          W
          
            l
          
        
      
    
      displaystyle W  l  
  
 affects the loss is through its effect on the next layer  and it does so linearly  
  
    
      
        
            x b  
          
            l
          
        
      
    
      displaystyle  delta   l  
  
 are the only data you need to compute the gradients of the weights at layer 
  
    
      
        l
      
    
      displaystyle l 
  
  and then the gradients of weights of previous layer can be computed by 
  
    
      
        
            x b  
          
            l
              x     
             
          
        
      
    
      displaystyle  delta   l    
  
 and repeated recursively  This avoids inefficiency in two ways  First  it avoids duplication because when computing the gradient at layer 
  
    
      
        l
      
    
      displaystyle l 
  
  it is unnecessary to recompute all derivatives on later layers 
  
    
      
        l
         
         
         
        l
         
         
         
          x     
      
    
      displaystyle l   l    ldots  
  
 each time  Second  it avoids unnecessary intermediate calculations  because at each stage it directly computes the gradient of the weights with respect to the ultimate output  the loss   rather than unnecessarily computing the derivatives of the values of hidden layers with respect to changes in weights 
  
    
      
          x     
        
          a
          
            
              j
                x     
            
          
          
            
              l
                x     
            
          
        
        
           
        
          x     
        
          w
          
            j
            k
          
          
            l
          
        
      
    
      displaystyle  partial a  j    l    partial w  jk   l  
  
 
Backpropagation can be expressed for simple feedforward networks in terms of matrix multiplication  or more generally in terms of the adjoint graph 

Matrix multiplication edit 
For the basic case of a feedforward network  where nodes in each layer are connected only to nodes in the immediate next layer  without skipping any layers   and there is a loss function that computes a scalar loss for the final output  backpropagation can be understood simply by matrix multiplication      c      Essentially  backpropagation evaluates the expression for the derivative of the cost function as a product of derivatives between each layer from right to left    backwards    with the gradient of the weights between each layer being a simple modification of the partial products  the  backwards propagated error   
Given an input output pair 
  
    
      
         
        x
         
        y
         
      
    
      displaystyle  x y  
  
  the loss is 


  
    
      
        C
         
        y
         
        
          f
          
            L
          
        
         
        
          W
          
            L
          
        
        
          f
          
            L
              x     
             
          
        
         
        
          W
          
            L
              x     
             
          
        
          x  ef 
        
          f
          
             
          
        
         
        
          W
          
             
          
        
        
          f
          
             
          
        
         
        
          W
          
             
          
        
        x
         
         
          x  ef 
         
         
         
      
    
      displaystyle C y f  L  W  L f  L    W  L    cdots f     W    f     W    x   cdots     
  

To compute this  one starts with the input 
  
    
      
        x
      
    
      displaystyle x 
  
 and works forward  denote the weighted input of each hidden layer as 
  
    
      
        
          z
          
            l
          
        
      
    
      displaystyle z  l  
  
 and the output of hidden layer 
  
    
      
        l
      
    
      displaystyle l 
  
 as the activation 
  
    
      
        
          a
          
            l
          
        
      
    
      displaystyle a  l  
  
  For backpropagation  the activation 
  
    
      
        
          a
          
            l
          
        
      
    
      displaystyle a  l  
  
 as well as the derivatives 
  
    
      
         
        
          f
          
            l
          
        
        
           
            x     
        
      
    
      displaystyle  f  l    
  
  evaluated at 
  
    
      
        
          z
          
            l
          
        
      
    
      displaystyle z  l  
  
  must be cached for use during the backwards pass 
The derivative of the loss in terms of the inputs is given by the chain rule  note that each term is a total derivative  evaluated at the value of the network  at each node  on the input 
  
    
      
        x
      
    
      displaystyle x 
  
 


  
    
      
        
          
            
              d
              C
            
            
              d
              
                a
                
                  L
                
              
            
          
        
          x  c  
        
          
            
              d
              
                a
                
                  L
                
              
            
            
              d
              
                z
                
                  L
                
              
            
          
        
          x  c  
        
          
            
              d
              
                z
                
                  L
                
              
            
            
              d
              
                a
                
                  L
                    x     
                   
                
              
            
          
        
          x  c  
        
          
            
              d
              
                a
                
                  L
                    x     
                   
                
              
            
            
              d
              
                z
                
                  L
                    x     
                   
                
              
            
          
        
          x  c  
        
          
            
              d
              
                z
                
                  L
                    x     
                   
                
              
            
            
              d
              
                a
                
                  L
                    x     
                   
                
              
            
          
        
          x  c  
          x     
          x  c  
        
          
            
              d
              
                a
                
                   
                
              
            
            
              d
              
                z
                
                   
                
              
            
          
        
          x  c  
        
          
            
                x     
              
                z
                
                   
                
              
            
            
                x     
              x
            
          
        
         
      
    
      displaystyle   frac  dC  da  L    cdot   frac  da  L   dz  L    cdot   frac  dz  L   da  L      cdot   frac  da  L     dz  L      cdot   frac  dz  L     da  L      cdot  ldots  cdot   frac  da      dz       cdot   frac   partial z       partial x    
  

where 
  
    
      
        
          
            
              d
              
                a
                
                  L
                
              
            
            
              d
              
                z
                
                  L
                
              
            
          
        
      
    
      displaystyle   frac  da  L   dz  L    
  
 is a diagonal matrix 
These terms are  the derivative of the loss function      d      the derivatives of the activation functions      e      and the matrices of weights      f     


  
    
      
        
          
            
              d
              C
            
            
              d
              
                a
                
                  L
                
              
            
          
        
          x     
         
        
          f
          
            L
          
        
        
           
            x     
        
          x  c  
        
          W
          
            L
          
        
          x     
         
        
          f
          
            L
              x     
             
          
        
        
           
            x     
        
          x  c  
        
          W
          
            L
              x     
             
          
        
          x     
          x  ef 
          x     
         
        
          f
          
             
          
        
        
           
            x     
        
          x  c  
        
          W
          
             
          
        
         
      
    
      displaystyle   frac  dC  da  L    circ  f  L    cdot W  L  circ  f  L      cdot W  L    circ  cdots  circ  f       cdot W      
  

The gradient 
  
    
      
          x     
      
    
      displaystyle  nabla  
  
 is the transpose of the derivative of the output in terms of the input  so the matrices are transposed and the order of multiplication is reversed  but the entries are the same 


  
    
      
        
            x     
          
            x
          
        
        C
         
         
        
          W
          
             
          
        
        
           
          
            T
          
        
          x  c  
         
        
          f
          
             
          
        
        
           
            x     
        
          x     
          x     
          x     
         
        
          W
          
            L
              x     
             
          
        
        
           
          
            T
          
        
          x  c  
         
        
          f
          
            L
              x     
             
          
        
        
           
            x     
        
          x     
         
        
          W
          
            L
          
        
        
           
          
            T
          
        
          x  c  
         
        
          f
          
            L
          
        
        
           
            x     
        
          x     
        
            x     
          
            
              a
              
                L
              
            
          
        
        C
         
      
    
      displaystyle  nabla   x C  W       T  cdot  f       circ  ldots  circ  W  L      T  cdot  f  L      circ  W  L    T  cdot  f  L    circ  nabla   a  L  C  
  

Backpropagation then consists essentially of evaluating this expression from right to left  equivalently  multiplying the previous expression for the derivative from left to right   computing the gradient at each layer on the way  there is an added step  because the gradient of the weights is not just a subexpression  there s an extra multiplication 
Introducing the auxiliary quantity 
  
    
      
        
            x b  
          
            l
          
        
      
    
      displaystyle  delta   l  
  
 for the partial products  multiplying from right to left   interpreted as the  error at level 
  
    
      
        l
      
    
      displaystyle l 
  
  and defined as the gradient of the input values at level 
  
    
      
        l
      
    
      displaystyle l 
  
 


  
    
      
        
            x b  
          
            l
          
        
          
         
        
          f
          
            l
          
        
        
           
            x     
        
          x     
         
        
          W
          
            l
             
             
          
        
        
           
          
            T
          
        
          x  c  
         
        
          f
          
            l
             
             
          
        
        
           
            x     
        
          x     
          x  ef 
          x     
         
        
          W
          
            L
              x     
             
          
        
        
           
          
            T
          
        
          x  c  
         
        
          f
          
            L
              x     
             
          
        
        
           
            x     
        
          x     
         
        
          W
          
            L
          
        
        
           
          
            T
          
        
          x  c  
         
        
          f
          
            L
          
        
        
           
            x     
        
          x     
        
            x     
          
            
              a
              
                L
              
            
          
        
        C
         
      
    
      displaystyle  delta   l    f  l    circ  W  l      T  cdot  f  l      circ  cdots  circ  W  L      T  cdot  f  L      circ  W  L    T  cdot  f  L    circ  nabla   a  L  C  
  

Note that 
  
    
      
        
            x b  
          
            l
          
        
      
    
      displaystyle  delta   l  
  
 is a vector  of length equal to the number of nodes in level 
  
    
      
        l
      
    
      displaystyle l 
  
  each component is interpreted as the  cost attributable to  the value of  that node  
The gradient of the weights in layer 
  
    
      
        l
      
    
      displaystyle l 
  
 is then 


  
    
      
        
            x     
          
            
              W
              
                l
              
            
          
        
        C
         
        
            x b  
          
            l
          
        
         
        
          a
          
            l
              x     
             
          
        
        
           
          
            T
          
        
         
      
    
      displaystyle  nabla   W  l  C  delta   l  a  l      T   
  

The factor of 
  
    
      
        
          a
          
            l
              x     
             
          
        
      
    
      displaystyle a  l    
  
 is because the weights 
  
    
      
        
          W
          
            l
          
        
      
    
      displaystyle W  l  
  
 between level 
  
    
      
        l
          x     
         
      
    
      displaystyle l   
  
 and 
  
    
      
        l
      
    
      displaystyle l 
  
 affect level 
  
    
      
        l
      
    
      displaystyle l 
  
 proportionally to the inputs  activations   the inputs are fixed  the weights vary 
The 
  
    
      
        
            x b  
          
            l
          
        
      
    
      displaystyle  delta   l  
  
 can easily be computed recursively  going from right to left  as 


  
    
      
        
            x b  
          
            l
              x     
             
          
        
          
         
        
          f
          
            l
              x     
             
          
        
        
           
            x     
        
          x     
         
        
          W
          
            l
          
        
        
           
          
            T
          
        
          x  c  
        
            x b  
          
            l
          
        
         
      
    
      displaystyle  delta   l      f  l      circ  W  l    T  cdot  delta   l   
  

The gradients of the weights can thus be computed using a few matrix multiplications for each level  this is backpropagation 
Compared with naively computing forwards  using the 
  
    
      
        
            x b  
          
            l
          
        
      
    
      displaystyle  delta   l  
  
 for illustration  


  
    
      
        
          
            
              
                
                    x b  
                  
                     
                  
                
              
              
                
                 
                 
                
                  f
                  
                     
                  
                
                
                   
                    x     
                
                  x     
                 
                
                  W
                  
                     
                  
                
                
                   
                  
                    T
                  
                
                  x  c  
                 
                
                  f
                  
                     
                  
                
                
                   
                    x     
                
                  x     
                  x  ef 
                  x     
                 
                
                  W
                  
                    L
                      x     
                     
                  
                
                
                   
                  
                    T
                  
                
                  x  c  
                 
                
                  f
                  
                    L
                      x     
                     
                  
                
                
                   
                    x     
                
                  x     
                 
                
                  W
                  
                    L
                  
                
                
                   
                  
                    T
                  
                
                  x  c  
                 
                
                  f
                  
                    L
                  
                
                
                   
                    x     
                
                  x     
                
                    x     
                  
                    
                      a
                      
                        L
                      
                    
                  
                
                C
              
            
            
              
                
                    x b  
                  
                     
                  
                
              
              
                
                 
                 
                
                  f
                  
                     
                  
                
                
                   
                    x     
                
                  x     
                  x  ef 
                  x     
                 
                
                  W
                  
                    L
                      x     
                     
                  
                
                
                   
                  
                    T
                  
                
                  x  c  
                 
                
                  f
                  
                    L
                      x     
                     
                  
                
                
                   
                    x     
                
                  x     
                 
                
                  W
                  
                    L
                  
                
                
                   
                  
                    T
                  
                
                  x  c  
                 
                
                  f
                  
                    L
                  
                
                
                   
                    x     
                
                  x     
                
                    x     
                  
                    
                      a
                      
                        L
                      
                    
                  
                
                C
              
            
            
              
              
                
                  x  ee 
              
            
            
              
                
                    x b  
                  
                    L
                      x     
                     
                  
                
              
              
                
                 
                 
                
                  f
                  
                    L
                      x     
                     
                  
                
                
                   
                    x     
                
                  x     
                 
                
                  W
                  
                    L
                  
                
                
                   
                  
                    T
                  
                
                  x  c  
                 
                
                  f
                  
                    L
                  
                
                
                   
                    x     
                
                  x     
                
                    x     
                  
                    
                      a
                      
                        L
                      
                    
                  
                
                C
              
            
            
              
                
                    x b  
                  
                    L
                  
                
              
              
                
                 
                 
                
                  f
                  
                    L
                  
                
                
                   
                    x     
                
                  x     
                
                    x     
                  
                    
                      a
                      
                        L
                      
                    
                  
                
                C
                 
              
            
          
        
      
    
      displaystyle   begin aligned  delta      amp   f       circ  W       T  cdot  f       circ  cdots  circ  W  L      T  cdot  f  L      circ  W  L    T  cdot  f  L    circ  nabla   a  L  C   delta      amp   f       circ  cdots  circ  W  L      T  cdot  f  L      circ  W  L    T  cdot  f  L    circ  nabla   a  L  C   amp  vdots    delta   L    amp   f  L      circ  W  L    T  cdot  f  L    circ  nabla   a  L  C   delta   L  amp   f  L    circ  nabla   a  L  C  end aligned   
  

There are two key differences with backpropagation 

Computing 
  
    
      
        
            x b  
          
            l
              x     
             
          
        
      
    
      displaystyle  delta   l    
  
 in terms of 
  
    
      
        
            x b  
          
            l
          
        
      
    
      displaystyle  delta   l  
  
 avoids the obvious duplicate multiplication of layers 
  
    
      
        l
      
    
      displaystyle l 
  
 and beyond 
Multiplying starting from 
  
    
      
        
            x     
          
            
              a
              
                L
              
            
          
        
        C
      
    
      displaystyle  nabla   a  L  C 
  
   propagating the error backwards   means that each step simply multiplies a vector  
  
    
      
        
            x b  
          
            l
          
        
      
    
      displaystyle  delta   l  
  
  by the matrices of weights 
  
    
      
         
        
          W
          
            l
          
        
        
           
          
            T
          
        
      
    
      displaystyle  W  l    T  
  
 and derivatives of activations 
  
    
      
         
        
          f
          
            l
              x     
             
          
        
        
           
            x     
        
      
    
      displaystyle  f  l      
  
  By contrast  multiplying forwards  starting from the changes at an earlier layer  means that each multiplication multiplies a matrix by a matrix  This is much more expensive  and corresponds to tracking every possible path of a change in one layer 
  
    
      
        l
      
    
      displaystyle l 
  
 forward to changes in the layer 
  
    
      
        l
         
         
      
    
      displaystyle l   
  
  for multiplying 
  
    
      
        
          W
          
            l
             
             
          
        
      
    
      displaystyle W  l    
  
 by 
  
    
      
        
          W
          
            l
             
             
          
        
      
    
      displaystyle W  l    
  
  with additional multiplications for the derivatives of the activations   which unnecessarily computes the intermediate quantities of how weight changes affect the values of hidden nodes 
Adjoint graph edit 
This section needs expansion  You can help by adding to it    November      
For more general graphs  and other advanced variations  backpropagation can be understood in terms of automatic differentiation  where backpropagation is a special case of reverse accumulation  or  reverse mode              

Intuition edit 
Motivation edit 
The goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output  The motivation for backpropagation is to train a multi layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output             

Learning as an optimization problem edit 
To understand the mathematical derivation of the backpropagation algorithm  it helps to first develop some intuition about the relationship between the actual output of a neuron and the correct output for a particular training example  Consider a simple neural network with two input units  one output unit and no hidden units  and in which each neuron uses a linear output  unlike most work on neural networks  in which mapping from inputs to outputs is non linear      g      that is the weighted sum of its input  A simple neural network with two input units  each with a single input  and one output unit  with two inputs 
Initially  before training  the weights will be set randomly  Then the neuron learns from training examples  which in this case consist of a set of tuples 
  
    
      
         
        
          x
          
             
          
        
         
        
          x
          
             
          
        
         
        t
         
      
    
      displaystyle  x     x     t  
  
 where 
  
    
      
        
          x
          
             
          
        
      
    
      displaystyle x     
  
 and 
  
    
      
        
          x
          
             
          
        
      
    
      displaystyle x     
  
 are the inputs to the network and t is the correct output  the output the network should produce given those inputs  when it has been trained   The initial network  given 
  
    
      
        
          x
          
             
          
        
      
    
      displaystyle x     
  
 and 
  
    
      
        
          x
          
             
          
        
      
    
      displaystyle x     
  
  will compute an output y that likely differs from t  given random weights   A loss function 
  
    
      
        L
         
        t
         
        y
         
      
    
      displaystyle L t y  
  
 is used for measuring the discrepancy between the target output t and the computed output y  For regression analysis problems the squared error can be used as a loss function  for classification the categorical cross entropy can be used 
As an example consider a regression problem using the square error as a loss 


  
    
      
        L
         
        t
         
        y
         
         
         
        t
          x     
        y
        
           
          
             
          
        
         
        E
         
      
    
      displaystyle L t y   t y      E  
  

where E is the discrepancy or error 

Consider the network on a single training case  
  
    
      
         
         
         
         
         
         
         
      
    
      displaystyle         
  
  Thus  the input 
  
    
      
        
          x
          
             
          
        
      
    
      displaystyle x     
  
 and 
  
    
      
        
          x
          
             
          
        
      
    
      displaystyle x     
  
 are   and   respectively and the correct output  t is    Now if the relation is plotted between the network s output y on the horizontal axis and the error E on the vertical axis  the result is a parabola  The minimum of the parabola corresponds to the output y which minimizes the error E  For a single training case  the minimum also touches the horizontal axis  which means the error will be zero and the network can produce an output y that exactly matches the target output t  Therefore  the problem of mapping inputs to outputs can be reduced to an optimization problem of finding a function that will produce the minimal error  Error surface of a linear neuron for a single training case
However  the output of a neuron depends on the weighted sum of all its inputs 


  
    
      
        y
         
        
          x
          
             
          
        
        
          w
          
             
          
        
         
        
          x
          
             
          
        
        
          w
          
             
          
        
         
      
    
      displaystyle y x    w     x    w      
  

where 
  
    
      
        
          w
          
             
          
        
      
    
      displaystyle w     
  
 and 
  
    
      
        
          w
          
             
          
        
      
    
      displaystyle w     
  
 are the weights on the connection from the input units to the output unit  Therefore  the error also depends on the incoming weights to the neuron  which is ultimately what needs to be changed in the network to enable learning 
In this example  upon injecting the training data 
  
    
      
         
         
         
         
         
         
         
      
    
      displaystyle         
  
  the loss function becomes

  
    
      
        E
         
         
        t
          x     
        y
        
           
          
             
          
        
         
        
          y
          
             
          
        
         
         
        
          x
          
             
          
        
        
          w
          
             
          
        
         
        
          x
          
             
          
        
        
          w
          
             
          
        
        
           
          
             
          
        
         
         
        
          w
          
             
          
        
         
        
          w
          
             
          
        
        
           
          
             
          
        
         
      
    
      displaystyle E  t y      y      x    w     x    w           w     w           
  

Then  the loss function 
  
    
      
        E
      
    
      displaystyle E 
  
 takes the form of a parabolic cylinder with its base directed along 
  
    
      
        
          w
          
             
          
        
         
          x     
        
          w
          
             
          
        
      
    
      displaystyle w      w     
  
  Since all sets of weights that satisfy 
  
    
      
        
          w
          
             
          
        
         
          x     
        
          w
          
             
          
        
      
    
      displaystyle w      w     
  
 minimize the loss function  in this case additional constraints are required to converge to a unique solution  Additional constraints could either be generated by setting specific conditions to the weights  or by injecting additional training data 
One commonly used algorithm to find the set of weights that minimizes the error is gradient descent  By backpropagation  the steepest descent direction is calculated of the loss function versus the present synaptic weights  Then  the weights can be modified along the steepest descent direction  and the error is minimized in an efficient way 

Derivation edit 
The gradient descent method involves calculating the derivative of the loss function with respect to the weights of the network  This is normally done using backpropagation  Assuming one output neuron      h      the squared error function is


  
    
      
        E
         
        L
         
        t
         
        y
         
      
    
      displaystyle E L t y  
  

where


  
    
      
        L
      
    
      displaystyle L 
  
 is the loss for the output 
  
    
      
        y
      
    
      displaystyle y 
  
 and target value 
  
    
      
        t
      
    
      displaystyle t 
  
 

  
    
      
        t
      
    
      displaystyle t 
  
 is the target output for a training sample  and

  
    
      
        y
      
    
      displaystyle y 
  
 is the actual output of the output neuron 
For each neuron 
  
    
      
        j
      
    
      displaystyle j 
  
  its output 
  
    
      
        
          o
          
            j
          
        
      
    
      displaystyle o  j  
  
 is defined as


  
    
      
        
          o
          
            j
          
        
         
          x c  
         
        
          
            net
          
          
            j
          
        
         
         
          x c  
        
           
          
            
                x     
              
                k
                 
                 
              
              
                n
              
            
            
              w
              
                k
                j
              
            
            
              x
              
                k
              
            
          
           
        
         
      
    
      displaystyle o  j   varphi    text net    j    varphi  left  sum   k     n w  kj x  k  right   
  

where the activation function 
  
    
      
          x c  
      
    
      displaystyle  varphi  
  
 is non linear and differentiable over the activation region  the ReLU is not differentiable at one point   A historically used activation function is the logistic function 


  
    
      
          x c  
         
        z
         
         
        
          
             
            
               
               
              
                e
                
                    x     
                  z
                
              
            
          
        
      
    
      displaystyle  varphi  z    frac       e   z    
  

which has a convenient derivative of 


  
    
      
        
          
            
              d
                x c  
            
            
              d
              z
            
          
        
         
          x c  
         
        z
         
         
         
          x     
          x c  
         
        z
         
         
      
    
      displaystyle   frac  d varphi   dz    varphi  z     varphi  z   
  

The input 
  
    
      
        
          
            net
          
          
            j
          
        
      
    
      displaystyle   text net    j  
  
 to a neuron is the weighted sum of outputs 
  
    
      
        
          o
          
            k
          
        
      
    
      displaystyle o  k  
  
 of previous neurons  If the neuron is in the first layer after the input layer  the 
  
    
      
        
          o
          
            k
          
        
      
    
      displaystyle o  k  
  
 of the input layer are simply the inputs 
  
    
      
        
          x
          
            k
          
        
      
    
      displaystyle x  k  
  
 to the network  The number of input units to the neuron is 
  
    
      
        n
      
    
      displaystyle n 
  
  The variable 
  
    
      
        
          w
          
            k
            j
          
        
      
    
      displaystyle w  kj  
  
 denotes the weight between neuron 
  
    
      
        k
      
    
      displaystyle k 
  
 of the previous layer and neuron 
  
    
      
        j
      
    
      displaystyle j 
  
 of the current layer 

Finding the derivative of the error edit 
Diagram of an artificial neural network to illustrate the notation used here
Calculating the partial derivative of the error with respect to a weight 
  
    
      
        
          w
          
            i
            j
          
        
      
    
      displaystyle w  ij  
  
 is done using the chain rule twice 


  
    
      
        
          
            
                x     
              E
            
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
         
        
          
            
                x     
              E
            
            
                x     
              
                o
                
                  j
                
              
            
          
        
        
          
            
                x     
              
                o
                
                  j
                
              
            
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
         
        
          
            
                x     
              E
            
            
                x     
              
                o
                
                  j
                
              
            
          
        
        
          
            
                x     
              
                o
                
                  j
                
              
            
            
                x     
              
                
                  net
                
                
                  j
                
              
            
          
        
        
          
            
                x     
              
                
                  net
                
                
                  j
                
              
            
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
      
    
      displaystyle   frac   partial E   partial w  ij      frac   partial E   partial o  j     frac   partial o  j    partial w  ij      frac   partial E   partial o  j     frac   partial o  j    partial   text net    j     frac   partial   text net    j    partial w  ij    
  
  Eq   
In the last factor of the right hand side of the above  only one term in the sum 
  
    
      
        
          
            net
          
          
            j
          
        
      
    
      displaystyle   text net    j  
  
 depends on 
  
    
      
        
          w
          
            i
            j
          
        
      
    
      displaystyle w  ij  
  
  so that


  
    
      
        
          
            
                x     
              
                
                  net
                
                
                  j
                
              
            
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
         
        
          
              x     
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
        
           
          
            
                x     
              
                k
                 
                 
              
              
                n
              
            
            
              w
              
                k
                j
              
            
            
              o
              
                k
              
            
          
           
        
         
        
          
              x     
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
        
          w
          
            i
            j
          
        
        
          o
          
            i
          
        
         
        
          o
          
            i
          
        
         
      
    
      displaystyle   frac   partial   text net    j    partial w  ij      frac   partial    partial w  ij    left  sum   k     n w  kj o  k  right    frac   partial    partial w  ij   w  ij o  i  o  i   
  
  Eq   
If the neuron is in the first layer after the input layer  
  
    
      
        
          o
          
            i
          
        
      
    
      displaystyle o  i  
  
 is just 
  
    
      
        
          x
          
            i
          
        
      
    
      displaystyle x  i  
  
 
The derivative of the output of neuron 
  
    
      
        j
      
    
      displaystyle j 
  
 with respect to its input is simply the partial derivative of the activation function 


  
    
      
        
          
            
                x     
              
                o
                
                  j
                
              
            
            
                x     
              
                
                  net
                
                
                  j
                
              
            
          
        
         
        
          
            
                x     
                x c  
               
              
                
                  net
                
                
                  j
                
              
               
            
            
                x     
              
                
                  net
                
                
                  j
                
              
            
          
        
      
    
      displaystyle   frac   partial o  j    partial   text net    j      frac   partial  varphi    text net    j     partial   text net    j    
  
  Eq   
which for the logistic activation function 


  
    
      
        
          
            
                x     
              
                o
                
                  j
                
              
            
            
                x     
              
                
                  net
                
                
                  j
                
              
            
          
        
         
        
          
              x     
            
                x     
              
                
                  net
                
                
                  j
                
              
            
          
        
          x c  
         
        
          
            net
          
          
            j
          
        
         
         
          x c  
         
        
          
            net
          
          
            j
          
        
         
         
         
          x     
          x c  
         
        
          
            net
          
          
            j
          
        
         
         
         
        
          o
          
            j
          
        
         
         
          x     
        
          o
          
            j
          
        
         
      
    
      displaystyle   frac   partial o  j    partial   text net    j      frac   partial    partial   text net    j    varphi    text net    j    varphi    text net    j      varphi    text net    j    o  j    o  j   
  

This is the reason why backpropagation requires that the activation function be differentiable   Nevertheless  the ReLU activation function  which is non differentiable at    has become quite popular  e g  in AlexNet 
The first factor is straightforward to evaluate if the neuron is in the output layer  because then 
  
    
      
        
          o
          
            j
          
        
         
        y
      
    
      displaystyle o  j  y 
  
 and


  
    
      
        
          
            
                x     
              E
            
            
                x     
              
                o
                
                  j
                
              
            
          
        
         
        
          
            
                x     
              E
            
            
                x     
              y
            
          
        
      
    
      displaystyle   frac   partial E   partial o  j      frac   partial E   partial y   
  
  Eq   
If half of the square error is used as loss function we can rewrite it as


  
    
      
        
          
            
                x     
              E
            
            
                x     
              
                o
                
                  j
                
              
            
          
        
         
        
          
            
                x     
              E
            
            
                x     
              y
            
          
        
         
        
          
              x     
            
                x     
              y
            
          
        
        
          
             
             
          
        
         
        t
          x     
        y
        
           
          
             
          
        
         
        y
          x     
        t
      
    
      displaystyle   frac   partial E   partial o  j      frac   partial E   partial y     frac   partial    partial y    frac         t y      y t 
  

However  if 
  
    
      
        j
      
    
      displaystyle j 
  
 is in an arbitrary inner layer of the network  finding the derivative 
  
    
      
        E
      
    
      displaystyle E 
  
 with respect to 
  
    
      
        
          o
          
            j
          
        
      
    
      displaystyle o  j  
  
 is less obvious 
Considering 
  
    
      
        E
      
    
      displaystyle E 
  
 as a function with the inputs being all neurons 
  
    
      
        L
         
         
        u
         
        v
         
          x     
         
        w
         
      
    
      displaystyle L   u v  dots  w   
  
 receiving input from neuron 
  
    
      
        j
      
    
      displaystyle j 
  
 


  
    
      
        
          
            
                x     
              E
               
              
                o
                
                  j
                
              
               
            
            
                x     
              
                o
                
                  j
                
              
            
          
        
         
        
          
            
                x     
              E
               
              
                
                  n
                  e
                  t
                
                
                  u
                
              
               
              
                
                  net
                
                
                  v
                
              
               
                x     
               
              
                
                  n
                  e
                  t
                
                
                  w
                
              
               
            
            
                x     
              
                o
                
                  j
                
              
            
          
        
      
    
      displaystyle   frac   partial E o  j     partial o  j      frac   partial E  mathrm  net    u    text net    v   dots   mathrm  net    w     partial o  j    
  

and taking the total derivative with respect to 
  
    
      
        
          o
          
            j
          
        
      
    
      displaystyle o  j  
  
  a recursive expression for the derivative is obtained 


  
    
      
        
          
            
                x     
              E
            
            
                x     
              
                o
                
                  j
                
              
            
          
        
         
        
            x     
          
              x     
              x     
            L
          
        
        
           
          
            
              
                
                    x     
                  E
                
                
                    x     
                  
                    
                      net
                    
                    
                        x     
                    
                  
                
              
            
            
              
                
                    x     
                  
                    
                      net
                    
                    
                        x     
                    
                  
                
                
                    x     
                  
                    o
                    
                      j
                    
                  
                
              
            
          
           
        
         
        
            x     
          
              x     
              x     
            L
          
        
        
           
          
            
              
                
                    x     
                  E
                
                
                    x     
                  
                    o
                    
                        x     
                    
                  
                
              
            
            
              
                
                    x     
                  
                    o
                    
                        x     
                    
                  
                
                
                    x     
                  
                    
                      net
                    
                    
                        x     
                    
                  
                
              
            
            
              
                
                    x     
                  
                    
                      net
                    
                    
                        x     
                    
                  
                
                
                    x     
                  
                    o
                    
                      j
                    
                  
                
              
            
          
           
        
         
        
            x     
          
              x     
              x     
            L
          
        
        
           
          
            
              
                
                    x     
                  E
                
                
                    x     
                  
                    o
                    
                        x     
                    
                  
                
              
            
            
              
                
                    x     
                  
                    o
                    
                        x     
                    
                  
                
                
                    x     
                  
                    
                      net
                    
                    
                        x     
                    
                  
                
              
            
            
              w
              
                j
                  x     
              
            
          
           
        
      
    
      displaystyle   frac   partial E   partial o  j     sum    ell  in L  left   frac   partial E   partial   text net     ell      frac   partial   text net     ell     partial o  j    right   sum    ell  in L  left   frac   partial E   partial o   ell      frac   partial o   ell     partial   text net     ell      frac   partial   text net     ell     partial o  j    right   sum    ell  in L  left   frac   partial E   partial o   ell      frac   partial o   ell     partial   text net     ell    w  j ell   right  
  
  Eq   
Therefore  the derivative with respect to 
  
    
      
        
          o
          
            j
          
        
      
    
      displaystyle o  j  
  
 can be calculated if all the derivatives with respect to the outputs 
  
    
      
        
          o
          
              x     
          
        
      
    
      displaystyle o   ell   
  
 of the next layer   the ones closer to the output neuron   are known   Note  if any of the neurons in set 
  
    
      
        L
      
    
      displaystyle L 
  
 were not connected to neuron 
  
    
      
        j
      
    
      displaystyle j 
  
  they would be independent of 
  
    
      
        
          w
          
            i
            j
          
        
      
    
      displaystyle w  ij  
  
 and the corresponding partial derivative under the summation would vanish to    
Substituting Eq     Eq    Eq   and Eq    in Eq    we obtain 


  
    
      
        
          
            
                x     
              E
            
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
         
        
          
            
                x     
              E
            
            
                x     
              
                o
                
                  j
                
              
            
          
        
        
          
            
                x     
              
                o
                
                  j
                
              
            
            
                x     
              
                
                  net
                
                
                  j
                
              
            
          
        
        
          
            
                x     
              
                
                  net
                
                
                  j
                
              
            
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
         
        
          
            
                x     
              E
            
            
                x     
              
                o
                
                  j
                
              
            
          
        
        
          
            
                x     
              
                o
                
                  j
                
              
            
            
                x     
              
                
                  net
                
                
                  j
                
              
            
          
        
        
          o
          
            i
          
        
      
    
      displaystyle   frac   partial E   partial w  ij      frac   partial E   partial o  j     frac   partial o  j    partial   text net    j     frac   partial   text net    j    partial w  ij      frac   partial E   partial o  j     frac   partial o  j    partial   text net    j   o  i  
  


  
    
      
        
          
            
                x     
              E
            
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
         
        
          o
          
            i
          
        
        
            x b  
          
            j
          
        
      
    
      displaystyle   frac   partial E   partial w  ij    o  i  delta   j  
  

with


  
    
      
        
            x b  
          
            j
          
        
         
        
          
            
                x     
              E
            
            
                x     
              
                o
                
                  j
                
              
            
          
        
        
          
            
                x     
              
                o
                
                  j
                
              
            
            
                x     
              
                
                  net
                
                
                  j
                
              
            
          
        
         
        
          
             
            
              
                
                  
                    
                      
                          x     
                        L
                         
                        t
                         
                        
                          o
                          
                            j
                          
                        
                         
                      
                      
                          x     
                        
                          o
                          
                            j
                          
                        
                      
                    
                  
                  
                    
                      
                        d
                          x c  
                         
                        
                          
                            net
                          
                          
                            j
                          
                        
                         
                      
                      
                        d
                        
                          
                            net
                          
                          
                            j
                          
                        
                      
                    
                  
                
                
                  
                    if  xa  
                  
                  j
                  
                      xa  is an output neuron 
                  
                
              
              
                
                   
                  
                      x     
                    
                        x     
                        x     
                      L
                    
                  
                  
                    w
                    
                      j
                        x     
                    
                  
                  
                      x b  
                    
                        x     
                    
                  
                   
                  
                    
                      
                        d
                          x c  
                         
                        
                          
                            net
                          
                          
                            j
                          
                        
                         
                      
                      
                        d
                        
                          
                            net
                          
                          
                            j
                          
                        
                      
                    
                  
                
                
                  
                    if  xa  
                  
                  j
                  
                      xa  is an inner neuron 
                  
                
              
            
            
          
        
      
    
      displaystyle  delta   j    frac   partial E   partial o  j     frac   partial o  j    partial   text net    j      begin cases   frac   partial L t o  j     partial o  j     frac  d varphi    text net    j    d  text net    j    amp   text if   j  text  is an output neuron       sum    ell  in L w  j ell   delta    ell     frac  d varphi    text net    j    d  text net    j    amp   text if   j  text  is an inner neuron    end cases   
  

if 
  
    
      
          x c  
      
    
      displaystyle  varphi  
  
 is the logistic function  and the error is the square error 


  
    
      
        
            x b  
          
            j
          
        
         
        
          
            
                x     
              E
            
            
                x     
              
                o
                
                  j
                
              
            
          
        
        
          
            
                x     
              
                o
                
                  j
                
              
            
            
                x     
              
                
                  net
                
                
                  j
                
              
            
          
        
         
        
          
             
            
              
                
                   
                  
                    o
                    
                      j
                    
                  
                    x     
                  
                    t
                    
                      j
                    
                  
                   
                  
                    o
                    
                      j
                    
                  
                   
                   
                    x     
                  
                    o
                    
                      j
                    
                  
                   
                
                
                  
                    if  xa  
                  
                  j
                  
                      xa  is an output neuron 
                  
                
              
              
                
                   
                  
                      x     
                    
                        x     
                        x     
                      L
                    
                  
                  
                    w
                    
                      j
                        x     
                    
                  
                  
                      x b  
                    
                        x     
                    
                  
                   
                  
                    o
                    
                      j
                    
                  
                   
                   
                    x     
                  
                    o
                    
                      j
                    
                  
                   
                
                
                  
                    if  xa  
                  
                  j
                  
                      xa  is an inner neuron 
                  
                
              
            
            
          
        
      
    
      displaystyle  delta   j    frac   partial E   partial o  j     frac   partial o  j    partial   text net    j      begin cases  o  j  t  j  o  j    o  j   amp   text if   j  text  is an output neuron       sum    ell  in L w  j ell   delta    ell   o  j    o  j   amp   text if   j  text  is an inner neuron    end cases   
  

To update the weight 
  
    
      
        
          w
          
            i
            j
          
        
      
    
      displaystyle w  ij  
  
 using gradient descent  one must choose a learning rate  
  
    
      
          x b  
         gt 
         
      
    
      displaystyle  eta  gt   
  
  The change in weight needs to reflect the impact on 
  
    
      
        E
      
    
      displaystyle E 
  
 of an increase or decrease in 
  
    
      
        
          w
          
            i
            j
          
        
      
    
      displaystyle w  ij  
  
  If 
  
    
      
        
          
            
                x     
              E
            
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
         gt 
         
      
    
      displaystyle   frac   partial E   partial w  ij    gt   
  
  an increase in 
  
    
      
        
          w
          
            i
            j
          
        
      
    
      displaystyle w  ij  
  
 increases 
  
    
      
        E
      
    
      displaystyle E 
  
  conversely  if 
  
    
      
        
          
            
                x     
              E
            
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
         lt 
         
      
    
      displaystyle   frac   partial E   partial w  ij    lt   
  
  an increase in 
  
    
      
        
          w
          
            i
            j
          
        
      
    
      displaystyle w  ij  
  
 decreases 
  
    
      
        E
      
    
      displaystyle E 
  
  The new 
  
    
      
          x    
        
          w
          
            i
            j
          
        
      
    
      displaystyle  Delta w  ij  
  
 is added to the old weight  and the product of the learning rate and the gradient  multiplied by 
  
    
      
          x     
         
      
    
      displaystyle    
  
 guarantees that 
  
    
      
        
          w
          
            i
            j
          
        
      
    
      displaystyle w  ij  
  
 changes in a way that always decreases 
  
    
      
        E
      
    
      displaystyle E 
  
  In other words  in the equation immediately below  
  
    
      
          x     
          x b  
        
          
            
                x     
              E
            
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
      
    
      displaystyle   eta   frac   partial E   partial w  ij    
  
 always changes 
  
    
      
        
          w
          
            i
            j
          
        
      
    
      displaystyle w  ij  
  
 in such a way that 
  
    
      
        E
      
    
      displaystyle E 
  
 is decreased 


  
    
      
          x    
        
          w
          
            i
            j
          
        
         
          x     
          x b  
        
          
            
                x     
              E
            
            
                x     
              
                w
                
                  i
                  j
                
              
            
          
        
         
          x     
          x b  
        
          o
          
            i
          
        
        
            x b  
          
            j
          
        
      
    
      displaystyle  Delta w  ij    eta   frac   partial E   partial w  ij      eta o  i  delta   j  
  

Second order gradient descent edit 

Using a Hessian matrix of second order derivatives of the error function  the Levenberg Marquardt algorithm often converges faster than first order gradient descent  especially when the topology of the error function is complicated                          It may also find solutions in smaller node counts for which other methods might not converge              The Hessian can be approximated by the Fisher information matrix             
As an example  consider a simple feedforward network  At the 
  
    
      
        l
      
    
      displaystyle l 
  
 th layer  we have
  
    
      
        
          x
          
            i
          
          
             
            l
             
          
        
         
        
        
          a
          
            i
          
          
             
            l
             
          
        
         
        f
         
        
          x
          
            i
          
          
             
            l
             
          
        
         
         
        
        
          x
          
            i
          
          
             
            l
             
             
             
          
        
         
        
            x     
          
            j
          
        
        
          W
          
            i
            j
          
        
        
          a
          
            j
          
          
             
            l
             
          
        
      
    
      displaystyle x  i    l    quad a  i    l   f x  i    l     quad x  i    l      sum   j W  ij a  j    l   
  
where 
  
    
      
        x
      
    
      displaystyle x 
  
 are the pre activations  
  
    
      
        a
      
    
      displaystyle a 
  
 are the activations  and 
  
    
      
        W
      
    
      displaystyle W 
  
 is the weight matrix  Given a loss function 
  
    
      
        L
      
    
      displaystyle L 
  
  the first order backpropagation states that
  
    
      
        
          
            
                x     
              L
            
            
                x     
              
                a
                
                  j
                
                
                   
                  l
                   
                
              
            
          
        
         
        
            x     
          
            j
          
        
        
          W
          
            i
            j
          
        
        
          
            
                x     
              L
            
            
                x     
              
                x
                
                  i
                
                
                   
                  l
                   
                   
                   
                
              
            
          
        
         
        
        
          
            
                x     
              L
            
            
                x     
              
                x
                
                  j
                
                
                   
                  l
                   
                
              
            
          
        
         
        
          f
            x     
        
         
        
          x
          
            j
          
          
             
            l
             
          
        
         
        
          
            
                x     
              L
            
            
                x     
              
                a
                
                  j
                
                
                   
                  l
                   
                
              
            
          
        
      
    
      displaystyle   frac   partial L   partial a  j    l      sum   j W  ij   frac   partial L   partial x  i    l        quad   frac   partial L   partial x  j    l     f  x  j    l     frac   partial L   partial a  j    l     
  
and the second order backpropagation states that
  
    
      
        
          
            
              
                  x     
                
                   
                
              
              L
            
            
                x     
              
                a
                
                  
                    j
                    
                       
                    
                  
                
                
                   
                  l
                   
                
              
                x     
              
                a
                
                  
                    j
                    
                       
                    
                  
                
                
                   
                  l
                   
                
              
            
          
        
         
        
            x     
          
            
              j
              
                 
              
            
            
              j
              
                 
              
            
          
        
        
          W
          
            
              i
              
                 
              
            
            
              j
              
                 
              
            
          
        
        
          W
          
            
              i
              
                 
              
            
            
              j
              
                 
              
            
          
        
        
          
            
              
                  x     
                
                   
                
              
              L
            
            
                x     
              
                x
                
                  
                    i
                    
                       
                    
                  
                
                
                   
                  l
                   
                   
                   
                
              
                x     
              
                x
                
                  
                    i
                    
                       
                    
                  
                
                
                   
                  l
                   
                   
                   
                
              
            
          
        
         
        
        
          
            
              
                  x     
                
                   
                
              
              L
            
            
                x     
              
                x
                
                  
                    j
                    
                       
                    
                  
                
                
                   
                  l
                   
                
              
                x     
              
                x
                
                  
                    j
                    
                       
                    
                  
                
                
                   
                  l
                   
                
              
            
          
        
         
        
          f
            x     
        
         
        
          x
          
            
              j
              
                 
              
            
          
          
             
            l
             
          
        
         
        
          f
            x     
        
         
        
          x
          
            
              j
              
                 
              
            
          
          
             
            l
             
          
        
         
        
          
            
              
                  x     
                
                   
                
              
              L
            
            
                x     
              
                a
                
                  
                    j
                    
                       
                    
                  
                
                
                   
                  l
                   
                
              
                x     
              
                a
                
                  
                    j
                    
                       
                    
                  
                
                
                   
                  l
                   
                
              
            
          
        
         
        
            x b  
          
            
              j
              
                 
              
            
            
              j
              
                 
              
            
          
        
        
          f
            x     
        
         
        
          x
          
            
              j
              
                 
              
            
          
          
             
            l
             
          
        
         
        
          
            
                x     
              L
            
            
                x     
              
                a
                
                  
                    j
                    
                       
                    
                  
                
                
                   
                  l
                   
                
              
            
          
        
      
    
      displaystyle   frac   partial     L   partial a  j        l   partial a  j        l      sum   j    j     W  i    j     W  i    j       frac   partial     L   partial x  i        l     partial x  i        l        quad   frac   partial     L   partial x  j        l   partial x  j        l     f  x  j        l   f  x  j        l     frac   partial     L   partial a  j        l   partial a  j        l      delta   j    j     f   x  j        l     frac   partial L   partial a  j        l     
  
where 
  
    
      
          x b  
      
    
      displaystyle  delta  
  
 is the Dirac delta symbol 
Arbitrary order derivatives in arbitrary computational graphs can be computed with backpropagation  but with more complex expressions for higher orders 

Loss function edit 
Further information  Loss function
The loss function is a function that maps values of one or more variables onto a real number intuitively representing some  cost  associated with those values  For backpropagation  the loss function calculates the difference between the network output and its expected output  after a training example has propagated through the network 

Assumptions edit 
The mathematical expression of the loss function must fulfill two conditions in order for it to be possibly used in backpropagation              The first is that it can be written as an average 
  
    
      
        E
         
        
          
             
            n
          
        
        
            x     
          
            x
          
        
        
          E
          
            x
          
        
      
    
      textstyle E   frac     n   sum   x E  x  
  
 over error functions 
  
    
      
        
          E
          
            x
          
        
      
    
      textstyle E  x  
  
  for 
  
    
      
        n
      
    
      textstyle n 
  
 individual training examples  
  
    
      
        x
      
    
      textstyle x 
  
  The reason for this assumption is that the backpropagation algorithm calculates the gradient of the error function for a single training example  which needs to be generalized to the overall error function  The second assumption is that it can be written as a function of the outputs from the neural network 

Example loss function edit 
Let 
  
    
      
        y
         
        
          y
            x     
        
      
    
      displaystyle y y  
  
 be vectors in 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
      displaystyle  mathbb  R    n  
  
 
Select an error function 
  
    
      
        E
         
        y
         
        
          y
            x     
        
         
      
    
      displaystyle E y y   
  
 measuring the difference between two outputs  The standard choice is the square of the Euclidean distance between the vectors 
  
    
      
        y
      
    
      displaystyle y 
  
 and 
  
    
      
        
          y
            x     
        
      
    
      displaystyle y  
  
 
  
    
      
        E
         
        y
         
        
          y
            x     
        
         
         
        
          
            
               
               
            
          
        
          x     
        y
          x     
        
          y
            x     
        
        
            x     
          
             
          
        
      
    
      displaystyle E y y     tfrac         lVert y y  rVert      
  
The error function over 
  
    
      
        n
      
    
      textstyle n 
  
 training examples can then be written as an average of losses over individual examples 
  
    
      
        E
         
        
          
             
            
               
              n
            
          
        
        
            x     
          
            x
          
        
          x     
         
        y
         
        x
         
          x     
        
          y
            x     
        
         
        x
         
         
        
            x     
          
             
          
        
      
    
      displaystyle E   frac      n   sum   x  lVert  y x  y  x   rVert      
  


Limitations edit 
Gradient descent may find a local minimum instead of the global minimum 
Gradient descent with backpropagation is not guaranteed to find the global minimum of the error function  but only a local minimum  also  it has trouble crossing plateaus in the error function landscape  This issue  caused by the non convexity of error functions in neural networks  was long thought to be a major drawback  but Yann LeCun et al  argue that in many practical problems  it is not             
Backpropagation learning does not require normalization of input vectors  however  normalization could improve performance             
Backpropagation requires the derivatives of activation functions to be known at network design time 
History edit 
See also  History of Perceptron
Precursors edit 
Backpropagation had been derived repeatedly  as it is essentially an efficient application of the chain rule  first written down by Gottfried Wilhelm Leibniz in                               to neural networks 
The terminology  back propagating error correction  was introduced in      by Frank Rosenblatt  but he did not know how to implement this              In any case  he only studied neurons whose outputs were discrete levels  which only had zero derivatives  making backpropagation impossible 
Precursors to backpropagation appeared in optimal control theory since     s  Yann LeCun et al credits     s work by Pontryagin and others in optimal control theory  especially the adjoint state method  for being a continuous time version of backpropagation              Hecht Nielsen             credits the Robbins Monro algorithm                    and Arthur Bryson and Yu Chi Ho s Applied Optimal Control        as presages of backpropagation  Other precursors were Henry J  Kelley                  and Arthur E  Bryson                    In       Stuart Dreyfus published a simpler derivation based only on the chain rule                                      In       he adapted parameters of controllers in proportion to error gradients              Unlike modern backpropagation  these precursors used standard Jacobian matrix calculations from one stage to the previous one  neither addressing direct links across several stages nor potential additional efficiency gains due to network sparsity             
The ADALINE        learning algorithm was gradient descent with a squared error loss for a single layer  The first multilayer perceptron  MLP  with more than one layer trained by stochastic gradient descent             was published in      by Shun ichi Amari              The MLP had   layers  with   learnable layers  and it learned to classify patterns not linearly separable             

Modern backpropagation edit 
Modern backpropagation was first published by Seppo Linnainmaa as  reverse mode of automatic differentiation                     for discrete connected networks of nested differentiable functions                                     
In       Paul Werbos applied backpropagation to MLPs in the way that has become standard                          Werbos described how he developed backpropagation in an interview  In       during his PhD work  he developed backpropagation to mathematicize Freud s  flow of psychic energy   He faced repeated difficulty in publishing the work  only managing in                   He also claimed that  the first practical application of back propagation was for estimating a dynamic model to predict nationalism and social communications in       by him             
Around                                         David E  Rumelhart independently developed                                   backpropagation and taught the algorithm to others in his research circle  He did not cite previous work as he was unaware of them  He published the algorithm first in a      paper  then in a      Nature paper an experimental analysis of the technique              These papers became highly cited  contributed to the popularization of backpropagation  and coincided with the resurging research interest in neural networks during the     s                                     
In       the method was also described by David Parker                          Yann LeCun proposed an alternative form of backpropagation for neural networks in his PhD thesis in                  
Gradient descent took a considerable amount of time to reach acceptance  Some early objections were  there were no guarantees that gradient descent could reach a global minimum  only local minimum  neurons were  known  by physiologists as making discrete signals        not continuous ones  and with discrete signals  there is no gradient to take  See the interview with Geoffrey Hinton              who was awarded the      Nobel Prize in Physics for his contributions to the field             

Early successes edit 
Contributing to the acceptance were several applications in training neural networks via backpropagation  sometimes achieving popularity outside the research circles 
In       NETtalk learned to convert English text into pronunciation  Sejnowski tried training it with both backpropagation and Boltzmann machine  but found the backpropagation significantly faster  so he used it for the final NETtalk                                    The NETtalk program became a popular success  appearing on the Today show             
In       Dean A  Pomerleau published ALVINN  a neural network trained to drive autonomously using backpropagation             
The LeNet was published in      to recognize handwritten zip codes 
In       TD Gammon achieved top human level play in backgammon  It was a reinforcement learning agent with a neural network with two layers  trained by backpropagation             
In       Eric Wan won an international pattern recognition contest through backpropagation                         

After backpropagation edit 
During the     s it fell out of favour     citation needed       but returned in the     s  benefiting from cheap  powerful GPU based computing systems  This has been especially so in speech recognition  machine vision  natural language processing  and language structure learning research  in which it has been used to explain a variety of phenomena related to first             and second language learning                          
Error backpropagation has been suggested to explain human brain event related potential  ERP  components like the N    and P                
In       a backpropagation algorithm was implemented on a photonic processor by a team at Stanford University             

See also edit 
Artificial neural network
Neural circuit
Catastrophic interference
Ensemble learning
AdaBoost
Overfitting
Neural backpropagation
Backpropagation through time
Backpropagation through structure
Three factor learning
Notes edit 


  Use 
  
    
      
        C
      
    
      displaystyle C 
  
 for the loss function to allow 
  
    
      
        L
      
    
      displaystyle L 
  
 to be used for the number of layers

  This follows Nielsen         and means  left  multiplication by the matrix 
  
    
      
        
          W
          
            l
          
        
      
    
      displaystyle W  l  
  
 corresponds to converting output values of layer 
  
    
      
        l
          x     
         
      
    
      displaystyle l   
  
 to input values of layer 
  
    
      
        l
      
    
      displaystyle l 
  
  columns correspond to input coordinates  rows correspond to output coordinates 

  This section largely follows and summarizes Nielsen        

  The derivative of the loss function is a covector  since the loss function is a scalar valued function of several variables 

  The activation function is applied to each node separately  so the derivative is just the diagonal matrix of the derivative on each node  This is often represented as the Hadamard product with the vector of derivatives  denoted by 
  
    
      
         
        
          f
          
            l
          
        
        
           
            x     
        
          x     
      
    
      displaystyle  f  l    odot  
  
  which is mathematically identical but better matches the internal representation of the derivatives as a vector  rather than a diagonal matrix 

  Since matrix multiplication is linear  the derivative of multiplying by a matrix is just the matrix  
  
    
      
         
        W
        x
        
           
            x     
        
         
        W
      
    
      displaystyle  Wx   W 
  
 

  One may notice that multi layer neural networks use non linear activation functions  so an example with linear neurons seems obscure  However  even though the error surface of multi layer networks are much more complicated  locally they can be approximated by a paraboloid  Therefore  linear neurons are used for simplicity and easier understanding 

  There can be multiple output neurons  in which case the error is the squared norm of the difference vector 


References edit 


  a b Kelley  Henry J           Gradient theory of optimal flight paths   ARS Journal                    doi                

  a b Bryson  Arthur E           A gradient method for optimizing multi stage allocation processes   Proceedings of the Harvard Univ  Symposium on digital computers and their applications      April       Cambridge  Harvard University Press  OCLC                

  Goodfellow  Bengio  amp  Courville       p             This table filling strategy is sometimes called dynamic programming  

  Goodfellow  Bengio  amp  Courville       p             The term back propagation is often misunderstood as meaning the whole learning algorithm for multilayer neural networks  Backpropagation refers only to the method for computing the gradient  while other algorithms  such as stochastic gradient descent  is used to perform learning using this gradient  

  Mohapatra  Rohan  Saha  Snehanshu  Coello  Carlos A  Coello  Bhattacharya  Anwesh  Dhavala  Soma S   Saha  Sriparna  April         AdaSwarm  Augmenting Gradient Based Optimizers in Deep Learning With Swarm Intelligence   IEEE Transactions on Emerging Topics in Computational Intelligence                  arXiv             doi         TETCI               hdl                    ISSN              X 

  Abdulkadirov  Ruslan I   Lyakhov  Pavel A   Baboshina  Valentina A   Nagornov  Nikolay N           Improving the Accuracy of Neural Network Pattern Recognition by Fractional Gradient Descent   IEEE Access                     Bibcode     IEEEA    p    A  doi         ACCESS               ISSN                

  a b Goodfellow  Bengio  amp  Courville        p                  The back propagation algorithm described here is only one approach to automatic differentiation  It is a special case of a broader class of techniques called reverse mode accumulation  

  Ramachandran  Prajit  Zoph  Barret  Le  Quoc V                 Searching for Activation Functions   arXiv             cs NE  

  Misra  Diganta                Mish  A Self Regularized Non Monotonic Activation Function   arXiv             cs LG  

  a b Rumelhart  David E   Hinton  Geoffrey E   Williams  Ronald J       a    Learning representations by back propagating errors   Nature                       Bibcode     Natur         R  doi               a   S CID                

  Tan  Hong Hui  Lim  King Han          Review of second order optimization techniques in artificial neural networks backpropagation   IOP Conference Series  Materials Science and Engineering                   Bibcode     MS     E     a    T  doi                 X               S CID                

  a b Wiliamowski  Bogdan  Yu  Hao  June         Improved Computation for Levenberg Marquardt Training   PDF   IEEE Transactions on Neural Networks and Learning Systems         

  Martens  James  August         New Insights and Perspectives on the Natural Gradient Method   Journal of Machine Learning Research       arXiv           

  Nielsen           W hat assumptions do we need to make about our cost function     in order that backpropagation can be applied  The first assumption we need is that the cost function can be written as an average     over cost functions     for individual training examples     The second assumption we make about the cost is that it can be written as a function of the outputs from the neural network     

  LeCun  Yann  Bengio  Yoshua  Hinton  Geoffrey          Deep learning   PDF   Nature                       Bibcode     Natur         L  doi         nature       PMID                S CID              

  Buckland  Matt  Collins  Mark         AI Techniques for Game Programming  Boston  Premier Press  ISBN                  X 

  Leibniz  Gottfried Wilhelm Freiherr von         The Early Mathematical Manuscripts of Leibniz  Translated from the Latin Texts Published by Carl Immanuel Gerhardt with Critical and Historical Notes  Leibniz published the chain rule in a      memoir   Open court publishing Company  ISBN                       cite book    ISBN   Date incompatibility  help 

  Rodr guez  Omar Hern ndez  L pez Fern ndez  Jorge M           A Semiotic Reflection on the Didactics of the Chain Rule   The Mathematics Enthusiast                  doi                          S CID                Retrieved            

  Rosenblatt  Frank         Principles of Neurodynamics  Spartan  New York  pp               

  LeCun  Yann  et al   A theoretical framework for back propagation   Proceedings of the      connectionist models summer school  Vol          

  Hecht Nielsen  Robert         Neurocomputing  Internet Archive  Reading  Mass         Addison Wesley Pub  Co  pp                ISBN                        

  a b Robbins  H   Monro  S           A Stochastic Approximation Method   The Annals of Mathematical Statistics               doi         aoms            

  Dreyfus  Stuart          The numerical solution of variational problems   Journal of Mathematical Analysis and Applications                doi                 x            

  Dreyfus  Stuart E           Artificial Neural Networks  Back Propagation  and the Kelley Bryson Gradient Procedure   Journal of Guidance  Control  and Dynamics                   Bibcode     JGCD          D  doi                 

  Mizutani  Eiji  Dreyfus  Stuart  Nishio  Kenichi  July         On derivation of MLP backpropagation from the Kelley Bryson optimal control gradient formula and its application   PDF   Proceedings of the IEEE International Joint Conference on Neural Networks 

  Dreyfus  Stuart          The computational solution of optimal control problems with time lag   IEEE Transactions on Automatic Control                   doi         tac              

  a b Schmidhuber  J rgen          Annotated History of Modern AI and Deep Learning   arXiv             cs NE  

  Amari  Shun ichi          A theory of adaptive pattern classifier   IEEE Transactions  EC               

  Linnainmaa  Seppo         The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors  Masters   in Finnish   University of Helsinki  pp           

  Linnainmaa  Seppo          Taylor expansion of the accumulated rounding error   BIT Numerical Mathematics                   doi         bf          S CID                

  Griewank  Andreas          Who Invented the Reverse Mode of Differentiation    Optimization Stories  Documenta Mathematica  Extra Volume ISMP  pp                S CID               

  Griewank  Andreas  Walther  Andrea         Evaluating Derivatives  Principles and Techniques of Algorithmic Differentiation  Second Edition  SIAM  ISBN                        

  Werbos  Paul          Applications of advances in nonlinear sensitivity analysis   PDF   System modeling and optimization  Springer  pp                Archived  PDF  from the original on    April       Retrieved   July      

  Werbos  Paul J          The Roots of Backpropagation        From Ordered Derivatives to Neural Networks and Political Forecasting  New York  John Wiley  amp  Sons  ISBN                    

  a b c d Anderson  James A   Rosenfeld  Edward  eds          Talking Nets  An Oral History of Neural Networks  The MIT Press  doi         mitpress                ISBN                        

  P  J  Werbos   Backpropagation through time  what it does and how to do it   in Proceedings of the IEEE  vol      no      pp             Oct        doi                

  Olazaran Rodriguez  Jose Miguel  A historical sociology of neural network research  PhD Dissertation  University of Edinburgh       

  Rumelhart  Hinton  Williams          Learning representations by back propagating errors   PDF   Nature                       Bibcode     Natur         R  doi               a   S CID                

  Rumelhart  David E   Hinton  Geoffrey E   Williams  Ronald J       b       Learning Internal Representations by Error Propagation   In Rumelhart  David E   McClelland  James L   eds    Parallel Distributed Processing        Explorations in the Microstructure of Cognition  Vol                Foundations  Cambridge  MIT Press  ISBN                    

  Alpaydin  Ethem         Introduction to Machine Learning  MIT Press  ISBN                        

  Parker  D B          Learning Logic  Casting the Cortex of the Human Brain in Silicon  Center for Computational Research in Economics and Management Science  Report   Cambridge MA  Massachusetts Institute of Technology  Technical Report TR    

  Hertz  John         Introduction to the theory of neural computation  Krogh  Anders   Palmer  Richard G  Redwood City  Calif   Addison Wesley  p          ISBN                     OCLC               

  Le Cun  Yann         Mod les connexionnistes de l apprentissage  Th se de doctorat d  tat thesis   Paris  France  Universit  Pierre et Marie Curie 

   The Nobel Prize in Physics        NobelPrize org  Retrieved            

  Sejnowski  Terrence J          The deep learning revolution  Cambridge  Massachusetts London  England  The MIT Press  ISBN                        

  Pomerleau  Dean A           ALVINN  An Autonomous Land Vehicle in a Neural Network   Advances in Neural Information Processing Systems     Morgan Kaufmann 

  Sutton  Richard S   Barto  Andrew G                TD Gammon   Reinforcement Learning  An Introduction   nd      ed    Cambridge  MA  MIT Press 

  Schmidhuber  J rgen          Deep learning in neural networks  An overview   Neural Networks              arXiv            doi         j neunet              PMID                S CID               

  Wan  Eric A           Time Series Prediction by Using a Connectionist Network with Internal Delay Lines   In Weigend  Andreas S   Gershenfeld  Neil A   eds    Time Series Prediction        Forecasting the Future and Understanding the Past  Proceedings of the NATO Advanced Research Workshop on Comparative Time Series Analysis  Vol           Reading  Addison Wesley  pp                ISBN                     S CID               

  Chang  Franklin  Dell  Gary S   Bock  Kathryn          Becoming syntactic   Psychological Review                    doi                 x            PMID               

  Janciauskas  Marius  Chang  Franklin          Input and Age Dependent Variation in Second Language Learning  A Connectionist Account   Cognitive Science      Suppl Suppl              doi         cogs        PMC               PMID               

   Decoding the Power of Backpropagation  A Deep Dive into Advanced Neural Network Techniques   janbasktraining com     January      

  Fitz  Hartmut  Chang  Franklin          Language ERPs reflect learning through prediction error propagation   Cognitive Psychology              doi         j cogpsych              hdl                       D    PMID                S CID               

   Photonic Chips Curb AI Training s Energy Appetite   IEEE Spectrum   IEEE  Retrieved            


Further reading edit 
Goodfellow  Ian  Bengio  Yoshua  Courville  Aaron              Back Propagation and Other Differentiation Algorithms   Deep Learning  MIT Press  pp                ISBN                    
Nielsen  Michael A           How the backpropagation algorithm works   Neural Networks and Deep Learning  Determination Press 
McCaffrey  James  October         Neural Network Back Propagation for Programmers   MSDN Magazine 
Rojas  Ra l          The Backpropagation Algorithm   PDF   Neural Networks        A Systematic Introduction  Berlin  Springer  ISBN                    
External links edit 
Backpropagation neural network tutorial at the Wikiversity
Bernacki  Mariusz  W odarczyk  Przemys aw          Principles of training multi layer neural network using backpropagation  
Karpathy  Andrej          Lecture    Backpropagation  Neural Networks     CS   n  Stanford University  Archived from the original on                    via YouTube 
 What is Backpropagation Really Doing     Blue Brown  November          Archived from the original on                    via YouTube 
Putta  Sudeep Raja          Yet Another Derivation of Backpropagation in Matrix Form  
vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects

Authority control databases  National United StatesIsrael





Retrieved from  https   en wikipedia org w index php title Backpropagation amp oldid