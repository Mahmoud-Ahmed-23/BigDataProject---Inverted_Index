Large language model by Meta AI
Not to be confused with LaMDA 
This article relies excessively on references to primary sources  Please improve this article by adding secondary or tertiary sources  Find sources        Llama       language model             news        newspapers        books        scholar        JSTOR   April        Learn how and when to remove this message 
LlamaScreenshot
  Screenshot of an example of Llama answer describing WikipediaDeveloper s Meta AIInitial releaseFebruary                          years ago                  Stable releaseLlama  
     April                          days ago                  
Repositorygithub com meta llama llama modelsWritten inPythonType
Large language model
GPT
Foundation model
LicenseSource available  Meta Llama     Community License            Websitewww llama com 
Llama  Large Language Model Meta AI  formerly stylized as LLaMA  is a family of large language models  LLMs  released by Meta AI starting in February                  The latest version is Llama    released in April                 
Llama models come in different sizes  ranging from   billion to   trillion parameters  Initially only a foundation model             starting with Llama    Meta AI released instruction fine tuned versions alongside foundation models            
Model weights for the first version of Llama were only available to researchers on a case by case basis  under a non commercial license                        Unauthorized copies of the first model were shared via BitTorrent             Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use                       
Alongside the release of Llama    Meta added virtual assistant features to Facebook and WhatsApp in select regions  and a standalone website  Both services use a Llama   model             


Background edit 
After the release of large language models such as GPT    a focus of research was up scaling models which in some instances showed major increases in emergent capabilities              The release of ChatGPT and its surprise success caused an increase in attention to large language models             
Compared with other responses to ChatGPT  Meta s Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing                                                 
An empirical investigation of the Llama series was the scaling laws  It was observed that the Llama   models showed that when a model is trained on data that is more than the  Chinchilla optimal  amount  the performance continues to scale log linearly  For example  the Chinchilla optimal dataset for Llama    B is     billion tokens  but performance continued to scale log linearly to the    times larger dataset of    trillion tokens             

Initial release edit 
LLaMA was announced on February           via a blog post and a paper describing the model s training  architecture  and performance                         The inference code used to run the model was publicly released under the open source GPLv  license              Access to the model s weights was managed by an application process  with access to be granted  on a case by case basis to academic researchers  those affiliated with organizations in government  civil society  and academia  and industry research laboratories around the world             
Llama was trained on only publicly available information  and was trained at various model sizes  with the intention to make it more accessible to different hardware  The model was exclusively a foundation model             although the paper contained examples of instruction fine tuned versions of the model             
Meta AI reported the   B parameter model performance on most NLP benchmarks exceeded that of the much larger GPT    with    B parameters   and the largest   B model was competitive with state of the art models such as PaLM and Chinchilla             

Leak edit 
On March          a torrent containing LLaMA s weights was uploaded  with a link to the torrent shared on the  chan imageboard and subsequently spread through online AI communities              That same day  a pull request on the main LLaMA repository was opened  requesting to add the magnet link to the official documentation                          On March    a pull request was opened to add links to HuggingFace repositories containing the model                          On March    Meta filed takedown requests to remove the HuggingFace repositories linked in the pull request  characterizing it as  unauthorized distribution  of the model  HuggingFace complied with the requests              On March     Meta filed a DMCA takedown request for copyright infringement against a repository containing a script that downloaded LLaMA from a mirror  and GitHub complied the next day             
Reactions to the leak varied  Some speculated that the model would be used for malicious purposes  such as more sophisticated spam  Some have celebrated the model s accessibility  as well as the fact that smaller versions of the model can be run relatively cheaply  suggesting that this will promote the flourishing of additional research developments              Multiple commentators  such as Simon Willison  compared LLaMA to Stable Diffusion  a text to image model which  unlike comparably sophisticated models which preceded it  was openly distributed  leading to a rapid proliferation of associated tools  techniques  and software                         

LLaMa   edit 
On July           in partnership with Microsoft  Meta announced LLaMa    the next generation of Llama  Meta trained and released Llama   in three model sizes         and    billion parameters             The model architecture remains largely unchanged from that of LLaMA   models  but     more data was used to train the foundational models              The accompanying preprint             also mentions a model with   B parameters that might be released in the future upon satisfying safety targets 
LLaMa   includes foundation models and models fine tuned for chat  In a further departure from the original version of LLaMa  all models are released with weights and may be used for many commercial use cases  However  because LLaMa s license enforces an acceptable use policy that prohibits Llama from being used for some purposes  Meta s use of the term open source to describe Llama has been disputed by the Open Source Initiative  which maintains The Open Source Definition  and others                         
Code Llama is a fine tune of LLaMa   with code specific datasets   B    B  and   B versions were released on August           with the   B releasing on the January                       Starting with the foundation models from LLaMa    Meta AI would train an additional    B tokens of code datasets  before an additional   B token of long context data  creating the Code Llama foundation models  This foundation model was further trained on  B instruction following token to create the instruct fine tune  Another foundation model was created for Python code  which trained on    B tokens of Python only code  before the long context data             

Llama   edit 
Example of an image generated by Meta AI Imagine  powered by Llama    Prompt  A representation of Meta AI and Llama
On April           Meta released Llama   with two sizes   B and   B parameters              The models have been pre trained on approximately    trillion tokens of text gathered from  publicly available sources  with the instruct models fine tuned on  publicly available instruction datasets  as well as over   M human annotated examples   Meta AI s testing showed in April      that Llama     B was beating Gemini Pro     and Claude   Sonnet on most benchmarks  Meta also announced plans to make Llama   multilingual and multimodal  better at coding and reasoning  and to increase its context window                         
During an interview with Dwarkesh Patel  Mark Zuckerberg said that the  B version of Llama   was nearly as powerful as the largest Llama    Compared to previous models  Zuckerberg stated the team was surprised that the   B model was still learning even at the end of the   T tokens training  The decision was made to end training to focus GPU power elsewhere             
Llama     was released on July           with three sizes   B    B  and    B parameters                         

Llama   edit 
The Llama   series was released in       The architecture was changed to a mixture of experts  They are multimodal  text and image input  text output  and multilingual     languages               Specifically  on   April       the following were released both as base and instruction tuned versions             

Scout     billion active parameter model with    experts  context window of   M  with    B parameters in total 
Maverick     billion active parameter model with     experts  context window of  M  with    B parameters in total 
Also claimed was Behemoth  not yet released       billion active parameter model with    experts and around  T parameters in total  The Behemoth version was still in training at that time  The Scout was trained from scratch  The Maverick was  codistilled  from Behemoth  Note that the Scout was trained for longer and had a longer context length than Maverick 
The training data included publicly available data  licensed data  and Meta proprietary data such as publicly shared posts from Instagram and Facebook and people s interactions with Meta AI  The data cutoff was August                  
Meta claimed in its release announcement that Llama   bested GPT  o s score on the LMArena AI benchmark              The company also stated that Llama   s benchmark score was achieved using an unreleased  experimental chat version  of the model that was  optimized for conversationality   which differed from the version of Llama   released to the public              LMArena indicated that it would change its policies to prevent this incident from reoccurring  and responded   Meta s interpretation of our policy did not match what we expect from model providers  Meta should have made it clearer that  Llama   Maverick       Experimental  was a customized model to optimize for human preference               Some users criticized Meta on social media for its use of a separate model version tailored for benchmarking  and some additionally accused Meta of training Llama   on test sets to further boost its benchmark scores which Meta denied             

Comparison of models edit 
For the training cost column  only the largest model s cost is written by default  So for example           is the training cost of Llama     B in units of petaFLOP day  Also    petaFLOP day     petaFLOP sec     day       E   FLOP   T  means  trillion  and  B  means  billion  
The following table lists the main model versions of Llama  describing the significant changes included with each version             




Name
Release date
Status
Parameters

Training cost  petaFLOP day 
Context length  tokens 
Corpus size  tokens 
Commercial viability 


LLaMA

February         

Discontinued


   B
  B
    B
    B

                 

    

     T

No


Llama  

July         

Discontinued


   B
  B
  B

                  

    

 T

Yes  subject to acceptable use policy


Code Llama

August         

Discontinued


   B
  B
    B
  B

 


Llama  

April         

Active


 B
    B

                               

    

  T


Llama    

July         

Active


 B
    B
   B

                               

       


Llama    

September         

Active


 B
 B
  B
  B                        

 

                   

 T


Llama    

December        

Active


  B

 

       

  T 


Llama  

April        

Active


   B
   B
 T


      
      
             


  M
 M
 


  T
  T
 

Architecture and training edit 

 recommendation letter for the Magic Unicorn Corporation 

Here is the recommendation letter that I wrote for an application to a dragon feeder position at the Magic Unicorn Corporation 
Dear recruiter 
I have known     for two years  and I believe that she would be an excellent dragon feeder for the Magic Unicorn Corporation      has an ability to remember and process large amounts of information  which is an important skill for a dragon feeder 
     as an accomplished knight  has a deep understanding of how to kill dragons and how to use each dragon s weaknesses against it  This means that she knows what kinds of foods each dragon likes and what kinds of foods are dangerous to each dragon  This knowledge and experience will be invaluable as she feeds the dragons 
I am confident that     s competence  skill  and experience will make her an excellent employee  Please contact me at               if you have any questions  I look forward to hearing from you 
Best regards 
Honorable Knight
Sir George


  Output of    billion parameter LLaMA model before instruction tuning  given the prompt  in bold             

Architecture edit 
Like GPT    the Llama series of models are autoregressive decoder only Transformers  but there are some minor differences 

SwiGLU             activation function instead of GeLU 
rotary positional embeddings  RoPE              instead of absolute positional embedding 
RMSNorm             instead of layer normalization             

Key hyperparameters of Llama    




 B

  B

   B


Layers

  

  

   


Model dimension

     

     

      


FFN dimension

      

      

      


Attention heads

  

  

   


Key value heads

 

 

 


Peak learning rate

        

          

          


Activation function

SwiGLU


Vocabulary size

       


Positional embeddings


  
    
      
        RoPE
          x     
         
          x b  
         
               
         
      
    
      displaystyle  operatorname  RoPE    theta             
  


Training datasets edit 
LLaMA s developers focused their effort on scaling the model s performance by increasing the volume of training data  rather than the number of parameters  reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process 
LLaMA   foundational models were trained on a data set with     trillion tokens  drawn from publicly available data sources  including             

Webpages scraped by CommonCrawl
Open source repositories of source code from GitHub
Wikipedia in    languages
Public domain books from Project Gutenberg
Books  books dataset
The LaTeX source code for scientific papers uploaded to ArXiv
Questions and answers from Stack Exchange websites
On April           TogetherAI launched a project named RedPajama to reproduce and distribute an open source version of the LLaMA dataset              The dataset has approximately     trillion tokens and is publicly available for download             
Llama   foundational models were trained on a data set with   trillion tokens  This data set was curated to remove Web sites that often disclose personal data of people  It also upsamples sources considered trustworthy              Llama     Chat was additionally fine tuned on        prompt response pairs created for this project  which performed better than larger but lower quality third party datasets  For AI alignment  reinforcement learning with human feedback  RLHF  was used with a combination of           Meta examples and seven smaller datasets  The average dialog depth was     in the Meta examples      for Anthropic Helpful and Anthropic Harmless sets  and     for five other sets  including OpenAI Summarize  StackExchange  etc 
Llama   consists of mainly English data  with over    in over    other languages  Its dataset was filtered by a text quality classifier  and the classifier was trained by text synthesized by Llama               
In a lawsuit brought by Richard Kadrey and others against Meta Platforms  CEO Mark Zuckerberg was alleged to have authorized the use of copyrighted content from Library Genesis to train Llama AI models and conceal its actions by removing copyright markers from the data             

Fine tuning edit 
Llama   models are only available as foundational models with self supervised learning and without fine tuning  Llama     Chat models were derived from foundational Llama   models  Unlike GPT   which increased context length during fine tuning  Llama   and Code Llama   Chat have the same context length of  K tokens  Supervised fine tuning used an autoregressive loss function with token loss on user prompts zeroed out  The batch size was    
For AI alignment  human annotators wrote prompts and then compared two model outputs  a binary protocol   giving confidence levels and separate safety labels with veto power  Two separate reward models were trained from these preferences for safety and helpfulness using Reinforcement learning from human feedback  RLHF   A major technical contribution is the departure from the exclusive use of Proximal Policy Optimization  PPO  for RLHF   a new technique based on Rejection sampling was used  followed by PPO 
Multi turn consistency in dialogs was targeted for improvement  to make sure that  system messages   initial instructions  such as  speak in French  and  act like Napoleon   are respected during the dialog  This was accomplished using the new  Ghost attention  technique during training  which concatenates relevant instructions to each new user message but zeros out the loss function for tokens in the prompt  earlier parts of the dialog  

Applications edit 
The Stanford University Institute for Human Centered Artificial Intelligence  HAI  Center for Research on Foundation Models  CRFM  released Alpaca  a training recipe based on the LLaMA  B model that uses the  Self Instruct  method of instruction tuning to acquire capabilities comparable to the OpenAI GPT   series text davinci     model at a modest cost                                      The model files were officially removed on March           over hosting costs and safety concerns  though the code and paper remain online for reference                                     
Meditron is a family of Llama based finetuned on a corpus of clinical guidelines  PubMed papers  and articles  It was created by researchers at  cole Polytechnique F d rale de Lausanne School of Computer and Communication Sciences  and the Yale School of Medicine  It shows increased performance on medical related benchmarks such as MedQA and MedMCQA                                     
Zoom used Meta Llama   to create an AI Companion that can summarize meetings  provide helpful presentation tips  and assist with message responses  This AI Companion is powered by multiple models  including Meta Llama               
Reuters reported in      that many Chinese foundation models relied on Llama models for their training             

llama cpp edit 
Main article  llama cpp
Software developer Georgi Gerganov released llama cpp as open source on March           It s a re implementation of LLaMA in C    allowing systems without a powerful GPU to run the model locally              The llama cpp project introduced the GGUF file format  a binary format that stores both tensors and metadata              The format focuses on supporting different quantization types  which can reduce memory usage  and increase speed at the expense of lower model precision             
llamafile created by Justine Tunney is an open source tool that bundles llama cpp with the model into a single executable file  Tunney et al  introduced new optimized matrix multiplication kernels for x   and ARM CPUs  improving prompt evaluation performance for FP   and   bit quantized data types             

Military edit 
In       researchers from the People s Liberation Army Academy of Military Sciences  top military academy of China  were reported to have developed a military tool using Llama  which Meta Platforms stated was unauthorized due to Llama s license prohibiting the use of the model for military purposes                          Meta granted the US government and US military contractors permission to use Llama in November       but continued to prohibit military use by non US entities                         

Reception edit 
Wired describes the  B parameter version of Llama   as being  surprisingly capable  given its size             
The response to Meta s integration of Llama into Facebook was mixed  with some users confused after Meta AI told a parental group that it had a child             
According to the Q       Earnings transcript  Meta adopted the strategy of open weights to improve on model safety  iteration speed  increase adoption among developers and researchers  and to become the industry standard  Llama       and   are planned for the future             
The release of Llama models has sparked significant debates on the benefits and misuse risks of open weight models  Such models can be fine tuned to remove safeguards  notably by cyber criminals  until they comply with harmful requests  Some experts contend that future models may facilitate causing damage more than defending against it  for example by making it relatively easy to engineer advanced bioweapons without specialized knowledge  Conversely  open weight models can be useful for a wide variety of purposes  including for safety research             
Open Source Initiative head Stefano Maffulli criticized Meta for describing Llama as open source  saying that it was causing confusion among users and  polluting  the term             

See also edit 
GPT  o
IBM Granite  an open source LLM made by IBM
Mistral AI  a French open source AI company
References edit 


   llama models models llama    LICENSE at main   meta llama llama models   GitHub   GitHub  Archived from the original on             Retrieved            

  Leswing  Kif                Mark Zuckerberg announces Meta s new large language model as A I  race heats up   CNBC  Retrieved            

  Franzen  Carl                Meta defends Llama   release against  reports of mixed quality   blames bugs   VentureBeat  Retrieved            

  a b Peters  Jay  Vincent  James     February         Meta has a new machine learning language model to remind you it does AI too   The Verge 

  a b c  Meta and Microsoft Introduce the Next Generation of LLaMA   Meta     July       Archived from the original on    September       Retrieved    July      

  Malik  Yuvraj  Paul  Katie     February         Meta heats up Big Tech s AI arms race with new language model   Reuters 

  a b c  Introducing LLaMA  A foundational     billion parameter large language model   Meta AI     February       Archived from the original on   March       Retrieved    March      

  Hern  Alex                TechScape  Will Meta s massive leak democratise AI   and at what cost    The Guardian  ISSN                 Retrieved            

  David  Emilia     October         Meta s AI research head wants open source licensing to change   The Verge  Archived from the original on    September       Retrieved    October      

  Heath  Alex                Meta s battle with ChatGPT begins now   The Verge  Retrieved            

   Examining Emergent Abilities in Large Language Models   hai stanford edu     September      

   The inside story of how ChatGPT was built from the people who made it   MIT Technology Review  Archived from the original on             Retrieved            

  Ray  Tiernan     January         ChatGPT is  not particularly innovative   and  nothing revolutionary   says Meta s chief AI scientist   ZDNET  Archived from the original on            

  Badminton  Nik     February         Meta s Yann LeCun on auto regressive Large Language Models  LLMs    Futurist com  Archived from the original on    July       Retrieved    October      

   Yann LeCun on LinkedIn  My unwavering opinion on current  auto regressive  LLMs   LinkedIn  Archived from the original on             Retrieved            

   Meta s Yann LeCun Asks How AIs will Match   and Exceed   Human level Intelligence      October      

  a b c  Introducing Meta Llama    The most capable openly available LLM to date   ai meta com  April           Archived from the original on             Retrieved            

  a b c d e Touvron  Hugo  Lavril  Thibaut  Izacard  Gautier  Martinet  Xavier  Lachaux  Marie Anne  Lacroix  Timoth e  Rozi re  Baptiste  Goyal  Naman  Hambro  Eric  Azhar  Faisal  Rodriguez  Aurelien  Joulin  Armand  Grave  Edouard  Lample  Guillaume          LLaMA  Open and Efficient Foundation Language Models   arXiv             cs CL  

   llama   GitHub  Archived from the original on    March       Retrieved    March      

  a b c Vincent  James    March         Meta s powerful AI language model has leaked online   what happens now    The Verge  Archived from the original on   November       Retrieved    March      

  a b VK  Anirudh    March         Meta s LLaMA Leaked to the Public  Thanks To  chan   Analytics India Magazine  Archived from the original on    March       Retrieved    March      

   Save bandwidth by using a torrent to distribute more efficiently by ChristopherKing     Pull Request       facebookresearch llama   GitHub  Archived from the original on    April       Retrieved    March      

   Download weights from hugging face to help us save bandwidth by Jainam      Pull Request        facebookresearch llama   GitHub  Archived from the original on    March       Retrieved    March      

  Cox  Joseph    March         Facebook s Powerful Large Language Model Leaks Online   Vice  Archived from the original on   April       Retrieved    March      

  OpSec Online LLC     March         github dmca   Notice of Claimed Infringement via Email   GitHub  Archived from the original on    April       Retrieved    March      

  Willison  Simon     March         Large language models are having their Stable Diffusion moment   Simon Willison s Weblog  Archived from the original on    March       Retrieved    March      

  a b c Touvron  Hugo  Martin  Louis  et      al      Jul         LLaMA    Open Foundation and Fine Tuned Chat Models   arXiv             cs CL  

  Edwards  Benj                Meta launches LLaMA    a source available AI model that allows commercial applications      Updated        Ars Technica  Archived from the original on             Retrieved            

  a b Thomas  Prasanth Aby    November         Meta offers Llama AI to US government for national security   CIO  Retrieved   December      

   Introducing Code Llama  a state of the art large language model for coding   ai meta com  Archived from the original on             Retrieved            

  Rozi re  Baptiste  Gehring  Jonas  Gloeckle  Fabian  Sootla  Sten  Gat  Itai  Tan  Xiaoqing Ellen  Adi  Yossi  Liu  Jingyu  Sauvestre  Romain                Code Llama  Open Foundation Models for Code   arXiv             cs CL  

  Wiggers  Kyle     April         Meta releases Llama    claims it s among the best open models available   TechCrunch  Archived from the original on    September       Retrieved    October      

  Mann  Tobias  April             Meta debuts third generation Llama large language model   The Register  Archived from the original on August           Retrieved October          

  Patel  Dwarkesh                Mark Zuckerberg   Llama    Open Sourcing    b Models   amp  Caesar Augustus   www dwarkeshpatel com  Archived from the original on             Retrieved             the   billion is nearly as powerful as the biggest version of Llama   that we released       even by the end  it was    still learning right it s like we probably could have fed it more tokens and it would have gotten somewhat better but i mean at some point you know you re running a company you need to do these meta reasoning questions of       how do I want to spend our GPUs

   Introducing Llama      Our most capable models to date   ai meta com  July           Archived from the original on             Retrieved            

  a b Dubey  Abhimanyu  Jauhri  Abhinav  Pandey  Abhinav  Kadian  Abhishek  Al Dahle  Ahmad  Letman  Aiesha  Mathur  Akhil  Schelten  Alan  Yang  Amy               The Llama   Herd of Models  arXiv           

  a b c  meta llama Llama   Maverick   B    E   Hugging Face   huggingface co              Retrieved            

   The Llama   herd  The beginning of a new era of natively multimodal AI innovation   ai meta com  Archived from the original on             Retrieved            

  a b Robison  Kylie    April         Meta got caught gaming AI benchmarks   The Verge  Retrieved   April      

  Wiggers  Kyle    April         Meta s benchmarks for its new AI models are a bit misleading   TechCrunch  Retrieved   April      

  Franzen  Carl    April         Meta defends Llama   release against  reports of mixed quality   blames bugs   VentureBeat  Retrieved   April      

   Llama Models   www llama com  Archived from the original on April          Retrieved April          

   The Falcon has landed in the Hugging Face ecosystem   huggingface co  Archived from the original on             Retrieved            

   llama MODEL CARD md at main   meta llama llama   GitHub  Archived from the original on             Retrieved            

   Andrej Karpathy  Apr            The model card has some more interesting info too   X  formerly Twitter   Archived from the original on August           Retrieved October          

   llama  MODEL CARD md at main   meta llama llama    GitHub  Archived from the original on             Retrieved            

   llama models models llama    MODEL CARD md at main   meta llama llama models   GitHub  Archived from the original on             Retrieved            

  Robison  Kylie                Meta releases its first open AI model that can process images   The Verge  Retrieved            

  Wiggers  Kyle                Meta s Llama AI models get multimodal   TechCrunch  Archived from the original on             Retrieved            

   Llama      Revolutionizing edge AI and vision with open  customizable models   ai meta com  Archived from the original on             Retrieved            

  Shazeer  Noam                GLU Variants Improve Transformer   arXiv             cs CL  

  Su  Jianlin  Lu  Yu  Pan  Shengfeng  Murtadha  Ahmed  Wen  Bo  Liu  Yunfeng                RoFormer  Enhanced Transformer with Rotary Position Embedding   arXiv             cs CL  

  Zhang  Biao  Sennrich  Rico                Root Mean Square Layer Normalization   arXiv             cs LG  

  Lei Ba  Jimmy  Kiros  Jamie Ryan  Hinton  Geoffrey E                 Layer Normalization   arXiv             stat ML  

   RedPajama Data  An Open Source Recipe to Reproduce LLaMA training dataset   GitHub  Together  Archived from the original on   November       Retrieved   May      

   RedPajama Data  T   Hugging Face  Together  Archived from the original on   November       Retrieved   May      

  Wiggers  Kyle  January            Mark Zuckerberg gave Meta s Llama team the OK to train on copyrighted works  filing claims   Techcrunch  Retrieved January          

  Taori  Rohan  Gulrajani  Ishaan  Zhang  Tianyi  Dubois  Yann  Li  Xuechen  Guestrin  Carlos  Liang  Percy  Hashimoto  Tatsunori B      March         Alpaca  A Strong  Replicable Instruction Following Model   Stanford Center for Research on Foundation Models  Archived from the original on   April      

  Wang  Yizhong  Kordi  Yeganeh  Mishra  Swaroop  Liu  Alisa  Smith  Noah A   Khashabi  Daniel  Hajishirzi  Hannaneh          Self Instruct  Aligning Language Models with Self Generated Instructions   arXiv             cs CL  

   Stanford CRFM   crfm stanford edu  Archived from the original on             Retrieved            

  Quach  Katyanna   Stanford takes costly  risky Alpaca AI model offline   www theregister com 

   Stanford Researchers Take Down Alpaca AI Over Cost and Hallucinations   Gizmodo     March       Archived from the original on    May       Retrieved    October      

   alpaca lora   GitHub  Archived from the original on   April       Retrieved   April      

   Meditron  An LLM suite for low resource medical settings leveraging Meta Llama   ai meta com 

  Petersen  Tanya     November         EPFL s new Large Language Model for Medical Knowledge   Archived from the original on    September       Retrieved    October      

   epfLLM meditron   epfLLM     May       Archived from the original on    September       Retrieved    October      

   How Companies Are Using Meta Llama   Meta    May       Archived from the original on    September       Retrieved    October      

   How dependent is China on US artificial intelligence technology    Reuters  May         

  Edwards  Benj                You can now run a GPT   level AI model on your laptop  phone  and Raspberry Pi   Ars Technica  Archived from the original on             Retrieved            

   GGUF   huggingface co  Retrieved   May      

  Labonne  Maxime     November         Quantize Llama models with GGUF and llama cpp   Medium  Towards Data Science  Archived from the original on   May       Retrieved   May      

  Connatser  Matthew   Llamafile LLM driver project boosts performance on CPU cores   www theregister com  Archived from the original on    May       Retrieved    May      

  Cheung  Sunny  October             PRC Adapts Meta s Llama for Military and Security AI Applications   Jamestown Foundation  Retrieved            

  Pomfret  James  Pang  Jessie  November            Chinese researchers develop AI model for military use on back of Meta s Llama   Reuters  Retrieved November         

  Smith  Matthew S      November         Meta Opens Its AI Model for the U S  Military   IEEE Spectrum   IEEE Spectrum  Retrieved   December      

  Knight  Will   Meta s Open Source Llama   Is Already Nipping at OpenAI s Heels   Wired  Archived from the original on             Retrieved            

   Meta s amped up AI agents confusing Facebook users   ABC News     April       Archived from the original on             Retrieved            

   Archived copy   PDF   Archived  PDF  from the original on             Retrieved              cite web     CS  maint  archived copy as title  link 

  Knight  Will   Meta s New Llama     AI Model Is Free  Powerful  and Risky   Wired  ISSN                 Archived from the original on             Retrieved            

  Waters  Richard  October             Meta under fire for  polluting  open source   Financial Times 


Further reading edit 

Huang  Kalley  O Regan  Sylvia Varnham  September            Inside Meta s AI Drama  Internal Feuds Over Compute Power   The Information  Archived from the original on September          Retrieved September         

External links edit 
Official website 
Official Hugging Face organization for Llama  Llama Guard  and Prompt Guard models
vteGenerative AIConcepts
Autoencoder
Deep learning
Generative adversarial network
Generative pre trained transformer
Large language model
Neural network
Prompt engineering
Retrieval augmented generation
Reinforcement learning from human feedback
Self supervised learning
Transformer
Variational autoencoder
Vision transformer
Word embedding
ModelsText
Claude
DBRX
DeepSeek
ERNIE
Gemini
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Granite
Grok
Llama
Manus
Mistral Large
PanGu  
Qwen
Image
Aurora
DALL E
Firefly
Flux
GPT Image  
Ideogram
Imagen
Midjourney
Stable Diffusion
Speech
   ai
WaveNet
Video
Dream Machine
Gen  
Hailuo AI
Kling
Sora
Veo
VideoPoet
Music
Endel
Suno AI
Udio
Companies
   AI
Alibaba
Anthropic
Baichuan
Baidu
DeepSeek
ElevenLabs
Google DeepMind
Hugging Face
Kuaishou
Meta AI
MiniMax
Mistral AI
Moonshot AI
OpenAI
Runway
Stability AI
Synthesia
xAI
Zhipu AI

 Category
 Commons

vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects






Retrieved from  https   en wikipedia org w index php title Llama  language model  amp oldid