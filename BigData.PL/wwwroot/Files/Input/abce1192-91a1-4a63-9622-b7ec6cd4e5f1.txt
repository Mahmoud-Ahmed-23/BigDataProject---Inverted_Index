Hypothetical agent surpassing human intelligence
For the book by Nick Bostrom  see Superintelligence  Paths  Dangers  Strategies  For the      film  see Superintelligence  film  
A superintelligence is a hypothetical agent that possesses intelligence surpassing that of the brightest and most gifted human minds              Superintelligence  may also refer to a property of problem solving systems  e g   superintelligent language translators or engineering assistants  whether or not these high level intellectual competencies are embodied in agents that act in the world  A superintelligence may or may not be created by an intelligence explosion and associated with a technological singularity 
University of Oxford philosopher Nick Bostrom defines superintelligence as  any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest              The program Fritz falls short of this conception of superintelligence even though it is much better than humans at chess because Fritz cannot outperform humans in other tasks            
Technological researchers disagree about how likely present day human intelligence is to be surpassed  Some argue that advances in artificial intelligence  AI  will probably result in general reasoning systems that lack human cognitive limitations  Others believe that humans will evolve or directly modify their biology to achieve radically greater intelligence                        Several future study scenarios combine elements from both of these possibilities  suggesting that humans are likely to interface with computers  or upload their minds to computers  in a way that enables substantial intelligence amplification 
Some researchers believe that superintelligence will likely follow shortly after the development of artificial general intelligence  The first generally intelligent machines are likely to immediately hold an enormous advantage in at least some forms of mental capability  including the capacity of perfect recall  a vastly superior knowledge base  and the ability to multitask in ways not possible to biological entities  This may allow them to   either as a single being or as a new species   become much more powerful than humans  and displace them            
Several scientists and forecasters have been arguing for prioritizing early research into the possible benefits and risks of human and machine cognitive enhancement  because of the potential social impact of such technologies            


Feasibility of artificial superintelligence edit 
Artificial intelligence  especially foundation models  has made rapid progress  surpassing human capabilities in various benchmarks 
The creation of artificial superintelligence  ASI  has been a topic of increasing discussion in recent years  particularly with the rapid advancements in artificial intelligence  AI  technologies                       

Progress in AI and claims of AGI edit 
Recent developments in AI  particularly in large language models  LLMs  based on the transformer architecture  have led to significant improvements in various tasks  Models like GPT    GPT    Claude     and others have demonstrated capabilities that some researchers argue approach or even exhibit aspects of artificial general intelligence  AGI             
However  the claim that current LLMs constitute AGI is controversial  Critics argue that these models  while impressive  still lack true understanding and are primarily sophisticated pattern matching systems             

Pathways to superintelligence edit 
Philosopher David Chalmers argues that AGI is a likely path to ASI  He posits that AI can achieve equivalence to human intelligence  be extended to surpass it  and then be amplified to dominate humans across arbitrary tasks             
More recent research has explored various potential pathways to superintelligence 

Scaling current AI systems   Some researchers argue that continued scaling of existing AI architectures  particularly transformer based models  could lead to AGI and potentially ASI             
Novel architectures   Others suggest that new AI architectures  potentially inspired by neuroscience  may be necessary to achieve AGI and ASI             
Hybrid systems   Combining different AI approaches  including symbolic AI and neural networks  could potentially lead to more robust and capable systems             
Computational advantages edit 
Artificial systems have several potential advantages over biological intelligence 

Speed   Computer components operate much faster than biological neurons  Modern microprocessors     GHz  are seven orders of magnitude faster than neurons       Hz              
Scalability   AI systems can potentially be scaled up in size and computational capacity more easily than biological brains 
Modularity   Different components of AI systems can be improved or replaced independently 
Memory   AI systems can have perfect recall and vast knowledge bases  It is also much less constrained than humans when it comes to working memory             
Multitasking   AI can perform multiple tasks simultaneously in ways not possible for biological entities 
Potential path through transformer models edit 
Recent advancements in transformer based models have led some researchers to speculate that the path to ASI might lie in scaling up and improving these architectures  This view suggests that continued improvements in transformer models or similar architectures could lead directly to ASI             
Some experts even argue that current large language models like GPT   may already exhibit early signs of AGI or ASI capabilities              This perspective suggests that the transition from current AI to ASI might be more continuous and rapid than previously thought  blurring the lines between narrow AI  AGI  and ASI 
However  this view remains controversial  Critics argue that current models  while impressive  still lack crucial aspects of general intelligence such as true understanding  reasoning  and adaptability across diverse domains             
The debate over whether the path to ASI will involve a distinct AGI phase or a more direct scaling of current technologies remains ongoing  with significant implications for AI development strategies and safety considerations 

Challenges and uncertainties edit 
Despite these potential advantages  there are significant challenges and uncertainties in achieving ASI 

Ethical and safety concerns   The development of ASI raises numerous ethical questions and potential risks that need to be addressed             
Computational requirements   The computational resources required for ASI might be far beyond current capabilities 
Fundamental limitations   There may be fundamental limitations to intelligence that apply to both artificial and biological systems 
Unpredictability   The path to ASI and its consequences are highly uncertain and difficult to predict 
As research in AI continues to advance rapidly  the question of the feasibility of ASI remains a topic of intense debate and study in the scientific community 

Feasibility of biological superintelligence edit 
Carl Sagan suggested that the advent of Caesarean sections and in vitro fertilization may permit humans to evolve larger heads  resulting in improvements via natural selection in the heritable component of human intelligence              By contrast  Gerald Crabtree has argued that decreased selection pressure is resulting in a slow  centuries long reduction in human intelligence and that this process instead is likely to continue  There is no scientific consensus concerning either possibility and in both cases  the biological change would be slow  especially relative to rates of cultural change 
Selective breeding  nootropics  epigenetic modulation  and genetic engineering could improve human intelligence more rapidly  Bostrom writes that if we come to understand the genetic component of intelligence  pre implantation genetic diagnosis could be used to select for embryos with as much as   points of IQ gain  if one embryo is selected out of two   or with larger gains  e g   up to      IQ points gained if one embryo is selected out of        If this process is iterated over many generations  the gains could be an order of magnitude improvement  Bostrom suggests that deriving new gametes from embryonic stem cells could be used to iterate the selection process rapidly              A well organized society of high intelligence humans of this sort could potentially achieve collective superintelligence             
Alternatively  collective intelligence might be constructional by better organizing humans at present levels of individual intelligence  Several writers have suggested that human civilization  or some aspect of it  e g   the Internet  or the economy   is coming to function like a global brain with capacities far exceeding its component agents  If this systemic superintelligence relies heavily on artificial components  however  it may qualify as an AI rather than as a biology based superorganism              A prediction market is sometimes considered as an example of a working collective intelligence system  consisting of humans only  assuming algorithms are not used to inform decisions              
A final method of intelligence amplification would be to directly enhance individual humans  as opposed to enhancing their social or reproductive dynamics  This could be achieved using nootropics  somatic gene therapy  or brain computer interfaces  However  Bostrom expresses skepticism about the scalability of the first two approaches and argues that designing a superintelligent cyborg interface is an AI complete problem             

Forecasts edit 
Most surveyed AI researchers expect machines to eventually be able to rival humans in intelligence  though there is little consensus on when this will likely happen  At the      AI    conference      of attendees reported expecting machines to be able  to simulate learning and every other aspect of human intelligence  by           of attendees expected this to happen sometime after       and     expected machines to never reach that milestone             
In a survey of the     most cited authors in AI  as of May       according to Microsoft academic search   the median year by which respondents expected machines  that can carry out most human professions at least as well as a typical human   assuming no global catastrophe occurs  with     confidence is       mean       st  dev     years   with     confidence is       mean       st  dev      years   and with     confidence is       mean       st  dev      years   These estimates exclude the      of respondents who said no year would ever reach     confidence  the      who said  never  for     confidence  and the       who said  never  for     confidence  Respondents assigned a median     probability to the possibility that machine superintelligence will be invented within    years of the invention of approximately human level machine intelligence             
In a      survey  the median year by which respondents expected  High level machine intelligence  with     confidence is       The survey defined the achievement of high level machine intelligence as when unaided machines can accomplish every task better and more cheaply than human workers             
In       OpenAI leaders Sam Altman  Greg Brockman and Ilya Sutskever published recommendations for the governance of superintelligence  which they believe may happen in less than    years              In       Ilya Sutskever left OpenAI to cofound the startup Safe Superintelligence  which focuses solely on creating a superintelligence that is safe by design  while avoiding  distraction by management overhead or product cycles               Despite still offering no product  the startup became valued at     billion in February                   In       the forecast scenario  AI       led by Daniel Kokotakjlo predicted rapid progress in the automation of coding and AI research  followed by ASI              

Design considerations edit 
The design of superintelligent AI systems raises critical questions about what values and goals these systems should have  Several proposals have been put forward             

Value alignment proposals edit 
Coherent extrapolated volition  CEV    The AI should have the values upon which humans would converge if they were more knowledgeable and rational 
Moral rightness  MR    The AI should be programmed to do what is morally right  relying on its superior cognitive abilities to determine ethical actions 
Moral permissibility  MP    The AI should stay within the bounds of moral permissibility while otherwise pursuing goals aligned with human values  similar to CEV  
Bostrom elaborates on these concepts 

instead of implementing humanity s coherent extrapolated volition  one could try to build an AI to do what is morally right  relying on the AI s superior cognitive capacities to figure out just which actions fit that description  We can call this proposal  moral rightness   MR           
MR would also appear to have some disadvantages  It relies on the notion of  morally right   a notoriously difficult concept  one with which philosophers have grappled since antiquity without yet attaining consensus as to its analysis  Picking an erroneous explication of  moral rightness  could result in outcomes that would be morally very wrong         

One might try to preserve the basic idea of the MR model while reducing its demandingness by focusing on moral permissibility  the idea being that we could let the AI pursue humanity s CEV so long as it did not act in morally impermissible ways             
Recent developments edit 
Since Bostrom s analysis  new approaches to AI value alignment have emerged 

Inverse Reinforcement Learning  IRL    This technique aims to infer human preferences from observed behavior  potentially offering a more robust approach to value alignment             
Constitutional AI   Proposed by Anthropic  this involves training AI systems with explicit ethical principles and constraints             
Debate and amplification   These techniques  explored by OpenAI  use AI assisted debate and iterative processes to better understand and align with human values             
Transformer LLMs and ASI edit 
The rapid advancement of transformer based LLMs has led to speculation about their potential path to ASI  Some researchers argue that scaled up versions of these models could exhibit ASI like capabilities             

Emergent abilities   As LLMs increase in size and complexity  they demonstrate unexpected capabilities not present in smaller models             
In context learning   LLMs show the ability to adapt to new tasks without fine tuning  potentially mimicking general intelligence             
Multi modal integration   Recent models can process and generate various types of data  including text  images  and audio             
However  critics argue that current LLMs lack true understanding and are merely sophisticated pattern matchers  raising questions about their suitability as a path to ASI             

Other perspectives on artificial superintelligence edit 
Additional viewpoints on the development and implications of superintelligence include 

Recursive self improvement   I  J  Good proposed the concept of an  intelligence explosion   where an AI system could rapidly improve its own intelligence  potentially leading to superintelligence             
Orthogonality thesis   Bostrom argues that an AI s level of intelligence is orthogonal to its final goals  meaning a superintelligent AI could have any set of motivations             
Instrumental convergence   Certain instrumental goals  e g   self preservation  resource acquisition  might be pursued by a wide range of AI systems  regardless of their final goals             
Challenges and ongoing research edit 
The pursuit of value aligned AI faces several challenges 

Philosophical uncertainty in defining concepts like  moral rightness 
Technical complexity in translating ethical principles into precise algorithms
Potential for unintended consequences even with well intentioned approaches
Current research directions include multi stakeholder approaches to incorporate diverse perspectives  developing methods for scalable oversight of AI systems  and improving techniques for robust value learning                         
Al research is rapidly progressing towards superintelligence  Addressing these design challenges remains crucial for creating ASI systems that are both powerful and aligned with human interests 

Potential threat to humanity edit 
Main articles  Existential risk from artificial general intelligence  AI alignment  and AI safety
The development of artificial superintelligence  ASI  has raised concerns about potential existential risks to humanity  Researchers have proposed various scenarios in which an ASI could pose a significant threat 

Intelligence explosion and control problem edit 
Some researchers argue that through recursive self improvement  an ASI could rapidly become so powerful as to be beyond human control  This concept  known as an  intelligence explosion   was first proposed by I  J  Good in      

Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever  Since the design of machines is one of these intellectual activities  an ultraintelligent machine could design even better machines  there would then unquestionably be an  intelligence explosion   and the intelligence of man would be left far behind  Thus the first ultraintelligent machine is the last invention that man need ever make  provided that the machine is docile enough to tell us how to keep it under control             
This scenario presents the AI control problem  how to create an ASI that will benefit humanity while avoiding unintended harmful consequences              Eliezer Yudkowsky argues that solving this problem is crucial before ASI is developed  as a superintelligent system might be able to thwart any subsequent attempts at control             

Unintended consequences and goal misalignment edit 
Even with benign intentions  an ASI could potentially cause harm due to misaligned goals or unexpected interpretations of its objectives  Nick Bostrom provides a stark example of this risk 

When we create the first superintelligent entity  we might make a mistake and give it goals that lead it to annihilate humankind  assuming its enormous intellectual advantage gives it the power to do so  For example  we could mistakenly elevate a subgoal to the status of a supergoal  We tell it to solve a mathematical problem  and it complies by turning all the matter in the solar system into a giant calculating device  in the process killing the person who asked the question             
Stuart Russell offers another illustrative scenario 

A system given the objective of maximizing human happiness might find it easier to rewire human neurology so that humans are always happy regardless of their circumstances  rather than to improve the external world             
These examples highlight the potential for catastrophic outcomes even when an ASI is not explicitly designed to be harmful  underscoring the critical importance of precise goal specification and alignment 

Potential mitigation strategies edit 
Researchers have proposed various approaches to mitigate risks associated with ASI 

Capability control   Limiting an ASI s ability to influence the world  such as through physical isolation or restricted access to resources             
Motivational control   Designing ASIs with goals that are fundamentally aligned with human values             
Ethical AI   Incorporating ethical principles and decision making frameworks into ASI systems             
Oversight and governance   Developing robust international frameworks for the development and deployment of ASI technologies             
Despite these proposed strategies  some experts  such as Roman Yampolskiy  argue that the challenge of controlling a superintelligent AI might be fundamentally unsolvable  emphasizing the need for extreme caution in ASI development             

Debate and skepticism edit 
Not all researchers agree on the likelihood or severity of ASI related existential risks  Some  like Rodney Brooks  argue that fears of superintelligent AI are overblown and based on unrealistic assumptions about the nature of intelligence and technological progress              Others  such as Joanna Bryson  contend that anthropomorphizing AI systems leads to misplaced concerns about their potential threats             

Recent developments and current perspectives edit 
The rapid advancement of LLMs and other AI technologies has intensified debates about the proximity and potential risks of ASI  While there is no scientific consensus  some researchers and AI practitioners argue that current AI systems may already be approaching AGI or even ASI capabilities 

LLM capabilities   Recent LLMs like GPT   have demonstrated unexpected abilities in areas such as reasoning  problem solving  and multi modal understanding  leading some to speculate about their potential path to ASI             
Emergent behaviors   Studies have shown that as AI models increase in size and complexity  they can exhibit emergent capabilities not present in smaller models  potentially indicating a trend towards more general intelligence             
Rapid progress   The pace of AI advancement has led some to argue that we may be closer to ASI than previously thought  with potential implications for existential risk             
A minority of researchers and observers  including some in the AI development community  believe that current AI systems may already be at or near AGI levels  with ASI potentially following in the near future  This view  while not widely accepted in the scientific community  is based on observations of rapid progress in AI capabilities and unexpected emergent behaviors in large models             
However  many experts caution against premature claims of AGI or ASI  arguing that current AI systems  despite their impressive capabilities  still lack true understanding and general intelligence              They emphasize the significant challenges that remain in achieving human level intelligence  let alone superintelligence 
The debate surrounding the current state and trajectory of AI development underscores the importance of continued research into AI safety and ethics  as well as the need for robust governance frameworks to manage potential risks as AI capabilities continue to advance             

See also edit 

Artificial general intelligence
AI safety
AI takeover
Artificial brain
Artificial intelligence arms race
Effective altruism
Ethics of artificial intelligence
Existential risk
Friendly artificial intelligence
Future of Humanity Institute
Intelligent agent
Machine ethics
Machine Intelligence Research Institute
Machine learning
Neural scaling law        Statistical law in machine learning
Noosphere        Philosophical concept of biosphere successor via humankind s rational activities
Outline of artificial intelligence
Posthumanism
Robotics
Self replication
Self replicating machine
Superintelligence  Paths  Dangers  Strategies

References edit 


  Mucci  Tim  Stryker  Cole                What Is Artificial Superintelligence    IBM   www ibm com  Retrieved            

  a b Bostrom       Chapter   

  Bostrom       p          

  Pearce  David         Eden  Amnon H   Moor  James H   S raker  Johnny H   Steinhart  Eric  eds     The Biointelligence Explosion  How Recursively Self Improving Organic Robots will Modify their Own Source Code and Bootstrap Our Way to Full Spectrum Superintelligence   Singularity Hypotheses  The Frontiers Collection  Berlin  Heidelberg  Springer Berlin Heidelberg  pp                doi                               ISBN                         retrieved           

  Gouveia  Steven S   ed           ch      Humans and Intelligent Machines  Co evolution  Fusion or Replacement    David Pearce   The Age of Artificial Intelligence  An Exploration  Vernon Press  ISBN                        

  Legg       pp               

    Superintelligence  is the next big thing for OpenAI  Sam Altman   The Economic Times              ISSN                 Retrieved            

   OpenAI co founder Sutskever sets up new AI company devoted to  safe superintelligence    AP News              Retrieved            

   Microsoft Researchers Claim GPT   Is Showing  Sparks  of AGI   Futurism     March       Retrieved            

  Marcus  Gary  Davis  Ernest          GPT   and Beyond  The Future of Artificial Intelligence   arXiv             econ GN  

  Chalmers       p         

  Kaplan  Jared  McCandlish  Sam  Henighan  Tom  Brown  Tom B   Chess  Benjamin  Child  Rewon  Gray  Scott  Radford  Alec  Wu  Jeffrey  Amodei  Dario          Scaling Laws for Neural Language Models   arXiv             cs LG  

  Hassabis  Demis  Kumaran  Dharshan  Summerfield  Christopher  Botvinick  Matthew          Neuroscience Inspired Artificial Intelligence   Neuron                   doi         j neuron              PMID               

  Garcez  Artur d Avila  Lamb  Luis C           Neurosymbolic AI  The  rd Wave   arXiv             cs AI  

  a b Bostrom       p          

  Sutskever  Ilya          A Brief History of Scaling   ACM Queue                 doi                          inactive   November         cite journal     CS  maint  DOI inactive as of November       link 

  Bubeck  S bastien  Chandrasekaran  Varun  Eldan  Ronen  Gehrke  Johannes  Horvitz  Eric  Kamar  Ece  Lee  Peter  Lee  Yin Tat  Li  Yuanzhi  Lundberg  Scott  Nori  Harsha  Palangi  Hamid  Precup  Doina  Sountsov  Pavel  Srivastava  Sanjana  Tessler  Catherine  Tian  Jianfeng  Zaheer  Manzil     March         Sparks of Artificial General Intelligence  Early experiments with GPT     arXiv             cs CL  

  Marcus  Gary          The Next Decade in AI  Four Steps Towards Robust Artificial Intelligence   arXiv             cs AI  

  a b Russell      

  Sagan  Carl         The Dragons of Eden  Random House 

  Bostrom       pp             

  Bostrom       p          

  Bostrom       pp             

  Watkins  Jennifer H          Prediction Markets as an Aggregation Mechanism for Collective Intelligence

  Bostrom       pp                     

  Maker  Meg Houston  July             AI     First Poll   Archived from the original on            

  M ller  amp  Bostrom       pp                    

  Roser  Max    February         AI timelines  What do experts in artificial intelligence expect for the future    Our World in Data  Retrieved            

   Governance of superintelligence   openai com  Retrieved            

  Vance  Ashlee  June             Ilya Sutskever Has a New Plan for Safe Superintelligence   Bloomberg  Retrieved            

   There s Something Very Weird About This     Billion AI Startup by a Man Who Said Neural Networks May Already Be Conscious   Futurism              Retrieved            

  Roose  Kevin                This A I  Forecast Predicts Storms Ahead   The New York Times  ISSN                 Retrieved            

  a b Bostrom       pp               

  Christiano  Paul  Leike  Jan  Brown  Tom B   Martic  Miljan  Legg  Shane  Amodei  Dario          Deep Reinforcement Learning from Human Preferences   PDF   NeurIPS  arXiv            

   Constitutional AI  Harmlessness from AI Feedback   Anthropic  December          

   Learning complex goals with iterated amplification   OpenAI  October          

  Bommasani  Rishi  et      al           On the Opportunities and Risks of Foundation Models   Stanford University  arXiv            

  a b Wei  Jason  Tay  Yi  Bommasani  Rishi  Raffel  Colin  Zoph  Barret  Borgeaud  Sebastian  Yogatama  Dani  Bosma  Maarten  Zhou  Denny  Metzler  Donald  Chi  Ed H   Hashimoto  Tatsunori  Vinyals  Oriol  Liang  Percy  Dean  Jeff  Fedus  William                Emergent Abilities of Large Language Models   Transactions on Machine Learning Research  arXiv             ISSN                

  Brown  Tom B   et      al           Language Models are Few Shot Learners   NeurIPS  arXiv            

  Alayrac  Jean Baptiste  Donahue  Jeff  Luc  Pauline  Miech  Antoine  Barr  Iain  Hasson  Yana  Lenc  Karel  Mensch  Arthur  Millican  Katie  Reynolds  Malcolm  Ring  Roman  Rutherford  Eliza  Cabi  Serkan  Han  Tengda  Gong  Zhitao  Samangooei  Sina  Monteiro  Marianne  Menick  Jacob  Borgeaud  Sebastian  Brock  Andrew  Nematzadeh  Aida  Sharifzadeh  Sahand  Binkowski  Mikolaj  Barreira  Ricardo  Vinyals  Oriol  Zisserman  Andrew  Simonyan  Karen          Flamingo  a Visual Language Model for Few Shot Learning   NeurIPS  arXiv            

  Marcus  Gary  August             Deep Learning Alone Isn t Getting Us To Human Like AI   Noema 

   The AI apocalypse  will the human race soon be terminated    The Irish Times  March          

  Bostrom  Nick          The Superintelligent Will  Motivation and Instrumental Rationality in Advanced Artificial Agents   PDF   Minds and Machines                 doi         s                 

  Omohundro  Stephen M   January         The basic AI drives   PDF   Frontiers in Artificial Intelligence and Applications 

  Gabriel  Iason                Artificial Intelligence  Values  and Alignment   Minds and Machines                   arXiv             doi         s                   ISSN                

  Good  I  J           Speculations Concerning the First Ultraintelligent Machine   Advances in Computers 

  Russell       pp               

  Yudkowsky  Eliezer          Artificial Intelligence as a Positive and Negative Factor in Global Risk   PDF   Global Catastrophic Risks  doi         oso                         ISBN                        

  Bostrom      

  Russell       p           

  Bostrom       pp               

  Bostrom       pp               

  Wallach  Wendell  Allen  Colin               Moral Machines  Teaching Robots Right from Wrong  Oxford University Press  ISBN                        

  a b Dafoe  Allan  August             AI Governance  A Research Agenda   PDF   Center for the Governance of AI 

  Yampolskiy  Roman V   July             On Controllability of Artificial Intelligence   PDF   arXiv            

  Brooks  Rodney  October            The Seven Deadly Sins of AI Predictions   MIT Technology Review  Retrieved            

  Bryson  Joanna J          The Past Decade and Future of AI s Impact on Society   Towards a New Enlightenment  A Transcendent Decade      ISBN                        

  Bubeck  S bastien  Chandrasekaran  Varun  Eldan  Ronen  Gehrke  Johannes  Horvitz  Eric  Kamar  Ece  Lee  Peter  Yin Tat Lee  Li  Yuanzhi  Lundberg  Scott  Nori  Harsha  Palangi  Hamid  Marco Tulio Ribeiro  Zhang  Yi  April         Sparks of Artificial General Intelligence  Early experiments with GPT     arXiv             cs CL  

  Ord  Toby         The precipice  existential risk and the future of humanity  london New York  N Y    Bloomsbury academic  ISBN                        

  Ngo  Richard  Chan  Lawrence  Mindermann  S ren          The Alignment Problem from a Deep Learning Perspective   ICLR  arXiv            

   How and Why Gary Marcus Became AI s Leading Critic  gt  Marcus says generative AI like ChatGPT poses immediate dangers   IEEE Spectrum     September      


Papers edit 
Bostrom  Nick          Existential Risks   Journal of Evolution and Technology     retrieved            
Chalmers  David          The Singularity  A Philosophical Analysis   PDF   Journal of Consciousness Studies           
Legg  Shane         Machine Super Intelligence  PDF   PhD   Department of Informatics  University of Lugano  Retrieved September          
M ller  Vincent C   Bostrom  Nick          Future Progress in Artificial Intelligence  A Survey of Expert Opinion   In M ller  Vincent C   ed    Fundamental Issues of Artificial Intelligence  Springer  pp               
Santos Lang  Christopher          Our responsibility to manage evaluative diversity   PDF   ACM SIGCAS Computers  amp  Society                 doi                          S CID               Archived from the original on July          
Books edit 
Hibbard  Bill         Super Intelligent Machines  Kluwer Academic Plenum Publishers 
Bostrom  Nick         Superintelligence  Paths  Dangers  Strategies  Oxford University Press 
Tegmark  Max         Life      being human in the age of artificial intelligence  London  England  ISBN                         OCLC                   cite book     CS  maint  location missing publisher  link 
Russell  Stuart J          Human compatible  artificial intelligence and the problem of control  New York  ISBN                         OCLC                   cite book     CS  maint  location missing publisher  link 
Sanders  Nada R          The humachine  humankind  machines  and the future of enterprise  John D  Wood  First      ed    New York  New York  ISBN                         OCLC                   cite book     CS  maint  location missing publisher  link 
External links edit 
Bill Gates Joins Stephen Hawking in Fears of a Coming Threat from  Superintelligence 
Will Superintelligent Machines Destroy Humanity 
Apple Co founder Has Sense of Foreboding About Artificial Superintelligence
vteExistential risk from artificial intelligenceConcepts
AGI
AI alignment
AI capability control
AI safety
AI takeover
Consequentialism
Effective accelerationism
Ethics of artificial intelligence
Existential risk from artificial intelligence
Friendly artificial intelligence
Instrumental convergence
Vulnerable world hypothesis
Intelligence explosion
Longtermism
Machine ethics
Suffering risks
Superintelligence
Technological singularity
Organizations
Alignment Research Center
Center for AI Safety
Center for Applied Rationality
Center for Human Compatible Artificial Intelligence
Centre for the Study of Existential Risk
EleutherAI
Future of Humanity Institute
Future of Life Institute
Google DeepMind
Humanity 
Institute for Ethics and Emerging Technologies
Leverhulme Centre for the Future of Intelligence
Machine Intelligence Research Institute
OpenAI
People
Scott Alexander
Sam Altman
Yoshua Bengio
Nick Bostrom
Paul Christiano
Eric Drexler
Sam Harris
Stephen Hawking
Dan Hendrycks
Geoffrey Hinton
Bill Joy
Shane Legg
Elon Musk
Steve Omohundro
Huw Price
Martin Rees
Stuart J  Russell
Jaan Tallinn
Max Tegmark
Frank Wilczek
Roman Yampolskiy
Eliezer Yudkowsky
Other
Statement on AI risk of extinction
Human Compatible
Open letter on artificial intelligence       
Our Final Invention
The Precipice
Superintelligence  Paths  Dangers  Strategies
Do You Trust This Computer 
Artificial Intelligence Act
 Category





Retrieved from  https   en wikipedia org w index php title Superintelligence amp oldid