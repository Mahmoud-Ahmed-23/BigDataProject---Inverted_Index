Software system


This article s lead section contains information that is not included elsewhere in the article  If the information is appropriate for the lead of the article  this information should also be included in the body of the article    February        Learn how and when to remove this message 
Part of a series onAlgocracy
Examples

AI in government
LLMs in government
ChatGPT in education
Broligarchy
Cybersyn
DAO
Digital dictatorship
Merit order
OGAS
Ofqual exam results algorithm
OIA
PMPs
Predictive policing
Gangs Matrix
VioG n
Predictive sentencing
COMPAS
OASys
OGRS
Robodebt scheme
Smart city
Surveillance capitalism
SyRI
Tariffs in the second Trump administration

vte
Correctional Offender Management Profiling for Alternative Sanctions  COMPAS             is a case management and decision support software developed and owned by Northpointe  now Equivant   used by U S  courts to assess the likelihood of a defendant becoming a recidivist                       
COMPAS has been used by the U S  states of New York  Wisconsin  California  Florida s Broward County  and other jurisdictions            


Risk assessment edit 
The COMPAS software uses an algorithm to assess potential recidivism risk  Northpointe created risk scales for general and violent recidivism  and for pretrial misconduct  According to the COMPAS Practitioner s Guide  the scales were designed using behavioral and psychological constructs  of very high relevance to recidivism and criminal careers             

Pretrial release risk scale
Pretrial risk is a measure of the potential for an individual to fail to appear and or to commit new felonies while on release  According to the research that informed the creation of the scale   current charges  pending charges  prior arrest history  previous pretrial failure  residential stability  employment status  community ties  and substance abuse  are the most significant indicators affecting pretrial risk scores            
General recidivism scale
The General recidivism scale is designed to predict new offenses upon release  and after the COMPAS assessment is given  The scale uses an individual s criminal history and associates  drug involvement  and indications of juvenile delinquency            
Violent recidivism scale
The violent recidivism score is meant to predict violent offenses following release  The scale uses data or indicators that include a person s  history of violence  history of non compliance  vocational educational problems  the person s age at intake and the person s age at first arrest             
The violent recidivism risk scale is calculated as follows 

  
    
      
        s
         
        a
         
          x     
        w
         
         
        
          a
          
            first
          
        
         
          x     
        w
         
         
        
          h
          
            violence
          
        
        w
         
        
          v
          
            edu
          
        
        w
         
        
          h
          
            nc
          
        
        w
      
    
      displaystyle s a  w  a   text first    w  h   text violence  w v   text edu  w h   text nc  w 
  

where 
  
    
      
        s
      
    
      displaystyle s 
  
 is the violent recidivism risk score  
  
    
      
        w
      
    
      displaystyle w 
  
 is a weight multiplier  
  
    
      
        a
      
    
      displaystyle a 
  
 is current age  
  
    
      
        
          a
          
            first
          
        
      
    
      displaystyle a   text first   
  
 is the age at first arrest  
  
    
      
        
          h
          
            violence
          
        
      
    
      displaystyle h   text violence   
  
 is the history of violence  
  
    
      
        
          v
          
            edu
          
        
      
    
      displaystyle v   text edu   
  
 is vocational education scale  and 
  
    
      
        
          h
          
            nc
          
        
      
    
      displaystyle h   text nc   
  
 is history of noncompliance  The weight  
  
    
      
        w
      
    
      displaystyle w 
  
  is  determined by the strength of the item s relationship to person offense recidivism that we observed in our study data             

Critiques and legal rulings edit 
Proponents of using AI and algorithms in the courtroom tend to argue that these solutions will mitigate predictable biases and errors in judges  reasoning  such as the hungry judge effect  the phenomenon that judges are more likely to make lenient decisions after eating a meal             
In July       the Wisconsin Supreme Court ruled that COMPAS risk scores can be considered by judges during sentencing  but there must be warnings given to the scores to represent the tool s  limitations and cautions             
A general critique of the use of proprietary software such as COMPAS is that since the algorithms it uses are trade secrets  they cannot be examined by the public and affected parties which may be a violation of due process  Additionally  simple  transparent and more interpretable algorithms  such as linear regression  have been shown to perform predictions approximately as well as the COMPAS algorithm                                     
Another general criticism of machine learning based algorithms is since they are data dependent if the data are biased  the software will likely yield biased results             
Specifically  COMPAS risk assessments have been argued to violate   th Amendment Equal Protection rights on the basis of race  since the algorithms are argued to be racially discriminatory  to result in disparate treatment  and to not be narrowly tailored             

Accuracy edit 
In       Julia Angwin was co author of a ProPublica investigation of the algorithm              The team found that  blacks are almost twice as likely as whites to be labeled a higher risk but not actually re offend   whereas COMPAS  makes the opposite mistake among whites  They are much more likely than blacks to be labeled lower risk but go on to commit other crimes                                       They also found that only    percent of people predicted to commit violent crimes actually went on to do so             
In a letter  Northpointe criticized ProPublica s methodology and stated that    The company  does not agree that the results of your analysis  or the claims being made based upon that analysis  are correct or that they accurately reflect the outcomes from the application of the model              
Another team at the Community Resources for Justice  a criminal justice think tank  published a rebuttal of the investigation s findings              Among several objections  the CRJ rebuttal concluded that the Propublica s results   contradict several comprehensive existing studies concluding that actuarial risk can be predicted free of racial and or gender bias              
A subsequent study has shown that COMPAS software is somewhat more accurate than individuals with little or no criminal justice expertise  yet less accurate than groups of such individuals              They found that   On average  they got the right answer    percent of their time  and the group s accuracy rose to    percent if their answers were pooled  COMPAS  by contrast  has an accuracy of    percent                Researchers from the University of Houston found that COMPAS does not conform to group fairness criteria and produces various kinds of unfair outcomes across sex  and race based demographic groups              In       a group of researchers from Williams College found that the use of COMPAS in Broward County led to a reduced rate of confinement across demographic groups  but that the use of the algorithm exacerbated differences in confinement between racial groups  thereby deepening racial disparity             

See also edit 
Algorithmic bias
Garbage in  garbage out
Legal expert systems
Loomis v  Wisconsin
Criminal sentencing in the United States
References edit 


   DOC COMPAS   Retrieved April         

  Sam Corbett Davies  Emma Pierson  Avi Feller and Sharad Goel  October             A computer program used for bail and sentencing decisions was labeled biased against blacks  It s actually not that clear   The Washington Post  Retrieved January           cite news     CS  maint  multiple names  authors list  link 

  Aaron M  Bornstein  December             Are Algorithms Building the New Infrastructure of Racism    Nautilus  No           Retrieved January         

  a b Kirkpatrick  Keith  January             It s not the algorithm  it s the data   Communications of the ACM                 doi                  S CID               

  a b Northpointe       p          

  Northpointe       p          

  Northpointe       p          

  Northpointe       p          

  Chatziathanasiou  Konstantin  May         Beware the Lure of Narratives   Hungry Judges  Should Not Motivate the Use of  Artificial Intelligence  in Law   German Law Journal                   doi         glj          ISSN                 S CID                

  a b c Yong  Ed  January             A Popular Algorithm Is No Better at Predicting Crimes Than Random People   Retrieved November          

  Angelino  Elaine  Larus Stone  Nicholas  Alabi  Daniel  Seltzer  Margo  Rudin  Cynthia  June         Learning Certifiably Optimal Rule Lists for Categorical Data   Journal of Machine Learning Research                  arXiv             Retrieved July          

  Robin A  Smith  Opening the lid on criminal sentencing software  Duke Today     July     

  O Neil  Cathy         Weapons of Math Destruction  Crown  p           ISBN                     

  Thomas  C   Nunez  A           Automating Judicial Discretion  How Algorithmic Risk Assessments in Pretrial Adjudications Violate Equal Protection Rights on the Basis of Race   Law  amp  Inequality                   doi                       

  a b c d Angwin  Julia  Larson  Jeff  May             Machine Bias   ProPublica  Retrieved November          

  Israni  Ellora  October             When an Algorithm Helps Send You to Prison  Opinion    The New York Times  Retrieved November          

  a b Flores  Anthony  Lowenkamp  Christopher  Bechtel  Kristin   False Positives  False Negatives  and False Analyses   PDF   Community Resources for Justice  Retrieved November          

  Dressel  Julia  Farid  Hany  January             The accuracy  fairness  and limits of predicting recidivism   Science Advances         eaao      Bibcode     SciA          D  doi         sciadv aao      PMC               PMID               

  Gursoy  Furkan  Kakadiaris  Ioannis A   November             Equal Confusion Fairness  Measuring Group Based Disparities in Automated Decision Systems        IEEE International Conference on Data Mining Workshops  ICDMW   IEEE  pp                arXiv             doi         ICDMW                  ISBN                         S CID                

  Bahl  Utsav  Topaz  Chad  Oberm ller  Lea  Goldstein  Sophie  Sneirson  Mira  May             Algorithms in Judges  Hands  Incarceration and Inequity in Broward County  Florida   UCLA Law Review  Retrieved March          


Further reading edit 
Northpointe  March             A Practitioner s Guide to COMPAS Core   PDF  
Angwin  Julia  Larson  Jeff  May             Machine Bias   ProPublica  Retrieved November          
Flores  Anthony  Lowenkamp  Christopher  Bechtel  Kristin   False Positives  False Negatives  and False Analyses   PDF   Community Resources for Justice  Retrieved November          
Sample COMPAS Risk Assessment





Retrieved from  https   en wikipedia org w index php title COMPAS  software  amp oldid