Erroneous material generated by AI
Not to be confused with Artificial imagination 


A Sora generated video of the Glenfinnan Viaduct  incorrectly showing a second track whereas the real viaduct has only one  a second chimney on its interpretation of the train The Jacobite  and some carriages much longer than others
In the field of artificial intelligence  AI   a hallucination or artificial hallucination  also called bullshitting                        confabulation            or delusion             is a response generated by AI that contains false or misleading information presented as fact                        This term draws a loose analogy with human psychology  where hallucination typically involves false percepts  However  there is a key difference  AI hallucination is associated with erroneously constructed responses  confabulation   rather than perceptual experiences            
For example  a chatbot powered by large language models  LLMs   like ChatGPT  may embed plausible sounding random falsehoods within its generated content  Researchers have recognized this issue  and by       analysts estimated that chatbots hallucinate as much as     of the time             with factual errors present in     of generated texts             Detecting and mitigating these hallucinations pose significant challenges for practical deployment and reliability of LLMs in real world scenarios                                   Some people believe the specific term  AI hallucination  unreasonably anthropomorphizes computers            


Term edit 
Origin edit 
In       Stephen Thaler demonstrated how hallucinations and phantom experiences emerge from artificial neural networks through random perturbation of their connection weights                                                             
In the early     s  the term  hallucination  was used in computer vision with a positive connotation to describe the process of adding detail to an image  For example  the task of generating high resolution face images from low resolution inputs is called face hallucination                         
In the late     s  the term underwent a semantic shift to signify the generation of factually incorrect or misleading outputs by AI systems in tasks like translation or object detection              For example  in       Google researchers used the term to describe the responses generated by neural machine translation  NMT  models when they are not related to the source text              and in       the term was used in computer vision to describe instances where non existent objects are erroneously detected because of adversarial attacks             
The term  hallucinations  in AI gained wider recognition during the AI boom  alongside the rollout of widely used chatbots based on large language models  LLMs               In July       Meta warned during its release of BlenderBot   that the system is prone to  hallucinations   which Meta defined as  confident statements that are not true                           Following OpenAI s ChatGPT release in beta version in November       some users complained that such chatbots often seem to pointlessly embed plausible sounding random falsehoods within their generated content              Many news outlets  including The New York Times  started to use the term  hallucinations  to describe these models  occasionally incorrect or inconsistent responses             
Some researchers have highlighted a lack of consistency in how the term is used  but also identified several alternative terms in the literature  such as confabulations  fabrications  factual errors  etc             
In       the Cambridge dictionary updated their definition of hallucination to include this new meaning specific to the field of AI             

Definitions and alternatives edit 
A list of uses of the term  hallucination   definitions or characterizations in the context of LLMs include 

 a tendency to invent facts in moments of uncertainty   OpenAI  May                  
 a model s logical mistakes   OpenAI  May                  
 fabricating information entirely  but behaving as if spouting facts   CNBC  May                  
 making up information   The Verge  February                  
 probability distributions   in scientific contexts             
Journalist Benj Edwards  in Ars Technica  writes that the term  hallucination  is controversial  but that some form of metaphor remains necessary  Edwards suggests  confabulation  as an analogy for processes that involve  creative gap filling              In July       a White House report on fostering public trust in AI research mentioned hallucinations only in the context of reducing them  Notably  when acknowledging David Baker s Nobel Prize winning work with AI generated proteins  the Nobel committee avoided the term entirely  instead referring to  imaginative protein creation              

Criticism edit 
In the scientific community  some researchers avoid the term  hallucination  as potentially misleading  It has been criticized by Usama Fayyad  executive director of the Institute for Experimental Artificial Intelligence at Northeastern University  on the grounds that it misleadingly personifies large language models  and that it is vague              Mary Shaw said  The current fashion for calling generative AI s errors  hallucinations  is appalling  It anthropomorphizes the software  and it spins actual errors as somehow being idiosyncratic quirks of the system even when they re objectively incorrect               In Salon  statistician Gary N  Smith argues that LLMs  do not understand what words mean  and consequently that the term  hallucination  unreasonably anthropomorphizes the machine              Some see the AI outputs not as illusory but as prospective  i e  having some chance of being true  similar to early stage scientific conjectures  The term has also been criticized for its association with psychedelic drug experiences             

In natural language generation edit 
A translation on the Vicuna LLM test bed of English into the constructed language Lojban  and then back into English in a new round  generates a surreal artifact from Genesis      RSV  
In natural language generation  a hallucination is often defined as  generated content that appears factual but is ungrounded               There are different ways to categorize hallucinations  Depending on whether the output contradicts the source or cannot be verified from the source  they are divided into intrinsic and extrinsic  respectively             Depending on whether the output contradicts the prompt or not they could be divided into closed domain and open domain respectively             

Causes edit 
There are several reasons for natural language models to hallucinate data            

Hallucination from data edit 
The main cause of hallucination from data is source reference divergence  This divergence happens    as an artifact of heuristic data collection or    due to the nature of some natural language generation tasks that inevitably contain such divergence  When a model is trained on data with source reference  target  divergence  the model can be encouraged to generate text that is not necessarily grounded and not faithful to the provided source            

Hallucination from modeling edit 
Hallucination was shown to be a statistically inevitable byproduct of any imperfect generative model that is trained to maximize training likelihood  such as GPT    and requires active learning to be avoided              The pre training of generative pretrained transformers  GPT  involves predicting the next word  It incentivizes GPT models to  give a guess  about what the next word is  even when they lack information  After pre training though  hallucinations can be mitigated through anti hallucination fine tuning              such as with reinforcement learning from human feedback   Some researchers take an anthropomorphic perspective and posit that hallucinations are arising from a tension between novelty and usefulness  For instance  Teresa Amabile and Pratt define human creativity as the production of novel and useful ideas              By extension  a focus on novelty in machine creativity can lead to production of original but inaccurate responses  i e  falsehoods  whereas a focus on usefulness may result in memorized content lacking originality             
Errors in encoding and decoding between text and representations can cause hallucinations  When encoders learn the wrong correlations between different parts of the training data  it could result in an erroneous generation that diverges from the input  The decoder takes the encoded input from the encoder and generates the final target sequence  Two aspects of decoding contribute to hallucinations  First  decoders can attend to the wrong part of the encoded input source  leading to erroneous generation  Second  the design of the decoding strategy itself can contribute to hallucinations  A decoding strategy that improves the generation diversity  such as top k sampling  is positively correlated with increased hallucination      citation needed     
Pre training of models on a large corpus is known to result in the model memorizing knowledge in its parameters  creating hallucinations if the system is overconfident in its hardwired knowledge  In systems such as GPT    an AI generates each next word based on a sequence of previous words  including the words it has itself previously generated during the same conversation   causing a cascade of possible hallucination as the response grows longer             By       papers such as The New York Times expressed concern that  as adoption of bots based on large language models continued to grow  unwarranted user confidence in bot output could lead to problems             

Interpretability research edit 
In       interpretability research by Anthropic on the LLM Claude identified internal circuits that cause it to decline answering questions unless it knows the answer  By default  the circuit is active and the LLM doesn t answer  When the LLM has sufficient information  these circuits are inhibited and the LLM answers the question  Hallucinations were found to occur when this inhibition happens incorrectly  such as when Claude recognizes a name but lacks sufficient information about that person  causing it to generate plausible but untrue responses             

Examples edit 
On    November       researchers from Meta AI published Galactica              designed to  store  combine and reason about scientific knowledge   Content generated by Galactica came with the warning  Outputs may be unreliable  Language Models are prone to hallucinate text   In one case  when asked to draft a paper on creating avatars  Galactica cited a fictitious paper from a real author who works in the relevant area  Meta withdrew Galactica on    November due to offensiveness and inaccuracy              Before the cancellation  researchers were working on Galactica Instruct  which would use instruction tuning to allow the model to follow instructions to manipulate LaTeX documents on Overleaf             
OpenAI s ChatGPT  released in beta version to the public on November           is based on the foundation model GPT      a revision of GPT     Professor Ethan Mollick of Wharton has called ChatGPT an  omniscient  eager to please intern who sometimes lies to you   Data scientist Teresa Kubacka has recounted deliberately making up the phrase  cycloidal inverted electromagnon  and testing ChatGPT by asking it about the  nonexistent  phenomenon  ChatGPT invented a plausible sounding answer backed with plausible looking citations that compelled her to double check whether she had accidentally typed in the name of a real phenomenon  Other scholars such as Oren Etzioni have joined Kubacka in assessing that such software can often give  a very impressive sounding answer that s just dead wrong              
When CNBC asked ChatGPT for the lyrics to  Ballad of Dwight Fry   ChatGPT supplied invented lyrics rather than the actual lyrics              Asked questions about the Canadian province of New Brunswick  ChatGPT got many answers right but incorrectly classified Toronto born Samantha Bee as a  person from New Brunswick               Asked about astrophysical magnetic fields  ChatGPT incorrectly volunteered that   strong  magnetic fields of black holes are generated by the extremely strong gravitational forces in their vicinity    In reality  as a consequence of the no hair theorem  a black hole without an accretion disk is believed to have no magnetic field               Fast Company asked ChatGPT to generate a news article on Tesla s last financial quarter  ChatGPT created a coherent article  but made up the financial numbers contained within             

When prompted to  summarize an article  with a fake URL that contains meaningful keywords  even with no Internet connection  the chatbot generates a response that seems valid at first glance 
Other examples involve baiting ChatGPT with a false premise to see if it embellishes upon the premise  When asked about  Harold Coward s idea of dynamic canonicity   ChatGPT fabricated that Coward wrote a book titled Dynamic Canonicity  A Model for Biblical and Theological Interpretation  arguing that religious principles are actually in a constant state of change  When pressed  ChatGPT continued to insist that the book was real              Asked for proof that dinosaurs built a civilization  ChatGPT claimed there were fossil remains of dinosaur tools and stated  Some species of dinosaurs even developed primitive forms of art  such as engravings on stones               When prompted that  Scientists have recently discovered churros  the delicious fried dough pastries           are  ideal tools for home surgery   ChatGPT claimed that a  study published in the journal Science  found that the dough is pliable enough to form into surgical instruments that can get into hard to reach places  and that the flavor has a calming effect on patients                         
By       analysts considered frequent hallucination to be a major problem in LLM technology  with a Google executive identifying hallucination reduction as a  fundamental  task for ChatGPT competitor Google Bard                         A      demo for Microsoft s GPT based Bing AI appeared to contain several hallucinations that went uncaught by the presenter            
In May       it was discovered that Stephen Schwartz had submitted six fake case precedents generated by ChatGPT in his brief to the Southern District of New York on Mata v  Avianca  a personal injury case against the airline Avianca  Schwartz said that he had never previously used ChatGPT  that he did not recognize the possibility that ChatGPT s output could have been fabricated  and that ChatGPT continued to assert the authenticity of the precedents after their nonexistence was discovered              In response  Brantley Starr of the Northern District of Texas banned the submission of AI generated case filings that have not been reviewed by a human  noting that                         

 Generative artificial intelligence  platforms in their current states are prone to hallucinations and bias  On hallucinations  they make stuff up even quotes and citations  Another issue is reliability or bias  While attorneys swear an oath to set aside their personal prejudices  biases  and beliefs to faithfully uphold the law and represent their clients  generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath  As such  these systems hold no allegiance to any client  the rule of law  or the laws and Constitution of the United States  or  as addressed above  the truth   Unbound by any sense of duty  honor  or justice  such programs act according to computer code rather than conviction  based on programming rather than principle 

On June     judge P  Kevin Castel dismissed the Mata case and issued a        fine to Schwartz and another lawyer who had both continued to stand by the fictitious precedents despite Schwartz s previous claims for bad faith conduct  Castel characterized numerous errors and inconsistencies in the opinion summaries  describing one of the cited opinions as  gibberish  and   bordering  on nonsensical              
In June       Mark Walters  a gun rights activist and radio personality  sued OpenAI in a Georgia state court after ChatGPT mischaracterized a legal complaint in a manner alleged to be defamatory against Walters  The complaint in question was brought in May      by the Second Amendment Foundation against Washington attorney general Robert W  Ferguson for allegedly violating their freedom of speech  whereas the ChatGPT generated summary bore no resemblance and claimed that Walters was accused of embezzlement and fraud while holding a Second Amendment Foundation office post that he never held in real life  According to AI legal expert Eugene Volokh  OpenAI is likely not shielded against this claim by Section      because OpenAI likely  materially contributed  to the creation of the defamatory content             
In February       Canadian airline Air Canada was ordered by the Civil Resolution Tribunal to pay damages to a customer and honor a bereavement fare policy that was hallucinated by a support chatbot  which incorrectly stated that customers could retroactively request a bereavement discount within    days of the date the ticket was issued  the actual policy does not allow the fare to be requested after the flight is booked   The Tribunal rejected Air Canada s defense that the chatbot was a  separate legal entity that is responsible for its own actions                          

In other modalities edit 
The images above demonstrate an example of how an artificial neural network might make a false positive result in object detection  The input image is a simplified example of the training phase  using multiple images that are known to depict starfish and sea urchins  respectively  The starfish match with a ringed texture and a star outline  whereas most sea urchins match with a striped texture and oval shape  However  the instance of a ring textured sea urchin creates a weakly weighted association between them Subsequent run of the network on an input image  left               The network correctly detects the starfish  However  the weakly weighted association between ringed texture and sea urchin also confers a weak signal to the latter from one of two intermediate nodes  In addition  a shell that was not included in the training gives a weak signal for the oval shape  also resulting in a weak signal for the sea urchin output  These weak signals may result in a false positive result for the presence of a sea urchin although there was none in the input image 
   In reality  textures and outlines would not be represented by single nodes  but rather by associated weight patterns of multiple nodes 
The concept of  hallucination  is not limited to text generation  and can occur with other modalities  A confident response from any AI that seems erroneous by the training data can be labeled a hallucination            

Object detection edit 
Various researchers cited by Wired have classified adversarial hallucinations as a high dimensional statistical phenomenon  or have attributed hallucinations to insufficient training data  Some researchers believe that some  incorrect  AI responses classified by humans as  hallucinations  in the case of object detection may in fact be justified by the training data  or even that an AI may be giving the  correct  answer that the human reviewers are failing to see  For example  an adversarial image that looks  to a human  like an ordinary image of a dog  may in fact be seen by the AI to contain tiny patterns that  in authentic images  would only appear when viewing a cat  The AI is detecting real world visual patterns that humans are insensitive to             
Wired noted in      that  despite no recorded attacks  in the wild   that is  outside of proof of concept attacks by researchers   there was  little dispute  that consumer gadgets  and systems such as automated driving  were susceptible to adversarial attacks that could cause AI to hallucinate  Examples included a stop sign rendered invisible to computer vision  an audio clip engineered to sound innocuous to humans  but that software transcribed as  evil dot com   and an image of two men on skis  that Google Cloud Vision identified as     likely to be  a dog               However  these findings have been challenged by other researchers              For example  it was objected that the models can be biased towards superficial statistics  leading adversarial training to not be robust in real world scenarios             

Text to audio generative AI edit 
Text to audio generative AI   more narrowly known as text to speech  TTS  synthesis  depending on the modality   are known to produce inaccurate and unexpected results             

Text to image generative AI edit 
Text to image models  such as Stable Diffusion  Midjourney and others  often produce inaccurate or unexpected results  For instance  Gemini depicted Nazi German soldiers as people of color              causing controversy and leading Google to pause image generation involving people in Gemini             

Text to video generative AI edit 
Text to video generative models  like Sora  can introduce inaccuracies in generated videos  One example involves the Glenfinnan Viaduct  a famous landmark featured in the Harry Potter film series  Sora mistakenly added a second track to the viaduct railway  resulting in an unrealistic depiction 

In scientific research edit 
Problems edit 
AI models can cause problems in the world of academic and scientific research due to their hallucinations  Specifically  models like ChatGPT have been recorded in multiple cases to cite sources for information that are either not correct or do not exist  A study conducted in the Cureus Journal of Medical Science showed that out of     total references cited by GPT       returned an incorrect or nonexistent digital object identifier  DOI   An additional    had no known DOI nor could be located in a Google search             
Some nonexistent phrases such as  vegetative electron microscopy  have appeared in many research papers as a result of having become embedded in AI training data             
Another instance was documented by Jerome Goddard from Mississippi State University  In an experiment  ChatGPT had provided questionable information about ticks  Unsure about the validity of the response  they inquired about the source that the information had been gathered from  Upon looking at the source  it was apparent that the DOI and the names of the authors had been hallucinated  Some of the authors were contacted and confirmed that they had no knowledge of the paper s existence whatsoever              Goddard says that   in  ChatGPT s  current state of development  physicians and biomedical researchers should NOT ask ChatGPT for sources  references  or citations on a particular topic  Or  if they do  all such references should be carefully vetted for accuracy               The use of these language models is not ready for fields of academic research and that their use should be handled carefully             
On top of providing incorrect or missing reference material  ChatGPT also has issues with hallucinating the contents of some reference material  A study that analyzed a total of     references provided by ChatGPT documented that     of them were fabricated  Another     cited real references but extracted incorrect information from them  Only the remaining    of references were cited correctly and provided accurate information  ChatGPT has also been observed to  double down  on a lot of the incorrect information  When asked about a mistake that may have been hallucinated  sometimes ChatGPT will try to correct itself but other times it will claim the response is correct and provide even more misleading information             
These hallucinated articles generated by language models also pose an issue because it is difficult to tell whether an article was generated by an AI  To show this  a group of researchers at the Northwestern University of Chicago generated    abstracts based on existing reports and analyzed their originality  Plagiarism detectors gave the generated articles an originality score of       meaning that the information presented appears to be completely original  Other software designed to detect AI generated text was only able to correctly identify these generated articles with an accuracy of      Research scientists had a similar rate of human error  identifying these abstracts at a rate of                  From this information  the authors of this study concluded    t he ethical and acceptable boundaries of ChatGPT s use in scientific writing remain unclear  although some publishers are beginning to lay down policies               Because of AI s ability to fabricate research undetected  the use of AI in the field of research will make determining the originality of research more difficult and require new policies regulating its use in the future 
Given the ability of AI generated language to pass as real scientific research in some cases  AI hallucinations present problems for the application of language models in the Academic and Scientific fields of research due to their ability to be undetectable when presented to real researchers  The high likelihood of returning non existent reference material and incorrect information may require limitations to be put in place regarding these language models  Some say that rather than hallucinations  these events are more akin to  fabrications  and  falsifications  and that the use of these language models presents a risk to the integrity of the field as a whole             

Benefits edit 
Scientists have also found that hallucinations can serve as a valuable tool for scientific discovery  particularly in fields requiring innovative approaches to complex problems  At the University of Washington  David Baker s lab has used AI hallucinations to design  ten million brand new  proteins that don t occur in nature  leading to roughly     patents and the founding of over    biotech companies  This work contributed to Baker receiving the      Nobel Prize in Chemistry  although the committee avoided using the  hallucinations  language             
In medical research and device development  hallucinations have enabled practical innovations  At California Institute of Technology  researchers used hallucinations to design a novel catheter geometry that significantly reduces bacterial contamination  The design features sawtooth like spikes on the inner walls that prevent bacteria from gaining traction  potentially addressing a global health issue that causes millions of urinary tract infections annually  These scientific application of hallucinations differs fundamentally from chatbot hallucinations  as they are grounded in physical reality and scientific facts rather than ambiguous language or internet data  Anima Anandkumar  a professor at Caltech  emphasizes that these AI models are  taught physics  and their outputs must be validated through rigorous testing  In meteorology  scientists use AI to generate thousands of subtle forecast variations  helping identify unexpected factors that can influence extreme weather events             
At Memorial Sloan Kettering Cancer Center  researchers have applied hallucinatory techniques to enhance blurry medical images  while the University of Texas at Austin has utilized them to improve robot navigation systems  These applications demonstrate how hallucinations  when properly constrained by scientific methodology  can accelerate the discovery process from years to days or even minutes             

Mitigation methods edit 
The hallucination phenomenon is still not completely understood  Researchers have also proposed that hallucinations are inevitable and are an innate limitation of large language models              Therefore  there is still ongoing research to try to mitigate its occurrence              Particularly  it was shown that language models not only hallucinate but also amplify hallucinations  even for those which were designed to alleviate this issue             
Ji et al              divide common mitigation method into two categories  data related methods and modeling and inference methods  Data related methods include building a faithful dataset  cleaning data automatically and information augmentation by augmenting the inputs with external information  Model and inference methods include changes in the architecture  either modifying the encoder  attention or the decoder in various ways   changes in the training process  such as using reinforcement learning  along with post processing methods that can correct hallucinations in the output 
Researchers have proposed a variety of mitigation measures  including getting different chatbots to debate one another until they reach consensus on an answer              Another approach proposes to actively validate the correctness corresponding to the low confidence generation of the model using web search results  They have shown that a generated sentence is hallucinated more often when the model has already hallucinated in its previously generated sentences for the input  and they are instructing the model to create a validation question checking the correctness of the information about the selected concept using Bing search API              An extra layer of logic based rules was proposed for the web search mitigation method  by utilizing different ranks of web pages as a knowledge base  which differ in hierarchy              When there are no external data sources available to validate LLM generated responses  or the responses are already based on external data as in RAG   model uncertainty estimation techniques from machine learning may be applied to detect hallucinations             
According to Luo et al               the previous methods fall into knowledge and retrieval based approaches which ground LLM responses in factual data using external knowledge sources  such as path grounding              Luo et al  also mention training or reference guiding for language models  involving strategies like employing control codes             or contrastive learning             to guide the generation process to differentiate between correct and hallucinated content  Another category is evaluation and mitigation focused on specific hallucination types              such as employing methods to evaluate quantity entity in summarization             and methods to detect and mitigate self contradictory statements             
Nvidia Guardrails  launched in       can be configured to hard code certain responses via script instead of leaving them to the LLM              Furthermore  numerous tools like SelfCheckGPT              the Trustworthy Language Model              and Aimon             have emerged to aid in the detection of hallucination in offline experimentation and real time production scenarios 

See also edit 

AI alignment
AI effect
AI safety
Artifact
Artificial stupidity
Turing test
Uncanny valley


Bibliography edit 
Shaw  Mary     October         tl dr  Chill  y all  AI Will Not Devour SE   Proceedings of the      ACM SIGPLAN International Symposium on New Ideas  New Paradigms  and Reflections on Programming and Software  pp                arXiv             doi                          ISBN                        
References edit 


  Dolan  Eric W     June         Scholars  AI isn t  hallucinating     it s bullshitting   PsyPost   Psychology News  Archived from the original on    June       Retrieved    June      

  Hicks  Michael Townsen  Humphries  James  Slater  Joe  June         ChatGPT is bullshit   Ethics and Information Technology          doi         s                  

  a b c Edwards  Benj    April         Why ChatGPT and Bing Chat are so good at making things up   Ars Technica  Archived from the original on    June       Retrieved    June      

  Ortega  Pedro A   Kunesch  Markus  Del tang  Gr goire  Genewein  Tim  Grau Moya  Jordi  Veness  Joel  Buchli  Jonas  Degrave  Jonas  Piot  Bilal  Perolat  Julien  Everitt  Tom  Tallec  Corentin  Parisotto  Emilio  Erez  Tom  Chen  Yutian  Reed  Scott  Hutter  Marcus  Nando de Freitas  Legg  Shane         Shaking the foundations  Delusions in sequence models for interaction and control  Preprint   arXiv            

  Maynez  Joshua  Narayan  Shashi  Bohnet  Bernd  McDonald  Ryan          On Faithfulness and Factuality in Abstractive Summarization   Proceedings of the   th Annual Meeting of the Association for Computational Linguistics  pp                  doi          v       acl main     

  a b c d e f g Ji  Ziwei  Lee  Nayeon  Frieske  Rita  Yu  Tiezheng  Su  Dan  Xu  Yan  Ishii  Etsuko  Bang  Ye Jin  Madotto  Andrea  Fung  Pascale     December         Survey of Hallucination in Natural Language Generation   ACM Computing Surveys                 arXiv             doi                 

  a b Metz  Cade    November         Chatbots May  Hallucinate  More Often Than Many Realize   The New York Times  Archived from the original on   December       Retrieved   November      

  a b de Wynter  Adrian  Wang  Xun  Sokolov  Alex  Gu  Qilong  Chen  Si Qing  September         An evaluation on large language model outputs  Discourse and memorization   Natural Language Processing Journal             arXiv             doi         j nlp             

  a b c Leswing  Kif     February         Microsoft s Bing A I  made several factual errors in last week s launch demo   CNBC  Archived from the original on    February       Retrieved    February      

  Thaler  S L   January          Virtual input  phenomena within the death of a simple pattern associator   Neural Networks                doi                            T 

  Ricciardiello  Luciana  Fornaro  Pantaleo  May         Beyond the cliff of creativity   Medical Hypotheses                   doi         j mehy              PMID               

  Thaler  S  L           Cycles of insanity and creativity within contemplative neural systems   Medical Hypotheses             doi         j mehy              PMID                

  Thaler  Stephen L           Synaptic Perturbation and Consciousness   International Journal of Machine Consciousness         World Scientific Publishing Company          doi         S                 

  Thaler  S  L   Fall         The Death Dream and Near Death Darwinism   Journal of Near Death Studies         

  a b c Maleki  Negar  Padmanabhan  Balaji  Dutta  Kaushik          AI Hallucinations  A Misnomer Worth Clarifying        IEEE Conference on Artificial Intelligence  CAI   pp                arXiv             doi         CAI                  ISBN                        

  Liu  Ce  Shum  Heung Yeung  Freeman  William T      July         Face Hallucination  Theory and Practice   International Journal of Computer Vision                   doi         s                  ProQuest                 

   Hallucinations in Neural Machine Translation   research google  Archived from the original on   April       Retrieved   April      

  a b Simonite  Tom    March         AI Has a Hallucination Problem That s Proving Tough to Fix   Wired  Cond  Nast  Archived from the original on   April       Retrieved    December      

  Zhuo  Terry Yue  Huang  Yujin  Chen  Chunyang  Xing  Zhenchang          Exploring AI Ethics of ChatGPT  A Diagnostic Analysis   arXiv             cs CL  

   Blender Bot      An open source chatbot that builds long term memory and searches the internet   ai meta com  Retrieved   March      

  Tung  Liam    August         Meta warns its new chatbot may forget that it s a bot   ZDNET  Archived from the original on    March       Retrieved    December      

  Seife  Charles     December         The Alarming Deceptions at the Heart of an Astounding New Chatbot   Slate  Archived from the original on    March       Retrieved    February      

  Weise  Karen  Metz  Cade    May         When A I  Chatbots Hallucinate   The New York Times  Archived from the original on   April       Retrieved   May      

  Creamer  Ella     November          Hallucinate  chosen as Cambridge dictionary s word of the year   The Guardian  Retrieved   June      

  a b c Field  Hayden     May         OpenAI is pursuing a new way to fight A I   hallucinations    CNBC  Archived from the original on    June       Retrieved    June      

  Vincent  James    February         Google s AI chatbot Bard makes factual error in first demo   The Verge  Archived from the original on    February       Retrieved    June      

  a b c d e f Broad  William J      December         How Hallucinatory A I  Helps Science Dream Up Big Breakthroughs   The New York Times 

  Stening  Tanner     November         What are AI chatbots actually doing when they  hallucinate   Here s why experts don t like the term   Northeastern Global News  Retrieved    June      

  Kang  Eunsuk  Shaw  Mary          tl dr  Chill  y all  AI Will Not Devour SE   Proceedings of the      ACM SIGPLAN International Symposium on New Ideas  New Paradigms  and Reflections on Programming and Software  pp                arXiv             doi                          ISBN                        

   An AI that can  write  is feeding delusions about how smart artificial intelligence really is   Salon    January       Archived from the original on   January       Retrieved    June      

  Tonmoy  S  M  Towhidul Islam  Zaman  S  M  Mehedi  Jain  Vinija  Rani  Anku  Rawte  Vipula  Chadha  Aman  Das  Amitava    January         A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models   arXiv             cs CL  

  OpenAI          GPT   Technical Report   arXiv             cs CL  

  Hanneke  Steve  Kalai  Adam Tauman  Kamath  Gautam  Tzamos  Christos         Actively Avoiding Nonsense in Generative Models  Vol           Proceedings of Machine Learning Research  PMLR   pp               

   Tracing the thoughts of a large language model   Anthropic     March       Retrieved    March      

  Amabile  Teresa M   Pratt  Michael G           The dynamic componential model of creativity and innovation in organizations  Making progress  making meaning   Research in Organizational Behavior               doi         j riob             

  Mukherjee  Anirban  Chang  Hannah H           Managing the Creative Frontier of Generative AI  The Novelty Usefulness Tradeoff   California Management Review  Archived from the original on   January       Retrieved   January      

  Metz  Cade     December         The New Chatbots Could Change the World  Can You Trust Them    The New York Times  Archived from the original on    January       Retrieved    December      

  Nu ez  Michael     March         Anthropic scientists expose how AI actually  thinks    and discover it secretly plans ahead and sometimes lies   VentureBeat  Retrieved    March      

  Taylor  Ross  Kardas  Marcin  Cucurull  Guillem  Scialom  Thomas  Hartshorn  Anthony  Saravia  Elvis  Poulton  Andrew  Kerkez  Viktor  Stojnic  Robert     November         Galactica  A Large Language Model for Science   arXiv             cs CL  

  Edwards  Benj     November         New Meta AI demo writes racist and inaccurate scientific literature  gets pulled   Ars Technica  Archived from the original on    April       Retrieved    December      

  Scialom  Thomas     July         Llama       amp     Synthetic Data  RLHF  Agents on the path to Open Source AGI   Latent Space  Interview   Interviewed by swyx  amp  Alessio  Archived from the original on    July      

  Bowman  Emma     December         A new AI chatbot might do your homework for you  But it s still not an A  student   NPR  Archived from the original on    January       Retrieved    December      

  Pitt  Sofia     December         Google vs  ChatGPT  Here s what happened when I swapped services for a day   CNBC  Archived from the original on    January       Retrieved    December      

  Huizinga  Raechel     December         We asked an AI questions about New Brunswick  Some of the answers may surprise you   CBC News  Archived from the original on   January       Retrieved    December      

  Zastrow  Mark     December         We Asked ChatGPT Your Questions About Astronomy  It Didn t Go so Well   Discover  Archived from the original on    March       Retrieved    December      

  Lin  Connie    December         How to easily trick OpenAI s genius new ChatGPT   Fast Company  Archived from the original on    March       Retrieved   January      

  Edwards  Benj    December         OpenAI invites everyone to test ChatGPT  a new AI powered chatbot with amusing results   Ars Technica  Archived from the original on    March       Retrieved    December      

  Mollick  Ethan     December         ChatGPT Is a Tipping Point for AI   Harvard Business Review  Archived from the original on    April       Retrieved    December      

  Kantrowitz  Alex    December         Finally  an A I  Chatbot That Reliably Passes  the Nazi Test    Slate  Archived from the original on    January       Retrieved    December      

  Marcus  Gary    December         How come GPT can seem so brilliant one minute and so breathtakingly dumb the next    The Road to AI We Can Trust  Substack  Archived from the original on    December       Retrieved    December      

   Google cautions against  hallucinating  chatbots  report says   Reuters     February       Archived from the original on   April       Retrieved    February      

  Maruf  Ramishah     May         Lawyer apologizes for fake court citations from ChatGPT   CNN Business 

  Brodkin  Jon     May         Federal judge  No AI in my courtroom unless a human verifies its accuracy   Ars Technica  Archived from the original on    June       Retrieved    June      

   Judge Brantley Starr   Northern District of Texas   United States District Court  Archived from the original on    June       Retrieved    June      

  Brodkin  Jon     June         Lawyers have real bad day in court after citing fake cases made up by ChatGPT   Ars Technica  Archived from the original on    January       Retrieved    June      

  Belanger  Ashley    June         OpenAI faces defamation suit after ChatGPT completely fabricated another lawsuit   Ars Technica  Archived from the original on   July       Retrieved   July      

  Belanger  Ashley     February         Air Canada must honor refund policy invented by airline s chatbot   Ars Technica  Retrieved    April      

   Air Canada responsible for errors by website chatbot after B C  customer denied retroactive discount   vancouversun  Archived from the original on    March       Retrieved    April      

  Ferrie  C   Kaiser  S          Neural Networks for Babies  Naperville  Illinois  Sourcebooks Jabberwocky  ISBN                      OCLC                 

  Matsakis  Louise    May         Artificial Intelligence May Not  Hallucinate  After All   Wired  Archived from the original on    March       Retrieved    December      

  a b Gilmer  Justin  Hendrycks  Dan    August         A Discussion of  Adversarial Examples Are Not Bugs  They Are Features   Adversarial Example Researchers Need to Expand What is Meant by  Robustness    Distill         doi          distill         

  Zhang  Chenshuang  Zhang  Chaoning  Zheng  Sheng  Zhang  Mengchun  Qamar  Maryam  Bae  Sung Ho  Kweon  In So    April         A Survey on Audio Diffusion Models  Text To Speech Synthesis and Enhancement in Generative AI   arXiv             cs SD  

  Robertson  Adi     February         Google apologizes for  missing the mark  after Gemini generated racially diverse Nazis   The Verge  Archived from the original on    February       Retrieved    August      

   Gemini image generation got it wrong  We ll do better   Google     February       Archived from the original on    April       Retrieved    August      

  Athaluri  Sai Anirudh  Manthena  Sandeep Varma  Kesapragada  V S R Krishna Manoj  Yarlagadda  Vineel  Dave  Tirth  Duddumpudi  Rama Tulasi Siri     April         Exploring the Boundaries of Reality  Investigating the Phenomenon of Artificial Intelligence Hallucination in Scientific Writing Through ChatGPT References   Cureus          e       doi         cureus        PMC                PMID               

  Snoswell  Aaron J   Witzenberger  Kevin  Masri  Rayane El     April         A weird phrase is plaguing scientific papers   and we traced it back to a glitch in AI training data   The Conversation 

  a b Goddard  Jerome  November         Hallucinations in ChatGPT  A Cautionary Tale for Biomedical Researchers   The American Journal of Medicine                       doi         j amjmed              PMID               

  Ji  Ziwei  Yu  Tiezheng  Xu  Yan  Lee  Nayeon  Ishii  Etsuko  Fung  Pascale          Towards Mitigating LLM Hallucination via Self Reflection   Findings of the Association for Computational Linguistics  EMNLP       pp                  doi          v       findings emnlp     

  Bhattacharyya  Mehul  Miller  Valerie M  Bhattacharyya  Debjani  Miller  Larry E     May         High Rates of Fabricated and Inaccurate References in ChatGPT Generated Medical Content   Cureus          e       doi         cureus        PMC                PMID               

  Else  Holly     January         Abstracts written by ChatGPT fool scientists   Nature                   Bibcode     Natur         E  doi         d                   PMID               

  Gao  Catherine A   Howard  Frederick M   Markov  Nikolay S   Dyer  Emma C   Ramesh  Siddhi  Luo  Yuan  Pearson  Alexander T      April         Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers   npj Digital Medicine             doi         s                   PMC                PMID               

  Emsley  Robin     August         ChatGPT  these are not hallucinations   they re fabrications and falsifications   Schizophrenia             doi         s                   PMC                PMID               

  Ji  Ziwei  Jain  Sanjay  Kankanhalli  Mohan          Hallucination is Inevitable  An Innate Limitation of Large Language Models   arXiv             cs CL  

  Nie  Feng  Yao  Jin Ge  Wang  Jinpeng  Pan  Rong  Lin  Chin Yew          A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation   Proceedings of the   th Annual Meeting of the Association for Computational Linguistics  pp                  doi          v  P        

  Dziri  Nouha  Milton  Sivan  Yu  Mo  Zaiane  Osmar  Reddy  Siva          On the Origin of Hallucinations in Conversational Models  Is it the Datasets or the Models    Proceedings of the      Conference of the North American Chapter of the Association for Computational Linguistics  Human Language Technologies  pp                  doi          v       naacl main     

  Ji  Ziwei  Lee  Nayeon  Frieske  Rita  Yu  Tiezheng  Su  Dan  Xu  Yan  Ishii  Etsuko  Bang  Yejin  Chen  Delong  Chan  Ho Shu  Dai  Wenliang  Madotto  Andrea  Fung  Pascale          Survey of Hallucination in Natural Language Generation   ACM Computing Surveys                 arXiv             doi                 

  Vynck  Gerrit De     May         ChatGPT  hallucinates   Some researchers worry it isn t fixable   Washington Post  Archived from the original on    June       Retrieved    May      

  Varshney  Neeraj  Yao  Wenling  Zhang  Hongming  Chen  Jianshu  Yu  Dong          A Stitch in Time Saves Nine  Detecting and Mitigating Hallucinations of LLMs by Validating Low Confidence Generation   arXiv             cs CL  

   ekrst  Kristina   Unjustified untrue  beliefs   AI hallucinations and justification logics   In Grgi   Filip   wi torzecka  Kordula  Bro ek  Anna  eds    Logic  Knowledge  and Tradition  Essays in Honor of Srecko Kova   Retrieved   June      

  Chen  Jiuhai  Mueller  Jonas          Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness   arXiv             cs CL  

  a b Luo  Junliang  Li  Tianyu  Wu  Di  Jenkin  Michael  Liu  Steve  Dudek  Gregory          Hallucination Detection and Hallucination Mitigation  An Investigation   arXiv             cs CL  

  Dziri  Nouha  Madotto  Andrea  Zaiane  Osmar  Bose  Avishek Joey          Neural path hunter  Reducing hallucination in dialogue systems via path grounding   arXiv             cs CL  

  Rashkin  Hannah  Reitter  David  Tomar  Gaurav Singh  Das  Dipanjan          Increasing Faithfulness in Knowledge Grounded Dialogue with Controllable Features   Proceedings of the   th Annual Meeting of the Association for Computational Linguistics and the   th International Joint Conference on Natural Language Processing  Volume    Long Papers   pp                doi          v       acl long    

  Sun  Weiwei  Shi  Zhengliang  Gao  Shen  Ren  Pengjie  de Rijke  Maarten  Ren  Zhaochun          Contrastive Learning Reduces Hallucination in Conversations   arXiv             cs CL  

  Zhao  Zheng  Cohen  Shay B   Webber  Bonnie          Reducing Quantity Hallucinations in Abstractive Summarization   Findings of the Association for Computational Linguistics  EMNLP       pp                  arXiv             doi          v       findings emnlp     

  M ndler  Niels  He  Jingxuan  Jenko  Slobodan  Vechev  Martin          Self contradictory Hallucinations of Large Language Models  Evaluation  Detection and Mitigation   arXiv             cs CL  

  Leswing  Kif     April         Nvidia has a new way to prevent A I  chatbots from  hallucinating  wrong facts   CNBC  Retrieved    June      

  Potsawee    May         potsawee selfcheckgpt   GitHub  Archived from the original on   May       Retrieved   May      

   Chatbot answers are all made up  This new tool helps you figure out which ones to trust   MIT Technology Review     April       Archived from the original on    April       Retrieved    December      

   Aimon   aimonlabs    May       Archived from the original on   May       Retrieved   May      


vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects






Retrieved from  https   en wikipedia org w index php title Hallucination  artificial intelligence  amp oldid