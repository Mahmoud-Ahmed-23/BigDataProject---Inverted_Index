Representation learning method
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vteSparse dictionary learning  also known as sparse coding or SDL  is a representation learning method which aims to find a sparse representation of the input data in the form of a linear combination of basic elements as well as those basic elements themselves  These elements are called atoms  and they compose a dictionary  Atoms in the dictionary are not required to be orthogonal  and they may be an over complete spanning set  This problem setup also allows the dimensionality of the signals being represented to be higher than any one of the signals being observed  These two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal  but also provide an improvement in sparsity and flexibility of the representation 
One of the most important applications of sparse dictionary learning is in the field of compressed sensing or signal recovery  In compressed sensing  a high dimensional signal can be recovered with only a few linear measurements  provided that the signal is sparse or near sparse  Since not all signals satisfy this condition  it is crucial to find a sparse representation of that signal such as the wavelet transform or the directional gradient of a rasterized matrix  Once a matrix or a high dimensional vector is transferred to a sparse space  different recovery algorithms like basis pursuit  CoSaMP             or fast non iterative algorithms            can be used to recover the signal 
One of the key principles of dictionary learning is that the dictionary has to be inferred from the input data  The emergence of sparse dictionary learning methods was stimulated by the fact that in signal processing  one typically wants to represent the input data using a minimal amount of components  Before this approach  the general practice was to use predefined dictionaries such as Fourier or wavelet transforms  However  in certain cases  a dictionary that is trained to fit the input data can significantly improve the sparsity  which has applications in data decomposition  compression  and analysis  and has been used in the fields of image denoising and classification  and video and audio processing  Sparsity and overcomplete dictionaries have immense applications in image compression  image fusion  and inpainting   

Image Denoising by Dictionary Learning

Problem statement edit 
Given the input dataset 
  
    
      
        X
         
         
        
          x
          
             
          
        
         
         
         
         
         
        
          x
          
            K
          
        
         
         
        
          x
          
            i
          
        
          x     
        
          
            R
          
          
            d
          
        
      
    
      displaystyle X  x         x  K   x  i  in  mathbb  R    d  
  
 we wish to find a dictionary 
  
    
      
        
          D
        
          x     
        
          
            R
          
          
            d
              xd  
            n
          
        
         
        D
         
         
        
          d
          
             
          
        
         
         
         
         
         
        
          d
          
            n
          
        
         
      
    
      displaystyle  mathbf  D   in  mathbb  R    d times n  D  d         d  n   
  
 and a representation 
  
    
      
        R
         
         
        
          r
          
             
          
        
         
         
         
         
         
        
          r
          
            K
          
        
         
         
        
          r
          
            i
          
        
          x     
        
          
            R
          
          
            n
          
        
      
    
      displaystyle R  r         r  K   r  i  in  mathbb  R    n  
  
 such that both 
  
    
      
          x     
        X
          x     
        
          D
        
        R
        
            x     
          
            F
          
          
             
          
        
      
    
      displaystyle   X  mathbf  D  R    F      
  
 is minimized and the representations 
  
    
      
        
          r
          
            i
          
        
      
    
      displaystyle r  i  
  
 are sparse enough  This can be formulated as the following optimization problem 

  
    
      
        
          
            argmin
            
              
                D
              
                x     
              
                
                  C
                
              
               
              
                r
                
                  i
                
              
                x     
              
                
                  R
                
                
                  n
                
              
            
          
        
        
            x     
          
            i
             
             
          
          
            K
          
        
          x     
        
          x
          
            i
          
        
          x     
        
          D
        
        
          r
          
            i
          
        
        
            x     
          
             
          
          
             
          
        
         
          x bb 
          x     
        
          r
          
            i
          
        
        
            x     
          
             
          
        
      
    
      displaystyle   underset   mathbf  D   in   mathcal  C   r  i  in  mathbb  R    n    text argmin    sum   i     K   x  i   mathbf  D  r  i             lambda   r  i        
  
  where 
  
    
      
        
          
            C
          
        
          x     
         
        
          D
        
          x     
        
          
            R
          
          
            d
              xd  
            n
          
        
         
          x     
        
          d
          
            i
          
        
        
            x     
          
             
          
        
          x     
         
        
        
          x     
        i
         
         
         
         
         
         
         
        n
         
      
    
      displaystyle   mathcal  C   equiv    mathbf  D   in  mathbb  R    d times n    d  i        leq       forall i       n   
  
  
  
    
      
          x bb 
         gt 
         
      
    
      displaystyle  lambda  gt   
  


  
    
      
        
          
            C
          
        
      
    
      displaystyle   mathcal  C   
  
 is required to constrain 
  
    
      
        
          D
        
      
    
      displaystyle  mathbf  D   
  
 so that its atoms would not reach arbitrarily high values allowing for arbitrarily low  but non zero  values of 
  
    
      
        
          r
          
            i
          
        
      
    
      displaystyle r  i  
  
  
  
    
      
          x bb 
      
    
      displaystyle  lambda  
  
 controls the trade off between the sparsity and the minimization error 
The minimization problem above is not convex because of the     norm  and solving this problem is NP hard             In some cases L  norm is known to ensure sparsity            and so the above becomes a convex optimization problem with respect to each of the variables 
  
    
      
        
          D
        
      
    
      displaystyle  mathbf  D   
  
 and 
  
    
      
        
          R
        
      
    
      displaystyle  mathbf  R   
  
 when the other one is fixed  but it is not jointly convex in 
  
    
      
         
        
          D
        
         
        
          R
        
         
      
    
      displaystyle   mathbf  D    mathbf  R    
  
 

Properties of the dictionary edit 
The dictionary 
  
    
      
        
          D
        
      
    
      displaystyle  mathbf  D   
  
 defined above can be  undercomplete  if 
  
    
      
        n
         lt 
        d
      
    
      displaystyle n lt d 
  
 or  overcomplete  in case 
  
    
      
        n
         gt 
        d
      
    
      displaystyle n gt d 
  
 with the latter being a typical assumption for a sparse dictionary learning problem  The case of a complete dictionary does not provide any improvement from a representational point of view and thus isn t considered 
Undercomplete dictionaries represent the setup in which the actual input data lies in a lower dimensional space  This case is strongly related to dimensionality reduction and techniques like principal component analysis which require atoms 
  
    
      
        
          d
          
             
          
        
         
         
         
         
         
        
          d
          
            n
          
        
      
    
      displaystyle d         d  n  
  
 to be orthogonal    The choice of these subspaces is crucial for efficient dimensionality reduction  but it is not trivial   And dimensionality reduction based on dictionary representation can be extended to address specific tasks such as data analysis or classification  However  their main downside is limiting the choice of atoms 
Overcomplete dictionaries  however  do not require the atoms to be orthogonal  they will never have a basis anyway  thus allowing for more flexible dictionaries and richer data representations 
An overcomplete dictionary which allows for sparse representation of signal can be a famous transform matrix  wavelets transform  fourier transform  or it can be formulated so that its elements are changed in such a way that it sparsely represents the given signal in a best way  Learned dictionaries are capable of giving sparser solutions as compared to predefined transform matrices 

Algorithms edit 
As the optimization problem described above can be solved as a convex problem with respect to either dictionary or sparse coding while the other one of the two is fixed  most of the algorithms are based on the idea of iteratively updating one and then the other 
The problem of finding an optimal sparse coding 
  
    
      
        R
      
    
      displaystyle R 
  
 with a given dictionary 
  
    
      
        
          D
        
      
    
      displaystyle  mathbf  D   
  
 is known as sparse approximation  or sometimes just sparse coding problem   A number of algorithms have been developed to solve it  such as matching pursuit and LASSO  and are incorporated in the algorithms described below 

Method of optimal directions  MOD  edit 
The method of optimal directions  or MOD  was one of the first methods introduced to tackle the sparse dictionary learning problem             The core idea of it is to solve the minimization problem subject to the limited number of non zero components of the representation vector 

  
    
      
        
          min
          
            
              D
            
             
            R
          
        
         
          x     
        X
          x     
        
          D
        
        R
        
            x     
          
            F
          
          
             
          
        
         
        
        
        
          s t 
        
        
        
          x     
        i
        
        
          x     
        
          r
          
            i
          
        
        
            x     
          
             
          
        
          x     
        T
      
    
      displaystyle  min    mathbf  D   R     X  mathbf  D  R    F             text s t        forall i      r  i        leq T 
  

Here  
  
    
      
        F
      
    
      displaystyle F 
  
 denotes the Frobenius norm  MOD alternates between getting the sparse coding using a method such as matching pursuit and updating the dictionary by computing the analytical solution of the problem given by 
  
    
      
        
          D
        
         
        X
        
          R
          
             
          
        
      
    
      displaystyle  mathbf  D   XR     
  
 where 
  
    
      
        
          R
          
             
          
        
      
    
      displaystyle R     
  
 is a Moore Penrose pseudoinverse  After this update 
  
    
      
        
          D
        
      
    
      displaystyle  mathbf  D   
  
 is renormalized to fit the constraints and the new sparse coding is obtained again  The process is repeated until convergence  or until a sufficiently small residue  
MOD has proved to be a very efficient method for low dimensional input data 
  
    
      
        X
      
    
      displaystyle X 
  
 requiring just a few iterations to converge  However  due to the high complexity of the matrix inversion operation  computing the pseudoinverse in high dimensional cases is in many cases intractable  This shortcoming has inspired the development of other dictionary learning methods 

K SVD edit 
Main article  K SVDK SVD is an algorithm that performs SVD at its core to update the atoms of the dictionary one by one and basically is a generalization of K means  It enforces that each element of the input data 
  
    
      
        
          x
          
            i
          
        
      
    
      displaystyle x  i  
  
 is encoded by a linear combination of not more than 
  
    
      
        
          T
          
             
          
        
      
    
      displaystyle T     
  
 elements in a way identical to the MOD approach 

  
    
      
        
          min
          
            
              D
            
             
            R
          
        
         
          x     
        X
          x     
        
          D
        
        R
        
            x     
          
            F
          
          
             
          
        
         
        
        
        
          s t 
        
        
        
          x     
        i
        
        
          x     
        
          r
          
            i
          
        
        
            x     
          
             
          
        
          x     
        
          T
          
             
          
        
      
    
      displaystyle  min    mathbf  D   R     X  mathbf  D  R    F             text s t        forall i      r  i        leq T     
  

This algorithm s essence is to first fix the dictionary  find the best possible 
  
    
      
        R
      
    
      displaystyle R 
  
 under the above constraint  using Orthogonal Matching Pursuit  and then iteratively update the atoms of dictionary 
  
    
      
        
          D
        
      
    
      displaystyle  mathbf  D   
  
 in the following manner 

  
    
      
          x     
        X
          x     
        
          D
        
        R
        
            x     
          
            F
          
          
             
          
        
         
        
          
             
            
              X
                x     
              
                  x     
                
                  i
                   
                   
                
                
                  K
                
              
              
                d
                
                  i
                
              
              
                x
                
                  T
                
                
                  i
                
              
            
             
          
          
            F
          
          
             
          
        
         
          x     
        
          E
          
            k
          
        
          x     
        
          d
          
            k
          
        
        
          x
          
            T
          
          
            k
          
        
        
            x     
          
            F
          
          
             
          
        
      
    
      displaystyle   X  mathbf  D  R    F       left X  sum   i     K d  i x  T   i  right   F        E  k  d  k x  T   k     F      
  

The next steps of the algorithm include rank   approximation of the residual matrix 
  
    
      
        
          E
          
            k
          
        
      
    
      displaystyle E  k  
  
  updating 
  
    
      
        
          d
          
            k
          
        
      
    
      displaystyle d  k  
  
 and enforcing the sparsity of 
  
    
      
        
          x
          
            k
          
        
      
    
      displaystyle x  k  
  
 after the update  This algorithm is considered to be standard for dictionary learning and is used in a variety of applications  However  it shares weaknesses with MOD being efficient only for signals with relatively low dimensionality and having the possibility for being stuck at local minima 

Stochastic gradient descent edit 
Main article  Stochastic gradient descentOne can also apply a widespread stochastic gradient descent method with iterative projection to solve this problem             The idea of this method is to update the dictionary using the first order stochastic gradient and project it on the constraint set 
  
    
      
        
          
            C
          
        
      
    
      displaystyle   mathcal  C   
  
  The step that occurs at i th iteration is described by this expression 

  
    
      
        
          
            D
          
          
            i
          
        
         
        
          
            proj
          
          
            
              C
            
          
        
        
           
          
            
              
                D
              
              
                i
                  x     
                 
              
            
              x     
            
                x b  
              
                i
              
            
            
                x     
              
                
                  D
                
              
            
            
                x     
              
                i
                  x     
                S
              
            
              x     
            
              x
              
                i
              
            
              x     
            
              D
            
            
              r
              
                i
              
            
            
                x     
              
                 
              
              
                 
              
            
             
              x bb 
              x     
            
              r
              
                i
              
            
            
                x     
              
                 
              
            
          
           
        
      
    
      displaystyle  mathbf  D    i    text proj     mathcal  C   left   mathbf  D    i     delta   i  nabla    mathbf  D    sum   i in S   x  i   mathbf  D  r  i             lambda   r  i        right   
  
  where 
  
    
      
        S
      
    
      displaystyle S 
  
 is a random subset of 
  
    
      
         
            
        K
         
      
    
      displaystyle       K   
  
 and 
  
    
      
        
            x b  
          
            i
          
        
      
    
      displaystyle  delta   i  
  
 is a gradient step 

Lagrange dual method edit 
An algorithm based on solving a dual Lagrangian problem provides an efficient way to solve for the dictionary having no complications induced by the sparsity function             Consider the following Lagrangian 

  
    
      
        
          
            L
          
        
         
        
          D
        
         
          x  b 
         
         
        
          tr
        
        
           
          
             
            X
              x     
            
              D
            
            R
            
               
              
                T
              
            
             
            X
              x     
            
              D
            
            R
             
          
           
        
         
        
            x     
          
            j
             
             
          
          
            n
          
        
        
            x bb 
          
            j
          
        
        
           
          
            
                x     
              
                i
                 
                 
              
              
                d
              
            
            
              
                D
              
              
                i
                j
              
              
                 
              
            
              x     
            c
          
           
        
      
    
      displaystyle   mathcal  L    mathbf  D    Lambda     text tr   left  X  mathbf  D  R   T  X  mathbf  D  R  right   sum   j     n  lambda   j  left   sum   i     d  mathbf  D    ij      c  right  
  
  where 
  
    
      
        c
      
    
      displaystyle c 
  
 is a constraint on the norm of the atoms and 
  
    
      
        
            x bb 
          
            i
          
        
      
    
      displaystyle  lambda   i  
  
 are the so called dual variables forming the diagonal matrix 
  
    
      
          x  b 
      
    
      displaystyle  Lambda  
  
 
We can then provide an analytical expression for the Lagrange dual after minimization over 
  
    
      
        
          D
        
      
    
      displaystyle  mathbf  D   
  
 

  
    
      
        
          
            D
          
        
         
          x  b 
         
         
        
          min
          
            
              D
            
          
        
        
          
            L
          
        
         
        
          D
        
         
          x  b 
         
         
        
          tr
        
         
        
          X
          
            T
          
        
        X
          x     
        X
        
          R
          
            T
          
        
         
        R
        
          R
          
            T
          
        
         
          x  b 
        
           
          
              x     
             
          
        
         
        X
        
          R
          
            T
          
        
        
           
          
            T
          
        
          x     
        c
          x  b 
         
      
    
      displaystyle   mathcal  D    Lambda    min    mathbf  D     mathcal  L    mathbf  D    Lambda     text tr   X  T X XR  T  RR  T   Lambda        XR  T    T  c Lambda   
  
 
After applying one of the optimization methods to the value of the dual  such as Newton s method or conjugate gradient  we get the value of 
  
    
      
        
          D
        
      
    
      displaystyle  mathbf  D   
  
 

  
    
      
        
          
            D
          
          
            T
          
        
         
         
        R
        
          R
          
            T
          
        
         
          x  b 
        
           
          
              x     
             
          
        
         
        X
        
          R
          
            T
          
        
        
           
          
            T
          
        
      
    
      displaystyle  mathbf  D    T   RR  T   Lambda        XR  T    T  
  

Solving this problem is less computational hard because the amount of dual variables 
  
    
      
        n
      
    
      displaystyle n 
  
 is a lot of times much less than the amount of variables in the primal problem 

LASSO edit 
Main article  Lasso  statistics 
In this approach  the optimization problem is formulated as 

  
    
      
        
          min
          
            r
              x     
            
              
                R
              
              
                n
              
            
          
        
         
        
        
          x     
        r
        
            x     
          
             
          
        
         
        
        
        
          subject to
        
        
        
          x     
        X
          x     
        
          D
        
        R
        
            x     
          
            F
          
          
             
          
        
         lt 
          x f  
      
    
      displaystyle  min   r in  mathbb  R    n          r              text subject to        X  mathbf  D  R    F      lt  epsilon  
  
  where 
  
    
      
          x f  
      
    
      displaystyle  epsilon  
  
 is the permitted error in the reconstruction LASSO 
It finds an estimate of 
  
    
      
        
          r
          
            i
          
        
      
    
      displaystyle r  i  
  
 by minimizing the least square error subject to a L  norm constraint in the solution vector  formulated as 

  
    
      
        
          min
          
            r
              x     
            
              
                R
              
              
                n
              
            
          
        
        
        
        
          
            
               
               
            
          
        
        
        
          x     
        X
          x     
        
          D
        
        r
        
            x     
          
            F
          
          
             
          
        
         
          x bb 
        
        
          x     
        r
        
            x     
          
             
          
        
      
    
      displaystyle  min   r in  mathbb  R    n        dfrac              X  mathbf  D  r    F       lambda       r       
  
  where 
  
    
      
          x bb 
         gt 
         
      
    
      displaystyle  lambda  gt   
  
 controls the trade off between sparsity and the reconstruction error  This gives the global optimal solution             See also Online dictionary learning for Sparse coding

Parametric training methods edit 
Parametric training methods are aimed to incorporate the best of both worlds   the realm of analytically constructed dictionaries and the learned ones             This allows to construct more powerful generalized dictionaries that can potentially be applied to the cases of arbitrary sized signals  Notable approaches include  

Translation invariant dictionaries              These dictionaries are composed by the translations of the atoms originating from the dictionary constructed for a finite size signal patch  This allows the resulting dictionary to provide a representation for the arbitrary sized signal 
Multiscale dictionaries              This method focuses on constructing a dictionary that is composed of differently scaled dictionaries to improve sparsity 
Sparse dictionaries              This method focuses on not only providing a sparse representation but also constructing a sparse dictionary which is enforced by the expression 
  
    
      
        
          D
        
         
        
          B
        
        
          A
        
      
    
      displaystyle  mathbf  D    mathbf  B   mathbf  A   
  
 where 
  
    
      
        
          B
        
      
    
      displaystyle  mathbf  B   
  
 is some pre defined analytical dictionary with desirable properties such as fast computation and 
  
    
      
        
          A
        
      
    
      displaystyle  mathbf  A   
  
 is a sparse matrix  Such formulation allows to directly combine the fast implementation of analytical dictionaries with the flexibility of sparse approaches 
Online dictionary learning  LASSO approach  edit 
Many common approaches to sparse dictionary learning rely on the fact that the whole input data 
  
    
      
        X
      
    
      displaystyle X 
  
  or at least a large enough training dataset  is available for the algorithm  However  this might not be the case in the real world scenario as the size of the input data might be too big to fit it into memory  The other case where this assumption can not be made is when the input data comes in a form of a stream  Such cases lie in the field of study of online learning which essentially suggests iteratively updating the model upon the new data points 
  
    
      
        x
      
    
      displaystyle x 
  
 becoming available 
A dictionary can be learned in an online manner the following way             

For 
  
    
      
        t
         
            
        T
         
      
    
      displaystyle t     T  
  

Draw a new sample 
  
    
      
        
          x
          
            t
          
        
      
    
      displaystyle x  t  
  

Find a sparse coding using LARS  
  
    
      
        
          r
          
            t
          
        
         
        
          
            argmin
            
              r
                x     
              
                
                  R
                
                
                  n
                
              
            
          
        
        
           
          
            
              
                 
                 
              
            
              x     
            
              x
              
                t
              
            
              x     
            
              
                D
              
              
                t
                  x     
                 
              
            
            r
              x     
             
              x bb 
              x     
            r
            
                x     
              
                 
              
            
          
           
        
      
    
      displaystyle r  t    underset  r in  mathbb  R    n    text argmin    left   frac          x  t   mathbf  D    t   r    lambda   r       right  
  

Update dictionary using block coordinate approach  
  
    
      
        
          
            D
          
          
            t
          
        
         
        
          
            argmin
            
              
                D
              
                x     
              
                
                  C
                
              
            
          
        
        
          
             
            t
          
        
        
            x     
          
            i
             
             
          
          
            t
          
        
        
           
          
            
              
                 
                 
              
            
              x     
            
              x
              
                i
              
            
              x     
            
              D
            
            
              r
              
                i
              
            
            
                x     
              
                 
              
              
                 
              
            
             
              x bb 
              x     
            
              r
              
                i
              
            
            
                x     
              
                 
              
            
          
           
        
      
    
      displaystyle  mathbf  D    t    underset   mathbf  D   in   mathcal  C     text argmin     frac     t   sum   i     t  left   frac          x  i   mathbf  D  r  i             lambda   r  i        right  
  

This method allows us to gradually update the dictionary as new data becomes available for sparse representation learning and helps drastically reduce the amount of memory needed to store the dataset  which often has a huge size  

Applications edit 
The dictionary learning framework  namely the linear decomposition of an input signal using a few basis elements learned from data itself  has led to state of art     citation needed      results in various image and video processing tasks  This technique can be applied to classification problems in a way that if we have built specific dictionaries for each class  the input signal can be classified by finding the dictionary corresponding to the sparsest representation 
It also has properties that are useful for signal denoising since usually one can learn a dictionary to represent the meaningful part of the input signal in a sparse way but the noise in the input will have a much less sparse representation             
Sparse dictionary learning has been successfully applied to various image  video and audio processing tasks as well as to texture synthesis             and unsupervised clustering              In evaluations with the Bag of Words model                          sparse coding was found empirically to outperform other coding approaches on the object category recognition tasks 
Dictionary learning is used to analyse medical signals in detail  Such medical signals include those from electroencephalography  EEG   electrocardiography  ECG   magnetic resonance imaging  MRI   functional MRI  fMRI   continuous glucose monitors              and ultrasound computer tomography  USCT   where different assumptions are used to analyze each signal 

See also edit 
Sparse approximation
Sparse PCA
K SVD
Matrix factorization
Neural sparse coding
References edit 

  Needell  D   Tropp  J A           CoSaMP  Iterative signal recovery from incomplete and inaccurate samples   Applied and Computational Harmonic Analysis                   arXiv            doi         j acha             

  Lotfi  M   Vidyasagar  M  A Fast Non iterative Algorithm for Compressive Sensing Using Binary Measurement Matrices 

  A  M  Tillmann   On the Computational Intractability of Exact and Approximate Dictionary Learning   IEEE Signal Processing Letters                    

  Donoho  David L                 For most large underdetermined systems of linear equations the minimal     norm solution is also the sparsest solution   Communications on Pure and Applied Mathematics                   doi         cpa        ISSN                 S CID              

  Engan  K   Aase  S O   Hakon Husoy  J                 Method of optimal directions for frame design        IEEE International Conference on Acoustics  Speech  and Signal Processing  Proceedings  ICASSP    Cat  No   CH        Vol          pp                 vol    doi         ICASSP              ISBN                         S CID               

  Aharon  Michal  Elad  Michael          Sparse and Redundant Modeling of Image Content Using an Image Signature Dictionary   SIAM Journal on Imaging Sciences                  CiteSeerX                       doi                 x 

  Lee  Honglak  et al   Efficient sparse coding algorithms   Advances in neural information processing systems       

  Kumar  Abhay  Kataria  Saurabh   Dictionary Learning Based Applications in Image Processing using Convex Optimisation   PDF  

  Rubinstein  R   Bruckstein  A M   Elad  M                 Dictionaries for Sparse Representation Modeling   Proceedings of the IEEE                     CiteSeerX                      doi         JPROC               ISSN                 S CID              

  Engan  Kjersti  Skretting  Karl  Hus y  John H a akon                Family of Iterative LS based Dictionary Learning Algorithms  ILS DLA  for Sparse Signal Representation   Digit  Signal Process                 Bibcode     DSP           E  doi         j dsp              ISSN                

  Mairal  J   Sapiro  G   Elad  M                 Learning Multiscale Sparse Representations for Image and Video Restoration   Multiscale Modeling  amp  Simulation                  CiteSeerX                      doi                    ISSN                

  Rubinstein  R   Zibulevsky  M   Elad  M                 Double Sparsity  Learning Sparse Dictionaries for Sparse Signal Approximation   IEEE Transactions on Signal Processing                     Bibcode     ITSP          R  CiteSeerX                      doi         TSP               ISSN              X  S CID              

  Mairal  Julien  Bach  Francis  Ponce  Jean  Sapiro  Guillermo                Online Learning for Matrix Factorization and Sparse Coding   J  Mach  Learn  Res             arXiv            Bibcode     arXiv         M  ISSN                

  Aharon  M  M Elad  and A Bruckstein         K SVD  An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation   Signal Processing  IEEE Transactions on                   

  Peyr   Gabriel                Sparse Modeling of Textures   PDF   Journal of Mathematical Imaging and Vision                 doi         s                  ISSN                 S CID               

  Ramirez  Ignacio  Sprechmann  Pablo  Sapiro  Guillermo                Classification and clustering via dictionary learning with structured incoherence and shared features        IEEE Computer Society Conference on Computer Vision and Pattern Recognition  Los Alamitos  CA  USA  IEEE Computer Society  pp                  doi         CVPR               ISBN                         S CID                

  Koniusz  Piotr  Yan  Fei  Mikolajczyk  Krystian                Comparison of mid level feature coding approaches and pooling strategies in visual concept detection   Computer Vision and Image Understanding                    CiteSeerX                       doi         j cviu              ISSN                

  Koniusz  Piotr  Yan  Fei  Gosselin  Philippe Henri  Mikolajczyk  Krystian                Higher order occurrence pooling for bags of words  Visual concept detection   PDF   IEEE Transactions on Pattern Analysis and Machine Intelligence                   doi         TPAMI               hdl                ISSN                 PMID                S CID               

  AlMatouq  Ali  LalegKirati  TaousMeriem  Novara  Carlo  Ivana  Rabbone  Vincent  Tyrone                Sparse Reconstruction of Glucose Fluxes Using Continuous Glucose Monitors   IEEE ACM Transactions on Computational Biology and Bioinformatics                     doi         TCBB               hdl               ISSN                 PMID                S CID               







Retrieved from  https   en wikipedia org w index php title Sparse dictionary learning amp oldid