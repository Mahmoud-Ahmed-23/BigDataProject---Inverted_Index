Computational model used in machine learning  based on connected  hierarchical functions
This article is about the computational models used for artificial intelligence  For other uses  see Neural network  disambiguation  


An artificial neural network is an interconnected group of nodes  inspired by a simplification of neurons in a brain  Here  each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another 
In machine learning  a neural network  also artificial neural network or neural net  abbreviated ANN or NN  is a computational model inspired by the structure and functions of biological neural networks                       
A neural network consists of connected units or nodes called artificial neurons  which loosely model the neurons in the brain  Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance  These are connected by edges  which model the synapses in the brain  Each artificial neuron receives signals from connected neurons  then processes them and sends a signal to other connected neurons  The  signal  is a real number  and the output of each neuron is computed by some non linear function of the sum of its inputs  called the activation function  The strength of the signal at each connection is determined by a weight  which adjusts during the learning process 
Typically  neurons are aggregated into layers  Different layers may perform different transformations on their inputs  Signals travel from the first layer  the input layer  to the last layer  the output layer   possibly passing through multiple intermediate layers  hidden layers   A network is typically called a deep neural network if it has at least two hidden layers            
Artificial neural networks are used for various tasks  including predictive modeling  adaptive control  and solving problems in artificial intelligence  They can learn from experience  and can derive conclusions from a complex and seemingly unrelated set of information 


Training edit 
Neural networks are typically trained through empirical risk minimization  This method is based on the idea of optimizing the network s parameters to minimize the difference  or empirical risk  between the predicted output and the actual target values in a given dataset             Gradient based methods such as backpropagation are usually used to estimate the parameters of the network             During the training phase  ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function             This method allows the network to generalize to unseen data Simplified example of training a neural network in object detection  The network is trained by multiple images that are known to depict starfish and sea urchins  which are correlated with  nodes  that represent visual features  The starfish match with a ringed texture and a star outline  whereas most sea urchins match with a striped texture and oval shape  However  the instance of a ring textured sea urchin creates a weakly weighted association between them Subsequent run of the network on an input image  left              The network correctly detects the starfish  However  the weakly weighted association between ringed texture and sea urchin also confers a weak signal to the latter from one of two intermediate nodes  In addition  a shell that was not included in the training gives a weak signal for the oval shape  also resulting in a weak signal for the sea urchin output  These weak signals may result in a false positive result for sea urchin In reality  textures and outlines would not be represented by single nodes  but rather by associated weight patterns of multiple nodes 
History edit 
Main article  History of artificial neural networks
Early work edit 
Today s deep neural networks are based on early work in statistics over     years ago  The simplest kind of feedforward neural network  FNN  is a linear network  which consists of a single layer of output nodes with linear activation functions  the inputs are fed directly to the outputs via a series of weights  The sum of the products of the weights and the inputs is calculated at each node  The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights  This technique has been known for over two centuries as the method of least squares or linear regression  It was used as a means of finding a good rough linear fit to a set of points by Legendre        and Gauss        for the prediction of planetary movement                                                          
Historically  digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors  Some neural networks  on the other hand  originated from efforts to model information processing in biological systems through the framework of connectionism  Unlike the von Neumann model  connectionist computing does not separate memory and processing 
Warren McCulloch and Walter Pitts                    considered a non learning computational model for neural networks              This model paved the way for research to split into two approaches  One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence 
In the late     s  D  O  Hebb             proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning  It was used in many early neural networks  such as Rosenblatt s perceptron and the Hopfield network  Farley and Clark                    used computational machines to simulate a Hebbian network  Other neural network computational machines were created by Rochester  Holland  Habit and Duda                     
In       psychologist Frank Rosenblatt described the perceptron  one of the first implemented artificial neural networks                                                  funded by the United States Office of Naval Research             
R  D  Joseph                    mentions an even earlier perceptron like device by Farley and Clark               Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron like device   However   they dropped the subject  
The perceptron raised public excitement for research in Artificial Neural Networks  causing the US government to drastically increase funding  This contributed to  the Golden Age of AI  fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence             
The first perceptrons did not have adaptive hidden units  However  Joseph                    also discussed multilayer perceptrons with an adaptive hidden layer  Rosenblatt                               section           cited and adopted these ideas  also crediting work by H  D  Block and B  W  Knight  Unfortunately  these early efforts did not lead to a working learning algorithm for hidden units  i e   deep learning 

Deep learning breakthroughs in the     s and     s edit 
Fundamental research was conducted on ANNs in the     s and     s  The first working deep learning algorithm was the Group method of data handling  a method to train arbitrarily deep neural networks  published by Alexey Ivakhnenko and Lapa in the Soviet Union         They regarded it as a form of polynomial regression              or a generalization of Rosenblatt s perceptron              A      paper described a deep network with eight layers trained by this method              which is based on layer by layer training through regression analysis  Superfluous hidden units are pruned using a separate validation set  Since the activation functions of the nodes are Kolmogorov Gabor polynomials  these were also the first deep networks with multiplicative units or  gates              
The first deep learning multilayer perceptron trained by stochastic gradient descent             was published in      by Shun ichi Amari              In computer experiments conducted by Amari s student Saito  a five layer MLP with two modifiable layers learned internal representations to classify non linearily separable pattern classes              Subsequent developments in hardware and hyperparameter tunings have made end to end stochastic gradient descent the currently dominant training technique 
In       Kunihiko Fukushima introduced the ReLU  rectified linear unit  activation function                                      The rectifier has become the most popular activation function for deep learning             
Nevertheless  research stagnated in the United States following the work of Minsky and Papert                     who emphasized that basic perceptrons were incapable of processing the exclusive or circuit  This insight was irrelevant for the deep networks of Ivakhnenko        and Amari        
In      transfer learning was introduced in neural networks learning                         
Deep learning architectures for convolutional neural networks  CNNs  with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in       though not trained by backpropagation                                     

Backpropagation edit 
Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in                  to networks of differentiable nodes  The terminology  back propagating errors  was actually introduced in      by Rosenblatt              but he did not know how to implement this  although Henry J  Kelley had a continuous precursor of backpropagation in      in the context of control theory              In       Seppo Linnainmaa published the modern form of backpropagation in his Master s thesis                                             G M  Ostrovski et al  republished it in                               Paul Werbos applied backpropagation to neural networks in                               his      PhD thesis  reprinted in a      book              did not yet describe the algorithm               In       David E  Rumelhart et al  popularised backpropagation but did not cite the original work             

Convolutional neural networks edit 
Kunihiko Fukushima s convolutional neural network  CNN  architecture of                  also introduced max pooling              a popular downsampling procedure for CNNs  CNNs have become an essential tool for computer vision 
The time delay neural network  TDNN  was introduced in      by Alex Waibel to apply CNN to phoneme recognition  It used convolutions  weight sharing  and backpropagation                          In       Wei Zhang applied a backpropagation trained CNN to alphabet recognition             
In       Yann LeCun et al  created a CNN called LeNet for recognizing handwritten ZIP codes on mail  Training required   days              In       Wei Zhang implemented a CNN on optical computing hardware              In       a CNN was applied to medical image object segmentation             and breast cancer detection in mammograms              LeNet           a   level CNN by Yann LeCun et al   that classifies digits  was applied by several banks to recognize hand written numbers on checks digitized in       pixel images             
From      onward                          the use of neural networks transformed the field of protein structure prediction  in particular when the first cascading networks were trained on profiles  matrices  produced by multiple sequence alignments             

Recurrent neural networks edit 
One origin of RNN was statistical mechanics  In       Shun ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory  adding in the component of learning              This was popularized as the Hopfield network by John Hopfield                     Another origin of RNN was neuroscience  The word  recurrent  is used to describe loop like structures in anatomy  In       Cajal observed  recurrent semicircles  in the cerebellar cortex              Hebb considered  reverberating circuit  as an explanation for short term memory              The McCulloch and Pitts paper        considered neural networks that contain cycles  and noted that the current activity of such networks can be affected by activity indefinitely far in the past             
In      a recurrent neural network with an array architecture  rather than a multilayer perceptron architecture   namely a Crossbar Adaptive Array                          used direct recurrent connections from the output to the supervisor  teaching  inputs  In addition of computing actions  decisions   it computed internal state evaluations  emotions  of the consequence situations  Eliminating the external supervisor  it introduced the self learning method in neural networks   
In cognitive psychology  the journal American Psychologist in early      s carried out a debate on the relation between cognition and emotion  Zajonc in      stated that emotion is computed first and is independent from cognition  while Lazarus in      stated that cognition is computed first and is inseparable from emotion                          In      the Crossbar Adaptive Array gave a neural network model of cognition emotion relation                          It was an example of a debate where an AI system  a recurrent neural network  contributed to an issue in the same time addressed by cognitive psychology 
Two early influential works were the Jordan network        and the Elman network         which applied RNN to study cognitive psychology  
In the     s  backpropagation did not work well for deep RNNs  To overcome this problem  in       J rgen Schmidhuber proposed the  neural sequence chunker  or  neural history compressor                          which introduced the important concepts of self supervised pre training  the  P  in ChatGPT  and neural knowledge distillation              In       a neural history compressor system solved a  Very Deep Learning  task that required more than      subsequent layers in an RNN unfolded in time             
In       Sepp Hochreiter s diploma thesis             identified and analyzed the vanishing gradient problem                         and proposed recurrent residual connections to solve it  He and Schmidhuber introduced long short term memory  LSTM   which set accuracy records in multiple applications domains                          This was not yet the modern version of LSTM  which required the forget gate  which was introduced in                   It became the default choice for RNN architecture 
During            inspired by statistical mechanics  several architectures and methods were developed by Terry Sejnowski  Peter Dayan  Geoffrey Hinton  etc   including the Boltzmann machine              restricted Boltzmann machine              Helmholtz machine              and the wake sleep algorithm              These were designed for unsupervised learning of deep generative models 

Deep learning edit 
Between      and       ANNs began winning prizes in image recognition contests  approaching human level performance on various tasks  initially in pattern recognition and handwriting recognition                          In       a CNN named DanNet                         by Dan Ciresan  Ueli Meier  Jonathan Masci  Luca Maria Gambardella  and J rgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest  outperforming traditional methods by a factor of                It then won more contests                          They also showed how max pooling CNNs on GPU improved performance significantly             
In October       AlexNet by Alex Krizhevsky  Ilya Sutskever  and Geoffrey Hinton             won the large scale ImageNet competition by a significant margin over shallow machine learning methods  Further incremental improvements included the VGG    network by Karen Simonyan and Andrew Zisserman             and Google s Inceptionv              
In       Ng and Dean created a network that learned to recognize higher level concepts  such as cats  only from watching unlabeled images              Unsupervised pre training and increased computing power from GPUs and distributed computing allowed the use of larger networks  particularly in image and visual recognition problems  which became known as  deep learning             
Radial basis function and wavelet networks were introduced in       These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications             
Generative adversarial network  GAN   Ian Goodfellow et al                     became state of the art in generative modeling during           period  The GAN principle was originally published in      by J rgen Schmidhuber who called it  artificial curiosity   two neural networks contest with each other in the form of a zero sum game  where one network s gain is the other network s loss                          The first network is a generative model that models a probability distribution over output patterns  The second network learns by gradient descent to predict the reactions of the environment to these patterns  Excellent image quality is achieved by Nvidia s StyleGAN                    based on the Progressive GAN by Tero Karras et al              Here  the GAN generator is grown from small to large scale in a pyramidal fashion  Image generation by GAN reached popular success  and provoked discussions concerning deepfakes              Diffusion models                     eclipsed GANs in generative modeling since then  with systems such as DALL E          and Stable Diffusion        
In       the state of the art was training  very deep neural network  with    to    layers               Stacking too many layers led to a steep reduction in training accuracy               known as the  degradation  problem               In       two techniques were developed to train very deep networks  the highway network was published in May                    and the residual neural network  ResNet  in December                                 ResNet behaves like an open gated Highway Net  

Main article  Transformer  deep learning architecture         History
During the     s  the seq seq model was developed  and attention mechanisms were added  It led to the modern Transformer architecture in      in Attention Is All You Need              
It requires computation time that is quadratic in the size of the context window  J rgen Schmidhuber s fast weight controller                     scales linearly and was later shown to be equivalent to the unnormalized linear Transformer                                       
Transformers have increasingly become the model of choice for natural language processing               Many modern large language models such as ChatGPT  GPT    and BERT use this architecture 

Models edit 
This section may be confusing or unclear to readers  Please help clarify the section  There might be a discussion about this on the talk page    April        Learn how and when to remove this message Further information  Mathematics of artificial neural networksNeuron and myelinated axon  with signal flow from inputs at dendrites to outputs at axon terminals
ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with  They soon reoriented towards improving empirical results  abandoning attempts to remain true to their biological precursors  ANNs have the ability to learn and model non linearities and complex relationships  This is achieved by neurons being connected in various patterns  allowing the output of some neurons to become the input of others  The network forms a directed  weighted graph              
An artificial neural network consists of simulated neurons  Each neuron is connected to other nodes via links like a biological axon synapse dendrite connection  All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data  Each link has a weight  determining the strength of one node s influence on another               allowing weights to choose the signal between neurons 

Artificial neurons edit 
Main article  Artificial neuron
ANNs are composed of artificial neurons which are conceptually derived from biological neurons  Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons               The inputs can be the feature values of a sample of external data  such as images or documents  or they can be the outputs of other neurons  The outputs of the final output neurons of the neural net accomplish the task  such as recognizing an object in an image      citation needed     
To find the output of the neuron we take the weighted sum of all the inputs  weighted by the weights of the connections from the inputs to the neuron  We add a bias term to this sum               This weighted sum is sometimes called the activation  This weighted sum is then passed through a  usually nonlinear  activation function to produce the output  The initial inputs are external data  such as images and documents  The ultimate outputs accomplish the task  such as recognizing an object in an image              

Organization edit 
The neurons are typically organized into multiple layers  especially in deep learning  Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers  The layer that receives external data is the input layer  The layer that produces the ultimate result is the output layer  In between them are zero or more hidden layers  Single layer and unlayered networks are also used  Between two layers  multiple connection patterns are possible  They can be  fully connected   with every neuron in one layer connecting to every neuron in the next layer  They can be pooling  where a group of neurons in one layer connects to a single neuron in the next layer  thereby reducing the number of neurons in that layer               Neurons with only such connections form a directed acyclic graph and are known as feedforward networks               Alternatively  networks that allow connections between neurons in the same or previous layers are known as recurrent networks              

Hyperparameter edit 
Main article  Hyperparameter  machine learning 
A hyperparameter is a constant parameter whose value is set before the learning process begins  The values of parameters are derived via learning  Examples of hyperparameters include learning rate  the number of hidden layers and batch size      citation needed      The values of some hyperparameters can be dependent on those of other hyperparameters  For example  the size of some layers can depend on the overall number of layers      citation needed     

Learning edit 
This section includes a list of references  related reading  or external links  but its sources remain unclear because it lacks inline citations  Please help improve this section by introducing more precise citations    August        Learn how and when to remove this message See also  Mathematical optimization  Estimation theory  and Machine learning
Learning is the adaptation of the network to better handle a task by considering sample observations  Learning involves adjusting the weights  and optional thresholds  of the network to improve the accuracy of the result  This is done by minimizing the observed errors  Learning is complete when examining additional observations does not usefully reduce the error rate  Even after learning  the error rate typically does not reach    If after learning  the error rate is too high  the network typically must be redesigned  Practically this is done by defining a cost function that is evaluated periodically during learning  As long as its output continues to decline  learning continues  The cost is frequently defined as a statistic whose value can only be approximated  The outputs are actually numbers  so when the error is low  the difference between the output  almost certainly a cat  and the correct answer  cat  is small  Learning attempts to reduce the total of the differences across the observations  Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation                           

Learning rate edit 
Main article  Learning rate
The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation               A high learning rate shortens the training time  but with lower ultimate accuracy  while a lower learning rate takes longer  but with the potential for greater accuracy  Optimizations such as Quickprop are primarily aimed at speeding up error minimization  while other improvements mainly try to increase reliability  In order to avoid oscillation inside the network such as alternating connection weights  and to improve the rate of convergence  refinements use an adaptive learning rate that increases or decreases as appropriate               The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change  A momentum close to   emphasizes the gradient  while a value close to   emphasizes the last change      citation needed     

Cost function edit 
While it is possible to define a cost function ad hoc  frequently the choice is determined by the function s desirable properties  such as convexity  or because it arises from the model  e g  in a probabilistic model the model s posterior probability can be used as an inverse cost       citation needed     

Backpropagation edit 
Main article  Backpropagation
Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning  The error amount is effectively divided among the connections  Technically  backpropagation calculates the gradient  the derivative  of the cost function associated with a given state with respect to the weights  The weight updates can be done via stochastic gradient descent or other methods  such as extreme learning machines                no prop  networks               training without backtracking                weightless  networks                            and non connectionist neural networks      citation needed     

Learning paradigms edit 
This section includes a list of references  related reading  or external links  but its sources remain unclear because it lacks inline citations  Please help improve this section by introducing more precise citations    August        Learn how and when to remove this message 
Machine learning is commonly separated into three main learning paradigms  supervised learning               unsupervised learning              and reinforcement learning               Each corresponds to a particular learning task 

Supervised learning edit 
Supervised learning uses a set of paired inputs and desired outputs  The learning task is to produce the desired output for each input  In this case  the cost function is related to eliminating incorrect deductions               A commonly used cost is the mean squared error  which tries to minimize the average squared error between the network s output and the desired output  Tasks suited for supervised learning are pattern recognition  also known as classification  and regression  also known as function approximation   Supervised learning is also applicable to sequential data  e g   for handwriting  speech and gesture recognition   This can be thought of as learning with a  teacher   in the form of a function that provides continuous feedback on the quality of solutions obtained thus far 

Unsupervised learning edit 
In unsupervised learning  input data is given along with the cost function  some function of the data 
  
    
      
        
          x
        
      
    
      displaystyle  textstyle x 
  
 and the network s output  The cost function is dependent on the task  the model domain  and any a priori assumptions  the implicit properties of the model  its parameters and the observed variables   As a trivial example  consider the model 
  
    
      
        
          f
           
          x
           
           
          a
        
      
    
      displaystyle  textstyle f x  a 
  
 where 
  
    
      
        
          a
        
      
    
      displaystyle  textstyle a 
  
 is a constant and the cost 
  
    
      
        
          C
           
          E
           
           
          x
            x     
          f
           
          x
           
          
             
            
               
            
          
           
        
      
    
      displaystyle  textstyle C E  x f x        
  
  Minimizing this cost produces a value of 
  
    
      
        
          a
        
      
    
      displaystyle  textstyle a 
  
 that is equal to the mean of the data  The cost function can be much more complicated  Its form depends on the application  for example  in compression it could be related to the mutual information between 
  
    
      
        
          x
        
      
    
      displaystyle  textstyle x 
  
 and 
  
    
      
        
          f
           
          x
           
        
      
    
      displaystyle  textstyle f x  
  
  whereas in statistical modeling  it could be related to the posterior probability of the model given the data  note that in both of those examples  those quantities would be maximized rather than minimized   Tasks that fall within the paradigm of unsupervised learning are in general estimation problems  the applications include clustering  the estimation of statistical distributions  compression and filtering 

Reinforcement learning edit 
Main article  Reinforcement learning
See also  Stochastic control
In applications such as playing video games  an actor takes a string of actions  receiving a generally unpredictable response from the environment after each one  The goal is to win the game  i e   generate the most positive  lowest cost  responses  In reinforcement learning  the aim is to weight the network  devise a policy  to perform actions that minimize long term  expected cumulative  cost  At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost  according to some  usually unknown  rules  The rules and the long term cost usually only can be estimated  At any juncture  the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly 
Formally the environment is modeled as a Markov decision process  MDP  with states 
  
    
      
        
          
            
              s
              
                 
              
            
             
             
             
             
             
            
              s
              
                n
              
            
          
            x     
          S
        
      
    
      displaystyle  textstyle  s         s  n   in S 
  
 and actions 
  
    
      
        
          
            
              a
              
                 
              
            
             
             
             
             
             
            
              a
              
                m
              
            
          
            x     
          A
        
      
    
      displaystyle  textstyle  a         a  m   in A 
  
  Because the state transitions are not known  probability distributions are used instead  the instantaneous cost distribution 
  
    
      
        
          P
           
          
            c
            
              t
            
          
          
             
          
          
            s
            
              t
            
          
           
        
      
    
      displaystyle  textstyle P c  t  s  t   
  
  the observation distribution 
  
    
      
        
          P
           
          
            x
            
              t
            
          
          
             
          
          
            s
            
              t
            
          
           
        
      
    
      displaystyle  textstyle P x  t  s  t   
  
 and the transition distribution 
  
    
      
        
          P
           
          
            s
            
              t
               
               
            
          
          
             
          
          
            s
            
              t
            
          
           
          
            a
            
              t
            
          
           
        
      
    
      displaystyle  textstyle P s  t    s  t  a  t   
  
  while a policy is defined as the conditional distribution over actions given the observations  Taken together  the two define a Markov chain  MC   The aim is to discover the lowest cost MC 
ANNs serve as the learning component in such applications                            Dynamic programming coupled with ANNs  giving neurodynamic programming               has been applied to problems such as those involved in vehicle routing               video games  natural resource management                           and medicine              because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems  Tasks that fall within the paradigm of reinforcement learning are control problems  games and other sequential decision making tasks 

Self learning edit 
Self learning in neural networks was introduced in      along with a neural network capable of self learning named crossbar adaptive array  CAA                It is a system with only one input  situation s  and only one output  action  or behavior  a  It has neither external advice input nor external reinforcement input from the environment  The CAA computes  in a crossbar fashion  both decisions about actions and emotions  feelings  about encountered situations  The system is driven by the interaction between cognition and emotion               Given the memory matrix  W    w a s     the crossbar self learning algorithm in each iteration performs the following computation 

 In situation s perform action a 
 Receive consequence situation s  
 Compute emotion of being in consequence situation v s   
 Update crossbar memory w  a s    w a s    v s   

The backpropagated value  secondary reinforcement  is the emotion toward the consequence situation  The CAA exists in two environments  one is behavioral environment where it behaves  and the other is genetic environment  where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment  Having received the genome vector  species vector  from the genetic environment  the CAA will learn a goal seeking behavior  in the behavioral environment that contains both desirable and undesirable situations              

Neuroevolution edit 
Main article  Neuroevolution
Neuroevolution can create neural network topologies and weights using evolutionary computation  It is competitive with sophisticated gradient descent approaches                            One advantage of neuroevolution is that it may be less prone to get caught in  dead ends               

Stochastic neural network edit 
Stochastic neural networks originating from Sherrington Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network  either by giving the network s artificial neurons stochastic transfer functions      citation needed       or by giving them stochastic weights  This makes them useful tools for optimization problems  since the random fluctuations help the network escape from local minima               Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks              

Topological deep learning edit 
Topological deep learning  first introduced in                    is an emerging approach in machine learning that integrates topology with deep neural networks to address highly intricate and high order data  Initially rooted in algebraic topology  TDL has since evolved into a versatile framework incorporating tools from other mathematical disciplines  such as differential topology and geometric topology  As a successful example of mathematical deep learning  TDL continues to inspire advancements in mathematical artificial intelligence  fostering a mutually beneficial relationship between AI and mathematics   

Other edit 
In a Bayesian framework  a distribution over the set of allowed models is chosen to minimize the cost  Evolutionary methods               gene expression programming               simulated annealing               expectation maximization  non parametric methods and particle swarm optimization              are other learning algorithms  Convergent recursion is a learning algorithm for cerebellar model articulation controller  CMAC  neural networks                           

Modes edit 
This section includes a list of references  related reading  or external links  but its sources remain unclear because it lacks inline citations  Please help improve this section by introducing more precise citations    August        Learn how and when to remove this message 
Two modes of learning are available  stochastic and batch  In stochastic learning  each input creates a weight adjustment  In batch learning weights are adjusted based on a batch of inputs  accumulating errors over the batch  Stochastic learning introduces  noise  into the process  using the local gradient calculated from one data point  this reduces the chance of the network getting stuck in local minima  However  batch learning typically yields a faster  more stable descent to a local minimum  since each update is performed in the direction of the batch s average error  A common compromise is to use  mini batches   small batches with samples in each batch selected stochastically from the entire data set 

Types edit 
Main article  Types of artificial neural networks
ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains  The simplest types have one or more static components  including number of units  number of layers  unit weights and topology  Dynamic types allow one or more of these to evolve via learning  The latter is much more complicated but can shorten learning periods and produce better results  Some types allow require learning to be  supervised  by the operator  while others operate independently  Some types operate purely in hardware  while others are purely software and run on general purpose computers 
Some of the main breakthroughs include  

Convolutional neural networks that have proven particularly successful in processing visual and other two dimensional data                            where long short term memory avoids the vanishing gradient problem              and can handle signals that have a mix of low and high frequency components aiding large vocabulary speech recognition                            text to speech synthesis                                         and photo real talking heads              
Competitive networks such as generative adversarial networks in which multiple networks  of varying structure  compete with each other  on tasks such as winning a game              or on deceiving the opponent about the authenticity of an input             
Network design edit 
Using artificial neural networks requires an understanding of their characteristics 

Choice of model  This depends on the data representation and the application  Model parameters include the number  type  and connectedness of network layers  as well as the size of each and the connection type  full  pooling  etc     Overly complex models learn slowly 
Learning algorithm  Numerous trade offs exist between learning algorithms  Almost any algorithm will work well with the correct hyperparameters              for training on a particular data set  However  selecting and tuning an algorithm for training on unseen data requires significant experimentation 
Robustness  If the model  cost function and learning algorithm are selected appropriately  the resulting ANN can become robust 
Neural architecture search  NAS  uses machine learning to automate ANN design  Various approaches to NAS have designed networks that compare well with hand designed systems  The basic search algorithm is to propose a candidate model  evaluate it against a dataset  and use the results as feedback to teach the NAS network               Available systems include AutoML and AutoKeras               scikit learn library provides functions to help with building a deep network from scratch  We can then implement a deep network with TensorFlow or Keras 

Hyperparameters must also be defined as part of the design  they are not learned   governing matters such as how many neurons are in each layer  learning rate  step  stride  depth  receptive field and padding  for CNNs   etc               The Python code snippet provides an overview of the training function  which uses the training dataset  number of hidden layer units  learning rate  and number of iterations as parameters def train X  y  n hidden  learning rate  n iter  
    m  n input   X shape

         random initialize weights and biases
    w    np random randn n input  n hidden 
    b    np zeros     n hidden  
    w    np random randn n hidden    
    b    np zeros        

         in each iteration  feed all layers with the latest weights and biases
    for i in range n iter      
        z    np dot X  w     b 
        a    sigmoid z  
        z    np dot a   w     b 
        a    z 
        dz    a    y
        dw    np dot a  T  dz  
        db    np sum dz   axis    keepdims True 
        dz    np dot dz   w  T    sigmoid derivative z  
        dw    np dot X Y  dz  
        db    np sum dz   axis   

             update weights and biases with gradients
        w     learning rate   dw    m
        w     learning rate   dw    m
        b     learning rate   db    m
        b     learning rate   db    m

        if i             
            print  quot Epoch quot   i   quot loss   quot   np mean np square dz    

    model     quot w  quot   w    quot b  quot   b    quot w  quot   w    quot b  quot   b  
    return model
     citation needed     
Applications edit 
Because of their ability to reproduce and model nonlinear processes  artificial neural networks have found applications in many disciplines  These include 

Function approximation               or regression analysis                including time series prediction  fitness approximation               and modeling 
Data processing               including filtering  clustering  blind source separation               and compression 
Nonlinear system identification             and control  including vehicle control  trajectory prediction               adaptive control  process control  and natural resource management 
Pattern recognition  including radar systems  face identification  signal classification               novelty detection   D reconstruction               object recognition  and sequential decision making              
Sequence recognition  including gesture  speech  and handwritten and printed text recognition              
Sensor data analysis               including image analysis 
Robotics  including directing manipulators and prostheses 
Data mining  including knowledge discovery in databases 
Finance               such as ex ante models for specific financial long run forecasts and artificial financial markets 
Quantum chemistry             
General game playing             
Generative AI             
Data visualization
Machine translation
Social network filtering             
E mail spam filtering
Medical diagnosis             
ANNs have been used to diagnose several types of cancers                           and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information                           
ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters                           and to predict foundation settlements               It can also be useful to mitigate flood by the use of ANNs for modelling rainfall runoff               ANNs have also been used for building black box models in geoscience  hydrology                            ocean modelling and coastal engineering                            and geomorphology               ANNs have been employed in cybersecurity  with the objective to discriminate between legitimate activities and malicious ones  For example  machine learning has been used for classifying Android malware               for identifying domains belonging to threat actors and for detecting URLs posing a security risk               Research is underway on ANN systems designed for penetration testing  for detecting botnets               credit cards frauds              and network intrusions 
ANNs have been proposed as a tool to solve partial differential equations in physics                                        and simulate the properties of many body open quantum systems                                                      In brain research ANNs have studied short term behavior of individual neurons               the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems  Studies considered long and short term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level 
It is possible to create a profile of a user s interests from pictures  using artificial neural networks trained for object recognition              
Beyond their traditional applications  artificial neural networks are increasingly being utilized in interdisciplinary research  such as materials science  For instance  graph neural networks  GNNs  have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals  This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence  opening new pathways for scientific discovery and innovation              

Theoretical properties edit 
Computational power edit 
The multilayer perceptron is a universal function approximator  as proven by the universal approximation theorem  However  the proof is not constructive regarding the number of neurons required  the network topology  the weights and the learning parameters 
A specific recurrent architecture with rational valued weights  as opposed to full precision real number valued weights  has the power of a universal Turing machine               using a finite number of neurons and standard linear connections  Further  the use of irrational values for weights results in a machine with super Turing power                                failed verification     

Capacity edit 
A model s  capacity  property corresponds to its ability to model any given function  It is related to the amount of information that can be stored in the network and to the notion of complexity 
Two notions of capacity are known by the community  The information capacity and the VC Dimension  The information capacity of a perceptron is intensively discussed in Sir David MacKay s book              which summarizes work by Thomas Cover               The capacity of a network of standard neurons  not convolutional  can be derived by four rules              that derive from understanding a neuron as an electrical element  The information capacity captures the functions modelable by the network given any data as input  The second notion  is the VC dimension  VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances  This is  given input data in a specific form  As noted in               the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron  The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity              

Convergence edit 
Models may not consistently converge on a single solution  firstly because local minima may exist  depending on the cost function and the model  Secondly  the optimization method used might not guarantee to converge when it begins far from any local minimum  Thirdly  for sufficiently large data or parameters  some methods become impractical 
Another issue worthy to mention is that training may cross some Saddle point which may lead the convergence to the wrong direction 
The convergence behavior of certain types of ANN architectures are more understood than others  When the width of network approaches to infinity  the ANN is well described by its first order Taylor expansion throughout training  and so inherits the convergence behavior of affine models                            Another example is when parameters are small  it is observed that ANNs often fits target functions from low to high frequencies  This behavior is referred to as the spectral bias  or frequency principle  of neural networks                                                      This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method  Deeper neural networks have been observed to be more biased towards low frequency functions              

Generalization and statistics edit 
This section includes a list of references  related reading  or external links  but its sources remain unclear because it lacks inline citations  Please help improve this section by introducing more precise citations    August        Learn how and when to remove this message 
Applications whose goal is to create a system that generalizes well to unseen examples  face the possibility of over training  This arises in convoluted or over specified systems when the network capacity significantly exceeds the needed free parameters  Two approaches address over training  The first is to use cross validation and similar techniques to check for the presence of over training and to select hyperparameters to minimize the generalization error 
The second is to use some form of regularization  This concept emerges in a probabilistic  Bayesian  framework  where regularization can be performed by selecting a larger prior probability over simpler models  but also in statistical learning theory  where the goal is to minimize over two quantities  the  empirical risk  and the  structural risk   which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting 

Confidence analysis of a neural network
Supervised neural networks that use a mean squared error  MSE  cost function can use formal statistical methods to determine the confidence of the trained model  The MSE on a validation set can be used as an estimate for variance  This value can then be used to calculate the confidence interval of network output  assuming a normal distribution  A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified 
By assigning a softmax activation function  a generalization of the logistic function  on the output layer of the neural network  or a softmax component in a component based network  for categorical target variables  the outputs can be interpreted as posterior probabilities  This is useful in classification as it gives a certainty measure on classifications 
The softmax activation function is 


  
    
      
        
          y
          
            i
          
        
         
        
          
            
              e
              
                
                  x
                  
                    i
                  
                
              
            
            
              
                  x     
                
                  j
                   
                   
                
                
                  c
                
              
              
                e
                
                  
                    x
                    
                      j
                    
                  
                
              
            
          
        
      
    
      displaystyle y  i    frac  e  x  i     sum   j     c e  x  j     
  



Criticism edit 
Training edit 
A common criticism of neural networks  particularly in robotics  is that they require too many training samples for real world operation              
Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases  Potential solutions include randomly shuffling training examples  by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example  grouping examples in so called mini batches and or introducing a recursive least squares algorithm for CMAC              
Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads  single lane  multi lane  dirt  etc    and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience  and preserving past training diversity so that the system does not become overtrained  if  for example  it is presented with a series of right turns it should not learn to always turn right               

Theory edit 
A central claim     citation needed      of ANNs is that they embody new and powerful general principles for processing information  These principles are ill defined  It is often claimed     by whom       that they are emergent from the network itself  This allows simple statistical association  the basic function of artificial neural networks  to be described as learning or recognition  In       Alexander Dewdney  a former Scientific American columnist  commented that as a result  artificial neural networks have a  something for nothing quality  one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are  No human hand  or mind  intervenes  solutions are found as if by magic  and no one  it seems  has learned anything                One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks  ranging from autonomously flying aircraft              to detecting credit card fraud to mastering the game of Go 
Technology writer Roger Bridgman commented 

Neural networks  for instance  are in the dock not only because they have been hyped to high heaven   what hasn t   but also because you could create a successful net without understanding how it worked  the bunch of numbers that captures its behaviour would in all probability be  an opaque  unreadable table   valueless as a scientific resource  
In spite of his emphatic declaration that science is not technology  Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers  An unreadable table that a useful machine could read would still be well worth having              


Although it is true that analyzing what has been learned by an artificial neural network is difficult  it is much easier to do so than to analyze what has been learned by a biological neural network  Moreover  recent emphasis on the explainability of AI has contributed towards the development of methods  notably those based on attention mechanisms  for visualizing and explaining learned neural networks  Furthermore  researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful  For example  Bengio and LeCun        wrote an article regarding local vs non local learning  as well as shallow vs deep architecture              
Biological brains use both shallow and deep circuits as reported by brain anatomy               displaying a wide variety of invariance  Weng              argued that the brain self wires largely according to signal statistics and therefore  a serial cascade cannot catch all major statistical dependencies 

Hardware edit 
Large and effective neural networks require considerable computing resources               While the brain has hardware tailored to the task of processing signals through a graph of neurons  simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage  Furthermore  the designer often needs to transmit signals through many of these connections and their associated neurons                   which require enormous CPU power and time      citation needed     
Some argue that the resurgence of neural networks in the twenty first century is largely attributable to advances in hardware  from      to       computing power  especially as delivered by GPGPUs  on GPUs   has increased around a million fold  making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before              The use of accelerators such as FPGAs and GPUs can reduce training times from months to days                           
Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly  by constructing non von Neumann chips to directly implement neural networks in circuitry  Another type of chip optimized for neural network processing is called a Tensor Processing Unit  or TPU              

Practical counterexamples edit 
Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network  Furthermore  researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful  For example  local vs  non local learning and shallow vs  deep architecture              

Hybrid approaches edit 
Advocates of hybrid models  combining neural networks and symbolic approaches  say that such a mixture can better capture the mechanisms of the human mind                           

Dataset bias edit 
Neural networks are dependent on the quality of the data they are trained on  thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases                            These inherited biases become especially critical when the ANNs are integrated into real world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race  gender or other attribute               This imbalance can result in the model having inadequate representation and understanding of underrepresented groups  leading to discriminatory outcomes that exacerbate societal inequalities  especially in applications like facial recognition  hiring processes  and law enforcement                            For example  in       Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field               The program would penalize any resume with the word  woman  or the name of any women s college  However  the use of synthetic data can help reduce dataset bias and increase representation in datasets              

Gallery edit 

		
			
			A single layer feedforward artificial neural network  Arrows originating from 
  
    
      
        
          
            x
            
               
            
          
        
      
    
      displaystyle  scriptstyle x     
  
 are omitted for clarity  There are p inputs to this network and q outputs  In this system  the value of the qth output  
  
    
      
        
          y
          
            q
          
        
      
    
      displaystyle y  q  
  
  is calculated as 
  
    
      
        
          
            y
            
              q
            
          
           
          K
            x     
           
          
              x     
            
              i
            
          
           
          
            x
            
              i
            
          
            x     
          
            w
            
              i
              q
            
          
           
            x     
          
            b
            
              q
            
          
           
           
        
      
    
      displaystyle  scriptstyle y  q  K   sum   i  x  i  w  iq   b  q    
  

		
		
			
			A two layer feedforward artificial neural network
		
		
			
			An artificial neural network
		
		
			
			An ANN dependency graph
		
		
			
			A single layer feedforward artificial neural network with   inputs    hidden nodes and   outputs  Given position state and direction  it outputs wheel based control values 
		
		
			
			A two layer feedforward artificial neural network with   inputs   x  hidden nodes and   outputs  Given position state  direction and other environment values  it outputs thruster based control values 
		
		
			
			Parallel pipeline structure of CMAC neural network  This learning algorithm can converge in one step  
		

Recent advancements and future directions edit 
Artificial neural networks  ANNs  have undergone significant advancements  particularly in their ability to model complex systems  handle large data sets  and adapt to various types of applications  Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing  speech recognition  natural language processing  finance  and medicine      citation needed     

Image processing edit 
In the realm of image processing  ANNs are employed in tasks such as image classification  object recognition  and image segmentation  For instance  deep convolutional neural networks  CNNs  have been important in handwritten digit recognition  achieving state of the art performance               This demonstrates the ability of ANNs to effectively process and interpret complex visual information  leading to advancements in fields ranging from automated surveillance to medical imaging              

Speech recognition edit 
By modeling speech signals  ANNs are used for tasks like speaker identification and speech to text conversion  Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition  outperforming traditional techniques                            These advancements have enabled the development of more accurate and efficient voice activated systems  enhancing user interfaces in technology products      citation needed     

Natural language processing edit 
In natural language processing  ANNs are used for tasks such as text classification  sentiment analysis  and machine translation  They have enabled the development of models that can accurately translate between languages  understand the context and sentiment in textual data  and categorize text based on content                            This has implications for automated customer service  content moderation  and language understanding technologies      citation needed     

Control systems edit 
In the domain of control systems  ANNs are used to model dynamic systems for tasks such as system identification  control design  and optimization  For instance  deep feedforward neural networks are important in system identification and control applications      citation needed     

Finance edit 
Further information  Applications of artificial intelligence        Trading and investment
ANNs are used for stock market prediction and credit scoring  

In investing  ANNs can process vast amounts of financial data  recognize complex patterns  and forecast stock market trends  aiding investors and risk managers in making informed decisions              
In credit scoring  ANNs offer data driven  personalized assessments of creditworthiness  improving the accuracy of default predictions and automating the lending process              
ANNs require high quality data and careful tuning  and their  black box  nature can pose challenges in interpretation  Nevertheless  ongoing advancements suggest that ANNs continue to play a role in finance  offering valuable insights and enhancing risk management strategies      citation needed     

Medicine edit 
ANNs are able to process and analyze vast medical datasets  They enhance diagnostic accuracy  especially by interpreting complex medical imaging for early disease detection  and by predicting patient outcomes for personalized treatment planning               In drug discovery  ANNs speed up the identification of potential drug candidates and predict their efficacy and safety  significantly reducing development time and costs               Additionally  their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management               Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability  as well as expanding the scope of ANN applications in medicine      citation needed     

Content creation edit 
ANNs such as generative adversarial networks  GAN  and transformers are used for content creation across numerous industries               This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions  For instance  DALL E is a deep neural network trained on     million pairs of images and texts across the internet that can create artworks based on text entered by the user               In the field of music  transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck               In the marketing industry generative models are used to create personalized advertisements for consumers               Additionally  major film companies are partnering with technology companies to analyze the financial success of a film  such as the partnership between Warner Bros and technology company Cinelytic established in                    Furthermore  neural networks have found uses in video game creation  where Non Player Characters  NPCs  can make decisions based on all the characters currently in the game              

See also edit 

ADALINE
Autoencoder
Bio inspired computing
Blue Brain Project
Catastrophic interference
Cognitive architecture
Connectionist expert system
Connectomics
Deep image prior
Digital morphogenesis
Efficiently updatable neural network
Evolutionary algorithm
Family of curves
Genetic algorithm
Hyperdimensional computing
In situ adaptive tabulation
Large width limits of neural networks
List of machine learning concepts
Memristor
Neural gas
Neural network software
Optical neural network
Parallel distributed processing
Philosophy of artificial intelligence
Predictive analytics
Quantum neural network
Support vector machine
Spiking neural network
Stochastic parrot
Tensor product network
Topological deep learning

References edit 


  Hardesty L     April         Explained  Neural networks   MIT News Office  Archived from the original on    March       Retrieved   June      

  Yang Z  Yang Z         Comprehensive Biomedical Physics  Karolinska Institute  Stockholm  Sweden  Elsevier  p          ISBN                         Archived from the original on    July       Retrieved    July      

  Bishop CM     August        Pattern Recognition and Machine Learning  New York  Springer  ISBN                        

  a b Vapnik VN  Vapnik VN         The nature of statistical learning theory  Corrected  nd print       ed    New York Berlin Heidelberg  Springer  ISBN                        

  a b Ian Goodfellow and Yoshua Bengio and Aaron Courville         Deep Learning  MIT Press  Archived from the original on    April       Retrieved   June      

  Ferrie  C   Kaiser  S          Neural Networks for Babies  Sourcebooks  ISBN                        

  Mansfield Merriman   A List of Writings Relating to the Method of Least Squares 

  Stigler SM          Gauss and the Invention of Least Squares   Ann  Stat                  doi         aos            

  Bretscher O         Linear Algebra With Applications   rd      ed    Upper Saddle River  NJ  Prentice Hall 

  a b c d e f g h Schmidhuber J          Annotated History of Modern AI and Deep Learning   arXiv             cs NE  

  
Stigler SM         The History of Statistics  The Measurement of Uncertainty before       Cambridge  Harvard  ISBN                    

  a b McCulloch WS  Pitts W  December         A logical calculus of the ideas immanent in nervous activity   The Bulletin of Mathematical Biophysics                  doi         BF          ISSN                 Archived from the original on    October       Retrieved   August      

  Kleene S          Representation of Events in Nerve Nets and Finite Automata   Annals of Mathematics Studies  No           Princeton University Press  pp             Archived from the original on    May       Retrieved    June      

  Hebb D         The Organization of Behavior  New York  Wiley  ISBN                           cite book    ISBN   Date incompatibility  help 

  Farley B  W A  Clark          Simulation of Self Organizing Systems by Digital Computer   IRE Transactions on Information Theory                doi         TIT              

  Rochester N  J H  Holland  L H  Habit  W L  Duda          Tests on a cell assembly theory of the action of the brain  using a large digital computer   IRE Transactions on Information Theory                doi         TIT              

  Haykin        Neural Networks and Learning Machines   rd edition

  Rosenblatt F          The Perceptron  A Probabilistic Model For Information Storage And Organization in the Brain   Psychological Review                   CiteSeerX                       doi         h         PMID                S CID               

  Werbos P         Beyond Regression  New Tools for Prediction and Analysis in the Behavioral Sciences 

  Rosenblatt F          The Perceptron a perceiving and recognizing automaton   Report           Cornell Aeronautical Laboratory 

  Olazaran M          A Sociological Study of the Official History of the Perceptrons Controversy   Social Studies of Science                   doi                             JSTOR              S CID               

  a b Joseph RD         Contributions to Perceptron Theory  Cornell Aeronautical Laboratory Report No  VG        G    Buffalo 

  Russel  Stuart  Norvig  Peter         Artificial Intelligence A Modern Approach  PDF    rd      ed    United States of America  Pearson Education  pp              ISBN                        

  a b Rosenblatt F         Principles of Neurodynamics  Spartan  New York 

  Ivakhnenko AG  Lapa VG         Cybernetics and Forecasting Techniques  American Elsevier Publishing Co  ISBN                        

  Ivakhnenko A  March         Heuristic self organization in problems of engineering cybernetics   Automatica                  doi                               Archived from the original on    August       Retrieved   August      

  Ivakhnenko A          Polynomial theory of complex systems   PDF   IEEE Transactions on Systems  Man  and Cybernetics  SMC                 doi         TSMC               Archived  PDF  from the original on    August       Retrieved   November      

  Robbins H  Monro S          A Stochastic Approximation Method   The Annals of Mathematical Statistics               doi         aoms            

  Amari S          A theory of adaptive pattern classifier   IEEE Transactions  EC               

  Fukushima K          Visual feature extraction by a multilayered network of analog threshold elements   IEEE Transactions on Systems Science and Cybernetics                  doi         TSSC             

  Sonoda S  Murata N          Neural network with unbounded activation functions is universal approximator   Applied and Computational Harmonic Analysis                   arXiv             doi         j acha              S CID               

  Ramachandran P  Barret Z  Quoc VL     October         Searching for Activation Functions   arXiv             cs NE  

  Minsky M  Papert S         Perceptrons  An Introduction to Computational Geometry  MIT Press  ISBN                        

  Bozinovski S  and Fulgosi A           The influence of pattern similarity and transfer learning on the base perceptron training   original in Croatian  Proceedings of Symposium Informatica          Bled 

  Bozinovski S         Reminder of the first paper on transfer learning in neural networks         Informatica             

  a b Fukushima K          Neural network model for a mechanism of pattern recognition unaffected by shift in position Neocognitron   Trans  IECE  In Japanese   J   A                doi         bf          PMID               S CID                

  Fukushima K          Neocognitron  A self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position   Biol  Cybern                   doi         bf          PMID               S CID                

  a b c Schmidhuber J          Deep Learning in Neural Networks  An Overview   Neural Networks              arXiv            doi         j neunet              PMID                S CID               

  Leibniz GW         The Early Mathematical Manuscripts of Leibniz  Translated from the Latin Texts Published by Carl Immanuel Gerhardt with Critical and Historical Notes  Leibniz published the chain rule in a      memoir   Open court publishing Company  ISBN                       cite book    ISBN   Date incompatibility  help 

  Kelley HJ          Gradient theory of optimal flight paths   ARS Journal                    doi                

  Linnainmaa S         The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors  Masters   in Finnish   University of Helsinki  p           

  Linnainmaa S          Taylor expansion of the accumulated rounding error   BIT Numerical Mathematics                   doi         bf          S CID                

  Ostrovski  G M   Volin Y M   and Boris  W W          On the computation of derivatives  Wiss  Z  Tech  Hochschule for Chemistry             

  a b Schmidhuber J     October         Who Invented Backpropagation    IDSIA  Switzerland  Archived from the original on    July       Retrieved    September      

  Werbos P          Applications of advances in nonlinear sensitivity analysis   PDF   System modeling and optimization  Springer  pp                Archived  PDF  from the original on    April       Retrieved   July      

  Anderson JA  Rosenfeld E  eds          Talking Nets  An Oral History of Neural Networks  The MIT Press  doi         mitpress                ISBN                         Archived from the original on    October       Retrieved   August      

  Werbos PJ         The Roots of Backpropagation        From Ordered Derivatives to Neural Networks and Political Forecasting  New York  John Wiley  amp  Sons  ISBN                    

  Rumelhart DE  Hinton GE  Williams RJ  October         Learning representations by back propagating errors   Nature                       Bibcode     Natur         R  doi               a   ISSN                 Archived from the original on   March       Retrieved    March      

  Fukushima K  Miyake S    January         Neocognitron  A new algorithm for pattern recognition tolerant of deformations and shifts in position   Pattern Recognition                   Bibcode     PatRe         F  doi                               ISSN                 Archived from the original on    October       Retrieved   September      

  Waibel A  December        Phoneme Recognition Using Time Delay Neural Networks  PDF   Meeting of the Institute of Electrical  Information and Communication Engineers  IEICE   Tokyo  Japan  Archived  PDF  from the original on    September       Retrieved    September      

  Alexander Waibel et al   Phoneme Recognition Using Time Delay Neural Networks Archived    December      at the Wayback Machine IEEE Transactions on Acoustics  Speech  and Signal Processing  Volume     No     pp             March      

  Zhang W          Shift invariant pattern recognition neural network and its optical architecture   Proceedings of Annual Conference of the Japan Society of Applied Physics  Archived from the original on    June       Retrieved    April      

  LeCun et al    Backpropagation Applied to Handwritten Zip Code Recognition   Neural Computation     pp                

  Zhang W          Parallel distributed processing model with local space invariant interconnections and its optical architecture   Applied Optics                   Bibcode     ApOpt         Z  doi         AO            PMID                Archived from the original on   February       Retrieved    April      

  Zhang W          Image processing of human corneal endothelium based on a learning network   Applied Optics                   Bibcode     ApOpt         Z  doi         AO            PMID                Archived from the original on    June       Retrieved    September      

  Zhang W          Computerized detection of clustered microcalcifications in digital mammograms using a shift invariant artificial neural network   Medical Physics                  Bibcode     MedPh         Z  doi                   PMID               Archived from the original on    June       Retrieved    September      

  LeCun Y  L on Bottou  Yoshua Bengio  Patrick Haffner          Gradient based learning applied to document recognition   PDF   Proceedings of the IEEE                      CiteSeerX                      doi                   S CID                Archived from the original  PDF  on    October       Retrieved   October      

  Qian  Ning  and Terrence J  Sejnowski   Predicting the secondary structure of globular proteins using neural network models   Journal of molecular biology      no                    

  Bohr  Henrik  Jakob Bohr  S ren Brunak  Rodney MJ Cotterill  Benny Lautrup  Leif N rskov  Ole H  Olsen  and Steffen B  Petersen   Protein secondary structure and homology by neural networks The   helices in rhodopsin   FEBS letters                     

  Rost  Burkhard  and Chris Sander   Prediction of protein secondary structure at better than     accuracy   Journal of molecular biology      no                    

  Amari SI  November         Learning Patterns and Pattern Sequences by Self Organizing Nets of Threshold Elements   IEEE Transactions on Computers  C                     doi         T C              ISSN                 Archived from the original on    October       Retrieved   August      

  Hopfield JJ          Neural networks and physical systems with emergent collective computational abilities   Proceedings of the National Academy of Sciences                     Bibcode     PNAS          H  doi         pnas            PMC              PMID              

  Espinosa Sanchez JM  Gomez Marin A  de Castro F    July         The Importance of Cajal s and Lorente de N  s Neuroscience to the Birth of Cybernetics   The Neuroscientist  doi                            hdl               ISSN                 PMID                Archived from the original on    October       Retrieved   August      

   reverberating circuit   Oxford Reference  Archived from the original on    October       Retrieved    July      

  a b  Bozinovski  S           A self learning system using secondary reinforcement   In Trappl  Robert  ed    Cybernetics and Systems Research  Proceedings of the Sixth European Meeting on Cybernetics and Systems Research  North Holland  pp           ISBN                  

  Bozinovski S          Neuro genetic agents and structural theory of self reinforcement learning systems   CMPSCI Technical Report         University of Massachusetts at Amherst     Archived   October      at the Wayback Machine

  R  Zajonc         Feeling and thinking  Preferences need no inferences   American Psychologist                

  Lazarus R          Thoughts on the relations between emotion and cognition  American Psychologist                  

  Bozinovski  S          Modeling mechanisms of cognition emotion interaction in artificial neural networks  since       Procedia Computer Science p           https   core ac uk download pdf          pdf Archived    March      at the Wayback Machine 

  Schmidhuber J  April         Neural Sequence Chunkers   PDF   TR FKI      TU Munich  Archived  PDF  from the original on    September       Retrieved    September      

  Schmidhuber J          Learning complex  extended sequences using the principle of history compression  based on TR FKI              PDF   Neural Computation                  doi         neco               S CID                Archived  PDF  from the original on    September       Retrieved    September      

  Schmidhuber J         Habilitation thesis  System modeling and optimization  PDF   Archived  PDF  from the original on   August       Retrieved    September       Page     ff demonstrates credit assignment across the equivalent of       layers in an unfolded RNN 

  a b S  Hochreiter    Untersuchungen zu dynamischen neuronalen Netzen   Archived   March      at the Wayback Machine  Diploma thesis  Institut f  Informatik  Technische Univ  Munich  Advisor  J  Schmidhuber       

  Hochreiter S  et      al      January         Gradient flow in recurrent nets  the difficulty of learning long term dependencies   In Kolen JF  Kremer SC  eds    A Field Guide to Dynamical Recurrent Networks  John Wiley  amp  Sons  ISBN                         Archived from the original on    May       Retrieved    June      

  Sepp Hochreiter  J rgen Schmidhuber     August        Long Short Term Memory  Wikidata      Q        

  Hochreiter S  Schmidhuber J    November         Long Short Term Memory   Neural Computation                    doi         neco                PMID               S CID              

  Gers F  Schmidhuber J  Cummins F          Learning to forget  Continual prediction with LSTM    th International Conference on Artificial Neural Networks  ICANN      Vol             pp                doi         cp           ISBN                    

  Ackley DH  Hinton GE  Sejnowski TJ    January         A learning algorithm for boltzmann machines   Cognitive Science                  doi         S                      ISSN                 Archived from the original on    September       Retrieved   August      

  Smolensky P          Chapter    Information Processing in Dynamical Systems  Foundations of Harmony Theory   PDF   In Rumelhart DE  McLelland JL  eds    Parallel Distributed Processing  Explorations in the Microstructure of Cognition  Volume    Foundations  MIT Press  pp                ISBN                  X  Archived  PDF  from the original on    July       Retrieved   August      

  Peter D  Hinton GE  Neal RM  Zemel RS          The Helmholtz machine   Neural Computation                  doi         neco               hdl                    D D  E  PMID               S CID               

  Hinton GE  Dayan P  Frey BJ  Neal R     May         The wake sleep algorithm for unsupervised neural networks   Science                         Bibcode     Sci           H  doi         science          PMID               S CID             

       Kurzweil AI Interview Archived    August      at the Wayback Machine with Juergen Schmidhuber on the eight competitions won by his Deep Learning team          

   How bio inspired deep learning keeps winning competitions   KurzweilAI   kurzweilai net  Archived from the original on    August       Retrieved    June      

  Cire an DC  Meier U  Gambardella LM  Schmidhuber J     September         Deep  Big  Simple Neural Nets for Handwritten Digit Recognition   Neural Computation                      arXiv            doi         neco a        ISSN                 PMID                S CID              

  Ciresan DC  Meier U  Masci J  Gambardella L  Schmidhuber J          Flexible  High Performance Convolutional Neural Networks for Image Classification   PDF   International Joint Conference on Artificial Intelligence  doi                           ijcai        Archived  PDF  from the original on    September       Retrieved    June      

  Ciresan D  Giusti A  Gambardella LM  Schmidhuber J         Pereira F  Burges CJ  Bottou L  Weinberger KQ  eds    Advances in Neural Information Processing Systems     PDF   Curran Associates  Inc  pp                  Archived  PDF  from the original on   August       Retrieved    June      

  Ciresan D  Giusti A  Gambardella L  Schmidhuber J          Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks   Medical Image Computing and Computer Assisted Intervention   MICCAI       Lecture Notes in Computer Science  Vol             pp                doi                               ISBN                         PMID               

  Ciresan D  Meier U  Schmidhuber J          Multi column deep neural networks for image classification        IEEE Conference on Computer Vision and Pattern Recognition  pp                  arXiv            doi         cvpr               ISBN                         S CID              

  Krizhevsky A  Sutskever I  Hinton G          ImageNet Classification with Deep Convolutional Neural Networks   PDF   NIPS       Neural Information Processing Systems  Lake Tahoe  Nevada  Archived  PDF  from the original on    January       Retrieved    May      

  Simonyan K  Andrew Z          Very Deep Convolution Networks for Large Scale Image Recognition   arXiv            cs CV  

  Szegedy C          Going deeper with convolutions   PDF   Cvpr      arXiv            Archived  PDF  from the original on    September       Retrieved   August      

  Ng A  Dean J          Building High level Features Using Large Scale Unsupervised Learning   arXiv            cs LG  

  a b Billings SA         Nonlinear System Identification  NARMAX Methods in the Time  Frequency  and Spatio Temporal Domains  Wiley  ISBN                        

  a b Goodfellow I  Pouget Abadie J  Mirza M  Xu B  Warde Farley D  Ozair S  et      al          Generative Adversarial Networks  PDF   Proceedings of the International Conference on Neural Information Processing Systems  NIPS        pp                  Archived  PDF  from the original on    November       Retrieved    August      

  Schmidhuber J          A possibility for implementing curiosity and boredom in model building neural controllers   Proc  SAB       MIT Press Bradford Books  pp               

  Schmidhuber J          Generative Adversarial Networks are Special Cases of Artificial Curiosity        and also Closely Related to Predictability Minimization          Neural Networks              arXiv             doi         j neunet              PMID                S CID                

   GAN      NVIDIA s Hyperrealistic Face Generator   SyncedReview com     December       Archived from the original on    September       Retrieved   October      

  Karras T  Aila T  Laine S  Lehtinen J     February         Progressive Growing of GANs for Improved Quality  Stability  and Variation   arXiv             cs NE  

   Prepare  Don t Panic  Synthetic Media and Deepfakes   witness org  Archived from the original on   December       Retrieved    November      

  Sohl Dickstein J  Weiss E  Maheswaranathan N  Ganguli S    June         Deep Unsupervised Learning using Nonequilibrium Thermodynamics   PDF   Proceedings of the   nd International Conference on Machine Learning      PMLR             arXiv             Archived  PDF  from the original on    September       Retrieved   August      

  Simonyan K  Zisserman A     April        Very Deep Convolutional Networks for Large Scale Image Recognition  arXiv          

  He K  Zhang X  Ren S  Sun J          Delving Deep into Rectifiers  Surpassing Human Level Performance on ImageNet Classification   arXiv             cs CV  

  He K  Zhang X  Ren S  Sun J     December        Deep Residual Learning for Image Recognition  arXiv            

  Srivastava RK  Greff K  Schmidhuber J    May         Highway Networks   arXiv             cs LG  

  He K  Zhang X  Ren S  Sun J         Deep Residual Learning for Image Recognition       IEEE Conference on Computer Vision and Pattern Recognition  CVPR   Las Vegas  NV  USA  IEEE  pp                arXiv             doi         CVPR          ISBN                         Archived from the original on   October       Retrieved    April      

  Linn A     December         Microsoft researchers win ImageNet computer vision challenge   The AI Blog  Archived from the original on    May       Retrieved    June      

  Vaswani A  Shazeer N  Parmar N  Uszkoreit J  Jones L  Gomez AN  et      al      June         Attention Is All You Need   arXiv             cs CL  

  Schmidhuber J          Learning to control fast weight memories  an alternative to recurrent nets   PDF   Neural Computation                  doi         neco               S CID               

  Katharopoulos A  Vyas A  Pappas N  Fleuret F          Transformers are RNNs  Fast autoregressive Transformers with linear attention   ICML       PMLR  pp                  Archived from the original on    July       Retrieved    September      

  Schlag I  Irie K  Schmidhuber J          Linear Transformers Are Secretly Fast Weight Programmers   ICML       Springer  pp                 

  Wolf T  Debut L  Sanh V  Chaumond J  Delangue C  Moi A  et      al           Transformers  State of the Art Natural Language Processing   Proceedings of the      Conference on Empirical Methods in Natural Language Processing  System Demonstrations  pp              doi          v       emnlp demos    S CID                

  a b Zell A          chapter       Simulation neuronaler Netze      Simulation of Neural Networks       in German    st      ed    Addison Wesley  ISBN                         OCLC                

  Artificial intelligence   rd      ed    Addison Wesley Pub  Co        ISBN                    

  Abbod MF          Application of Artificial Intelligence to the Management of Urological Cancer   The Journal of Urology                      doi         j juro              PMID               

  Dawson CW          An artificial neural network approach to rainfall runoff modelling   Hydrological Sciences Journal                 Bibcode     HydSJ         D  doi                           

   The Machine Learning Dictionary   cse unsw edu au  Archived from the original on    August       Retrieved   November      

  Ciresan D  Ueli Meier  Jonathan Masci  Luca M  Gambardella  Jurgen Schmidhuber          Flexible  High Performance Convolutional Neural Networks for Image Classification   PDF   Proceedings of the Twenty Second International Joint Conference on Artificial Intelligence Volume Volume Two                Archived  PDF  from the original on   April       Retrieved   July      

  Zell A         Simulation Neuronaler Netze      Simulation of Neural Networks       in German    st      ed    Addison Wesley  p           ISBN                    

  Miljanovic M  February March         Comparative analysis of Recurrent and Finite Impulse Response Neural Networks in Time Series Prediction   PDF   Indian Journal of Computer and Engineering         Archived  PDF  from the original on    May       Retrieved    August      

  Kelleher JD  Mac Namee B  D Arcy A                Fundamentals of machine learning for predictive data analytics  algorithms  worked examples  and case studies   nd      ed    Cambridge  MA  The MIT Press  ISBN                         OCLC                 

  Wei J     April         Forget the Learning Rate  Decay Loss   arXiv             cs LG  

  Li Y  Fu Y  Li H  Zhang SW    June         The Improved Training Algorithm of Back Propagation Neural Network with Self adaptive Learning Rate        International Conference on Computational Intelligence and Natural Computing  Vol          pp              doi         CINC           ISBN                         S CID               

  Huang GB  Zhu QY  Siew CK          Extreme learning machine  theory and applications   Neurocomputing                   CiteSeerX                       doi         j neucom              S CID             

  Widrow B  et      al           The no prop algorithm  A new learning algorithm for multilayer neural networks   Neural Networks               doi         j neunet              PMID               

  Ollivier Y  Charpiat G          Training recurrent networks without backtracking   arXiv             cs NE  

  Hinton GE          A Practical Guide to Training Restricted Boltzmann Machines   Tech  Rep  UTML TR           Archived from the original on   May       Retrieved    June      

  ESANN            full citation needed     

  Bernard E         Introduction to machine learning  Champaign  Wolfram Media  p          ISBN                         Archived from the original on    May       Retrieved    March      

  Bernard E         Introduction to machine learning  Champaign  Wolfram Media  p           ISBN                         Archived from the original on    May       Retrieved    March      

  Bernard E         Introduction to Machine Learning  Wolfram Media Inc  p          ISBN                         Archived from the original on    May       Retrieved    July      

  Ojha VK  Abraham A  Sn  el V    April         Metaheuristic design of feedforward neural networks  A review of two decades of research   Engineering Applications of Artificial Intelligence              arXiv             Bibcode     arXiv         O  doi         j engappai              S CID               

  Dominic  S   Das  R   Whitley  D   Anderson  C   July         Genetic reinforcement learning for neural networks   IJCNN    Seattle International Joint Conference on Neural Networks  IJCNN    Seattle International Joint Conference on Neural Networks  Seattle  Washington  US  IEEE  pp              doi         IJCNN              ISBN                    

  Hoskins J  Himmelblau  D M           Process control via artificial neural networks and reinforcement learning   Computers  amp  Chemical Engineering                   doi                            B 

  Bertsekas D  Tsitsiklis J         Neuro dynamic programming  Athena Scientific  p            ISBN                         Archived from the original on    June       Retrieved    June      

  Secomandi N          Comparing neuro dynamic programming algorithms for the vehicle routing problem with stochastic demands   Computers  amp  Operations Research                         CiteSeerX                       doi         S                   X 

  de Rigo  D   Rizzoli  A  E   Soncini Sessa  R   Weber  E   Zenesi  P           Neuro dynamic programming for the efficient management of reservoir networks   Proceedings of MODSIM       International Congress on Modelling and Simulation  MODSIM       International Congress on Modelling and Simulation  Canberra  Australia  Modelling and Simulation Society of Australia and New Zealand  doi         zenodo       ISBN                     Archived from the original on   August       Retrieved    July      

  Damas  M   Salmeron  M   Diaz  A   Ortega  J   Prieto  A   Olivares  G           Genetic algorithms and neuro dynamic programming  application to water supply networks   Proceedings of      Congress on Evolutionary Computation       Congress on Evolutionary Computation  Vol          La Jolla  California  US  IEEE  pp             doi         CEC              ISBN                    

  Deng G  Ferris  M C           Neuro dynamic programming for fractionated radiotherapy planning   Optimization in Medicine  Springer Optimization and Its Applications  Vol           pp              CiteSeerX                       doi                              ISBN                        

  Bozinovski  S           A self learning system using secondary reinforcement   In R  Trappl  ed   Cybernetics and Systems Research  Proceedings of the Sixth European Meeting on Cybernetics and Systems Research  North Holland  pp           ISBN                        

  Bozinovski  S          Modeling mechanisms of cognition emotion interaction in artificial neural networks  since      Archived    March      at the Wayback Machine   Procedia Computer Science p         

  Bozinovski S  Bozinovska L          Self learning agents  A connectionist theory of emotion based on crossbar value judgment   Cybernetics and Systems                   doi                         S CID              

  Salimans T  Ho J  Chen X  Sidor S  Sutskever I    September         Evolution Strategies as a Scalable Alternative to Reinforcement Learning   arXiv             stat ML  

  Such FP  Madhavan V  Conti E  Lehman J  Stanley KO  Clune J     April         Deep Neuroevolution  Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning   arXiv             cs NE  

   Artificial intelligence can  evolve  to solve problems   Science   AAAS     January       Archived from the original on   December       Retrieved   February      

  Turchetti C         Stochastic Models of Neural Networks  Frontiers in artificial intelligence and applications  Knowledge based intelligent engineering systems  vol            IOS Press  ISBN                       

  Jospin LV  Laga H  Boussaid F  Buntine W  Bennamoun M          Hands On Bayesian Neural Networks A Tutorial for Deep Learning Users   IEEE Computational Intelligence Magazine  Vol           no          pp              arXiv             doi         mci               ISSN              X  S CID                

  Cang Z  Wei GW     July         TopologyNet  Topology based deep convolutional and multi task neural networks for biomolecular property predictions   PLOS Computational Biology          e         arXiv             Bibcode     PLSCB    E    C  doi         journal pcbi          ISSN                 PMC               PMID               

  de Rigo  D   Castelletti  A   Rizzoli  A  E   Soncini Sessa  R   Weber  E   January         A selective improvement technique for fastening Neuro Dynamic Programming in Water Resources Network Management   In Pavel Z tek  ed    Proceedings of the   th IFAC World Congress   IFAC PapersOnLine    th IFAC World Congress  Vol           Prague  Czech Republic  IFAC  pp             doi                    CZ             hdl               ISBN                         Archived from the original on    April       Retrieved    December      

  Ferreira C          Designing Neural Networks Using Gene Expression Programming   In A  Abraham  B  de Baets  M  K ppen  B  Nickolay  eds    Applied Soft Computing Technologies  The Challenge of Complexity  PDF   Springer Verlag  pp                Archived  PDF  from the original on    December       Retrieved   October      

  Da  Y   Xiurun  G   July         An improved PSO based ANN with simulated annealing technique   In T  Villmann  ed    New Aspects in Neurocomputing    th European Symposium on Artificial Neural Networks  Vol           Elsevier  pp                doi         j neucom              Archived from the original on    April       Retrieved    December      

  Wu  J   Chen  E   May         A Novel Nonparametric Regression Ensemble for Rainfall Forecasting Using Particle Swarm Optimization Technique Coupled with Artificial Neural Network   In Wang  H   Shen  Y   Huang  T   Zeng  Z   eds     th International Symposium on Neural Networks  ISNN       Lecture Notes in Computer Science  Vol             Springer  pp              doi                              ISBN                         Archived from the original on    December       Retrieved   January      

  a b Ting Qin  Zonghai Chen  Haitao Zhang  Sifu Li  Wei Xiang  Ming Li          A learning algorithm of CMAC based on RLS   PDF   Neural Processing Letters                 doi         B NEPL                      S CID               Archived  PDF  from the original on    April       Retrieved    January      

  Ting Qin  Haitao Zhang  Zonghai Chen  Wei Xiang          Continuous CMAC QRLS and its systolic array   PDF   Neural Processing Letters                doi         s                  S CID                Archived  PDF  from the original on    November       Retrieved    January      

  LeCun Y  Boser B  Denker JS  Henderson D  Howard RE  Hubbard W  et      al           Backpropagation Applied to Handwritten Zip Code Recognition   Neural Computation                  doi         neco               S CID               

  Yann LeCun         Slides on Deep Learning Online Archived    April      at the Wayback Machine

  Hochreiter S  Schmidhuber J    November         Long Short Term Memory   Neural Computation                    doi         neco                ISSN                 PMID               S CID              

  Sak H  Senior A  Beaufays F          Long Short Term Memory recurrent neural network architectures for large scale acoustic modeling   PDF   Archived from the original  PDF  on    April      

  Li X  Wu X     October         Constructing Long Short Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition   arXiv            cs CL  

  Fan Y  Qian Y  Xie F  Soong FK          TTS synthesis with bidirectional LSTM based Recurrent Neural Networks   Proceedings of the Annual Conference of the International Speech Communication Association  Interspeech             Retrieved    June      

  Schmidhuber J          Deep Learning   Scholarpedia                   Bibcode     SchpJ         S  doi         scholarpedia       

  Zen H  Sak H          Unidirectional Long Short Term Memory Recurrent Neural Network with Recurrent Output Layer for Low Latency Speech Synthesis   PDF   Google com  ICASSP  pp                  Archived  PDF  from the original on   May       Retrieved    June      

  Fan B  Wang L  Soong FK  Xie L          Photo Real Talking Head with Deep Bidirectional LSTM   PDF   Proceedings of ICASSP  Archived  PDF  from the original on   November       Retrieved    June      

  Silver D  Hubert T  Schrittwieser J  Antonoglou I  Lai M  Guez A  et      al     December         Mastering Chess and Shogi by Self Play with a General Reinforcement Learning Algorithm   arXiv             cs AI  

  Probst P  Boulesteix AL  Bischl B     February         Tunability  Importance of Hyperparameters of Machine Learning Algorithms   J  Mach  Learn  Res                  S CID               

  Zoph B  Le QV    November         Neural Architecture Search with Reinforcement Learning   arXiv             cs LG  

  Haifeng Jin  Qingquan Song  Xia Hu          Auto keras  An efficient neural architecture search system   Proceedings of the   th ACM SIGKDD International Conference on Knowledge Discovery  amp  Data Mining  ACM  arXiv             Archived from the original on    August       Retrieved    August              via autokeras com 

  Claesen M  De Moor B          Hyperparameter Search in Machine Learning   arXiv             cs LG   Bibcode     arXiv         C

  Esch R          Functional Approximation   Handbook of Applied Mathematics  Springer US      ed    Boston  MA  Springer US  pp                doi                               ISBN                        

  Sarstedt M  Moo E          Regression Analysis   A Concise Guide to Market Research  Springer Texts in Business and Economics  Springer Berlin Heidelberg  pp                doi                              ISBN                         S CID                 Archived from the original on    March       Retrieved    March      

  Tian J  Tan Y  Sun C  Zeng J  Jin Y  December         A self adaptive similarity based fitness approximation for evolutionary optimization        IEEE Symposium Series on Computational Intelligence  SSCI   pp            doi         SSCI               ISBN                         S CID                Archived from the original on    May       Retrieved    March      

  Alaloul WS  Qureshi AH          Data Processing Using Artificial Neural Networks   Dynamic Data Assimilation   Beating the Uncertainties  doi         intechopen        ISBN                         S CID                 Archived from the original on    March       Retrieved    March      

  Pal M  Roy R  Basu J  Bepari MS          Blind source separation  A review and analysis        International Conference Oriental COCOSDA held jointly with      Conference on Asian Spoken Language Research and Evaluation  O COCOSDA CASLRE   IEEE  pp            doi         ICSDA               ISBN                         S CID                Archived from the original on    March       Retrieved    March      

  Zissis D  October         A cloud based architecture capable of perceiving and predicting multiple vessel behaviour   Applied Soft Computing               doi         j asoc              Archived from the original on    July       Retrieved    July      

  Sengupta N  Sahidullah  Md  Saha  Goutam  August         Lung sound classification using cepstral based statistical features   Computers in Biology and Medicine                   doi         j compbiomed              PMID               

  Choy  Christopher B   et al    d r n   A unified approach for single and multi view  d object reconstruction Archived    July      at the Wayback Machine   European conference on computer vision  Springer  Cham       

  Turek  Fred D   March         Introduction to Neural Net Machine Vision   Vision Systems Design          Archived from the original on    May       Retrieved   March      

  Maitra DS  Bhattacharya U  Parui SK  August         CNN based common approach to handwritten character recognition of multiple scripts          th International Conference on Document Analysis and Recognition  ICDAR   pp                  doi         ICDAR               ISBN                         S CID                Archived from the original on    October       Retrieved    March      

  Gessler J  August         Sensor for food analysis applying impedance spectroscopy and artificial neural networks   RiuNet UPV            Archived from the original on    October       Retrieved    October      

  French J          The time traveller s CAPM   Investment Analysts Journal                 doi                                S CID                

  Roman M  Balabin  Ekaterina I  Lomakina          Neural network approach to quantum chemistry data  Accurate prediction of density functional theory energies   J  Chem  Phys                   Bibcode     JChPh    g    B  doi                    PMID               

  Silver D  et      al           Mastering the game of Go with deep neural networks and tree search   PDF   Nature                       Bibcode     Natur         S  doi         nature       PMID                S CID              Archived  PDF  from the original on    November       Retrieved    January      

  Pasick A     March         Artificial Intelligence Glossary  Neural Networks and Other Terms Explained   The New York Times  ISSN                 Archived from the original on   September       Retrieved    April      

  Schechner S     June         Facebook Boosts A I  to Block Terrorist Propaganda   The Wall Street Journal  ISSN                 Archived from the original on    May       Retrieved    June      

  Ciaramella A  Ciaramella M         Introduction to Artificial Intelligence  from data analysis to generative AI  Intellisemantic Editions  ISBN                        

  Ganesan N          Application of Neural Networks in Diagnosing Cancer Disease Using Demographic Data   International Journal of Computer Applications                 Bibcode     IJCA     z    G  doi                 

  Bottaci L          Artificial Neural Networks Applied to Outcome Prediction for Colorectal Cancer Patients in Separate Institutions   PDF   Lancet              The Lancet          doi         S                   X  PMID               S CID                Archived from the original  PDF  on    November       Retrieved   May      

  Alizadeh E  Lyons SM  Castle JM  Prasad A          Measuring systematic changes in invasive cancer cell shape using Zernike moments   Integrative Biology                     doi         C IB     A  PMID                Archived from the original on    May       Retrieved    March      

  Lyons S          Changes in cell shape are correlated with metastatic potential in murine   Biology Open                  doi         bio         PMC               PMID               

  Nabian MA  Meidani H     August         Deep Learning for Accelerated Reliability Analysis of Infrastructure Networks   Computer Aided Civil and Infrastructure Engineering                   arXiv             Bibcode     arXiv         N  doi         mice        S CID               

  Nabian MA  Meidani H          Accelerating Stochastic Assessment of Post Earthquake Transportation Network Connectivity via Machine Learning Based Surrogates   Transportation Research Board   th Annual Meeting  Archived from the original on   March       Retrieved    March      

  D az E  Brotons V  Tom s R  September         Use of artificial neural networks to predict   D elastic settlement of foundations on soils with inclined bedrock   Soils and Foundations                     Bibcode     SoFou         D  doi         j sandf              hdl              ISSN                

  Tayebiyan A  Mohammad TA  Ghazali AH  Mashohor S   Artificial Neural Network for Modelling Rainfall Runoff   Pertanika Journal of Science  amp  Technology                   Archived from the original on    May       Retrieved    May      

  Govindaraju RS    April         Artificial Neural Networks in Hydrology  I  Preliminary Concepts   Journal of Hydrologic Engineering                  doi          ASCE                         

  Govindaraju RS    April         Artificial Neural Networks in Hydrology  II  Hydrologic Applications   Journal of Hydrologic Engineering                  doi          ASCE                         

  Peres DJ  Iuppa C  Cavallaro L  Cancelliere A  Foti E    October         Significant wave height record extension by neural networks and reanalysis wind data   Ocean Modelling               Bibcode     OcMod         P  doi         j ocemod             

  Dwarakish GS  Rakshith S  Natesan U          Review on Applications of Neural Network in Coastal Engineering   Artificial Intelligent Systems and Machine Learning                  Archived from the original on    August       Retrieved   July      

  Ermini L  Catani F  Casagli N    March         Artificial Neural Networks applied to landslide susceptibility assessment   Geomorphology  Geomorphological hazard and human impact in mountain environments                   Bibcode     Geomo         E  doi         j geomorph             

  Nix R  Zhang J  May         Classification of Android apps and malware using deep neural networks        International Joint Conference on Neural Networks  IJCNN   pp                  doi         IJCNN               ISBN                         S CID              

   Detecting Malicious URLs   The systems and networking group at UCSD  Archived from the original on    July       Retrieved    February      

  Homayoun S  Ahmadzadeh M  Hashemi S  Dehghantanha A  Khayami R         Dehghantanha A  Conti M  Dargahi T  eds     BoTShark  A Deep Learning Approach for Botnet Traffic Detection   Cyber Threat Intelligence  Advances in Information Security  vol           Springer International Publishing  pp                doi                              ISBN                       

  Ghosh  Reilly  January         Credit card fraud detection with a neural network   Proceedings of the Twenty Seventh Hawaii International Conference on System Sciences HICSS     Vol          pp                doi         HICSS              ISBN                         S CID               

  Ananthaswamy A     April         Latest Neural Nets Solve World s Hardest Equations Faster Than Ever Before   Quanta Magazine  Archived from the original on    May       Retrieved    May      

   AI has cracked a key mathematical puzzle for understanding our world   MIT Technology Review  Archived from the original on    May       Retrieved    November      

   Caltech Open Sources AI for Solving Partial Differential Equations   InfoQ  Archived from the original on    January       Retrieved    January      

  Nagy A     June         Variational Quantum Monte Carlo Method with a Neural Network Ansatz for Open Quantum Systems   Physical Review Letters                    arXiv             Bibcode     PhRvL    y    N  doi         PhysRevLett             PMID                S CID                

  Yoshioka N  Hamazaki R     June         Constructing neural stationary states for open quantum many body systems   Physical Review B                   arXiv             Bibcode     PhRvB    u    Y  doi         PhysRevB            S CID                

  Hartmann MJ  Carleo G     June         Neural Network Approach to Dissipative Quantum Many Body Dynamics   Physical Review Letters                    arXiv             Bibcode     PhRvL    y    H  doi         PhysRevLett             PMID                S CID                

  Vicentini F  Biella A  Regnault N  Ciuti C     June         Variational Neural Network Ansatz for Steady States in Open Quantum Systems   Physical Review Letters                    arXiv             Bibcode     PhRvL    y    V  doi         PhysRevLett             PMID                S CID                

  Forrest MD  April         Simulation of alcohol action upon a detailed Purkinje neuron model and a simpler surrogate model that runs  gt     times faster   BMC Neuroscience               doi         s                  PMC               PMID               

  Wieczorek S  Filipiak D  Filipowska A          Semantic Image Based Profiling of Users  Interests with Neural Networks   Studies on the Semantic Web      Emerging Topics in Semantic Technologies   doi                                Archived from the original on    May       Retrieved    January      

  Merchant A  Batzner S  Schoenholz SS  Aykol M  Cheon G  Cubuk ED  December         Scaling deep learning for materials discovery   Nature                     Bibcode     Natur         M  doi         s                   ISSN                 PMC                PMID               

  Siegelmann H  Sontag E          Turing computability with neural nets   PDF   Appl  Math  Lett                doi                            F  Archived  PDF  from the original on    May       Retrieved    January      

  Bains S    November         Analog computer trumps Turing model   EE Times  Archived from the original on    May       Retrieved    May      

  Balc zar J  July         Computational Power of Neural Networks  A Kolmogorov Complexity Characterization   IEEE Transactions on Information Theory                     CiteSeerX                       doi                   

  a b MacKay DJ         Information Theory  Inference  and Learning Algorithms  PDF   Cambridge University Press  ISBN                         Archived  PDF  from the original on    October       Retrieved    June      

  Cover T          Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition   PDF   IEEE Transactions on Electronic Computers  EC         IEEE           doi         PGEC              Archived  PDF  from the original on   March       Retrieved    March      

  Gerald F          Reproducibility and Experimental Design for Machine Learning on Audio and Multimedia Data   Proceedings of the   th ACM International Conference on Multimedia  ACM  pp                  doi                          ISBN                         S CID                

   Stop tinkering  start measuring  Predictable experimental design of Neural Network experiments   The Tensorflow Meter  Archived from the original on    April       Retrieved    March      

  Lee J  Xiao L  Schoenholz SS  Bahri Y  Novak R  Sohl Dickstein J  et      al           Wide neural networks of any depth evolve as linear models under gradient descent   Journal of Statistical Mechanics  Theory and Experiment                     arXiv             Bibcode     JSMTE    l    L  doi                   abc  b  S CID               

  Arthur Jacot  Franck Gabriel  Clement Hongler         Neural Tangent Kernel  Convergence and Generalization in Neural Networks  PDF     nd Conference on Neural Information Processing Systems  NeurIPS        Montreal  Canada  Archived  PDF  from the original on    June       Retrieved   June      

  Xu ZJ  Zhang Y  Xiao Y          Training Behavior of Deep Neural Network in Frequency Domain   In Gedeon T  Wong K  Lee M  eds    Neural Information Processing  Lecture Notes in Computer Science  Vol              Springer  Cham  pp                arXiv             doi                               ISBN                         S CID               

  Nasim Rahaman  Aristide Baratin  Devansh Arpit  Felix Draxler  Min Lin  Fred Hamprecht  et      al           On the Spectral Bias of Neural Networks   PDF   Proceedings of the   th International Conference on Machine Learning                 arXiv             Archived  PDF  from the original on    October       Retrieved   June      

  Zhi Qin John Xu  Yaoyu Zhang  Tao Luo  Yanyang Xiao  Zheng Ma          Frequency Principle  Fourier Analysis Sheds Light on Deep Neural Networks   Communications in Computational Physics                     arXiv             Bibcode     CCoPh         X  doi         cicp OA            S CID               

  Tao Luo  Zheng Ma  Zhi Qin John Xu  Yaoyu Zhang          Theory of the Frequency Principle for General Deep Neural Networks   arXiv             cs LG  

  Xu ZJ  Zhou H     May         Deep Frequency Principle Towards Understanding Why Deeper Learning is Faster   Proceedings of the AAAI Conference on Artificial Intelligence                        arXiv             doi         aaai v  i          ISSN                 S CID                 Archived from the original on   October       Retrieved   October      

  Parisi GI  Kemker R  Part JL  Kanan C  Wermter S    May         Continual lifelong learning with neural networks  A review   Neural Networks              arXiv             doi         j neunet              ISSN                 PMID               

  Dean Pomerleau   Knowledge based Training of Artificial Neural Networks for Autonomous Robot Driving 

  Dewdney AK    April        Yes  we have no neutrons  an eye opening tour through the twists and turns of bad science  Wiley  p           ISBN                        

  NASA   Dryden Flight Research Center   News Room  News Releases  NASA NEURAL NETWORK PROJECT PASSES MILESTONE Archived   April      at the Wayback Machine  Nasa gov  Retrieved on    November      

   Roger Bridgman s defence of neural networks   Archived from the original on    March       Retrieved    July      

   Scaling Learning Algorithms towards  AI    LISA   Publications   Aigaion       iro umontreal ca 

  D  J  Felleman and D  C  Van Essen   Distributed hierarchical processing in the primate cerebral cortex   Cerebral Cortex     pp             

  J  Weng   Natural and Artificial Intelligence  Introduction to Computational Brain Mind Archived    May      at the Wayback Machine   BMI Press  ISBN                              

  a b Edwards C     June         Growing pains for deep learning   Communications of the ACM                 doi                  S CID               

   The Bitter Lesson   incompleteideas net  Retrieved   August      

  Cade Metz     May         Google Built Its Very Own Chips to Power Its AI Bots   Wired  Archived from the original on    January       Retrieved   March      

   Scaling Learning Algorithms towards AI   PDF   Archived  PDF  from the original on    August       Retrieved   July      

  Tahmasebi  Hezarkhani          A hybrid neural networks fuzzy logic genetic algorithm for grade estimation   Computers  amp  Geosciences             Bibcode     CG            T  doi         j cageo              PMC               PMID               

  Sun and Bookman      

  a b Norori N  Hu Q  Aellen FM  Faraci FD  Tzovara A  October         Addressing bias in big data and AI for health care  A call for open science   Patterns                  doi         j patter              PMC               PMID               

  a b Carina W     October         Failing at Face Value  The Effect of Biased Facial Recognition Technology on Racial Discrimination in Criminal Justice   Scientific and Social Research                 doi          ssr v i         ISSN                

  a b Chang X     September         Gender Bias in Hiring  An Analysis of the Impact of Amazon s Recruiting Algorithm   Advances in Economics  Management and Political Sciences                   doi                                 ISSN                 Archived from the original on   December       Retrieved   December      

  Kortylewski A  Egger B  Schneider A  Gerig T  Morel Forster A  Vetter T  June         Analyzing and Reducing the Damage of Dataset Bias to Face Recognition with Synthetic Data        IEEE CVF Conference on Computer Vision and Pattern Recognition Workshops  CVPRW   PDF   IEEE  pp                  doi         cvprw             ISBN                         S CID                 Archived  PDF  from the original on    May       Retrieved    December      

  a b c d e f Huang Y          Advances in Artificial Neural Networks   Methodological Development and Application   Algorithms                   doi         algor         ISSN                

  a b c d e Kariri E  Louati H  Louati A  Masmoudi F          Exploring the Advancements and Future Research Directions of Artificial Neural Networks  A Text Mining Approach   Applied Sciences                doi         app          ISSN                

  a b Fui Hoon Nah F  Zheng R  Cai J  Siau K  Chen L    July         Generative AI and ChatGPT  Applications  challenges  and AI human collaboration   Journal of Information Technology Case and Application Research                   doi                                ISSN                

   DALL E   s Failures Are the Most Interesting Thing About It   IEEE Spectrum   IEEE  Archived from the original on    July       Retrieved   December      

  Briot JP  January         From artificial neural networks to deep learning for music generation  history  concepts and trends   Neural Computing and Applications                 doi         s                   ISSN                

  Chow PS    July         Ghost in the  Hollywood  machine  Emergent applications of artificial intelligence in the film industry   NECSUS European Journal of Media Studies  doi          MEDIAREP        ISSN                

  Yu X  He S  Gao Y  Yang J  Sha L  Zhang Y  et      al   June         Dynamic difficulty adjustment of game AI for video game Dead End   The  rd International Conference on Information Sciences and Interaction Sciences  IEEE  pp                doi         icicis               ISBN                         S CID               


Bibliography edit 

Bhadeshia H  K  D  H           Neural Networks in Materials Science   PDF   ISIJ International                    doi         isijinternational        
Bishop CM         Neural networks for pattern recognition  Clarendon Press  ISBN                         OCLC               
Borgelt C         Neuro Fuzzy Systeme  von den Grundlagen k nstlicher Neuronaler Netze zur Kopplung mit Fuzzy Systemen  Vieweg  ISBN                         OCLC               
Cybenko G          Approximation by Superpositions of a Sigmoidal function   In van Schuppen JH  ed    Mathematics of Control  Signals  and Systems  Springer International  pp                PDF
Dewdney AK         Yes  we have no neutrons  an eye opening tour through the twists and turns of bad science  New York  Wiley  ISBN                         OCLC               
Duda RO  Hart PE  Stork DG         Pattern classification         ed    Wiley  ISBN                         OCLC               
Egmont Petersen M  de Ridder D  Handels H          Image processing with neural networks   a review   Pattern Recognition                      CiteSeerX                      doi         S                     
Fahlman S  Lebiere C          The Cascade Correlation Learning Architecture   PDF   Archived from the original  PDF  on   May       Retrieved    August      
created for National Science Foundation  Contract Number EET          and Defense Advanced Research Projects Agency  DOD   ARPA Order No       under Contract F         C      
Gurney K         An introduction to neural networks  UCL Press  ISBN                         OCLC               
Haykin SS         Neural networks  a comprehensive foundation  Prentice Hall  ISBN                         OCLC               
Hertz J  Palmer RG  Krogh AS         Introduction to the theory of neural computation  Addison Wesley  ISBN                         OCLC               
Information theory  inference  and learning algorithms  Cambridge University Press     September       Bibcode     itil book     M  ISBN                         OCLC               
Kruse R  Borgelt C  Klawonn F  Moewes C  Steinbrecher M  Held P         Computational intelligence  a methodological introduction  Springer  ISBN                         OCLC                
Lawrence J         Introduction to neural networks  design  theory and applications  California Scientific Software  ISBN                         OCLC               
Masters T         Signal and image processing with neural networks  a C   sourcebook  J  Wiley  ISBN                         OCLC               
Maurer H         Cognitive science  integrative synchronization mechanisms in cognitive neuroarchitectures of the modern connectionism  CRC Press  doi                        ISBN                         S CID                
Ripley BD         Pattern Recognition and Neural Networks  Cambridge University Press  ISBN                        
Siegelmann H  Sontag ED          Analog computation via neural networks   Theoretical Computer Science                    doi                               S CID              
Smith M         Neural networks for statistical modeling  Van Nostrand Reinhold  ISBN                         OCLC               
Wasserman PD         Advanced methods in neural computing  Van Nostrand Reinhold  ISBN                         OCLC               
Wilson H         Artificial intelligence  Grey House Publishing  ISBN                        

External links edit 
Listen to this article     minutes 
This audio file was created from a revision of this article dated         November                              and does not reflect subsequent edits  Audio help        More spoken articles 
A Brief Introduction to Neural Networks  D  Kriesel    Illustrated  bilingual manuscript about artificial neural networks  Topics so far  Perceptrons  Backpropagation  Radial Basis Functions  Recurrent Neural Networks  Self Organizing Maps  Hopfield Networks 
Review of Neural Networks in Materials Science Archived   June      at the Wayback Machine
Artificial Neural Networks Tutorial in three languages  Univ  Polit cnica de Madrid 
Another introduction to ANN
Next Generation of Neural Networks Archived    January      at the Wayback Machine   Google Tech Talks
Performance of Neural Networks
Neural Networks and Information Archived   July      at the Wayback Machine
Sanderson G    October         But what is a Neural Network     Blue Brown  Archived from the original on   November              via YouTube 
Links to related articles
vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects

vteComplex systemsBackground
Emergence
Self organization
Collective behavior
Social dynamics
Collective intelligence
Collective action
Collective consciousness
Self organized criticality
Herd mentality
Phase transition
Agent based modelling
Synchronization
Ant colony optimization
Particle swarm optimization
Swarm behaviour
Evolution and adaptation
Artificial neural network
Evolutionary computation
Genetic algorithms
Genetic programming
Artificial life
Machine learning
Evolutionary developmental biology
Artificial intelligence
Evolutionary robotics
Evolvability
Game theory
Prisoner s dilemma
Rational choice theory
Bounded rationality
Evolutionary game theory
Networks
Social network analysis
Small world networks
Centrality
Motifs
Graph theory
Scaling
Robustness
Systems biology
Dynamic networks
Adaptive networks
Nonlinear dynamics
Time series analysis
Ordinary differential equations
Phase space
Attractor
Population dynamics
Chaos
Multistability
Bifurcation
Coupled map lattices
Pattern formation
Reaction diffusion systems
Partial differential equations
Dissipative structures
Percolation
Cellular automata
Spatial ecology
Self replication
Geomorphology
Systems theory
Homeostasis
Operationalization
Feedback
Self reference
Goal oriented
System dynamics
Sensemaking
Entropy
Cybernetics
Autopoiesis
Information theory
Computation theory

vteControl theoryBranches
Adaptive control
Control theory
Digital control
Energy shaping control
Fuzzy control
Hybrid control
Intelligent control
Model predictive control
Multivariable control
Neural control
Nonlinear control
Optimal control
Real time control
Robust control
Stochastic control
System properties
Bode plot
Block diagram
Closed loop transfer function
Controllability
Fourier transform
Frequency response
Laplace transform
Negative feedback
Observability
Performance
Positive feedback
Root locus ethod
Servomechanism
Signal flow graph
State space representation
Stability theory
Steady state analysis  amp  design
System dynamics
Transfer function
Digital control
Discrete time signal
Digital signal processing
Quantization
Real time software
Sampled data
System identification
Z transform
Advanced techniques
Artificial neural network
Coefficient diagram method
Control reconfiguration
Distributed parameter systems
Fractional order control
Fuzzy logic
H infinity loop shaping
Hankel singular value
Kalman filter
Krener s theorem
Least squares
Lyapunov stability
Minor loop feedback
Perceptual control theory
State observer
Vector control
Controllers
Embedded controller
Closed loop controller
Lead lag compensator
Numerical control
PID controller
Programmable logic controller
Control applications
Automation and Remote Control
Distributed control system
Electric motors
Industrial control systems
Mechatronics
Motion control
Process control
Robotics
Supervisory control  SCADA 

vteNeuroscience
Outline
History
Basicscience
Behavioral epigenetics
Behavioral genetics
Brain mapping
Brain reading
Cellular neuroscience
Computational neuroscience
Connectomics
Imaging genetics
Integrative neuroscience
Molecular neuroscience
Neural decoding
Neural engineering
Neuroanatomy
Neurobiology
Neurochemistry
Neuroendocrinology
Neurogenetics
Neuroinformatics
Neurometrics
Neuromorphology
Neurophysics
Neurophysiology
Systems neuroscience
Clinicalneuroscience
Behavioral neurology
Clinical neurophysiology
Epileptology
Neurocardiology
Neuroepidemiology
Neurogastroenterology
Neuroimmunology
Neurointensive care
Neurology
Neuro oncology
Neuro ophthalmology
Neuropathology
Neuropharmacology
Neuroprosthetics
Neuropsychiatry
Neuroradiology
Neurorehabilitation
Neurosurgery
Neurotology
Neurovirology
Nutritional neuroscience
Psychiatry
Cognitiveneuroscience
Affective neuroscience
Behavioral neuroscience
Chronobiology
Molecular cellular cognition
Motor control
Neurolinguistics
Neuropsychology
Sensory neuroscience
Social cognitive neuroscience
Interdisciplinaryfields
Consumer neuroscience
Cultural neuroscience
Educational neuroscience
Evolutionary neuroscience
Global neurosurgery
Neuroanthropology
Neural engineering
Neurobiotics
Neurocinema
Neurocriminology
Neuroeconomics
Neuroepistemology
Neuroesthetics
Neuroethics
Neuroethology
Neurohistory
Neurolaw
Neuromarketing
Neuromorphic engineering
Neuroscience of music
Neurophenomenology
Neurophilosophy
Neuropolitics
Neurorobotics
Neurotheology
Paleoneurobiology
Social neuroscience
Concepts
Brain computer interface
Development of the nervous system
Neural network  artificial 
Neural network  biological 
Detection theory
Intraoperative neurophysiological monitoring
Neurochip
Neurodegenerative disease
Neurodevelopmental disorder
Neurodiversity
Neurogenesis
Neuroimaging
Neuroimmune system
Neuromanagement
Neuromodulation
Neuroplasticity
Neurotechnology
Neurotoxin
Self awareness

 Category
 Commons

vteSelf driving cars  self driving vehicles and enabling technologiesOverview and context
History of self driving cars
Impact of self driving cars
Intelligent transportation system
Context aware pervasive systems
Mobile computing
Smart  connected products
Ubiquitous computing
Ambient intelligence
Internet of things
List of self driving system suppliers
SAE LevelsHuman driver monitors the driving environment Levels       
Lane departure warning system
Automatic parking
Automated emergency braking system
Collision avoidance system
Cruise control
Adaptive cruise control
Advanced driver assistance system
Driver drowsiness detection
Intelligent speed adaptation
Blind spot monitor
System monitors the driving environment Levels       
Automated Lane Keeping Systems
Vehicular ad hoc network  V V 
Connected car
Automotive navigation system
VehiclesCars
VaMP       
Spirit of Berlin       
General Motors EN V       
MadeInGermany       
Waymo  formerly Google Car       
Tesla Model S with Autopilot       
LUTZ Pathfinder       
Avride       
Honda Legend       
Buses and commercial vehicles
Automated guideway transit
CAVForth
ParkShuttle
Navia shuttle
NuTonomy taxi
Freightliner Inspiration
Driverless tractor
Self driving truck
Mobility as a service
Regulation
Legislation
IEEE       p
 Safe speed automotive common law
Automated lane keeping system  unece regulation     
Regulation  EU           
LiabilitySelf driving car liabilityEnabling technologies
Radar
Laser
LIDAR
Artificial neural network
Computer stereo vision
Image recognition
Dedicated short range communications
Real time Control System
rFpro
Eye tracking
Radio frequency identification
Automotive navigation system
Organizations  Projects  amp  PeopleOrganizations  projects and events
American Center for Mobility
DAVI
European Land Robot Trial
Navlab
DARPA Grand Challenge
VisLab Intercontinental Autonomous Challenge
Eureka Prometheus Project
IEEE Intelligent Transportation Systems Society
People
Harold Goddijn
Alberto Broggi
Anthony Levandowski

Authority control databases  National GermanyUnited StatesJapanCzech RepublicLatviaIsrael





Retrieved from  https   en wikipedia org w index php title Neural network  machine learning  amp oldid