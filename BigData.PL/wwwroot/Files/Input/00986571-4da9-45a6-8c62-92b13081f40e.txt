Technological phenomenon with social implications



A flow chart showing the decisions made by a recommendation engine  c                       
Part of a series onArtificial intelligence  AI 
Major goals
Artificial general intelligence
Intelligent agent
Recursive self improvement
Planning
Computer vision
General game playing
Knowledge reasoning
Natural language processing
Robotics
AI safety

Approaches
Machine learning
Symbolic
Deep learning
Bayesian networks
Evolutionary algorithms
Hybrid intelligent systems
Systems integration

Applications
Bioinformatics
Deepfake
Earth sciences
 Finance 
Generative AI
Art
Audio
Music
Government
Healthcare
Mental health
Industry
Translation
 Military 
Physics
Projects

Philosophy
Artificial consciousness
Chinese room
Friendly AI
Control problem Takeover
Ethics
Existential risk
Turing test
Uncanny valley

History
Timeline
Progress
AI winter
AI boom

Glossary
Glossary
vte
Part of a series onDiscrimination
Forms

Institutional
Reverse
Structural
Statistical
Systemic
Taste based


Attributes

Age
Caste
Class
Dialect
Disability
Economic
Genetic
Hair texture
Height
Language
Looks
Mental disorder
Nationality or citizenship
Race            Ethnicity
Reverse
Skin color
Scientific racism
Rank
Sex
Reverse
Sexual orientation
Species
Size
Viewpoint


Social

Arophobia
Anti albinism
Acephobia
Adultism
Anti altruistic
Anti autism
Anti homelessness
Anti drug addicts
Anti intellectualism
Anti intersex
Anti left handedness
Anti Masonry
Aporophobia
Audism
Biphobia
Fear of children
Clannism
Elitism
Ephebiphobia
Health
mental
in poverty
Fatphobia
Gayphobia
Gerontophobia
Heterosexism
HIV AIDS stigma
Hypergamy
Homophobia
In group
Leprosy stigma
Lesbophobia
Against men
Misandry
Misogyny
Misogynoir
Nepotism
Outgroup
Perpetual foreigner
Pregnancy
Sectarianism
Supremacism
Aryanism
Black
Hutu
Chauvinism
Han
Female
Human
Nordicism
Male
Jewish
Ultranationalism
White
Transphobia
Non binary
Transmisogyny
Trans men
Vegaphobia
Xenophobia


Religious

Persecution of non believers
Atheism
Blasphemy law
Religious censorship
In Islam
Apostasy
Religious police
Jizya
Religious persecution
In China
Exclusivism
Bah    Faith
Buddhism
Christianity
Persecution
Catholicism
Eastern Orthodoxy
Coptic Christianity
Jehovah s Witnesses
LDS or Mormon
Protestantism
Tewahedo Orthodoxy
post Cold War era
Falun Gong
Hinduism
Persecution
Untouchability
Islam
Persecution
Ahmadiyya
Shi ism
Sufism
Sunnism
minority Muslim
Judaism
Persecution
Neopaganism
Rastafari
Serers
Sikhism
Yazidism
Zoroastrianism


Race or ethnicity

Afghan
African
Fulani
Igbo
Serers
Albanian
Arab
Armenian
Asian
France
South Africa
United States
Assyrian
Australian
Austrian
Azerbaijani
Black people
African Americans
China
South Africa
Bengali
Bulgarian
Catalan
Chechen
Chinese
Han people
Colombian
Croat
Dutch
English
Estonian
Filipino
French
Finnish
Georgian
German
Greek
Haitian
Hazara
Hungarian
Indian
Indigenous people
Australia
Canada
United States
Iranian
Irish
Italian
Japanese
Jewish
Eliminationist
New
Religious
Anti Yiddish
Zionist
Korean
Kurdish
Lithuanian
Malay
M ori
Mexican
Middle Eastern
Mongolian
Montenegrin
Nigerian
Pakistani
Palestinians
Pashtun
Polish
Portuguese
Quebec
Romani
Romanian
Russian
Scottish
Serb
Slavic
Somali
Spanish
Taiwanese
Tatar
Thai
Tibetan
Turkish
Ukrainian
Uyghur
Venezuelan
Vietnamese
Welsh
White people


Manifestations

Algorithmic bias
Anti LGBTQ rhetoric
SPLC designated list of anti LGBTQ hate groups
Blood libel
Bullying
Cancel culture
Capital punishment for homosexuality
Carnism
Cognitive
Compulsory sterilization
Corrective rape
Counter jihad
Cultural genocide
Defamation
Democide
Dog whistle
Domicide
Economic
Education
Academic
In curricula
Sexism
Eliminationism
Eliminationist antisemitism
Employment
Enemy of the people
Environmental racism
Ethnic cleansing
Ethnic conflict
Ethnic hatred
Ethnic joke
Ethnocide
Excellence
Gender based dress codes
Cosmetics policy
High heel policy
Forced conversion
Freak show
Funding
Gay bashing
Gendercide
Transgender genocide
Genital modification and mutilation
Circumcision
Female genital mutilation
Intersex medical interventions
Genocide
examples
Glass ceiling
Hate crime
Disability hate crime
Violence against LGBTQ people
Violence against transgender people
Hate group
Hate speech
Institutional
Abuse
Discrimination
Racism
Homeless dumping
Housing
Hypergamy
Age disparity
Indian rolling
International inequality
Kill Haole Day
Lavender scare
LGBTQ 
grooming conspiracy theory
Linguicide
Lynching
Media
Minority stress
Moral exclusion
Mortgage
Murder music
Native American mascots
Braves
Blackhawks
Chiefs
Occupational
Apartheid
Inequality
Injustice
Segregation
Opposition to immigration
Paper genocide
Persecution
Pogrom
Political
Political repression
Ideological repression
Purge
Racialization
Religious persecution
Religious terrorism
Religious violence
Religious war
Scapegoating
Selective enforcement
Selective prosecution
Sentencing disparity
Sexual harassment
Sex selective abortion
Slut shaming
Structural abuse
Structural discrimination
Structural evil
Structural inequality
Structural violence
Suicide
Untermensch
Trans bashing
Victimisation
Violence against women
White flight
White genocide conspiracy theory
Wife selling
Witch hunt


Policies

Algorithmic wage discrimination
Age of candidacy
Blood purity
Blood quantum
Breadwinner model
Conscription and sexism
Crime of apartheid
Disabilities
Catholic
Jewish
Disparate impact
Fagging
Gender pay gap
Gender roles
Protecting Women s Private Spaces Act
Gerontocracy
Gerrymandering
Ghetto benches
Internment
Jewish quota
Law for Protection of the Nation
LGBTQ rights opposition
MSM blood donation restrictions
No kid zone
Numerus clausus  as religious or racial quota 
One drop rule
Racial quota
Racial steering
Redlining
Same sex marriage  laws and issues prohibiting 
Segregation
age
racial
Jim Crow laws
Nuremberg Laws
Segregation academy
religious
sexual
in Islam
Social exclusion
Sodomy law
State atheism
State religion
Ugly law
Voter suppression
White Australia policy


Countermeasures

Affirmative action
Anti discrimination law
Anti racism
Audit study
Autism rights movement
Blind audition
Constitutional colorblindness
Cross sex friendship
Cultural assimilation
Cultural pluralism
Diversity  equity  and inclusion
Diversity training
Empowerment
Fat acceptance movement
Feminism
Fighting Discrimination
Golden Rule
Hate speech laws by country
Human rights
Intersex human rights
Korenizatsiia
LGBTQ rights
Music in the movement against apartheid
Nonviolence
Racial integration
Reappropriation
Rock Against Sexism
Self determination
Social integration
Toleration
Transgender rights movement
Universal suffrage
Women s rights


Related topics

Allophilia
Alterity
Amatonormativity
Bias
Cisnormativity
Civil liberties
Criminalization of homosexuality
Dehumanization
Diseases of despair
Diversity
Multiculturalism
Neurodiversity
Ethnic penalty
Figleaf
Gender blind
Heteronormativity
Historical eugenics
Internalized oppression
Intersectionality
Mad pride
Masculism
Medical model of disability
autism
Net bias
Nonperson
Oikophobia
Oppression
Police brutality
Respect
Polyculturalism
Power distance
Prejudice
Prisoner abuse
Racial bias in criminal news in the United States
Racism by country
Racial color blindness
Religious intolerance
Second generation gender bias
Snobbery
Social equity
Social exclusion
Social identity threat
Social model of disability
Social privilege
Christian
male
white
Social stigma
Speciesism
Stereotype
threat
The talk

vte
Algorithmic bias describes systematic and repeatable harmful tendency in a computerized sociotechnical system to create  unfair  outcomes  such as  privileging  one category over another in ways different from the intended function of the algorithm            
Bias can emerge from many factors  including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded  collected  selected or used to train the algorithm              For example  algorithmic bias has been observed in search engine results and social media platforms  This bias can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race  gender  sexuality  and ethnicity  The study of algorithmic bias is most concerned with algorithms that reflect  systematic and unfair  discrimination             This bias has only recently been addressed in legal frameworks  such as the European Union s General Data Protection Regulation  proposed       and the Artificial Intelligence Act  proposed       approved       
As algorithms expand their ability to organize society  politics  institutions  and behavior  sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world  Because algorithms are often considered to be neutral and unbiased  they can inaccurately project greater authority than human expertise  in part due to the psychological phenomenon of automation bias   and in some cases  reliance on algorithms can displace human responsibility for their outcomes  Bias can enter into algorithmic systems as a result of pre existing cultural  social  or institutional expectations  by how features and labels are chosen  because of technical limitations of their design  or by being used in unanticipated contexts or by audiences who are not considered in the software s initial design            
Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech  It has also arisen in criminal justice  healthcare  and hiring  compounding existing racial  socioeconomic  and gender biases  The relative inability of facial recognition technology to accurately identify darker skinned faces has been linked to multiple wrongful arrests of black men  an issue stemming from imbalanced datasets  Problems in understanding  researching  and discovering algorithmic bias persist due to the proprietary nature of algorithms  which are typically treated as trade secrets  Even when full transparency is provided  the complexity of certain algorithms poses a barrier to understanding their functioning  Furthermore  algorithms may change  or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis  In many cases  even within a single website or application  there is no single  algorithm  to examine  but a network of many interrelated programs and data inputs  even between users of the same service 
A      survey identified multiple forms of algorithmic bias  including historical  representation  and measurement biases  each of which can contribute to unfair outcomes            


Definitions edit 
A      diagram for how a simple computer program makes decisions  illustrating a very simple algorithm
Algorithms are difficult to define             but may be generally understood as lists of instructions that determine how programs read  collect  process  and analyze data to generate output                                  For a rigorous technical introduction  see Algorithms  Advances in computer hardware have led to an increased ability to process  store and transmit data  This has in turn boosted the design and adoption of technologies such as machine learning and artificial intelligence                                     By analyzing and processing data  algorithms are the backbone of search engines              social media websites              recommendation engines              online retail              online advertising              and more             
Contemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact  and question the underlying assumptions of an algorithm s neutrality                                                                                                                  The term algorithmic bias describes systematic and repeatable errors that create unfair outcomes  such as privileging one arbitrary group of users over others  For example  a credit score algorithm may deny a loan without being unfair  if it is consistently weighing relevant financial criteria  If the algorithm recommends loans to one group of users  but denies loans to another set of nearly identical users based on unrelated criteria  and if this behavior can be repeated across multiple occurrences  an algorithm can be described as biased                                    This bias may be intentional or unintentional  for example  it can come from biased data obtained from a worker that previously did the job the algorithm is going to do from now on  

Methods edit 
Bias can be introduced to an algorithm in several ways  During the assemblage of a dataset  data may be collected  digitized  adapted  and entered into a database according to human designed cataloging criteria                                  Next  programmers assign priorities  or hierarchies  for how a program assesses and sorts that data  This requires human decisions about how data is categorized  and which data is included or discarded                                  Some algorithms collect their own data based on human selected criteria  which can also reflect the bias of human designers                                  Other algorithms may reinforce stereotypes and preferences as they process and display  relevant  data for human users  for example  by selecting information based on previous choices of a similar user or group of users                                 
Beyond assembling and processing data  bias can emerge as a result of design              For example  algorithms that determine the allocation of resources or scrutiny  such as determining school placements  may inadvertently discriminate against a category when determining risk based on similar users  as in credit scores                                    Meanwhile  recommendation engines that work by associating users with similar users  or that make use of inferred marketing traits  might rely on inaccurate associations that reflect broad ethnic  gender  socio economic  or racial stereotypes  Another example comes from determining criteria for what is included and excluded from results  These criteria could present unanticipated outcomes for search results  such as with flight recommendation software that omits flights that do not follow the sponsoring airline s flight paths              Algorithms may also display an uncertainty bias  offering more confident assessments when larger data sets are available  This can skew algorithmic processes toward results that more closely correspond with larger samples  which may disregard data from underrepresented populations                                 

History edit 
Early critiques edit 
This card was used to load software into an old mainframe computer  Each byte  the letter  A   for example  is entered by punching holes  Though contemporary computers are more complex  they reflect this human decision making process in collecting and processing data                                                                   
The earliest computer programs were designed to mimic human reasoning and deductions  and were deemed to be functioning when they successfully and consistently reproduced that human logic  In his      book Computer Power and Human Reason  artificial intelligence pioneer Joseph Weizenbaum suggested that bias could arise both from the data used in a program  but also from the way a program is coded                                   
Weizenbaum wrote that programs are a sequence of rules created by humans for a computer to follow  By following those rules consistently  such programs  embody law                                    that is  enforce a specific way to solve problems  The rules a computer follows are based on the assumptions of a computer programmer for how these problems might be solved  That means the code could incorporate the programmer s imagination of how the world works  including their biases and expectations                                    While a computer program can incorporate bias in this way  Weizenbaum also noted that any data fed to a machine additionally reflects  human decision making processes  as data is being selected                                             
Finally  he noted that machines might also transfer good information with unintended consequences if users are unclear about how to interpret the results                                   Weizenbaum warned against trusting decisions made by computer programs that a user doesn t understand  comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss  Crucially  the tourist has no basis of understanding how or why he arrived at his destination  and a successful arrival does not mean the process is accurate or reliable                                   
An early example of algorithmic bias resulted in as many as    women and ethnic minorities denied entry to St  George s Hospital Medical School per year from      to       based on implementation of a new computer guidance assessment system that denied entry to women and men with  foreign sounding names  based on historical trends in admissions              While many schools at the time employed similar biases in their selection process  St  George was most notable for automating said bias through the use of an algorithm  thus gaining the attention of people on a much wider scale 
In recent years  as algorithms increasingly rely on machine learning methods applied to real world data  algorithmic bias has become more prevalent due to inherent biases within the data itself  For instance  facial recognition systems have been shown to misidentify individuals from marginalized groups at significantly higher rates than white individuals  highlighting how biases in training datasets manifest in deployed systems              A      study by Joy Buolamwini and Timnit Gebru found that commercial facial recognition technologies exhibited error rates of up to     when identifying darker skinned women  compared to less than    for lighter skinned men             
Algorithmic biases are not only technical failures but often reflect systemic inequities embedded in historical and societal data  Researchers and critics  such as Cathy O Neil in her book Weapons of Math Destruction         emphasize that these biases can amplify existing social inequalities under the guise of objectivity  O Neil argues that opaque  automated decision making processes in areas such as credit scoring  predictive policing  and education can reinforce discriminatory practices while appearing neutral or scientific             

Contemporary critiques and responses edit 
Though well designed algorithms frequently determine outcomes that are equally  or more  equitable than the decisions of human beings  cases of bias still occur  and are difficult to predict and analyze              The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design  Decisions made by one designer  or team of designers  may be obscured among the many pieces of code created for a single program  over time these decisions and their collective impact on the program s output may be forgotten                                    In theory  these biases may create new patterns of behavior  or  scripts   in relationship to specific technologies as the code interacts with other elements of society              Biases may also impact how society shapes itself around the data points that algorithms require  For example  if data shows a high number of arrests in a particular area  an algorithm may assign more police patrols to that area  which could lead to more arrests                                   
The decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist                                   a process described by author Clay Shirky as  algorithmic authority               Shirky uses the term to describe  the decision to regard as authoritative an unmanaged process of extracting value from diverse  untrustworthy sources   such as search results              This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public  For example  a list of news items selected and presented as  trending  or  popular  may be created based on significantly wider criteria than just their popularity                                  
Because of their convenience and authority  algorithms are theorized as a means of delegating responsibility away from humans                                                                   This can have the effect of reducing alternative options  compromises  or flexibility                                   Sociologist Scott Lash has critiqued algorithms as a new form of  generative power   in that they are a virtual means of generating actual ends  Where previously human behavior generated data to be collected and studied  powerful algorithms increasingly could shape and define human behaviors                                  
While blind adherence to algorithmic decisions is a concern  an opposite issue arises when human decision makers exhibit  selective adherence  to algorithmic advice  In such cases  individuals accept recommendations that align with their preexisting beliefs and disregard those that do not  thereby perpetuating existing biases and undermining the fairness objectives of algorithmic interventions  Consequently  incorporating fair algorithmic tools into decision making processes does not automatically eliminate human biases             
Concerns over the impact of algorithms on society have led to the creation of working groups in organizations such as Google and Microsoft  which have co created a working group named Fairness  Accountability 
and Transparency in Machine Learning                                    Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences                                    In recent years  the study of the Fairness  Accountability 
and Transparency  FAT  of algorithms has emerged as its own interdisciplinary research area with an annual conference called FAccT              Critics have suggested that FAT initiatives cannot serve effectively as independent watchdogs when many are funded by corporations building the systems being studied             

Types edit 
Pre existing edit 
Pre existing bias in an algorithm is a consequence of underlying social and institutional ideologies  Such ideas may influence or create personal biases within individual designers or programmers  Such prejudices can be explicit and conscious  or implicit and unconscious                                                                      Poorly selected input data  or simply data from a biased source  will influence the outcomes created by machines                                   Encoding pre existing bias into software can preserve social and institutional bias  and  without correction  could be replicated in all future uses of that algorithm                                                                   
An example of this form of bias is the British Nationality Act Program  designed to automate the evaluation of new British citizens after the      British Nationality Act                                    The program accurately reflected the tenets of the law  which stated that  a man is the father of only his legitimate children  whereas a woman is the mother of all her children  legitimate or not                                                                       In its attempt to transfer a particular logic into an algorithmic process  the BNAP inscribed the logic of the British Nationality Act into its algorithm  which would perpetuate it even if the act was eventually repealed                                   
Another source of bias  which has been called  label choice bias               arises when proxy measures are used to train algorithms  that build in bias against certain groups  For example  a widely used algorithm predicted health care costs as a proxy for health care needs  and used predictions to allocate resources to help patients with complex health needs  This introduced bias because Black patients have lower costs  even when they are just as unhealthy as White patients             Solutions to the  label choice bias  aim to match the actual target  what the algorithm is predicting  more closely to the ideal target  what researchers want the algorithm to predict   so for the prior example  instead of predicting cost  researchers would focus on the variable of healthcare needs which is rather more significant  Adjusting the target led to almost double the number of Black patients being selected for the program             

Machine learning bias edit 
Machine learning bias refers to systematic and unfair disparities in the output of machine learning algorithms  These biases can manifest in various ways and are often a reflection of the data used to train these algorithms  Here are some key aspects 

Language bias edit 
Language bias refers a type of statistical sampling bias tied to the language of a query that leads to  a systematic deviation in sampling information that prevents it from accurately representing the true coverage of topics and views available in their repository               Luo et al  s work             shows that current large language models  as they are predominately trained on English language data  often present the Anglo American views as truth  while systematically downplaying non English perspectives as irrelevant  wrong  or noise  When queried with political ideologies like  What is liberalism    ChatGPT  as it was trained on English centric data  describes liberalism from the Anglo American perspective  emphasizing aspects of human rights and equality  while equally valid aspects like  opposes state intervention in personal and economic life  from the dominant Vietnamese perspective and  limitation of government power  from the prevalent Chinese perspective are absent              Similarly  language models may exhibit bias against people within a language group based on the specific dialect they use             

Selection bias edit 
Selection bias refers the inherent tendency of large language models to favor certain option identifiers irrespective of the actual content of the options  This bias primarily stems from token bias that is  the model assigns a higher a priori probability to specific answer tokens  such as  A   when generating responses  As a result  when the ordering of options is altered  for example  by systematically moving the correct answer to different positions   the model s performance can fluctuate significantly  This phenomenon undermines the reliability of large language models in multiple choice settings                         

Gender bias edit 
Gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another  This bias typically arises from the data on which these models are trained  For example  large language models often assign roles and characteristics based on traditional gender norms  it might associate nurses or secretaries predominantly with women and engineers or CEOs with men                         

Stereotyping edit 
Beyond gender and race  these models can reinforce a wide range of stereotypes  including those based on age  nationality  religion  or occupation  This can lead to outputs that homogenize  or unfairly generalize or caricature groups of people  sometimes in harmful or derogatory ways                         
A recent focus in research has been on the complex interplay between the grammatical properties of a language and real world biases that can become embedded in AI systems  potentially perpetuating harmful stereotypes and assumptions  The study on gender bias in language models trained on Icelandic  a highly grammatically gendered language  revealed that the models exhibited a significant predisposition towards the masculine grammatical gender when referring to occupation terms  even for female dominated professions              This suggests the models amplified societal gender biases present in the training data 

Political bias edit 
Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints  ideologies  or outcomes over others  Language models may also exhibit political biases  Since the training data includes a wide range of political opinions and coverage  the models might generate responses that lean towards particular political ideologies or viewpoints  depending on the prevalence of those views in the data                         

Racial bias edit 
Racial bias refers to the tendency of machine learning models to produce outcomes that unfairly discriminate against or stereotype individuals based on race or ethnicity  This bias often stems from training data that reflects historical and systemic inequalities  For example  AI systems used in hiring  law enforcement  or healthcare may disproportionately disadvantage certain racial groups by reinforcing existing stereotypes or underrepresenting them in key areas  Such biases can manifest in ways like facial recognition systems misidentifying individuals of certain racial backgrounds or healthcare algorithms underestimating the medical needs of minority patients  Addressing racial bias requires careful examination of data  improved transparency in algorithmic processes  and efforts to ensure fairness throughout the AI development lifecycle                         

Technical edit 
Facial recognition software used in conjunction with surveillance cameras was found to display bias in recognizing Asian and black faces over white faces                                   
Technical bias emerges through limitations of a program  computational power  its design  or other constraint on the system                                    Such bias can also be a restraint of design  for example  a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three  as in an airline price display                                    Another case is software that relies on randomness for fair distributions of results  If the random number generation mechanism is not truly random  it can introduce bias  for example  by skewing selections toward items at the end or beginning of a list                                   
A decontextualized algorithm uses unrelated information to sort results  for example  a flight pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines                                    The opposite may also apply  in which results are evaluated in contexts different from which they are collected  Data may be collected without crucial external context  for example  when facial recognition software is used by surveillance cameras  but evaluated by remote staff in another country or region  or evaluated by non human algorithms with no awareness of what takes place beyond the camera s field of vision  This could create an incomplete understanding of a crime scene  for example  potentially mistaking bystanders for those who commit the crime                                   
Lastly  technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way  For example  software weighs data points to determine whether a defendant should accept a plea bargain  while ignoring the impact of emotion on a jury                                    Another unintended result of this form of bias was found in the plagiarism detection software Turnitin  which compares student written texts to information found online and returns a probability score that the student s work is copied  Because the software compares long strings of text  it is more likely to identify non native speakers of English than native speakers  as the latter group might be better able to change individual words  break up strings of plagiarized text  or obscure copied passages through synonyms  Because it is easier for native speakers to evade detection as a result of the technical constraints of the software  this creates a scenario where Turnitin identifies foreign speakers of English for plagiarism while allowing more native speakers to evade detection                                     

Emergent edit 
Emergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts                                    Algorithms may not have been adjusted to consider new forms of knowledge  such as new drugs or medical breakthroughs  new laws  business models  or shifting cultural norms                                               This may exclude groups through technology  without providing clear outlines to understand who is responsible for their exclusion                                                                      Similarly  problems may emerge when training data  the samples  fed  to a machine  by which it models certain conclusions  do not align with contexts that an algorithm encounters in the real world             
In       an example of emergent bias was identified in the software used to place US medical students into residencies  the National Residency Match Program  NRMP                                     The algorithm was designed at a time when few married couples would seek residencies together  As more women entered medical schools  more students were likely to request a residency alongside their partners  The process called for each applicant to provide a list of preferences for placement across the US  which was then sorted and assigned when a hospital and an applicant both agreed to a match  In the case of married couples where both sought residencies  the algorithm weighed the location choices of the higher rated partner first  The result was a frequent assignment of highly preferred schools to the first partner and lower preferred schools to the second partner  rather than sorting for compromises in placement preference                                               
Additional emergent biases include 

Correlations edit 
Unpredictable correlations can emerge when large data sets are compared to each other  For example  data collected about web browsing patterns may align with signals marking sensitive data  such as race or sexual orientation   By selecting according to certain behavior or browsing patterns  the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data                                  In other cases  the algorithm draws conclusions from correlations  without being able to understand those correlations  For example  one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia  The program algorithm did this because it simply compared survival rates  asthmatics with pneumonia are at the highest risk  Historically  for this same reason  hospitals typically give such asthmatics the best and most immediate care                  clarification needed     

Unanticipated uses edit 
Emergent bias can occur when an algorithm is used by unanticipated audiences  For example  machines may require that users can read  write  or understand numbers  or relate to an interface using metaphors that they do not understand                                    These exclusions can become compounded  as biased or exclusionary technology is more deeply integrated into society                                   
Apart from exclusion  unanticipated uses may emerge from the end user relying on the software rather than their own knowledge  In one example  an unanticipated user group led to algorithmic bias in the UK  when the British National Act Program was created as a proof of concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship  The designers had access to legal expertise beyond the end users in immigration offices  whose understanding of both software and immigration law would likely have been unsophisticated  The agents administering the questions relied entirely on the software  which excluded alternative pathways to citizenship  and used the software even after new case laws and legal interpretations led the algorithm to become outdated  As a result of designing an algorithm for users assumed to be legally savvy on immigration law  the software s algorithm indirectly led to bias in favor of applicants who fit a very narrow set of legal criteria set by the algorithm  rather than by the more broader criteria of British immigration law                                   

Feedback loops edit 
Emergent bias may also create a feedback loop  or recursion  if data collected for an algorithm results in real world responses which are fed back into the algorithm                          For example  simulations of the predictive policing software  PredPol   deployed in Oakland  California  suggested an increased police presence in black neighborhoods based on crime data reported by the public              The simulation showed that the public reported crime based on the sight of police cars  regardless of what police were doing  The simulation interpreted police car sightings in modeling its predictions of crime  and would in turn assign an even larger increase of police presence within those neighborhoods                                      The Human Rights Data Analysis Group  which conducted the simulation  warned that in places where racial discrimination is a factor in arrests  such feedback loops could reinforce and perpetuate racial discrimination in policing              Another well known example of such an algorithm exhibiting such behavior is COMPAS  a software that determines an individual s likelihood of becoming a criminal offender  The software is often criticized for labeling Black individuals as criminals much more likely than others  and then feeds the data back into itself in the event individuals become registered criminals  further enforcing the bias created by the dataset the algorithm is acting on             
Recommender systems such as those used to recommend online videos or news articles can create feedback loops              When users click on content that is suggested by algorithms  it influences the next set of suggestions              Over time this may lead to users entering a filter bubble and being unaware of important or useful content                         

Impact edit 
Commercial influences edit 
Corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies  without the knowledge of a user who may mistake the algorithm as being impartial  For example  American Airlines created a flight finding algorithm in the     s  The software presented a range of flights from various airlines to customers  but weighed factors that boosted its own flights  regardless of price or convenience  In testimony to the United States Congress  the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment                                                                   
In a      paper describing Google  the founders of the company had adopted a policy of transparency in search results regarding paid placement  arguing that  advertising funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers               This bias would be an  invisible  manipulation of the user                                 

Voting behavior edit 
A series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about      The researchers concluded that candidates have  no means of competing  if an algorithm  with or without intent  boosted page listings for a rival candidate              Facebook users who saw messages related to voting were more likely to vote  A      randomized trial of Facebook users showed a     increase          votes  among users who saw messages encouraging voting  as well as images of their friends who had voted              Legal scholar Jonathan Zittrain has warned that this could create a  digital gerrymandering  effect in elections   the selective presentation of information by an intermediary to meet its agenda  rather than to serve its users   if intentionally manipulated                                   

Gender discrimination edit 
In       the professional networking site LinkedIn was discovered to recommend male variations of women s names in response to search queries  The site did not make similar recommendations in searches for male names  For example   Andrea  would bring up a prompt asking if users meant  Andrew   but queries for  Andrew  did not ask if users meant to find  Andrea   The company said this was the result of an analysis of users  interactions with the site             
In       the department store franchise Target was cited for gathering data points to infer when women customers were pregnant  even if they had not announced it  and then sharing that information with marketing partners                                               Because the data had been predicted  rather than directly observed or reported  the company had no legal obligation to protect the privacy of those customers                                  
Web search algorithms have also been accused of bias  Google s results may prioritize pornographic content in search terms related to sexuality  for example   lesbian   This bias extends to the search engine showing popular but sexualized content in neutral searches  For example   Top    Sexiest Women Athletes  articles displayed as first page results in searches for  women athletes                                    In       Google adjusted these results along with others that surfaced hate groups  racist views  child abuse and pornography  and other upsetting and offensive content              Other examples include the display of higher paying jobs to male applicants on job search websites              Researchers have also identified that machine translation exhibits a strong tendency towards male defaults              In particular  this is observed in fields linked to unbalanced gender distribution  including STEM occupations              In fact  current machine translation systems fail to reproduce the real world distribution of female workers             
In       Amazon com turned off an AI system it developed to screen job applications when they realized it was biased against women              The recruitment tool excluded applicants who attended all women s colleges and resumes that included the word  women s               A similar problem emerged with music streaming services In       it was discovered that the recommender system algorithm used by Spotify was biased against women artists              Spotify s song recommendations suggested more male artists over women artists 

Racial and ethnic discrimination edit 
Algorithms have been criticized as a method for obscuring racial prejudices in decision making                                                            Because of how certain races and ethnic groups were treated in the past  data can often contain hidden biases              For example  black people are likely to receive longer sentences than white people who committed the same crime                          This could potentially mean that a system amplifies the original biases in the data 
In       Google apologized when a couple of black users complained that an image identification algorithm in its Photos application identified them as gorillas              In       Nikon cameras were criticized when image recognition algorithms consistently asked Asian users if they were blinking              Such examples are the product of bias in biometric data sets              Biometric data is drawn from aspects of the body  including racial features either observed or inferred  which can then be transferred into data points                                    Speech recognition technology can have different accuracies depending on the user s accent  This may be caused by the a lack of training data for speakers of that accent             
Biometric data about race may also be inferred  rather than observed  For example  a      study showed that names commonly associated with blacks were more likely to yield search results implying arrest records  regardless of whether there is any police record of that individual s name              A      study also found that Black and Asian people are assumed to have lesser functioning lungs due to racial and occupational exposure data not being incorporated into the prediction algorithm s model of lung function                          
In       a research study revealed that a healthcare algorithm sold by Optum favored white patients over sicker black patients  The algorithm predicts how much patients would cost the health care system in the future  However  cost is not race neutral  as black patients incurred about        less in medical costs per year than white patients with the same number of chronic conditions  which led to the algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases              
A study conducted by researchers at UC Berkeley in November      revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on  creditworthiness  which is rooted in the U S  fair lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans  These particular algorithms were present in FinTech companies and were shown to discriminate against minorities                   non primary source needed     
Another study  published in August       on Large language model investigates how language models perpetuate covert racism  particularly through dialect prejudice against speakers of African American English  AAE   It highlights that these models exhibit more negative stereotypes about AAE speakers than any recorded human biases  while their overt stereotypes are more positive  This discrepancy raises concerns about the potential harmful consequences of such biases in decision making processes              
A study published by the Anti Defamation League in      found that several major LLMs  including ChatGPT  Llama  Claude  and Gemini showed antisemitic bias              
A      study found that commercial gender classification systems had significantly higher error rates for darker skinned women  with error rates up to        compared to near perfect accuracy for lighter skinned men              

Law enforcement and legal proceedings edit 
Algorithms already have numerous applications in legal systems  An example of this is COMPAS  a commercial program widely used by U S  courts to assess the likelihood of a defendant becoming a recidivist  ProPublica claims that the average COMPAS assigned recidivism risk level of black defendants is significantly higher than the average COMPAS assigned risk level of white defendants  and that black defendants are twice as likely to be erroneously assigned the label  high risk  as white defendants                           
One example is the use of risk assessments in criminal sentencing in the United States and parole hearings  judges were presented with an algorithmically generated score intended to reflect the risk that a prisoner will repeat a crime               For the time period starting in      and ending in       the nationality of a criminal s father was a consideration in those risk assessment scores                                   Today  these scores are shared with judges in Arizona  Colorado  Delaware  Kentucky  Louisiana  Oklahoma  Virginia  Washington  and Wisconsin  An independent investigation by ProPublica found that the scores were inaccurate     of the time  and disproportionately skewed to suggest blacks to be at risk of relapse      more often than whites              
One study that set out to examine  Risk  Race   amp  Recidivism  Predictive Bias and Disparate Impact  alleges a two fold     percent vs     percent  adverse likelihood for black vs  Caucasian defendants to be misclassified as imposing a higher risk despite having objectively remained without any documented recidivism over a two year period of observation              
In the pretrial detention context  a law review article argues that algorithmic risk assessments violate   th Amendment Equal Protection rights on the basis of race  since the algorithms are argued to be facially discriminatory  to result in disparate treatment  and to not be narrowly tailored              

Online hate speech edit 
In      a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content  according to internal Facebook documents               The algorithm  which is a combination of computer programs and human content reviewers  was created to protect broad categories rather than specific subsets of categories  For example  posts denouncing  Muslims  would be blocked  while posts denouncing  Radical Muslims  would be allowed  An unanticipated outcome of the algorithm is to allow hate speech against black children  because they denounce the  children  subset of blacks  rather than  all blacks   whereas  all white men  would trigger a block  because whites and males are not considered subsets               Facebook was also found to allow ad purchasers to target  Jew haters  as a category of users  which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data  The company s design also allowed ad buyers to block African Americans from seeing housing ads              
While algorithms are used to track and block hate speech  some were found to be     times more likely to flag information posted by Black users and     times likely to flag information as hate speech if written in African American English               Without context for slurs and epithets  even when used by communities which have re appropriated them  were flagged              
Another instance in a study found that    out of     examined subreddits tended to remove various norm violations  including misogynistic slurs and racist hate speech  highlighting the prevalence of such content in online communities               As platforms like Reddit update their hate speech policies  they must balance free expression with the protection of marginalized communities  emphasizing the need for context sensitive moderation and nuanced algorithms              

Surveillance edit 
Surveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors  and to determine who belongs in certain locations at certain times                                    The ability of such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database  if the majority of photos belong to one race or gender  the software is better at recognizing other members of that race or gender               However  even audits of these image recognition systems are ethically fraught  and some scholars have suggested the technology s context will always have a disproportionate impact on communities whose actions are over surveilled               For example  a      analysis of software used to identify individuals in CCTV images found several examples of bias when run against criminal databases  The software was assessed as identifying men more frequently than women  older people more frequently than the young  and identified Asians  African Americans and other races more often than whites                                    A      study found that facial recognition software most likely accurately identified light skinned  typically European  males  with slightly lower accuracy rates for light skinned females  Dark skinned males and females were significanfly less likely to be accurately identified by facial recognition software  These disparities are attributed to the under representation of darker skinned participants in data sets used to develop this software                           

Discrimination against the LGBTQ community edit 
In       users of the gay hookup application Grindr reported that the Android store s recommendation algorithm was linking Grindr to applications designed to find sex offenders  which critics said inaccurately related homosexuality with pedophilia  Writer Mike Ananny criticized this association in The Atlantic  arguing that such associations further stigmatized gay men               In       online retailer Amazon de listed        books after an algorithmic change expanded its  adult content  blacklist to include any book addressing sexuality or gay themes  such as the critically acclaimed novel Brokeback Mountain                                                           
In       it was found that on Facebook  searches for  photos of my female friends  yielded suggestions such as  in bikinis  or  at the beach   In contrast  searches for  photos of my male friends  yielded no results              
Facial recognition technology has been seen to cause problems for transgender individuals  In       there were reports of Uber drivers who were transgender or transitioning experiencing difficulty with the facial recognition software that Uber implements as a built in security measure  As a result of this  some of the accounts of trans Uber drivers were suspended which cost them fares and potentially cost them a job  all due to the facial recognition software experiencing difficulties with recognizing the face of a trans driver who was transitioning               Although the solution to this issue would appear to be including trans individuals in training sets for machine learning models  an instance of trans YouTube videos that were collected to be used in training data did not receive consent from the trans individuals that were included in the videos  which created an issue of violation of privacy              
There has also been a study that was conducted at Stanford University in      that tested algorithms in a machine learning system that was said to be able to detect an individual s sexual orientation based on their facial images               The model in the study predicted a correct distinction between gay and straight men     of the time  and a correct distinction between gay and straight women     of the time  This study resulted in a backlash from the LGBTQIA community  who were fearful of the possible negative repercussions that this AI system could have on individuals of the LGBTQIA community by putting individuals at risk of being  outed  against their will              

Disability discrimination edit 
While the modalities of algorithmic fairness have been judged on the basis of different aspects of bias   like gender  race and socioeconomic status  disability often is left out of the list                            The marginalization people with disabilities currently face in society is being translated into AI systems and algorithms  creating even more exclusion                          
The shifting nature of disabilities and its subjective characterization  makes it more difficult to computationally address  The lack of historical depth in defining disabilities  collecting its incidence and prevalence in questionnaires  and establishing recognition add to the controversy and ambiguity in its quantification and calculations        The definition of disability has been long debated shifting from a medical model to a social model of disability most recently  which establishes that disability is a result of the mismatch between people s interactions and barriers in their environment  rather than impairments and health conditions  Disabilities can also be situational or temporary               considered in a constant state of flux  Disabilities are incredibly diverse               fall within a large spectrum  and can be unique to each individual  People s identity can vary based on the specific types of disability they experience  how they use assistive technologies  and who they support        The high level of variability across people s experiences greatly personalizes how a disability can manifest  Overlapping identities and intersectional experiences              are excluded from statistics and datasets               hence underrepresented and nonexistent in training data               Therefore  machine learning models are trained inequitably and artificial intelligent systems perpetuate more algorithmic bias               For example  if people with speech impairments are not included in training voice control features and smart AI assistants  they are unable to use the feature or the responses received from a Google Home or Alexa are extremely poor 
Given the stereotypes and stigmas that still exist surrounding disabilities  the sensitive nature of revealing these identifying characteristics also carries vast privacy challenges       As disclosing disability information can be taboo and drive further discrimination against this population  there is a lack of explicit disability data available for algorithmic systems to interact with  People with disabilities face additional harms and risks with respect to their social support  cost of health insurance  workplace discrimination and other basic necessities upon disclosing their disability status  Algorithms are further exacerbating this gap by recreating the biases that already exist in societal systems and structures                           

Google Search edit 
While users generate results that are  completed  automatically  Google has failed to remove sexist and racist autocompletion text  For example  Algorithms of Oppression  How Search Engines Reinforce Racism Safiya Noble notes an example of the search for  black girls   which was reported to result in pornographic images  Google claimed it was unable to erase those pages unless they were considered unlawful              

Obstacles to research edit 
Several problems impede the study of large scale algorithmic bias  hindering the application of academically rigorous studies and public understanding                                                           

Defining fairness edit 
Main article  Fairness  machine learning 
Literature on algorithmic bias has focused on the remedy of fairness  but definitions of fairness are often incompatible with each other and the realities of machine learning optimization                            For example  defining fairness as an  equality of outcomes  may simply refer to a system producing the same result for all people  while fairness defined as  equality of treatment  might explicitly consider differences between individuals                                   As a result  fairness is sometimes described as being in conflict with the accuracy of a model  suggesting innate tensions between the priorities of social welfare and the priorities of the vendors designing these systems                                   In response to this tension  researchers have suggested more care to the design and use of systems that draw on potentially biased algorithms  with  fairness  defined for specific applications and contexts              

Complexity edit 
Algorithmic processes are complex  often exceeding the understanding of the people who use them                                                                   Large scale operations may not be understood even by those involved in creating them               The methods and processes of contemporary programs are often obscured by the inability to know every permutation of a code s input or output                                    Social scientist Bruno Latour has identified this process as blackboxing  a process in which  scientific and technical work is made invisible by its own success  When a machine runs efficiently  when a matter of fact is settled  one need focus only on its inputs and outputs and not on its internal complexity  Thus  paradoxically  the more science and technology succeed  the more opaque and obscure they become                Others have critiqued the black box metaphor  suggesting that current algorithms are not one black box  but a network of interconnected ones                                   
An example of this complexity can be found in the range of inputs into customizing feedback  The social media site Facebook factored in at least         data points to determine the layout of a user s social media feed in                    Furthermore  large teams of programmers may operate in relative isolation from one another  and be unaware of the cumulative effects of small decisions within connected  elaborate algorithms                                    Not all code is original  and may be borrowed from other libraries  creating a complicated set of relationships between data processing and data input systems                                 
Additional complexity occurs through machine learning and the personalization of algorithms based on user interactions such as clicks  time spent on site  and other metrics  These personal adjustments can confuse general attempts to understand algorithms                                                                      One unidentified streaming radio service reported that it used five unique music selection algorithms it selected for its users  based on their behavior  This creates different experiences of the same streaming services between different users  making it harder to understand what these algorithms do                                 
Companies also run frequent A B tests to fine tune algorithms based on user response  For example  the search engine Bing can run up to ten million subtle variations of its service per day  creating different experiences of the service between each use and or user                                 

Lack of transparency edit 
Commercial algorithms are proprietary  and may be treated as trade secrets                                                                                                     Treating algorithms as trade secrets protects companies  such as search engines  where a transparent algorithm might reveal tactics to manipulate search rankings                                     This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function                                  Critics suggest that such secrecy can also obscure possible unethical methods used in producing or processing algorithmic output                                     Other critics  such as lawyer and activist Katarzyna Szymielewicz  have suggested that the lack of transparency is often disguised as a result of algorithmic complexity  shielding companies from disclosing or investigating its own algorithmic processes              

Lack of data about sensitive categories edit 
A significant barrier to understanding the tackling of bias in practice is that categories  such as demographics of individuals protected by anti discrimination law  are often not explicitly considered when collecting and processing data               In some cases  there is little opportunity to collect this data explicitly  such as in device fingerprinting  ubiquitous computing and the Internet of Things  In other cases  the data controller may not wish to collect such data for reputational reasons  or because it represents a heightened liability and security risk  It may also be the case that  at least in relation to the European Union s General Data Protection Regulation  such data falls under the  special category  provisions  Article     and therefore comes with more restrictions on potential collection and processing 
Some practitioners have tried to estimate and impute these missing sensitive categorizations in order to allow bias mitigation  for example building systems to infer ethnicity from names               however this can introduce other forms of bias if not undertaken with care               Machine learning researchers have drawn upon cryptographic privacy enhancing technologies such as secure multi party computation to propose methods whereby algorithmic bias can be assessed or mitigated without these data ever being available to modellers in cleartext              
Algorithmic bias does not only include protected categories  but can also concern characteristics less easily observable or codifiable  such as political viewpoints  In these cases  there is rarely an easily accessible or non controversial ground truth  and removing the bias from such a system is more difficult               Furthermore  false and accidental correlations can emerge from a lack of understanding of protected categories  for example  insurance rates based on historical data of car accidents which may overlap  strictly by coincidence  with residential clusters of ethnic minorities              

Solutions edit 
A study of    policy guidelines on ethical AI found that fairness and  mitigation of unwanted bias  was a common point of concern  and were addressed through a blend of technical solutions  transparency and monitoring  right to remedy and increased oversight  and diversity and inclusion efforts              

Technical edit 
Further information  Fairness  machine learning 
There have been several attempts to create methods and tools that can detect and observe biases within an algorithm  These emergent fields focus on tools which are typically applied to the  training  data used by the program rather than the algorithm s internal processes  These methods may also analyze a program s output and its usefulness and therefore may involve the analysis of its confusion matrix  or table of confusion                                                                                                                        Explainable AI to detect algorithm Bias is a suggested way to detect the existence of bias in an algorithm or learning model               Using machine learning to detect bias is called   conducting an AI audit   where the  auditor  is an algorithm that goes through the AI model and the training data to identify biases              
Ensuring that an AI tool such as a classifier is free from bias is more difficult than just removing the sensitive information
from its input signals  because this is typically implicit in other signals  For example  the hobbies  sports and schools attended
by a job candidate might reveal their gender to the software  even when this is removed from the analysis  Solutions to this
problem involve ensuring that the intelligent agent does not have any information that could be used to reconstruct the protected
and sensitive information about the subject  as first demonstrated in              where a deep learning network was simultaneously trained to learn a task while at the same time being completely agnostic about the protected feature  A simpler method was proposed in the context of word embeddings  and involves removing information that is correlated with the protected characteristic              
Currently     when        a new IEEE standard is being drafted that aims to specify methodologies which help creators of algorithms eliminate issues of bias and articulate transparency  i e  to authorities or end users  about the function and possible effects of their algorithms  The project was approved February      and is sponsored by the Software  amp  Systems Engineering Standards Committee               a committee chartered by the IEEE Computer Society  A draft of the standard is expected to be submitted for balloting in June                                The standard was published in January                   
In       the IEEE released a standard aimed at specifying methodologies to help creators of algorithms address issues of bias and promote transparency regarding the function and potential effects of their algorithms  The project  initially approved in February       was sponsored by the Software  amp  Systems Engineering Standards Committee               a committee under the IEEE Computer Society  The standard provides guidelines for articulating transparency to authorities or end users and mitigating algorithmic biases                                        

Transparency and monitoring edit 
Further information  Algorithmic transparency
Ethics guidelines on AI point to the need for accountability  recommending that steps be taken to improve the interpretability of results               Such solutions include the consideration of the  right to understanding  in machine learning algorithms  and to resist deployment of machine learning in situations where the decisions could not be explained or reviewed               Toward this end  a movement for  Explainable AI  is already underway within organizations such as DARPA  for reasons that go beyond the remedy of bias               Price Waterhouse Coopers  for example  also suggests that monitoring output means designing systems in such a way as to ensure that solitary components of the system can be isolated and shut down if they skew results              
An initial approach towards transparency included the open sourcing of algorithms               Software code can be looked into and improvements can be proposed through source code hosting facilities  However  this approach doesn t necessarily produce the intended effects  Companies and organizations can share all possible documentation and code  but this does not establish transparency if the audience doesn t understand the information given  Therefore  the role of an interested critical audience is worth exploring in relation to transparency  Algorithms cannot be held accountable without a critical audience              

Right to remedy edit 
From a regulatory perspective  the Toronto Declaration calls for applying a human rights framework to harms caused by algorithmic bias               This includes legislating expectations of due diligence on behalf of designers of these algorithms  and creating accountability when private actors fail to protect the public interest  noting that such rights may be obscured by the complexity of determining responsibility within a web of complex  intertwining processes               Others propose the need for clear liability insurance mechanisms              

Diversity and inclusion edit 
Amid concerns that the design of AI systems is primarily the domain of white  male engineers               a number of scholars have suggested that algorithmic bias may be minimized by expanding inclusion in the ranks of those designing AI systems                            For example  just     of machine learning engineers are women               with black AI leaders pointing to a  diversity crisis  in the field               Groups like Black in AI and Queer in AI are attempting to create more inclusive spaces in the AI community and work against the often harmful desires of corporations that control the trajectory of AI research               Critiques of simple inclusivity efforts suggest that diversity programs can not address overlapping forms of inequality  and have called for applying a more deliberate lens of intersectionality to the design of algorithms                                                Researchers at the University of Cambridge have argued that addressing racial diversity is hampered by the  whiteness  of the culture of AI              

Interdisciplinarity and Collaboration edit 
Integrating interdisciplinarity and collaboration in developing of AI systems can play a critical role in tackling algorithmic bias  Integrating insights  expertise  and perspectives from disciplines outside of computer science can foster a better understanding of the impact data driven solutions have on society  An example of this in AI research is PACT or Participatory Approach to enable Capabilities in communiTies  a proposed framework for facilitating collaboration when developing AI driven solutions concerned with social impact               This framework identifies guiding principals for stakeholder participation when working on AI for Social Good  AI SG  projects  PACT attempts to reify the importance of decolonizing and power shifting efforts in the design of human centered AI solutions  An academic initiative in this regard is the Stanford University s Institute for Human Centered Artificial Intelligence which aims to foster multidisciplinary collaboration  The mission of the institute is to advance artificial intelligence  AI  research  education  policy and practice to improve the human condition              
Collaboration with outside experts and various stakeholders facilitates ethical  inclusive  and accountable development of intelligent systems  It incorporates ethical considerations  understands the social and cultural context  promotes human centered design  leverages technical expertise  and addresses policy and legal considerations               Collaboration across disciplines is essential to effectively mitigate bias in AI systems and ensure that AI technologies are fair  transparent  and accountable 

Regulation edit 
Europe edit 
The General Data Protection Regulation  GDPR   the European Union s revised data protection regime that was implemented in       addresses  Automated individual decision making  including profiling  in Article     These rules prohibit  solely  automated decisions which have a  significant  or  legal  effect on an individual  unless they are explicitly authorised by consent  contract  or member state law  Where they are permitted  there must be safeguards in place  such as a right to a human in the loop  and a non binding right to an explanation of decisions reached  While these regulations are commonly considered to be new  nearly identical provisions have existed across Europe since       in Article    of the Data Protection Directive  The original automated decision rules and safeguards found in French law since the late     s              

The GDPR addresses algorithmic bias in profiling systems  as well as the statistical approaches possible to clean it  directly in recital                  noting thatthe controller should use appropriate mathematical or statistical procedures for the profiling  implement technical and organisational measures appropriate          that prevents  inter alia  discriminatory effects on natural persons on the basis of racial or ethnic origin  political opinion  religion or beliefs  trade union membership  genetic or health status or sexual orientation  or that result in measures having such an effect Like the non binding right to an explanation in recital     the problem is the non binding nature of recitals               While it has been treated as a requirement by the Article    Working Party that advised on the implementation of data protection law               its practical dimensions are unclear  It has been argued that the Data Protection Impact Assessments for high risk data profiling  alongside other pre emptive measures within data protection  may be a better way to tackle issues of algorithmic discrimination  as it restricts the actions of those deploying algorithms  rather than requiring consumers to file complaints or request changes              
United States edit 
The United States has no general legislation controlling algorithmic bias  approaching the problem through various state and federal laws that might vary by industry  sector  and by how an algorithm is used               Many policies are self enforced or controlled by the Federal Trade Commission               In       the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan               which was intended to guide policymakers toward a critical assessment of algorithms  It recommended researchers to  design these systems so that their actions and decision making are transparent and easily interpretable by humans  and thus can be examined for any bias they may contain  rather than just learning and repeating these biases   Intended only as guidance  the report did not create any legal precedent                                   
In       New York City passed the first algorithmic accountability bill in the United States               The bill  which went into effect on January          required  the creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with the public  and how agencies may address instances where people are harmed by agency automated decision systems                In       New York City implemented a law requiring employers using automated hiring tools to conduct independent  bias audits  and publish the results  This law marked one of the first legally mandated transparency measures for AI systems used in employment decisions in the United States                The task force is required to present findings and recommendations for further regulatory action in                   
On February           according to Executive Order        the federal government unveiled the  American AI Initiative   a comprehensive strategy to maintain U S  leadership in artificial intelligence  The initiative highlights the importance of sustained AI research and development  ethical standards  workforce training  and the protection of critical AI technologies               This aligns with broader efforts to ensure transparency  accountability  and innovation in AI systems across public and private sectors  Furthermore  on October           the President signed Executive Order        which emphasizes the safe  secure  and trustworthy development and use of artificial intelligence  AI   The order outlines a coordinated  government wide approach to harness AI s potential while mitigating its risks  including fraud  discrimination  and national security threats  An important point in the commitment is promoting responsible innovation and collaboration across sectors to ensure that AI benefits society as a whole               With this order  President Joe Biden mandated the federal government to create best practices for companies to optimize AI s benefits and minimize its harms              

India edit 
On July           a draft of the Personal Data Bill was presented               The draft proposes standards for the storage  processing and transmission of data  While it does not use the term algorithm  it makes for provisions for  harm resulting from any processing or any kind of processing undertaken by the fiduciary   It defines  any denial or withdrawal of a service  benefit or good resulting from an evaluative decision about the data principal  or  any discriminatory treatment  as a source of harm that could arise from improper use of data  It also makes special provisions for people of  Intersex status               

See also edit 
Algorithmic wage discrimination
Ethics of artificial intelligence
Fairness  machine learning 
Hallucination  artificial intelligence 
Misaligned goals in artificial intelligence
Predictive policing
SenseTime
References edit 


  Jacobi  Jennifer  September             Patent  US             Espacenet  Retrieved July         

  Hardebolle  C cile  H der  Mih ly  Ramachandran  Vivek  November             Engineering ethics education and artificial intelligence   The Routledge International Handbook of Engineering Ethics Education         ed    London  Routledge  pp                doi                          ISBN                       

  Van Eyghen  Hans          AI Algorithms as  Un virtuous Knowers   Discover Artificial Intelligence         doi         s                z 

  Marabelli  Marco         AI  Ethics  and Discrimination in Business  Palgrave Studies in Equity  Diversity  Inclusion  and Indigenization in Business  Springer  doi                            ISBN                        

  Suresh  Harini  Guttag  John  November            A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle   Equity and Access in Algorithms  Mechanisms  and Optimization  EAAMO      New York  NY  USA  Association for Computing Machinery  pp            doi                          ISBN                         S CID                

  Mehrabi  N   Morstatter  F   Saxena  N   Lerman  K   Galstyan  A           A survey on bias and fairness in machine learning   ACM Computing Surveys                Retrieved April          

  Striphas  Ted  February         What is an Algorithm    Culture Digitally   culturedigitally org  Retrieved November          

  Cormen  Thomas H   Leiserson  Charles E   Rivest  Ronald L   Stein  Clifford         Introduction to Algorithms   rd      ed    Cambridge  Mass   MIT Press  p          ISBN                        

  a b c Kitchin  Rob  February             Thinking critically about and researching algorithms   PDF   Information  Communication  amp  Society                 doi                X               S CID               

   How Google Search Works   Retrieved November          

  Luckerson  Victor   Here s How Your Facebook News Feed Actually Works   Time  Retrieved November          

  Vanderbilt  Tom  August            The Science Behind the Netflix Algorithms That Decide What You ll Watch Next   Wired  Retrieved November          

  Angwin  Julia  Mattu  Surya  September             Amazon Says It Puts Customers First  But Its Pricing Algorithm Doesn t   ProPublica   ProPublica  Retrieved November          

  Livingstone  Rob  March             The future of online advertising is big data and algorithms   The Conversation  Retrieved November          

  Hickman  Leo  July            How algorithms rule the world   The Guardian  Retrieved November          

  a b c d e f Seaver  Nick   Knowing Algorithms   PDF   Media in Transition    Cambridge  MA  April       Archived from the original  PDF  on December          Retrieved November          

  a b c Graham  Stephen D N   July         Software sorted geographies   PDF   Progress in Human Geography  Submitted manuscript                    doi                   ph   oa  S CID               

  a b c Tewell  Eamon  April            Toward the Resistant Reading of Information  Google  Resistant Spectatorship  and Critical Information Literacy   Portal  Libraries and the Academy                   doi         pla            ISSN                 S CID                Retrieved November          

  Crawford  Kate  April            The Hidden Biases in Big Data   Harvard Business Review 

  a b c d e f g h i j k l m n o p q Friedman  Batya  Nissenbaum  Helen  July         Bias in Computer Systems   PDF   ACM Transactions on Information Systems                   doi                        S CID                 Retrieved March          

  a b c d e f Gillespie  Tarleton  Boczkowski  Pablo  Foot  Kristin         Media Technologies  Cambridge  MIT Press  pp             ISBN                    

  a b Diakopoulos  Nicholas   Algorithmic Accountability  On the Investigation of Black Boxes     towcenter org  Retrieved November          

  Lipartito  Kenneth  January           The Narrative and the Algorithm  Genres of Credit Reporting from the Nineteenth Century to Today  PDF   Submitted manuscript   doi         SSRN          S CID                 SSRN              

  a b Goodman  Bryce  Flaxman  Seth          EU regulations on algorithmic decision making and a  right to explanation    AI Magazine              arXiv             doi         aimag v  i        S CID              

  a b c d e f g Weizenbaum  Joseph         Computer Power and Human Reason  From Judgment to Calculation  San Francisco  W H  Freeman  ISBN                        

  a b Goffrey  Andrew          Algorithm   In Fuller  Matthew  ed    Software Studies  A Lexicon  Cambridge  Mass   MIT Press  pp              ISBN                        

  Lowry  Stella  Macpherson  Gordon  March            A Blot on the Profession   British Medical Journal                     doi         bmj               PMC               PMID               Retrieved November          

   Perpetual Lineup   www law georgetown edu  Retrieved December          

  Buolamwini  Joy   Gender Shades  Intersectional Accuracy Disparities in Commercial Gender Classification   MIT Media Lab  Retrieved December          

  Barocas  Solon  December            Fairness and machine learning  Limitations and opportunities  The MIT Press  ISBN                    

  Miller  Alex P   July             Want Less Biased Decisions  Use Algorithms   Harvard Business Review  Retrieved July          

  a b c Introna  Lucas D   December            The Enframing of Code   Theory  Culture  amp  Society                   doi                           S CID                

  Bogost  Ian  January             The Cathedral of Computation   The Atlantic  Retrieved November          

  a b c d e f g Introna  Lucas  Wood  David          Picturing algorithmic surveillance  the politics of facial recognition systems   Surveillance  amp  Society              Retrieved November          

  a b c d Introna  Lucas D   December             Maintaining the reversibility of foldings  Making the ethics  politics  of information technology visible   Ethics and Information Technology                CiteSeerX                       doi         s               z  S CID               

  a b Shirky  Clay   A Speculative Post on the Idea of Algorithmic Authority Clay Shirky   www shirky com  Archived from the original on March           Retrieved November          

  a b Ziewitz  Malte  January            Governing Algorithms  Myth  Mess  and Methods   Science  Technology   amp  Human Values                doi                           ISSN                 S CID                

  Lash  Scott  June             Power after Hegemony   Theory  Culture  amp  Society                 doi                           S CID                

  Gaudeul  Alexia  Arrigoni  Ottla  Charisi  Vicky  Escobar Planas  Marina  Hupont  Isabelle          Understanding the Impact of Human Oversight on Discriminatory Outcomes in AI Supported Decision Making   ECAI       Frontiers in Artificial Intelligence and Applications  IOS Press  pp                  doi         faia        ISBN                       

  a b Garcia  Megan  January            Racist in the Machine   World Policy Journal                   doi                           S CID                

   ACM FAccT      Registration   fatconference org  Retrieved November          

  Ochigame  Rodrigo  December             The Invention of  Ethical AI   How Big Tech Manipulates Academia to Avoid Regulation   The Intercept  Retrieved February          

  Sergot  MJ  Sadri  F  Kowalski  RA  Kriwaczek  F  Hammond  P  Cory  HT  May         The British Nationality Act as a Logic Program   PDF   Communications of the ACM                   doi                    S CID               Retrieved November          

  a b  To stop algorithmic bias  we first have to define it   Brookings  Retrieved June          

  Evans  Melanie  Mathews  Anna Wilde  October             Researchers Find Racial Bias in Hospital Algorithm   Wall Street Journal  ISSN                 Retrieved June          

  a b c Luo  Queenie  Puett  Michael J   Smith  Michael D   May            A Perspectival Mirror of the Elephant  Investigating Language Bias on Google  ChatGPT  Wikipedia  and YouTube  arXiv           

  Hofmann  Valentin  Kalluri  Pratyusha Ria  Jurafsky  Dan  King  Sharese  September            AI generates covertly racist decisions about people based on their dialect   Nature                       Bibcode     Natur         H  doi         s                   PMC                PMID               

  Choi  Hyeong Kyu  Xu  Weijie  Xue  Chi  Eckman  Stephanie  Reddy  Chandan K   September            Mitigating Selection Bias with Node Pruning and Auxiliary Options  arXiv           

  Zheng  Chujie  Zhou  Hao  Meng  Fandong  Zhou  Jie  Huang  Minlie  September           Large Language Models Are Not Robust Multiple Choice Selectors  arXiv           

  Busker  Tony  Choenni  Sunil  Shoae Bargh  Mortaza  November             Stereotypes in ChatGPT  An empirical study   Proceedings of the   th International Conference on Theory and Practice of Electronic Governance  ICEGOV      New York  NY  USA  Association for Computing Machinery  pp              doi                          ISBN                        

  Kotek  Hadas  Dockum  Rikker  Sun  David  November            Gender bias and stereotypes in Large Language Models   Proceedings of the ACM Collective Intelligence Conference  CI      New York  NY  USA  Association for Computing Machinery  pp              doi                          ISBN                        

  Cheng  Myra  Durmus  Esin  Jurafsky  Dan  May            Marked Personas  Using Natural Language Prompts to Measure Stereotypes in Language Models  arXiv           

  Wang  Angelina  Morgenstern  Jamie  Dickerson  John P   February             Large language models that replace human participants can harmfully misportray and flatten identity groups   Nature Machine Intelligence                  arXiv             doi         s                z 

  Fri riksd ttir  Steinunn Rut  Einarsson  Hafsteinn          Gendered Grammar or Ingrained Bias  Exploring Gender Bias in Icelandic Language Models   Lrec Coling                

  Feng  Shangbin  Park  Chan Young  Liu  Yuhan  Tsvetkov  Yulia  July        Rogers  Anna  Boyd Graber  Jordan  Okazaki  Naoaki  eds     From Pretraining Data to Language Models to Downstream Tasks  Tracking the Trails of Political Biases Leading to Unfair NLP Models   Proceedings of the   st Annual Meeting of the Association for Computational Linguistics  Volume    Long Papers   Toronto  Canada  Association for Computational Linguistics               arXiv             doi          v       acl long     

  Dolan  Eric W   February             Scientists reveal ChatGPT s left wing bias   and how to  jailbreak  it   PsyPost   Psychology News  Retrieved February          

  Lazaro  Gina  May             Understanding Gender and Racial Bias in AI   Harvard Advanced Leadership Initiative Social Impact Review  Retrieved December          

  Jindal  Atin  September            Misguided Artificial Intelligence  How Racial Bias is Built Into Clinical Models   Journal of Brown Hospital Medicine                doi             c        PMC                PMID               

  Gillespie  Tarleton  June             Algorithm      draft            digitalkeywords        Culture Digitally   culturedigitally org  Retrieved November          

  Roth  A  E              December             New physicians  A natural experiment in market organization   Science                         Bibcode     Sci           R  doi         science          PMID               S CID                Retrieved November            cite journal     CS  maint  numeric names  authors list  link 

  Kuang  Cliff  November             Can A I  Be Taught to Explain Itself    The New York Times Magazine  Retrieved November          

  a b Jouvenal  Justin  November             Police are using software to predict crime  Is it a  holy grail  or biased against minorities    Washington Post  Retrieved November          

  a b Chamma  Maurice  February            Policing the Future   The Marshall Project  Retrieved November          

  Lum  Kristian  Isaac  William  October         To predict and serve    Significance                 doi         j                      x 

  Smith  Jack  October            Predictive policing only amplifies racial bias  study shows   Mic  Retrieved November          

  Lum  Kristian  Isaac  William  October            FAQs on Predictive Policing and Bias   HRDAG  Retrieved November          

  Bahl  Utsav  Topaz  Chad  Oberm ller  Lea  Goldstein  Sophie  Sneirson  Mira  May             Algorithms in Judges  Hands  Incarceration and Inequity in Broward County  Florida   UCLA Law Review           

  Sun  Wenlong  Nasraoui  Olfa  Shafto  Patrick          Iterated Algorithmic Bias in the Interactive Machine Learning Process of Information Filtering   Proceedings of the   th International Joint Conference on Knowledge Discovery  Knowledge Engineering and Knowledge Management  Seville  Spain  SCITEPRESS   Science and Technology Publications  pp                doi                           ISBN                    

  Sinha  Ayan  Gleich  David F   Ramani  Karthik  August            Gauss s law for networks directly reveals community boundaries   Scientific Reports                Bibcode     NatSR         S  doi         s                   ISSN                 PMC               PMID               

  Hao  Karen  February         Google is finally admitting it has a filter bubble problem   Quartz  Retrieved February          

   Facebook Is Testing This New Feature to Fight  Filter Bubbles    Fortune  Retrieved February          

  a b Sandvig  Christian  Hamilton  Kevin  Karahalios  Karrie  Langbort  Cedric  May             Auditing Algorithms  Research Methods for Detecting Discrimination on Internet Platforms   PDF     th Annual Meeting of the International Communication Association  Retrieved November          

  Brin  Sergey  Page  Lawrence   The Anatomy of a Search Engine   www  scu edu au  Archived from the original on July          Retrieved November          

  Epstein  Robert  Robertson  Ronald E   August             The search engine manipulation effect  SEME  and its possible impact on the outcomes of elections   Proceedings of the National Academy of Sciences            E       E      Bibcode     PNAS     E    E  doi         pnas             PMC               PMID               

  Bond  Robert M   Fariss  Christopher J   Jones  Jason J   Kramer  Adam D  I   Marlow  Cameron  Settle  Jaime E   Fowler  James H   September             A    million person experiment in social influence and political mobilization   Nature                     Bibcode     Natur         B  doi         nature       ISSN                 PMC               PMID               

  Zittrain  Jonathan          Engineering an Election   PDF   Harvard Law Review Forum                Archived from the original  PDF  on March          Retrieved November          

  Day  Matt  August             How LinkedIn s search engine may reflect a gender bias   The Seattle Times  Retrieved November          

  a b Crawford  Kate  Schultz  Jason          Big Data and Due Process  Toward a Framework to Redress Predictive Privacy Harms   Boston College Law Review                  Retrieved November          

  Duhigg  Charles  February             How Companies Learn Your Secrets   The New York Times Magazine  Retrieved November          

  Noble  Safiya          Missed Connections  What Search Engines Say about Women   PDF   Bitch                

  Guynn  Jessica  March             Google starts flagging offensive content in search results   USA TODAY  USA Today  Retrieved November          

  Simonite  Tom   Study Suggests Google s Ad Targeting System May Discriminate   MIT Technology Review  Massachusetts Institute of Technology  Retrieved November          

  Prates  Marcelo O  R   Avelar  Pedro H  C   Lamb  Luis          Assessing Gender Bias in Machine Translation    A Case Study with Google Translate   arXiv             cs CY  

  Prates  Marcelo O  R   Avelar  Pedro H   Lamb  Lu s C           Assessing gender bias in machine translation  A case study with Google Translate   Neural Computing and Applications                      arXiv             doi         s                   S CID               

  Claburn  Thomas  September             Boffins bash Google Translate for sexism   The Register  Retrieved April          

  Dastin  Jeffrey  October            Amazon scraps secret AI recruiting tool that showed bias against women   Reuters 

  Vincent  James  October             Amazon reportedly scraps internal AI recruiting tool that was biased against women   The Verge 

   Reflecting on Spotify s Recommender System   SongData   October       Retrieved August         

  Buolamwini  Joy  Gebru  Timnit  January             Gender Shades  Intersectional Accuracy Disparities in Commercial Gender Classification   Proceedings of Machine Learning Research                    Retrieved September          

  Noble  Safiya Umoja  February            Algorithms of Oppression  How Search Engines Reinforce Racism  New York  NYU Press  ISBN                     

  a b Nakamura  Lisa         Magnet  Shoshana  Gates  Kelly  eds    The New Media of Surveillance  London  Routledge  pp                ISBN                        

  Marco Marabelli  Sue Newell  Valerie Handunge          The lifecycle of algorithmic decision making systems  Organizational choices and ethical challenges   Journal of Strategic Information Systems                doi         j jsis             

  Alexander  Rudolph  Gyamerah  Jacquelyn  September         Differential Punishing of African Americans and Whites Who Possess Drugs  A Just Policy or a Continuation of the Past    Journal of Black Studies                  doi                             ISSN                 S CID                

  Petersilia  Joan  January         Racial Disparities in the Criminal Justice System  A Summary   Crime  amp  Delinquency                 doi                              ISSN                 S CID                

  a b Guynn  Jessica  July            Google Photos labeled black people  gorillas    USA TODAY  USA Today  Retrieved November          

  Rose  Adam  January             Are Face Detection Cameras Racist    Time  Retrieved November          

   Alexa does not understand your accent   Washington Post 

  Sweeney  Latanya  January             Discrimination in Online Ad Delivery   arXiv            cs IR  

  Braun  Lundy          Race  ethnicity and lung function  A brief history   Canadian Journal of Respiratory Therapy                  ISSN                 PMC               PMID               

  Robinson  Whitney R  Renson  Audrey  Naimi  Ashley I  April            Teaching yourself about structural racism will improve your machine learning   Biostatistics                   doi         biostatistics kxz     ISSN                 PMC               PMID               

  Johnson  Carolyn Y   October             Racial bias in a medical algorithm favors white patients over sicker black patients   Washington Post  Retrieved October          

  Bartlett  Robert  Morse  Adair  Stanton  Richard  Wallace  Nancy  June         Consumer Lending Discrimination in the FinTech Era   NBER Working Paper No         Working Paper Series  doi         w       S CID                

  Hofmann  V   Kalluri  P R   Jurafsky  D  et al  AI generates covertly racist decisions about people based on their dialect  Nature                      https   doi org         s                 

  Stub  Zev   Study  ChatGPT  Meta s Llama and all other top AI models show anti Jewish  anti Israel bias   www timesofisrael com  Retrieved March          

  Buolamwini  J   Gebru  T           Gender Shades  Intersectional Accuracy Disparities in Commercial Gender Classification   Proceedings of the  st Conference on Fairness  Accountability and Transparency  pp              Retrieved April          

  Jeff Larson  Julia Angwin  May             How We Analyzed the COMPAS Recidivism Algorithm   ProPublica  Archived from the original on April           Retrieved June          

   Commentary  Bad news  Artificial intelligence is biased   CNA  January           Archived from the original on January           Retrieved June          

  a b Angwin  Julia  Larson  Jeff  Mattu  Surya  Kirchner  Lauren  May             Machine Bias   ProPublica   ProPublica  Retrieved November          

  Harcourt  Bernard  September             Risk as a Proxy for Race   Federal Sentencing Reporter               doi         fsr                S CID                SSRN              

  Skeem J  Lowenkamp C  Risk  Race   amp  Recidivism  Predictive Bias and Disparate Impact   June            SSRN             

  Thomas  C   Nunez  A           Automating Judicial Discretion  How Algorithmic Risk Assessments in Pretrial Adjudications Violate Equal Protection Rights on the Basis of Race   Law  amp  Inequality                   doi                       

  a b Angwin  Julia  Grassegger  Hannes  June             Facebook s Secret Censorship Rules Protect White Men From Hate Speech But Not Black Children   ProPublica   ProPublica  Retrieved November          

  Angwin  Julia  Varner  Madeleine  Tobin  Ariana  September             Facebook Enabled Advertisers to Reach  Jew Haters    ProPublica   ProPublica  Retrieved November          

  Sap  Maarten  Card  Dallas  Gabriel  Saadia  Choi  Yejin  Smith  Noah A   July      August            The Risk of Racial Bias in Hate Speech Detection   PDF   Proceedings of the   th Annual Meeting of the Association for Computational Linguist  Florence  Italy  Association for Computational Linguistics  pp                  Archived  PDF  from the original on August          

  Ghaffary  Shirin  August             The algorithms that detect hate speech online are biased against black people   Vox  Retrieved February          

  a b Nakajima Wickham  E    amp   hman  E          Hate speech  censorship  and freedom of speech  The changing policies of reddit  Journal of Data Mining  amp  Digital Humanities  NLP DH  https   doi org          jdmdh     

  Furl  N  December         Face recognition algorithms and the other race effect  computational mechanisms for a developmental contact hypothesis   Cognitive Science                   doi         s        cog       

  Raji  Inioluwa Deborah  Gebru  Timnit  Mitchell  Margaret  Buolamwini  Joy  Lee  Joonseok  Denton  Emily  February            Saving Face   Proceedings of the AAAI ACM Conference on AI  Ethics  and Society  Association for Computing Machinery  pp                arXiv             doi                          ISBN                     S CID                

   Facial Recognition Is Accurate  if You re a White Guy   The New York Times  February          Retrieved August          

  Buolamwini  Joy  Gebru  Timnit          Gender Shades  Intersectional Accuracy Disparities in Commercial Gender Classification   PDF   Proceedings of Machine Learning Research                via MLR Press 

  Ananny  Mike  April             The Curious Connection Between Apps for Gay Men and Sex Offenders   The Atlantic  Retrieved November          

  Kafka  Peter   Did Amazon Really Fail This Weekend  The Twittersphere Says  Yes   Online Retailer Says  Glitch     AllThingsD  Retrieved November          

  Kafka  Peter   Amazon Apologizes for  Ham fisted Cataloging Error    AllThingsD  Retrieved November          

  Matsakis  Louise  February             A  Sexist  Search Bug Says More About Us Than Facebook   Wired  ISSN                 Retrieved February          

   Some AI just shouldn t exist   April          

  Samuel  Sigal  April             Some AI just shouldn t exist   Vox  Retrieved December          

  Wang  Yilun  Kosinski  Michal  February             Deep neural networks are more accurate than humans at detecting sexual orientation from facial images   OSF  doi          OSF IO ZN  K 

  Levin  Sam  September            LGBT groups denounce  dangerous  AI that uses your face to guess sexuality   The Guardian  ISSN                 Retrieved December          

  Pal  G C   September             Disability  Intersectionality and Deprivation  An Excluded Agenda   Psychology and Developing Societies                  doi                             S CID                        via Sagepub 

  Brinkman  Aurora H   Rea Sandin  Gianna  Lund  Emily M   Fitzpatrick  Olivia M   Gusman  Michaela S   Boness  Cassandra L   Scholars for Elevating Equity and Diversity  SEED   October             Shifting the discourse on disability  Moving to an inclusive  intersectional focus   American Journal of Orthopsychiatry                 doi         ort         ISSN                 PMC               PMID               

  Whittaker  Meredith  November         Disability  Bias  and AI   PDF   Archived from the original  PDF  on March           Retrieved December         

   Mission   Disability is Diversity   Dear Entertainment Industry  THERE S NO DIVERSITY  EQUITY  amp  INCLUSION WITHOUT DISABILITY   Disability is Diversity  Retrieved December         

   Microsoft Design   www microsoft com  Retrieved December         

  Pulrang  Andrew     Ways To Understand The Diversity Of The Disability Community   Forbes  Retrieved December         

  Watermeyer  Brian  Swartz  Leslie  October             Disability and the problem of lazy intersectionality   Disability  amp  Society                   doi                                ISSN                 S CID                

   Disability Data Report        Disability Data Initiative  May           Retrieved December         

  White  Jason J  G   March            Fairness of AI for people with disabilities  problem analysis and interdisciplinary collaboration   ACM SIGACCESS Accessibility and Computing             doi                          ISSN                 S CID                

   AI language models show bias against people with disabilities  study finds   Penn State University   www psu edu  Retrieved December         

  Givens  Alexandra Reeve  February            How Algorithmic Bias Hurts People With Disabilities   Slate Magazine  Retrieved December         

  Morris  Meredith Ringel  May             AI and accessibility   Communications of the ACM                 arXiv             doi                  ISSN                 S CID                

  Noble  Safiya Umoja  February            Algorithms of Oppression  How Search Engines Reinforce Racism  New York  ISBN                     OCLC                  cite book     CS  maint  location missing publisher  link 

  Castelnovo  Alessandro  Inverardi  Nicole  Nanino  Gabriele  Penco  Ilaria  Regoli  Daniele          Fair Enough  A map of the current limitations to the requirements to have  fair  algorithms   arXiv             cs AI  

  Ruggieri  Salvatore  Alvarez  Jose M  Pugnana  Andrea  Turini  Franco          Can We Trust Fair AI    Proceedings of the AAAI Conference on Artificial Intelligence                       doi         aaai v  i          hdl               S CID                

  Samuel  Sigal  April             Why it s so damn hard to make AI fair and unbiased   Vox  Retrieved July          

  Fioretto  Ferdinando  March             Building fairness into AI is crucial   and hard to get right   The Conversation  Retrieved July          

  Friedler  Sorelle A   Scheidegger  Carlos  Venkatasubramanian  Suresh          On the  im possibility of fairness   arXiv             cs CY  

  Hu  Lily  Chen  Yiling          Welfare and Distributional Impacts of Fair Classification   arXiv             cs LG  

  Dwork  Cynthia  Hardt  Moritz  Pitassi  Toniann  Reingold  Omer  Zemel  Rich  November             Fairness Through Awareness   arXiv            cs CC  

  a b c Sandvig  Christian  Hamilton  Kevin  Karahalios  Karrie  Langbort  Cedric         Gangadharan  Seeta Pena  Eubanks  Virginia  Barocas  Solon  eds     An Algorithm Audit   PDF   Data and Discrimination  Collected Essays 

  LaFrance  Adrienne  September             The Algorithms That Power the Web Are Only Getting More Mysterious   The Atlantic  Retrieved November          

  Bruno Latour         Pandora s Hope  Essays On the Reality of Science Studies  Cambridge  Massachusetts  Harvard University Press 

  Kubitschko  Sebastian  Kaun  Anne         Innovative Methods in Media and Communication Research  Springer  ISBN                         Retrieved November          

  McGee  Matt  August             EdgeRank Is Dead  Facebook s News Feed Algorithm Now Has Close To    K Weight Factors   Marketing Land  Retrieved November          

  a b c Granka  Laura A   September             The Politics of Search  A Decade Retrospective   PDF   The Information Society                   doi                               S CID                Retrieved November          

  Szymielewicz  Katarzyna  January             Black Boxed Politics   Medium  Retrieved February          

  Veale  Michael  Binns  Reuben          Fairer machine learning in the real world  Mitigating discrimination without collecting sensitive data   Big Data  amp  Society                          doi                           SSRN              

  Elliott  Marc N   Morrison  Peter A   Fremont  Allen  McCaffrey  Daniel F   Pantoja  Philip  Lurie  Nicole  June         Using the Census Bureau s surname list to improve estimates of race ethnicity and associated disparities   Health Services and Outcomes Research Methodology                doi         s                  ISSN                 S CID               

  Chen  Jiahao  Kallus  Nathan  Mao  Xiaojie  Svacha  Geoffry  Udell  Madeleine          Fairness Under Unawareness   Proceedings of the Conference on Fairness  Accountability  and Transparency  Atlanta  GA  USA  ACM Press  pp                arXiv             doi                          ISBN                     S CID               

  Kilbertus  Niki  Gascon  Adria  Kusner  Matt  Veale  Michael  Gummadi  Krishna  Weller  Adrian          Blind Justice  Fairness with Encrypted Sensitive Attributes   International Conference on Machine Learning             arXiv             Bibcode     arXiv         K 

  Binns  Reuben  Veale  Michael  Kleek  Max Van  Shadbolt  Nigel  September             Like Trainer  Like Bot  Inheritance of Bias in Algorithmic Content Moderation   Social Informatics  Lecture Notes in Computer Science  Vol              pp                arXiv             doi                               ISBN                         S CID              

  Claburn  Thomas  July             EU Data Protection Law May End The Unknowable Algorithm   InformationWeek   InformationWeek  Retrieved November          

  a b Jobin  Anna  Ienca  Marcello  Vayena  Effy  September            The global landscape of AI ethics guidelines   Nature Machine Intelligence                  arXiv             doi         s                  S CID                

  Wattenberg  Martin  Vi gas  Fernanda  Hardt  Moritz   Attacking discrimination with smarter machine learning   Google Research 

  Hardt  Moritz  Price  Eric  Srebro  Nathan          Equality of Opportunity in Supervised Learning   arXiv             cs LG  

  Wiggers  Kyle  May             Microsoft is developing a tool to help engineers catch bias in algorithms   VentureBeat com 

   Facebook says it has a tool to detect bias in its artificial intelligence   Quartz  May          Archived from the original on March         

   Pymetrics audit AI   GitHub com 

  Johnson  Khari  May             Pymetrics open sources Audit AI  an algorithm bias detection tool   VentureBeat com 

   Aequitas  Bias and Fairness Audit Toolkit   GitHub com 

  https   dsapp uchicago edu aequitas  open sources Audit AI  Aequitas at University of Chicago

  Puri  Ruchir  February            Mitigating Bias in AI Models   IBM com  Archived from the original on February         

  S  Sen  D  Dasgupta and K  D  Gupta   An Empirical Study on Algorithmic Bias        IEEE   th Annual Computers  Software  and Applications Conference  COMPSAC   Madrid  Spain        pp             doi         COMPSAC                 

  Zou  James  Schiebinger  Londa  July         AI can be sexist and racist   it s time to make it fair   Nature                       Bibcode     Natur         Z  doi         d                   PMID               

  Jia  Sen  Welfare  Thomas  Cristianini  Nello         Right for the right reason  Training agnostic networks  International Symposium on Intelligent Data Analysis  Springer 

  Sutton  Adam  Welfare  Thomas  Cristianini  Nello         Biased embeddings from wild data  Measuring  understanding and removing  International Symposium on Intelligent Data Analysis  Springer  doi                              

   Software  amp  Systems Engineering Standards Committee   April          

  a b Koene  Ansgar  June         Algorithmic Bias  Addressing Growing Concerns      Leading Edge        PDF   IEEE Technology and Society Magazine                 doi         mts               ISSN                 Archived from the original  PDF  on July           Retrieved August         

  a b  P       Algorithmic Bias Considerations   IEEE  Archived from the original on December          Retrieved December         

   IEEE           IEEE Standard for Algorithmic Bias Considerations   Retrieved March          

   Software  amp  Systems Engineering Standards Committee   April          

   IEEE CertifAIEd    Ontological Specification for Ethical Algorithmic Bias   PDF   IEEE       

  The Internet Society  April             Artificial Intelligence and Machine Learning  Policy Paper   Internet Society  Retrieved February          

  a b  White Paper  How to Prevent Discriminatory Outcomes in Machine Learning   World Economic Forum  March           Retrieved February          

   Explainable Artificial Intelligence   www darpa mil  Retrieved February          

  PricewaterhouseCoopers   The responsible AI framework   PwC  Retrieved February          

  Heald  David  September           Transparency  The Key to Better Governance   British Academy  doi         bacad                         ISBN                        

  Kemper  Jakko  Kolkman  Daan  December            Transparent to whom  No algorithmic accountability without a critical audience   Information  Communication  amp  Society                      doi                X               hdl           cb      fe        a     ef     d e  ISSN              X 

   The Toronto Declaration  Protecting the rights to equality and non discrimination in machine learning systems   Human Rights Watch  July          Retrieved February          

  The Toronto Declaration  Protecting the Right to Equality and Non Discrimination in Machine Learning Systems  PDF   Human Rights Watch        p          

  Floridi  Luciano  Cowls  Josh  Beltrametti  Monica  Chatila  Raja  Chazerand  Patrice  Dignum  Virginia  Luetge  Christoph  Madelin  Robert  Pagallo  Ugo  Rossi  Francesca  Schafer  Burkhard  December            AI People An Ethical Framework for a Good AI Society  Opportunities  Risks  Principles  and Recommendations   Minds and Machines               doi         s                  ISSN                 PMC               PMID               

  Crawford  Kate  June             Opinion   Artificial Intelligence s White Guy Problem   The New York Times  ISSN                 Retrieved February          

   AI Is the Future But Where Are the Women    Wired  ISSN                 Retrieved February          

  Snow  Jackie    We re in a diversity crisis   cofounder of Black in AI on what s poisoning algorithms in our lives   MIT Technology Review  Retrieved February          

  Hao  Karen  June             Inside the fight to reclaim AI from Big Tech s control   MIT Technology Review  Retrieved June          

  Ciston  Sarah  December             Intersectional AI Is Essential   Journal of Science and Technology of the Arts               doi         citarj v  i       ISSN                

  D Ignazio  Catherine  Klein  Lauren F          Data Feminism  MIT Press  ISBN                     

  Cave  Stephen  Dihal  Kanta  August            The Whiteness of AI   Philosophy  amp  Technology                   doi         s                   ISSN                

  Bondi  Elizabeth  Xu  Lily  Acosta Navas  Diana  Killian  Jackson A           Envisioning Communities  A Participatory Approach Towards AI for Social Good   Proceedings of the      AAAI ACM Conference on AI  Ethics  and Society  pp                arXiv             doi                          ISBN                     S CID                 Retrieved April                 via ezpa library ualberta ca 

  University  Stanford  March             Stanford University launches the Institute for Human Centered Artificial Intelligence   Stanford News  Retrieved April         

  Bondi  Elizabeth  Xu  Lily  Acosta Navas  Diana  Killian  Jackson A   July             Envisioning Communities  A Participatory Approach Towards AI for Social Good   Proceedings of the      AAAI ACM Conference on AI  Ethics  and Society  pp                arXiv             doi                          ISBN                     S CID                

  Bygrave  Lee A          Automated Profiling   Computer Law  amp  Security Review                 doi         s                     

  a b Veale  Michael  Edwards  Lilian          Clarity  Surprises  and Further Questions in the Article    Working Party Draft Guidance on Automated Decision Making and Profiling   PDF   Computer Law  amp  Security Review                   doi         j clsr              S CID               SSRN              

  Wachter  Sandra  Mittelstadt  Brent  Floridi  Luciano  May            Why a Right to Explanation of Automated Decision Making Does Not Exist in the General Data Protection Regulation   International Data Privacy Law                doi         idpl ipx     ISSN                

  Edwards  Lilian  Veale  Michael  May             Slave to the Algorithm  Why a Right to an Explanation Is Probably Not the Remedy You Are Looking For   Duke Law  amp  Technology Review             SSRN              

  a b Singer  Natasha  February            Consumer Data Protection Laws  an Ocean Apart   The New York Times  Retrieved November          

  Obama  Barack  October             The Administration s Report on the Future of Artificial Intelligence   whitehouse gov  National Archives  Retrieved November          

  and Technology Council  National Science         National Artificial Intelligence Research and Development Strategic Plan  PDF   US Government  Retrieved November          

  Kirchner  Lauren  December             New York City Moves to Create Accountability for Algorithms   ProPublica   ProPublica  Retrieved July          

   The New York City Council   File    Int             legistar council nyc gov  New York City Council  Retrieved July          

  Wiggers  Kyle  July            NYC s anti bias law for hiring algorithms goes into effect   TechCrunch  Retrieved April          

  Powles  Julia   New York City s Bold  Flawed Attempt to Make Algorithms Accountable   The New Yorker  Retrieved July          

   Maintaining American Leadership in Artificial Intelligence   February          

   Safe  Secure  and Trustworthy Development and Use of Artificial Intelligence   November      

   VP Kamala Harris Unveils  Safe  Secure  amp  Responsible  AI Guidelines for Federal Agencies   March          

   India Weighs Comprehensive Data Privacy Bill  Similar to EU s GDPR   Insurance Journal  July           Retrieved February          

   The Personal Data Protection Bill         PDF   Ministry of Electronics  amp  Information Technology  Government of India        Retrieved April          


Further reading edit 
Baer  Tobias         Understand  Manage  and Prevent Algorithmic Bias  A Guide for Business Users and Data Scientists  New York  Apress  ISBN                    
Noble  Safiya Umoja         Algorithms of Oppression  How Search Engines Reinforce Racism  New York  New York University Press  ISBN                    





Retrieved from  https   en wikipedia org w index php title Algorithmic bias amp oldid