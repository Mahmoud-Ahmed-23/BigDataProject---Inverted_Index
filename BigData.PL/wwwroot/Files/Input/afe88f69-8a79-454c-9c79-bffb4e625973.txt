An alternating decision tree  ADTree  is a machine learning method for classification  It generalizes decision trees and has connections to boosting 
An ADTree consists of an alternation of decision nodes  which specify a predicate condition  and prediction nodes  which contain a single number   An instance is classified by an ADTree by following all paths for which all decision nodes are true  and summing any prediction nodes that are traversed 


History edit 
ADTrees were introduced by Yoav Freund and Llew Mason              However  the algorithm as presented had several typographical errors   Clarifications and optimizations were later presented by Bernhard Pfahringer  Geoffrey Holmes and Richard Kirkby              Implementations are available in Weka and JBoost 

Motivation edit 
Original boosting algorithms typically used either decision stumps 
or decision trees as weak hypotheses   As an example  boosting decision stumps creates
a set of 
  
    
      
        T
      
    
      displaystyle T 
  
 weighted decision stumps  where 
  
    
      
        T
      
    
      displaystyle T 
  

is the number of boosting iterations   which then vote on the final classification according to their weights   Individual decision stumps are weighted according to their ability to classify the data   
Boosting a simple learner results in an unstructured set of 
  
    
      
        T
      
    
      displaystyle T 
  
 hypotheses  making it difficult to infer correlations between attributes   Alternating decision trees introduce structure to the set of hypotheses by requiring that they build off a hypothesis that was produced in an earlier iteration   The resulting set of hypotheses can be visualized in a tree based on the relationship between a hypothesis and its  parent  
Another important feature of boosted algorithms is that the data is given a different distribution at each iteration   Instances that are misclassified are given a larger weight while accurately classified instances are given reduced weight 

Alternating decision tree structure edit 
An alternating decision tree consists of decision nodes and prediction nodes   Decision nodes specify a predicate condition   Prediction nodes contain a single number   ADTrees always have prediction nodes as both root and leaves   An instance is classified by an ADTree by following all paths for which all decision nodes are true and summing any prediction nodes that are traversed   This is different from binary classification trees such as CART  Classification and regression tree  or C    in which an instance follows only one path through the tree 

Example edit 
The following tree was constructed using JBoost on the spambase dataset             available from the UCI Machine Learning Repository               In this example  spam is coded as   and regular email is coded as      

An ADTree for   iterations on the Spambase dataset 
The following table contains part of the information for a single instance 


An instance to be classified


Feature

Value


char freq bang

    


word freq hp

   


capital run length longest

 


char freq dollar

 


word freq remove

   


word freq george

 


Other features

   

The instance is scored by summing all of the prediction nodes through which it passes   In the case of the instance above  the score is
calculated as


Score for the above instance


Iteration

 

 

 

 

 

 

 


Instance values

 

     lt         f

    lt         f

   lt        t

   lt          t

 

    lt         f


Prediction

      

    

      

     

     

 

    

The final score of       is positive  so the instance is classified as spam   The magnitude of the value is a measure of confidence in the prediction   The original authors list three potential levels of interpretation for the set of attributes identified by an ADTree 

Individual nodes can be evaluated for their own predictive ability 
Sets of nodes on the same path may be interpreted as having a joint effect
The tree can be interpreted as a whole 
Care must be taken when interpreting individual nodes as the scores reflect a re weighting of the data in each iteration 

Description of the algorithm edit 
The inputs to the alternating decision tree algorithm are 

A set of inputs 
  
    
      
         
        
          x
          
             
          
        
         
        
          y
          
             
          
        
         
         
          x     
         
         
        
          x
          
            m
          
        
         
        
          y
          
            m
          
        
         
      
    
      displaystyle  x     y       ldots   x  m  y  m   
  
 where 
  
    
      
        
          x
          
            i
          
        
      
    
      displaystyle x  i  
  
 is a vector of attributes and 
  
    
      
        
          y
          
            i
          
        
      
    
      displaystyle y  i  
  
 is either    or     Inputs are also called instances 
A set of weights 
  
    
      
        
          w
          
            i
          
        
      
    
      displaystyle w  i  
  
 corresponding to each instance 
The fundamental element of the ADTree algorithm is the rule   A single
rule consists of a precondition  a condition  and two scores   A
condition is a predicate of the form  attribute  lt comparison gt  value  
A precondition is simply a logical conjunction of conditions 
Evaluation of a rule involves a pair of nested if statements 

   if  precondition 
       if  condition 
           return score one
       else
           return score two
       end if
   else
       return  
   end if

Several auxiliary functions are also required by the algorithm 


  
    
      
        
          W
          
             
          
        
         
        c
         
      
    
      displaystyle W     c  
  
 returns the sum of the weights of all positively labeled examples that satisfy predicate 
  
    
      
        c
      
    
      displaystyle c 
  


  
    
      
        
          W
          
              x     
          
        
         
        c
         
      
    
      displaystyle W     c  
  
 returns the sum of the weights of all negatively labeled examples that satisfy predicate 
  
    
      
        c
      
    
      displaystyle c 
  


  
    
      
        W
         
        c
         
         
        
          W
          
             
          
        
         
        c
         
         
        
          W
          
              x     
          
        
         
        c
         
      
    
      displaystyle W c  W     c  W     c  
  
 returns the sum of the weights of all  examples that satisfy predicate 
  
    
      
        c
      
    
      displaystyle c 
  

The algorithm is as follows 

   function ad tree
   input Set of m training instances
  
   wi     m for all i
   
  
    
      
        a
         
        
          
             
             
          
        
        
          
            ln
          
        
        
          
            
              
                W
                
                   
                
              
               
              t
              r
              u
              e
               
            
            
              
                W
                
                    x     
                
              
               
              t
              r
              u
              e
               
            
          
        
      
    
      displaystyle a   frac          textrm  ln    frac  W     true   W     true    
  

   R    a rule with scores a and    precondition  true  and condition  true  
   
  
    
      
        
          
            P
          
        
         
         
        t
        r
        u
        e
         
      
    
      displaystyle   mathcal  P     true   
  

   
  
    
      
        
          
            C
          
        
         
      
    
      displaystyle   mathcal  C    
  
 the set of all possible conditions
   for 
  
    
      
        j
         
         
          x     
        T
      
    
      displaystyle j   dots T 
  

        
  
    
      
        p
          x     
        
          
            P
          
        
         
        c
          x     
        
          
            C
          
        
      
    
      displaystyle p in   mathcal  P   c in   mathcal  C   
  
 get values that minimize 
  
    
      
        z
         
         
        
           
          
            
              
                
                  W
                  
                     
                  
                
                 
                p
                  x     
                c
                 
                
                  W
                  
                      x     
                  
                
                 
                p
                  x     
                c
                 
              
            
             
            
              
                
                  W
                  
                     
                  
                
                 
                p
                  x     
                  xac 
                c
                 
                
                  W
                  
                      x     
                  
                
                 
                p
                  x     
                  xac 
                c
                 
              
            
          
           
        
         
        W
         
          xac 
        p
         
      
    
      displaystyle z   left   sqrt  W     p wedge c W     p wedge c      sqrt  W     p wedge  neg c W     p wedge  neg c    right  W  neg p  
  

        
  
    
      
        
          
            P
          
        
         
         
        p
          x     
        c
         
        p
          x     
          xac 
        c
      
    
      displaystyle   mathcal  P    p wedge c p wedge  neg c 
  

        
  
    
      
        
          a
          
             
          
        
         
        
          
             
             
          
        
        
          
            ln
          
        
        
          
            
              
                W
                
                   
                
              
               
              p
                x     
              c
               
               
               
            
            
              
                W
                
                    x     
                
              
               
              p
                x     
              c
               
               
               
            
          
        
      
    
      displaystyle a       frac          textrm  ln    frac  W     p wedge c     W     p wedge c      
  

        
  
    
      
        
          a
          
             
          
        
         
        
          
             
             
          
        
        
          
            ln
          
        
        
          
            
              
                W
                
                   
                
              
               
              p
                x     
                xac 
              c
               
               
               
            
            
              
                W
                
                    x     
                
              
               
              p
                x     
                xac 
              c
               
               
               
            
          
        
      
    
      displaystyle a       frac          textrm  ln    frac  W     p wedge  neg c     W     p wedge  neg c      
  

        Rj   new rule with precondition p  condition c  and weights a  and a 
        
  
    
      
        
          w
          
            i
          
        
         
        
          w
          
            i
          
        
        
          e
          
              x     
            
              y
              
                i
              
            
            
              R
              
                j
              
            
             
            
              x
              
                i
              
            
             
          
        
      
    
      displaystyle w  i  w  i e   y  i R  j  x  i    
  

    end for
    return set of Rj

The set 
  
    
      
        
          
            P
          
        
      
    
      displaystyle   mathcal  P   
  
 grows by two preconditions in each iteration  and it is possible to derive the tree structure of a set of rules by making note of the precondition that is used in each successive rule 

Empirical results edit 
Figure   in the original paper            demonstrates that ADTrees are typically as robust as boosted decision trees and boosted decision stumps   Typically  equivalent accuracy can be achieved with a much simpler tree structure than recursive partitioning algorithms 

References edit 


  a b Freund  Y   Mason  L           The alternating decision tree learning algorithm   PDF   Proceedings of the Sixteenth International Conference on Machine Learning  ICML       Morgan Kaufmann  pp                ISBN                        

  Pfahringer  Bernhard  Holmes  Geoffrey  Kirkby  Richard          Optimizing the Induction of Alternating Decision Trees   PDF   Advances in Knowledge Discovery and Data Mining  PAKDD       Lecture Notes in Computer Science  Vol             Springer  pp                doi                           ISBN                        

   Spambase Data Set   UCI Machine Learning Repository       

  Dua  D   Graff  C           UCI Machine Learning Repository   University of California  Irvine  School of Information and Computer Sciences 


External links edit 
An introduction to Boosting and ADTrees   Has many graphical examples of alternating decision trees in practice  
JBoost software implementing ADTrees 





Retrieved from  https   en wikipedia org w index php title Alternating decision tree amp oldid