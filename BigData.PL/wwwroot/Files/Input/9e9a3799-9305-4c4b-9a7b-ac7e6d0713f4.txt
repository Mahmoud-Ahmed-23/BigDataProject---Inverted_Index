American AI researcher and writer  born      


Eliezer YudkowskyYudkowsky at Stanford University in     BornEliezer Shlomo     a      Yudkowsky              September           age         OrganizationMachine Intelligence Research InstituteKnown      forCoining the term friendly artificial intelligenceResearch on AI safetyRationality writingFounder of LessWrongWebsitewww yudkowsky net
Eliezer S  Yudkowsky     li  z r j d ka ski  EL ee EZ  r yud KOW skee             born September           is an American artificial intelligence researcher                                             and writer on decision theory and ethics  best known for popularizing ideas related to friendly artificial intelligence                        He is the founder of and a research fellow at the Machine Intelligence Research Institute  MIRI   a private research nonprofit based in Berkeley  California             His work on the prospect of a runaway intelligence explosion influenced philosopher Nick Bostrom s      book Superintelligence  Paths  Dangers  Strategies            


Work in artificial intelligence safety edit 
See also  Machine Intelligence Research Institute
Goal learning and incentives in software systems edit 
Yudkowsky s views on the safety challenges future generations of AI systems pose are discussed in Stuart Russell s and Peter Norvig s undergraduate textbook Artificial Intelligence  A Modern Approach  Noting the difficulty of formally specifying general purpose goals by hand  Russell and Norvig cite Yudkowsky s proposal that autonomous and adaptive systems be designed to learn correct behavior over time 

Yudkowsky                    goes into more detail about how to design a Friendly AI  He asserts that friendliness  a desire not to harm humans  should be designed in from the start  but that the designers should recognize both that their own designs may be flawed  and that the robot will learn and evolve over time  Thus the challenge is one of mechanism design to design a mechanism for evolving AI under a system of checks and balances  and to give the systems utility functions that will remain friendly in the face of such changes            
In response to the instrumental convergence concern  that autonomous decision making systems with poorly designed goals would have default incentives to mistreat humans  Yudkowsky and other MIRI researchers have recommended that work be done to specify software agents that converge on safe default behaviors even when their goals are misspecified                        

Capabilities forecasting edit 
In the intelligence explosion scenario hypothesized by I  J  Good  recursively self improving AI systems quickly transition from subhuman general intelligence to superintelligent  Nick Bostrom s      book Superintelligence  Paths  Dangers  Strategies sketches out Good s argument in detail  while citing Yudkowsky on the risk that anthropomorphizing advanced AI systems will cause people to misunderstand the nature of an intelligence explosion   AI might make an apparently sharp jump in intelligence purely as the result of anthropomorphism  the human tendency to think of  village idiot  and  Einstein  as the extreme ends of the intelligence scale  instead of nearly indistinguishable points on the scale of minds in general                                     
In Artificial Intelligence  A Modern Approach  Russell and Norvig raise the objection that there are known limits to intelligent problem solving from computational complexity theory  if there are strong limits on how efficiently algorithms can solve various tasks  an intelligence explosion may not be possible            

Time op ed edit 
In a      op ed for Time magazine  Yudkowsky discussed the risk of artificial intelligence and advocated for international agreements to limit it  including a total halt on the development of AI                          He suggested that participating countries should be willing to take military action  such as  destroy ing  a rogue datacenter by airstrike   to enforce such a moratorium             The article helped introduce the debate about AI alignment to the mainstream  leading a reporter to ask President Joe Biden a question about AI safety at a press briefing            

Rationality writing edit 
Between      and       Yudkowsky and Robin Hanson were the principal contributors to Overcoming Bias  a cognitive and social science blog sponsored by the Future of Humanity Institute of Oxford University  In February       Yudkowsky founded LessWrong  a  community blog devoted to refining the art of human rationality                           Overcoming Bias has since functioned as Hanson s personal blog 
Over     blog posts by Yudkowsky on philosophy and science  originally written on LessWrong and Overcoming Bias  were released as an ebook  Rationality  From AI to Zombies  by MIRI in                   MIRI has also published Inadequate Equilibria  Yudkowsky s      ebook on societal inefficiencies             
Yudkowsky has also written several works of fiction  His fanfiction novel Harry Potter and the Methods of Rationality uses plot elements from J  K  Rowling s Harry Potter series to illustrate topics in science and rationality                          The New Yorker described Harry Potter and the Methods of Rationality as a retelling of Rowling s original  in an attempt to explain Harry s wizardry through the scientific method              

Personal life edit 
Yudkowsky is an autodidact             and did not attend high school or college              He is Jewish and was raised as a Modern Orthodox Jew  but is now secular                          

Academic publications edit 
Yudkowsky  Eliezer          Levels of Organization in General Intelligence   PDF   Artificial General Intelligence  Berlin  Springer  doi                              
Yudkowsky  Eliezer          Cognitive Biases Potentially Affecting Judgement of Global Risks   PDF   In Bostrom  Nick   irkovi   Milan  eds    Global Catastrophic Risks  Oxford University Press  ISBN                     
Yudkowsky  Eliezer          Artificial Intelligence as a Positive and Negative Factor in Global Risk   PDF   In Bostrom  Nick   irkovi   Milan  eds    Global Catastrophic Risks  Oxford University Press  ISBN                     
Yudkowsky  Eliezer          Complex Value Systems in Friendly AI   PDF   Artificial General Intelligence   th International Conference  AGI       Mountain View  CA  USA  August            Berlin  Springer 
Yudkowsky  Eliezer          Friendly Artificial Intelligence   In Eden  Ammon  Moor  James  S raker  John  et      al   eds    Singularity Hypotheses  A Scientific and Philosophical Assessment  The Frontiers Collection  Berlin  Springer  pp                doi                               ISBN                        
Bostrom  Nick  Yudkowsky  Eliezer          The Ethics of Artificial Intelligence   PDF   In Frankish  Keith  Ramsey  William  eds    The Cambridge Handbook of Artificial Intelligence  New York  Cambridge University Press  ISBN                        
LaVictoire  Patrick  Fallenstein  Benja  Yudkowsky  Eliezer  B r sz  Mih ly  Christiano  Paul  Herreshoff  Marcello          Program Equilibrium in the Prisoner s Dilemma via L b s Theorem   Multiagent Interaction without Prior Coordination  Papers from the AAAI    Workshop  AAAI Publications  Archived from the original on April           Retrieved October          
Soares  Nate  Fallenstein  Benja  Yudkowsky  Eliezer          Corrigibility   PDF   AAAI Workshops  Workshops at the Twenty Ninth AAAI Conference on Artificial Intelligence  Austin  TX  January              AAAI Publications 
See also edit 
AI box
Friendly artificial intelligence
Open Letter on Artificial Intelligence
Notes edit 


  Or Solomon


References edit 


   Eliezer Yudkowsky on  Three Major Singularity Schools   on YouTube  February           Timestamp      

  a b Silver  Nate  April             How Concerned Are Americans About The Pitfalls Of AI    FiveThirtyEight  Archived from the original on April           Retrieved April          

  Ocampo  Rodolfo  April            I used to work at Google and now I m an AI researcher  Here s why slowing down AI development is wise   The Conversation  Archived from the original on April           Retrieved June          

  Gault  Matthew  March             AI Theorist Says Nuclear War Preferable to Developing Advanced AI   Vice  Archived from the original on May           Retrieved June          

  a b Hutson  Matthew  May             Can We Stop Runaway A I     The New Yorker  ISSN              X  Archived from the original on May           Retrieved May           Eliezer Yudkowsky  a researcher at the Machine Intelligence Research Institute  in the Bay Area  has likened A I  safety recommendations to a fire alarm system  A classic experiment found that  when smoky mist began filling a room containing multiple people  most didn t report it  They saw others remaining stoic and downplayed the danger  An official alarm may signal that it s legitimate to take action  But  in A I   there s no one with the clear authority to sound such an alarm  and people will always disagree about which advances count as evidence of a conflagration   There will be no fire alarm that is not an actual running AGI   Yudkowsky has written  Even if everyone agrees on the threat  no company or country will want to pause on its own  for fear of being passed by competitors      That may require quitting A I  cold turkey before we feel it s time to stop  rather than getting closer and closer to the edge  tempting fate  But shutting it all down would call for draconian measures perhaps even steps as extreme as those espoused by Yudkowsky  who recently wrote  in an editorial for Time  that we should  be willing to destroy a rogue datacenter by airstrike   even at the risk of sparking  a full nuclear exchange  

  a b c d Russell  Stuart  Norvig  Peter         Artificial Intelligence  A Modern Approach  Prentice Hall  ISBN                        

  a b Leighton  Jonathan         The Battle for Compassion  Ethics in an Apathetic Universe  Algora  ISBN                        

  Kurzweil  Ray         The Singularity Is Near  New York City  Viking Penguin  ISBN                        

  Ford  Paul  February             Our Fear of Artificial Intelligence   MIT Technology Review  Archived from the original on March           Retrieved April         

  a b Yudkowsky  Eliezer          Artificial Intelligence as a Positive and Negative Factor in Global Risk   PDF   In Bostrom  Nick   irkovi   Milan  eds    Global Catastrophic Risks  Oxford University Press  ISBN                      Archived  PDF  from the original on March          Retrieved October          

  Soares  Nate  Fallenstein  Benja  Yudkowsky  Eliezer          Corrigibility   AAAI Workshops  Workshops at the Twenty Ninth AAAI Conference on Artificial Intelligence  Austin  TX  January              AAAI Publications  Archived from the original on January           Retrieved October          

  Bostrom  Nick         Superintelligence  Paths  Dangers  Strategies  Oxford University Press  ISBN                     

  Moss  Sebastian  March              Be willing to destroy a rogue data center by airstrike    leading AI alignment researcher pens Time piece calling for ban on large GPU clusters   Data Center Dynamics  Archived from the original on April           Retrieved April          

  Ferguson  Niall  April            The Aliens Have Landed  and We Created Them   Bloomberg  Archived from the original on April          Retrieved April          

  a b Miller  James         Singularity Rising  BenBella Books  Inc  ISBN                     

  Miller  James  July             You Can Learn How To Become More Rational   Business Insider  Archived from the original on August           Retrieved March          

  Miller  James D   Rifts in Rationality   New Rambler Review   newramblerreview com  Archived from the original on July           Retrieved July          

  Machine Intelligence Research Institute   Inadequate Equilibria  Where and How Civilizations Get Stuck   Archived from the original on September           Retrieved May          

  Snyder  Daniel D   July              Harry Potter  and the Key to Immortality   The Atlantic  Archived from the original on December           Retrieved June          

  Packer  George          No Death  No Taxes  The Libertarian Futurism of a Silicon Valley Billionaire   The New Yorker  p           Archived from the original on December           Retrieved October          

  Matthews  Dylan  Pinkerton  Byrd  June             He co founded Skype  Now he s spending his fortune on stopping dangerous AI   Vox  Archived from the original on March          Retrieved March          

  Saperstein  Gregory  August              Minutes With a Visionary  Eliezer Yudkowsky   CNBC  Archived from the original on August          Retrieved September         

  Elia Shalev  Asaf  December            Synagogues are joining an  effective altruism  initiative  Will the Sam Bankman Fried scandal stop them    Jewish Telegraphic Agency  Retrieved December         

  Yudkowsky  Eliezer  October            Avoiding your belief s real weak points   LessWrong  Archived from the original on May          Retrieved April          


External links edit 



Wikimedia Commons has media related to Eliezer Yudkowsky 




Wikiquote has quotations related to Eliezer Yudkowsky 

Official website 
Rationality  From AI to Zombies  entire book online 
 ESYudkowsky   Yudkowsky on X
vteLessWrongPeople
Scott Alexander
Paul Christiano
Wei Dai
Robin Hanson
Zvi Mowshowitz
Eliezer Yudkowsky
Organizations
Center for Applied Rationality
Future of Humanity Institute
Machine Intelligence Research Institute
MetaMed
Works
Harry Potter and the Methods of Rationality
Rationality  From AI to Zombies
Concepts
Friendly artificial intelligence
Pascal s mugging
Roko s basilisk
Waluigi effect

vteExistential risk from artificial intelligenceConcepts
AGI
AI alignment
AI capability control
AI safety
AI takeover
Consequentialism
Effective accelerationism
Ethics of artificial intelligence
Existential risk from artificial intelligence
Friendly artificial intelligence
Instrumental convergence
Vulnerable world hypothesis
Intelligence explosion
Longtermism
Machine ethics
Suffering risks
Superintelligence
Technological singularity
Organizations
Alignment Research Center
Center for AI Safety
Center for Applied Rationality
Center for Human Compatible Artificial Intelligence
Centre for the Study of Existential Risk
EleutherAI
Future of Humanity Institute
Future of Life Institute
Google DeepMind
Humanity 
Institute for Ethics and Emerging Technologies
Leverhulme Centre for the Future of Intelligence
Machine Intelligence Research Institute
OpenAI
People
Scott Alexander
Sam Altman
Yoshua Bengio
Nick Bostrom
Paul Christiano
Eric Drexler
Sam Harris
Stephen Hawking
Dan Hendrycks
Geoffrey Hinton
Bill Joy
Shane Legg
Elon Musk
Steve Omohundro
Huw Price
Martin Rees
Stuart J  Russell
Jaan Tallinn
Max Tegmark
Frank Wilczek
Roman Yampolskiy
Eliezer Yudkowsky
Other
Statement on AI risk of extinction
Human Compatible
Open letter on artificial intelligence       
Our Final Invention
The Precipice
Superintelligence  Paths  Dangers  Strategies
Do You Trust This Computer 
Artificial Intelligence Act
 Category
vteEffective altruismConcepts
Aid effectiveness
Charity assessment
Demandingness objection
Disability adjusted life year
Disease burden
Distributional cost effectiveness analysis
Earning to give
Equal consideration of interests
Longtermism
Marginal utility
Moral circle expansion
Psychological barriers to effective altruism
Quality adjusted life year
Utilitarianism
Venture philanthropy
Key figures
Sam Bankman Fried
Liv Boeree
Nick Bostrom
Hilary Greaves
Holden Karnofsky
William MacAskill
Dustin Moskovitz
Yew Kwang Ng
Toby Ord
Derek Parfit
Peter Singer
Cari Tuna
Eliezer Yudkowsky
Organizations
       Hours
Against Malaria Foundation
Animal Charity Evaluators
Animal Ethics
Centre for Effective Altruism
Centre for Enabling EA Learning  amp  Research
Center for High Impact Philanthropy
Centre for the Study of Existential Risk
Development Media International
Evidence Action
Faunalytics
Fistula Foundation
Future of Humanity Institute
Future of Life Institute
Founders Pledge
GiveDirectly
GiveWell
Giving Multiplier
Giving What We Can
Good Food Fund
The Good Food Institute
Good Ventures
The Humane League
Mercy for Animals
Machine Intelligence Research Institute
Malaria Consortium
Open Philanthropy
Raising for Effective Giving
Sentience Institute
Unlimit Health
Wild Animal Initiative
Focus areas
Biotechnology risk
Climate change
Cultured meat
Economic stability
Existential risk from artificial general intelligence
Global catastrophic risk
Global health
Global poverty
Intensive animal farming
Land use reform
Life extension
Malaria prevention
Mass deworming
Neglected tropical diseases
Risk of astronomical suffering
Wild animal suffering
Literature
Doing Good Better
The End of Animal Farming
Famine  Affluence  and Morality
The Life You Can Save
Living High and Letting Die
The Most Good You Can Do
Practical Ethics
The Precipice
Superintelligence  Paths  Dangers  Strategies
What We Owe the Future
Events
Effective Altruism Global

vteTranshumanismOverviews
Transhuman
Transhumanism in fiction
Currents
Accelerationism
Effective
Antinaturalism
Cypherpunk
Dataism
Eradication of suffering
Extropianism
Immortalism
Postgenderism
Posthumanism
Postpoliticism
Russian cosmism
Singularitarianism
Technogaianism
Technolibertarianism
Technological utopianism
Techno progressivism
Organizations
Foresight Institute
Humanity 
Institute for Ethics and Emerging Technologies
Future of Humanity Institute
LessWrong
US Transhumanist Party
People
Andrews
Bostrom
Church
Jos  Luis Cordeiro
K  Eric Drexler
Fahy
FM     
Freitas
Fuller
Fyodorov
de Garis
Gasson
David Gobel
Ben Goertzel
de Grey
Haldane
Hanson
Harari
Harbisson
Harris
Huxley
Hughes
Zoltan Istvan
Ray Kurzweil
Land
Ole Martin Moen
Hans Moravec
Max More
Elon Musk
Osborn
David Pearce
Martine Rothblatt
Anders Sandberg
Savulescu
Sorgner
Spencer
Stock
Gennady Stolyarov II
Teilhard de Chardin
Vernor Vinge
Natasha Vita More
Mark Alan Walker
Warwick
Eliezer Yudkowsky

 Category

Authority control databases InternationalISNIVIAFNationalUnited StatesCzech RepublicNorwayPolandIsraelAcademicsDBLPArtistsMusicBrainzOtherIdRef





Retrieved from  https   en wikipedia org w index php title Eliezer Yudkowsky amp oldid