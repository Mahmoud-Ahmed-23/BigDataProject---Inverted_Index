Method of statistical inference
Part of a series onBayesian statistics
Posterior   Likelihood   Prior   Evidence

Background
Bayesian inference
Bayesian probability
Bayes  theorem
Bernstein von Mises theorem
Coherence
Cox s theorem
Cromwell s rule
Likelihood principle
Principle of indifference
Principle of maximum entropy

Model building
Conjugate prior
Linear regression
Empirical Bayes
Hierarchical model

Posterior approximation
Markov chain Monte Carlo
Laplace s approximation
Integrated nested Laplace approximations
Variational inference
Approximate Bayesian computation

Estimators
Bayesian estimator
Credible interval
Maximum a posteriori estimation

Evidence approximation
Evidence lower bound
Nested sampling

Model evaluation
Bayes factor  Schwarz criterion 
Model averaging
Posterior predictive

 Mathematics     portalvte
Bayesian inference    be zi n  BAY zee  n or   be   n  BAY zh n             is a method of statistical inference in which Bayes  theorem is used to calculate a probability of a hypothesis  given prior evidence  and update it as more information becomes available  Fundamentally  Bayesian inference uses a prior distribution to estimate posterior probabilities  Bayesian inference is an important technique in statistics  and especially in mathematical statistics  Bayesian updating is particularly important in the dynamic analysis of a sequence of data  Bayesian inference has found application in a wide range of activities  including science  engineering  philosophy  medicine  sport  and law  In the philosophy of decision theory  Bayesian inference is closely related to subjective probability  often called  Bayesian probability  


Introduction to Bayes  rule edit 
A geometric visualisation of Bayes  theorem  In the table  the values         and   give the relative weights of each corresponding condition and case  The figures denote the cells of the table involved in each metric  the probability being the fraction of each figure that is shaded  This shows that 
  
    
      
        P
         
        A
        
           
        
        B
         
        P
         
        B
         
         
        P
         
        B
        
           
        
        A
         
        P
         
        A
         
      
    
      displaystyle P A B P B  P B A P A  
  
 i e  
  
    
      
        P
         
        A
        
           
        
        B
         
         
        
          
            
              P
               
              B
              
                 
              
              A
               
              P
               
              A
               
            
            
              P
               
              B
               
            
          
        
      
    
      displaystyle P A B    frac  P B A P A   P B    
  
  Similar reasoning can be used to show that 
  
    
      
        P
         
          xac 
        A
        
           
        
        B
         
         
        
          
            
              P
               
              B
              
                 
              
                xac 
              A
               
              P
               
                xac 
              A
               
            
            
              P
               
              B
               
            
          
        
      
    
      displaystyle P  neg A B    frac  P B  neg A P  neg A   P B    
  
 etc 
Main article  Bayes  theorem
See also  Bayesian probability
Formal explanation edit 

Contingency table


HypothesisEvidence
SatisfieshypothesisH
Violateshypothesis       
  
    
      
          xac 
        H
      
    
      displaystyle  neg H 
  
       

Total


Has evidenceE


  
    
      
        P
         
        H
        
           
        
        E
         
          x  c  
        P
         
        E
         
      
    
      displaystyle P H E  cdot P E  
  

  
    
      
         
        P
         
        E
        
           
        
        H
         
          x  c  
        P
         
        H
         
      
    
      displaystyle  P E H  cdot P H  
  


  
    
      
        P
         
          xac 
        H
        
           
        
        E
         
          x  c  
        P
         
        E
         
      
    
      displaystyle P  neg H E  cdot P E  
  

  
    
      
         
        P
         
        E
        
           
        
          xac 
        H
         
          x  c  
        P
         
          xac 
        H
         
      
    
      displaystyle  P E  neg H  cdot P  neg H  
  

       
  
    
      
        P
         
        E
         
      
    
      displaystyle P E  
  
       


No evidence       
  
    
      
          xac 
        E
      
    
      displaystyle  neg E 
  
       


  
    
      
        P
         
        H
        
           
        
          xac 
        E
         
          x  c  
        P
         
          xac 
        E
         
      
    
      displaystyle P H  neg E  cdot P  neg E  
  

  
    
      
         
        P
         
          xac 
        E
        
           
        
        H
         
          x  c  
        P
         
        H
         
      
    
      displaystyle  P  neg E H  cdot P H  
  


  
    
      
        P
         
          xac 
        H
        
           
        
          xac 
        E
         
          x  c  
        P
         
          xac 
        E
         
      
    
      displaystyle P  neg H  neg E  cdot P  neg E  
  

  
    
      
         
        P
         
          xac 
        E
        
           
        
          xac 
        H
         
          x  c  
        P
         
          xac 
        H
         
      
    
      displaystyle  P  neg E  neg H  cdot P  neg H  
  


  
    
      
        P
         
          xac 
        E
         
      
    
      displaystyle P  neg E  
  
 
  
    
      
         
          x     
        P
         
        E
         
      
    
      displaystyle   P E  
  






Total

                    
  
    
      
        P
         
        H
         
      
    
      displaystyle P H  
  
       

  
    
      
        P
         
          xac 
        H
         
         
         
          x     
        P
         
        H
         
      
    
      displaystyle P  neg H    P H  
  

 

Bayesian inference derives the posterior probability as a consequence of two antecedents  a prior probability and a  likelihood function  derived from a statistical model for the observed data  Bayesian inference computes the posterior probability according to Bayes  theorem 

  
    
      
        P
         
        H
          x     
        E
         
         
        
          
            
              P
               
              E
                x     
              H
               
                x  c  
              P
               
              H
               
            
            
              P
               
              E
               
            
          
        
         
      
    
      displaystyle P H mid E    frac  P E mid H  cdot P H   P E     
  

where

H stands for any hypothesis whose probability may be affected by data  called evidence below   Often there are competing hypotheses  and the task is to determine which is the most probable 

  
    
      
        P
         
        H
         
      
    
      displaystyle P H  
  
  the prior probability  is the estimate of the probability of the hypothesis H before the data E  the current evidence  is observed 
E  the evidence  corresponds to new data that were not used in computing the prior probability 

  
    
      
        P
         
        H
          x     
        E
         
      
    
      displaystyle P H mid E  
  
  the posterior probability  is the probability of H given E  i e   after E is observed   This is what we want to know  the probability of a hypothesis given the observed evidence 

  
    
      
        P
         
        E
          x     
        H
         
      
    
      displaystyle P E mid H  
  
 is the probability of observing E given H and is called the likelihood  As a function of E with H fixed  it indicates the compatibility of the evidence with the given hypothesis  The likelihood function is a function of the evidence  E  while the posterior probability is a function of the hypothesis  H 

  
    
      
        P
         
        E
         
      
    
      displaystyle P E  
  
 is sometimes termed the marginal likelihood or  model evidence   This factor is the same for all possible hypotheses being considered  as is evident from the fact that the hypothesis H does not appear anywhere in the symbol  unlike for all the other factors  and hence does not factor into determining the relative probabilities of different hypotheses 

  
    
      
        P
         
        E
         
         gt 
         
      
    
      displaystyle P E  gt   
  
  Else one has 
  
    
      
         
        
           
        
         
      
    
      displaystyle     
  
  
For different values of H  only the factors 
  
    
      
        P
         
        H
         
      
    
      displaystyle P H  
  
 and 
  
    
      
        P
         
        E
          x     
        H
         
      
    
      displaystyle P E mid H  
  
  both in the numerator  affect the value of 
  
    
      
        P
         
        H
          x     
        E
         
      
    
      displaystyle P H mid E  
  
                   the posterior probability of a hypothesis is proportional to its prior probability  its inherent likeliness  and the newly acquired likelihood  its compatibility with the new observed evidence  
In cases where 
  
    
      
          xac 
        H
      
    
      displaystyle  neg H 
  
   not H    the logical negation of H  is a valid likelihood  Bayes  rule can be rewritten as follows 

  
    
      
        
          
            
              
                P
                 
                H
                  x     
                E
                 
              
              
                
                 
                
                  
                    
                      P
                       
                      E
                        x     
                      H
                       
                      P
                       
                      H
                       
                    
                    
                      P
                       
                      E
                       
                    
                  
                
              
            
            
              
            
            
              
              
                
                 
                
                  
                    
                      P
                       
                      E
                        x     
                      H
                       
                      P
                       
                      H
                       
                    
                    
                      P
                       
                      E
                        x     
                      H
                       
                      P
                       
                      H
                       
                       
                      P
                       
                      E
                        x     
                        xac 
                      H
                       
                      P
                       
                        xac 
                      H
                       
                    
                  
                
              
            
            
              
            
            
              
              
                
                 
                
                  
                     
                    
                       
                       
                      
                         
                        
                          
                            
                               
                              
                                P
                                 
                                H
                                 
                              
                            
                          
                            x     
                           
                        
                         
                      
                      
                        
                          
                            P
                             
                            E
                              x     
                              xac 
                            H
                             
                          
                          
                            P
                             
                            E
                              x     
                            H
                             
                          
                        
                      
                    
                  
                
              
            
          
        
      
    
      displaystyle   begin aligned P H mid E  amp    frac  P E mid H P H   P E        amp    frac  P E mid H P H   P E mid H P H  P E mid  neg H P  neg H        amp    frac        left   frac     P H      right   frac  P E mid  neg H   P E mid H        end aligned   
  

because

  
    
      
        P
         
        E
         
         
        P
         
        E
          x     
        H
         
        P
         
        H
         
         
        P
         
        E
          x     
          xac 
        H
         
        P
         
          xac 
        H
         
      
    
      displaystyle P E  P E mid H P H  P E mid  neg H P  neg H  
  

and

  
    
      
        P
         
        H
         
         
        P
         
          xac 
        H
         
         
          
      
    
      displaystyle P H  P  neg H     
  
  This focuses attention on the term 
  
    
      
        
           
          
            
              
                
                   
                  
                    P
                     
                    H
                     
                  
                
              
            
              x     
             
          
           
        
        
          
            
              
                P
                 
                E
                  x     
                  xac 
                H
                 
              
              
                P
                 
                E
                  x     
                H
                 
              
            
          
        
         
      
    
      displaystyle  left   tfrac     P H      right   tfrac  P E mid  neg H   P E mid H     
  
  If that term is approximately    then the probability of the hypothesis given the evidence  
  
    
      
        P
         
        H
          x     
        E
         
      
    
      displaystyle P H mid E  
  
  is about 
  
    
      
        
          
            
               
               
            
          
        
      
    
      displaystyle   tfrac         
  
  about     likely   equally likely or not likely   If that term is very small  close to zero  then the probability of the hypothesis  given the evidence  
  
    
      
        P
         
        H
          x     
        E
         
      
    
      displaystyle P H mid E  
  
 is close to   or the conditional hypothesis is quite likely   If that term is very large  much larger than    then the hypothesis  given the evidence  is quite unlikely   If the hypothesis  without consideration of evidence  is unlikely  then 
  
    
      
        P
         
        H
         
      
    
      displaystyle P H  
  
 is small  but not necessarily astronomically small  and 
  
    
      
        
          
            
               
              
                P
                 
                H
                 
              
            
          
        
      
    
      displaystyle   tfrac     P H    
  
 is much larger than   and this term can be approximated as 
  
    
      
        
          
            
              
                P
                 
                E
                  x     
                  xac 
                H
                 
              
              
                P
                 
                E
                  x     
                H
                 
                  x  c  
                P
                 
                H
                 
              
            
          
        
      
    
      displaystyle   tfrac  P E mid  neg H   P E mid H  cdot P H    
  
 and relevant probabilities can be compared directly to each other 
One quick and easy way to remember the equation would be to use rule of multiplication 

  
    
      
        P
         
        E
          x     
        H
         
         
        P
         
        E
          x     
        H
         
        P
         
        H
         
         
        P
         
        H
          x     
        E
         
        P
         
        E
         
         
      
    
      displaystyle P E cap H  P E mid H P H  P H mid E P E   
  


Alternatives to Bayesian updating edit 
Bayesian updating is widely used and computationally convenient  However  it is not the only updating rule that might be considered rational 
Ian Hacking noted that traditional  Dutch book  arguments did not specify Bayesian updating  they left open the possibility that non Bayesian updating rules could avoid Dutch books  Hacking wrote              And neither the Dutch book argument nor any other in the personalist arsenal of proofs of the probability axioms entails the dynamic assumption  Not one entails Bayesianism  So the personalist requires the dynamic assumption to be Bayesian  It is true that in consistency a personalist could abandon the Bayesian model of learning from experience  Salt could lose its savour  
Indeed  there are non Bayesian updating rules that also avoid Dutch books  as discussed in the literature on  probability kinematics   following the publication of Richard C       Jeffrey s rule  which applies Bayes  rule to the case where the evidence itself is assigned a probability             The additional hypotheses needed to uniquely require Bayesian updating have been deemed to be substantial  complicated  and unsatisfactory            

Inference over exclusive and exhaustive possibilities edit 
If evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions  Bayesian inference may be thought of as acting on this belief distribution as a whole 

General formulation edit 
Diagram illustrating event space 
  
    
      
          x a  
      
    
      displaystyle  Omega  
  
 in general formulation of Bayesian inference  Although this diagram shows discrete models and events  the continuous case may be visualized similarly using probability densities 
Suppose a process is generating independent and identically distributed events 
  
    
      
        
          E
          
            n
          
        
         
          xa  
        n
         
         
         
         
         
         
         
          x     
      
    
      displaystyle E  n    n        ldots  
  
  but the probability distribution is unknown  Let the event space 
  
    
      
          x a  
      
    
      displaystyle  Omega  
  
 represent the current state of belief for this process  Each model is represented by event 
  
    
      
        
          M
          
            m
          
        
      
    
      displaystyle M  m  
  
  The conditional probabilities 
  
    
      
        P
         
        
          E
          
            n
          
        
          x     
        
          M
          
            m
          
        
         
      
    
      displaystyle P E  n  mid M  m   
  
 are specified to define the models  
  
    
      
        P
         
        
          M
          
            m
          
        
         
      
    
      displaystyle P M  m   
  
 is the degree of belief in 
  
    
      
        
          M
          
            m
          
        
      
    
      displaystyle M  m  
  
  Before the first inference step  
  
    
      
         
        P
         
        
          M
          
            m
          
        
         
         
      
    
      displaystyle   P M  m     
  
 is a set of initial prior probabilities  These must sum to    but are otherwise arbitrary 
Suppose that the process is observed to generate 
  
    
      
        E
          x     
         
        
          E
          
            n
          
        
         
      
    
      displaystyle E in   E  n    
  
  For each 
  
    
      
        M
          x     
         
        
          M
          
            m
          
        
         
      
    
      displaystyle M in   M  m    
  
  the prior 
  
    
      
        P
         
        M
         
      
    
      displaystyle P M  
  
 is updated to the posterior 
  
    
      
        P
         
        M
          x     
        E
         
      
    
      displaystyle P M mid E  
  
  From Bayes  theorem            

  
    
      
        P
         
        M
          x     
        E
         
         
        
          
            
              P
               
              E
                x     
              M
               
            
            
              
                  x     
                
                  m
                
              
              
                P
                 
                E
                  x     
                
                  M
                  
                    m
                  
                
                 
                P
                 
                
                  M
                  
                    m
                  
                
                 
              
            
          
        
          x  c  
        P
         
        M
         
         
      
    
      displaystyle P M mid E    frac  P E mid M    sum   m  P E mid M  m  P M  m      cdot P M   
  

Upon observation of further evidence  this procedure may be repeated 

Multiple observations edit 
For a sequence of independent and identically distributed observations 
  
    
      
        
          E
        
         
         
        
          e
          
             
          
        
         
          x     
         
        
          e
          
            n
          
        
         
      
    
      displaystyle  mathbf  E    e      dots  e  n   
  
  it can be shown by induction that repeated application of the above is equivalent to

  
    
      
        P
         
        M
          x     
        
          E
        
         
         
        
          
            
              P
               
              
                E
              
                x     
              M
               
            
            
              
                  x     
                
                  m
                
              
              
                P
                 
                
                  E
                
                  x     
                
                  M
                  
                    m
                  
                
                 
                P
                 
                
                  M
                  
                    m
                  
                
                 
              
            
          
        
          x  c  
        P
         
        M
         
         
      
    
      displaystyle P M mid  mathbf  E      frac  P  mathbf  E   mid M    sum   m  P  mathbf  E   mid M  m  P M  m      cdot P M   
  

where

  
    
      
        P
         
        
          E
        
          x     
        M
         
         
        
            x   f 
          
            k
          
        
        
          P
           
          
            e
            
              k
            
          
            x     
          M
           
        
         
      
    
      displaystyle P  mathbf  E   mid M   prod   k  P e  k  mid M    
  


Parametric formulation  motivating the formal description edit 
By parameterizing the space of models  the belief in all models may be updated in a single step  The distribution of belief over the model space may then be thought of as a distribution of belief over the parameter space  The distributions in this section are expressed as continuous  represented by probability densities  as this is the usual situation  The technique is  however  equally applicable to discrete distributions 
Let the vector 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
 span the parameter space  Let the initial prior distribution over 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
 be 
  
    
      
        p
         
        
            x b  
        
          x     
        
            x b  
        
         
      
    
      displaystyle p   boldsymbol   theta    mid   boldsymbol   alpha     
  
  where 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   alpha    
  
 is a set of parameters to the prior itself  or hyperparameters  Let 
  
    
      
        
          E
        
         
         
        
          e
          
             
          
        
         
          x     
         
        
          e
          
            n
          
        
         
      
    
      displaystyle  mathbf  E    e      dots  e  n   
  
 be a sequence of independent and identically distributed event observations  where all 
  
    
      
        
          e
          
            i
          
        
      
    
      displaystyle e  i  
  
 are distributed as 
  
    
      
        p
         
        e
          x     
        
            x b  
        
         
      
    
      displaystyle p e mid   boldsymbol   theta     
  
 for some 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
  Bayes  theorem is applied to find the posterior distribution over 
  
    
      
        
            x b  
        
      
    
      displaystyle   boldsymbol   theta    
  
 

  
    
      
        
          
            
              
                p
                 
                
                    x b  
                
                  x     
                
                  E
                
                 
                
                    x b  
                
                 
              
              
                
                 
                
                  
                    
                      p
                       
                      
                        E
                      
                        x     
                      
                          x b  
                      
                       
                      
                          x b  
                      
                       
                    
                    
                      p
                       
                      
                        E
                      
                        x     
                      
                          x b  
                      
                       
                    
                  
                
                  x  c  
                p
                 
                
                    x b  
                
                  x     
                
                    x b  
                
                 
              
            
            
              
              
                
                 
                
                  
                    
                      p
                       
                      
                        E
                      
                        x     
                      
                          x b  
                      
                       
                      
                          x b  
                      
                       
                    
                    
                        x   b 
                      p
                       
                      
                        E
                      
                        x     
                      
                          x b  
                      
                       
                      
                          x b  
                      
                       
                      p
                       
                      
                          x b  
                      
                        x     
                      
                          x b  
                      
                       
                      
                      d
                      
                          x b  
                      
                    
                  
                
                  x  c  
                p
                 
                
                    x b  
                
                  x     
                
                    x b  
                
                 
                 
              
            
          
        
      
    
      displaystyle   begin aligned p   boldsymbol   theta    mid  mathbf  E     boldsymbol   alpha     amp    frac  p  mathbf  E   mid   boldsymbol   theta      boldsymbol   alpha      p  mathbf  E   mid   boldsymbol   alpha       cdot p   boldsymbol   theta    mid   boldsymbol   alpha       amp    frac  p  mathbf  E   mid   boldsymbol   theta      boldsymbol   alpha       int p  mathbf  E   mid   boldsymbol   theta      boldsymbol   alpha    p   boldsymbol   theta    mid   boldsymbol   alpha      d  boldsymbol   theta      cdot p   boldsymbol   theta    mid   boldsymbol   alpha      end aligned   
  

where

  
    
      
        p
         
        
          E
        
          x     
        
            x b  
        
         
        
            x b  
        
         
         
        
            x   f 
          
            k
          
        
        p
         
        
          e
          
            k
          
        
          x     
        
            x b  
        
         
         
      
    
      displaystyle p  mathbf  E   mid   boldsymbol   theta      boldsymbol   alpha      prod   k p e  k  mid   boldsymbol   theta      
  


Formal description of Bayesian inference edit 
Definitions edit 

  
    
      
        x
      
    
      displaystyle x 
  
  a data point in general   This may in fact be a vector of values 

  
    
      
          x b  
      
    
      displaystyle  theta  
  
  the parameter of the data point s distribution  i e   
  
    
      
        x
          x   c 
        p
         
        x
          x     
          x b  
         
      
    
      displaystyle x sim p x mid  theta   
  
   This may be a vector of parameters 

  
    
      
          x b  
      
    
      displaystyle  alpha  
  
  the hyperparameter of the parameter distribution  i e   
  
    
      
          x b  
          x   c 
        p
         
          x b  
          x     
          x b  
         
      
    
      displaystyle  theta  sim p  theta  mid  alpha   
  
   This may be a vector of hyperparameters 

  
    
      
        
          X
        
      
    
      displaystyle  mathbf  X   
  
 is the sample  a set of 
  
    
      
        n
      
    
      displaystyle n 
  
 observed data points  i e   
  
    
      
        
          x
          
             
          
        
         
          x     
         
        
          x
          
            n
          
        
      
    
      displaystyle x      ldots  x  n  
  
 

  
    
      
        
          
            
              x
                x e 
            
          
        
      
    
      displaystyle   tilde  x   
  
  a new data point whose distribution is to be predicted 
Bayesian inference edit 
The prior distribution is the distribution of the parameter s  before any data is observed  i e  
  
    
      
        p
         
          x b  
          x     
          x b  
         
      
    
      displaystyle p  theta  mid  alpha   
  
   The prior distribution might not be easily determined  in such a case  one possibility may be to use the Jeffreys prior to obtain a prior distribution before updating it with newer observations 
The sampling distribution is the distribution of the observed data conditional on its parameters  i e  
  
    
      
        p
         
        
          X
        
          x     
          x b  
         
      
    
      displaystyle p  mathbf  X   mid  theta   
  
   This is also termed the likelihood  especially when viewed as a function of the parameter s   sometimes written 
  
    
      
        L
          x     
         
          x b  
          x     
        
          X
        
         
         
        p
         
        
          X
        
          x     
          x b  
         
      
    
      displaystyle  operatorname  L    theta  mid  mathbf  X    p  mathbf  X   mid  theta   
  
 
The marginal likelihood  sometimes also termed the evidence  is the distribution of the observed data marginalized over the parameter s   i e  
  
    
      
        p
         
        
          X
        
          x     
          x b  
         
         
          x   b 
        p
         
        
          X
        
          x     
          x b  
         
        p
         
          x b  
          x     
          x b  
         
        d
          x b  
         
      
    
      displaystyle p  mathbf  X   mid  alpha    int p  mathbf  X   mid  theta  p  theta  mid  alpha  d theta   
  
 It quantifies the agreement between data and expert opinion  in a geometric sense that can be made precise             If the marginal likelihood is   then there is no agreement between the data and expert opinion and Bayes  rule cannot be applied 
The posterior distribution is the distribution of the parameter s  after taking into account the observed data   This is determined by Bayes  rule  which forms the heart of Bayesian inference  
  
    
      
        p
         
          x b  
          x     
        
          X
        
         
          x b  
         
         
        
          
            
              p
               
                x b  
               
              
                X
              
               
                x b  
               
            
            
              p
               
              
                X
              
               
                x b  
               
            
          
        
         
        
          
            
              p
               
              
                X
              
                x     
                x b  
               
                x b  
               
              p
               
                x b  
               
                x b  
               
            
            
              p
               
              
                X
              
                x     
                x b  
               
              p
               
                x b  
               
            
          
        
         
        
          
            
              p
               
              
                X
              
                x     
                x b  
               
                x b  
               
              p
               
                x b  
                x     
                x b  
               
            
            
              p
               
              
                X
              
                x     
                x b  
               
            
          
        
          x   d 
        p
         
        
          X
        
          x     
          x b  
         
          x b  
         
        p
         
          x b  
          x     
          x b  
         
         
      
    
      displaystyle p  theta  mid  mathbf  X    alpha     frac  p  theta   mathbf  X    alpha    p  mathbf  X    alpha       frac  p  mathbf  X   mid  theta   alpha  p  theta   alpha    p  mathbf  X   mid  alpha  p  alpha       frac  p  mathbf  X   mid  theta   alpha  p  theta  mid  alpha    p  mathbf  X   mid  alpha     propto p  mathbf  X   mid  theta   alpha  p  theta  mid  alpha    
  
 This is expressed in words as  posterior is proportional to likelihood times prior   or sometimes as  posterior   likelihood times prior  over evidence  
In practice  for almost all complex Bayesian models used in machine learning  the posterior distribution 
  
    
      
        p
         
          x b  
          x     
        
          X
        
         
          x b  
         
      
    
      displaystyle p  theta  mid  mathbf  X    alpha   
  
 is not obtained in a closed form distribution  mainly because the parameter space for 
  
    
      
          x b  
      
    
      displaystyle  theta  
  
 can be very high  or the Bayesian model retains certain hierarchical structure formulated from the observations 
  
    
      
        
          X
        
      
    
      displaystyle  mathbf  X   
  
 and parameter 
  
    
      
          x b  
      
    
      displaystyle  theta  
  
  In such situations  we need to resort to approximation techniques            
General case  Let 
  
    
      
        
          P
          
            Y
          
          
            x
          
        
      
    
      displaystyle P  Y   x  
  
 be the conditional distribution of 
  
    
      
        Y
      
    
      displaystyle Y 
  
 given 
  
    
      
        X
         
        x
      
    
      displaystyle X x 
  
 and let 
  
    
      
        
          P
          
            X
          
        
      
    
      displaystyle P  X  
  
 be the distribution of 
  
    
      
        X
      
    
      displaystyle X 
  
  The joint distribution is then 
  
    
      
        
          P
          
            X
             
            Y
          
        
         
        d
        x
         
        d
        y
         
         
        
          P
          
            Y
          
          
            x
          
        
         
        d
        y
         
        
          P
          
            X
          
        
         
        d
        x
         
      
    
      displaystyle P  X Y  dx dy  P  Y   x  dy P  X  dx  
  
  The conditional distribution 
  
    
      
        
          P
          
            X
          
          
            y
          
        
      
    
      displaystyle P  X   y  
  
 of 
  
    
      
        X
      
    
      displaystyle X 
  
  given 
  
    
      
        Y
         
        y
      
    
      displaystyle Y y 
  
 is then determined by

  
    
      
        
          P
          
            X
          
          
            y
          
        
         
        A
         
         
        E
         
        
           
          
            A
          
        
         
        X
         
        
           
        
        Y
         
        y
         
      
    
      displaystyle P  X   y  A  E    A  X  Y y  
  
Existence and uniqueness of the needed conditional expectation is a consequence of the Radon Nikodym theorem  This was formulated by Kolmogorov in his famous book from       Kolmogorov underlines the importance of conditional probability by writing  I wish to call attention to      and especially the theory of conditional probabilities and conditional expectations      in the Preface             The Bayes theorem determines the posterior distribution from the prior distribution  Uniqueness requires continuity assumptions             Bayes  theorem can be generalized to include improper prior distributions such as the uniform distribution on the real line              Modern Markov chain Monte Carlo methods have boosted the importance of Bayes  theorem including cases with improper priors             

Bayesian prediction edit 
The posterior predictive distribution is the distribution of a new data point  marginalized over the posterior  
  
    
      
        p
         
        
          
            
              x
                x e 
            
          
        
          x     
        
          X
        
         
          x b  
         
         
          x   b 
        p
         
        
          
            
              x
                x e 
            
          
        
          x     
          x b  
         
        p
         
          x b  
          x     
        
          X
        
         
          x b  
         
        d
          x b  
      
    
      displaystyle p   tilde  x   mid  mathbf  X    alpha    int p   tilde  x   mid  theta  p  theta  mid  mathbf  X    alpha  d theta  
  

The prior predictive distribution is the distribution of a new data point  marginalized over the prior  
  
    
      
        p
         
        
          
            
              x
                x e 
            
          
        
          x     
          x b  
         
         
          x   b 
        p
         
        
          
            
              x
                x e 
            
          
        
          x     
          x b  
         
        p
         
          x b  
          x     
          x b  
         
        d
          x b  
      
    
      displaystyle p   tilde  x   mid  alpha    int p   tilde  x   mid  theta  p  theta  mid  alpha  d theta  
  

Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference  i e   to predict the distribution of a new  unobserved data point  That is  instead of a fixed point as a prediction  a distribution over possible points is returned   Only this way is the entire posterior distribution of the parameter s  used   By comparison  prediction in frequentist statistics often involves finding an optimum point estimate of the parameter s  e g   by maximum likelihood or maximum a posteriori estimation  MAP  and then plugging this estimate into the formula for the distribution of a data point  This has the disadvantage that it does not account for any uncertainty in the value of the parameter  and hence will underestimate the variance of the predictive distribution 
In some instances  frequentist statistics can work around this problem  For example  confidence intervals and prediction intervals in frequentist statistics when constructed from a normal distribution with unknown mean and variance are constructed using a Student s t distribution   This correctly estimates the variance  due to the facts that          the average of normally distributed random variables is also normally distributed  and     the predictive distribution of a normally distributed data point with unknown mean and variance  using conjugate or uninformative priors  has a Student s t distribution  In Bayesian statistics  however  the posterior predictive distribution can always be determined exactly or at least to an arbitrary level of precision when numerical methods are used 
Both types of predictive distributions have the form of a compound probability distribution  as does the marginal likelihood   In fact  if the prior distribution is a conjugate prior  such that the prior and posterior distributions come from the same family  it can be seen that both prior and posterior predictive distributions also come from the same family of compound distributions  The only difference is that the posterior predictive distribution uses the updated values of the hyperparameters  applying the Bayesian update rules given in the conjugate prior article   while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution 


Mathematical properties edit 
This section includes a list of general references  but it lacks sufficient corresponding inline citations  Please help to improve this section by introducing more precise citations    February        Learn how and when to remove this message 
Interpretation of factor edit 

  
    
      
        
          
            
              P
               
              E
                x     
              M
               
            
            
              P
               
              E
               
            
          
        
         gt 
         
          x  d  
        P
         
        E
          x     
        M
         
         gt 
        P
         
        E
         
      
    
      textstyle   frac  P E mid M   P E    gt   Rightarrow P E mid M  gt P E  
  
  That is  if the model were true  the evidence would be more likely than is predicted by the current state of belief  The reverse applies for a decrease in belief  If the belief does not change  
  
    
      
        
          
            
              P
               
              E
                x     
              M
               
            
            
              P
               
              E
               
            
          
        
         
         
          x  d  
        P
         
        E
          x     
        M
         
         
        P
         
        E
         
      
    
      textstyle   frac  P E mid M   P E      Rightarrow P E mid M  P E  
  
  That is  the evidence is independent of the model  If the model were true  the evidence would be exactly as likely as predicted by the current state of belief 

Cromwell s rule edit 
Main article  Cromwell s rule
If 
  
    
      
        P
         
        M
         
         
         
      
    
      displaystyle P M    
  
 then 
  
    
      
        P
         
        M
          x     
        E
         
         
         
      
    
      displaystyle P M mid E    
  
  If 
  
    
      
        P
         
        M
         
         
         
      
    
      displaystyle P M    
  
 and 
  
    
      
        P
         
        E
         
         gt 
         
      
    
      displaystyle P E  gt   
  
  then 
  
    
      
        P
         
        M
        
           
        
        E
         
         
         
      
    
      displaystyle P M E    
  
  This can be interpreted to mean that hard convictions are insensitive to counter evidence 
The former follows directly from Bayes  theorem  The latter can be derived by applying the first rule to the event  not 
  
    
      
        M
      
    
      displaystyle M 
  
  in place of  
  
    
      
        M
      
    
      displaystyle M 
  
   yielding  if 
  
    
      
         
          x     
        P
         
        M
         
         
         
      
    
      displaystyle   P M    
  
  then 
  
    
      
         
          x     
        P
         
        M
          x     
        E
         
         
         
      
    
      displaystyle   P M mid E    
  
   from which the result immediately follows 

Asymptotic behaviour of posterior edit 
Consider the behaviour of a belief distribution as it is updated a large number of times with independent and identically distributed trials  For sufficiently nice prior probabilities  the Bernstein von Mises theorem gives that in the limit of infinite trials  the posterior converges to a Gaussian distribution independent of the initial prior under some conditions firstly outlined and rigorously proven by Joseph L  Doob in       namely if the random variable in consideration has a finite probability space  The more general results were obtained later by the statistician David A  Freedman who published in two seminal research papers in                   and                   when and under what circumstances the asymptotic behaviour of posterior is guaranteed  His      paper treats  like Doob         the finite case and comes to a satisfactory conclusion  However  if the random variable has an infinite but countable probability space  i e   corresponding to a die with infinite many faces  the      paper demonstrates that for a dense subset of priors the Bernstein von Mises theorem is not applicable  In this case there is almost surely no asymptotic convergence  Later in the     s and     s Freedman and Persi Diaconis continued to work on the case of infinite countable probability spaces              To summarise  there may be insufficient trials to suppress the effects of the initial choice  and especially for large  but finite  systems the convergence might be very slow 

Conjugate priors edit 
Main article  Conjugate prior
In parameterized form  the prior distribution is often assumed to come from a family of distributions called conjugate priors  The usefulness of a conjugate prior is that the corresponding posterior distribution will be in the same family  and the calculation may be expressed in closed form 

Estimates of parameters and predictions edit 
It is often desired to use a posterior distribution to estimate a parameter or variable  Several methods of Bayesian estimation select measurements of central tendency from the posterior distribution 
For one dimensional problems  a unique median exists for practical continuous problems  The posterior median is attractive as a robust estimator             
If there exists a finite mean for the posterior distribution  then the posterior mean is a method of estimation             

  
    
      
        
          
            
                x b  
                x e 
            
          
        
         
        E
          x     
         
          x b  
         
         
          x   b 
          x b  
        
        p
         
          x b  
          x     
        
          X
        
         
          x b  
         
        
        d
          x b  
      
    
      displaystyle   tilde   theta     operatorname  E    theta    int  theta   p  theta  mid  mathbf  X    alpha    d theta  
  

Taking a value with the greatest probability defines maximum a      posteriori  MAP  estimates             

  
    
      
         
        
            x b  
          
            MAP
          
        
         
          x     
        arg
          x     
        
          max
          
              x b  
          
        
        p
         
          x b  
          x     
        
          X
        
         
          x b  
         
         
      
    
      displaystyle    theta    text MAP     subset  arg  max    theta  p  theta  mid  mathbf  X    alpha    
  

There are examples where no maximum is attained  in which case the set of MAP estimates is empty 
There are other methods of estimation that minimize the posterior risk  expected posterior loss  with respect to a loss function  and these are of interest to statistical decision theory using the sampling distribution   frequentist statistics               
The posterior predictive distribution of a new observation 
  
    
      
        
          
            
              x
                x e 
            
          
        
      
    
      displaystyle   tilde  x   
  
  that is independent of previous observations  is determined by            

  
    
      
        p
         
        
          
            
              x
                x e 
            
          
        
        
           
        
        
          X
        
         
          x b  
         
         
          x   b 
        p
         
        
          
            
              x
                x e 
            
          
        
         
          x b  
          x     
        
          X
        
         
          x b  
         
        
        d
          x b  
         
          x   b 
        p
         
        
          
            
              x
                x e 
            
          
        
          x     
          x b  
         
        p
         
          x b  
          x     
        
          X
        
         
          x b  
         
        
        d
          x b  
         
      
    
      displaystyle p   tilde  x    mathbf  X    alpha    int p   tilde  x    theta  mid  mathbf  X    alpha    d theta   int p   tilde  x   mid  theta  p  theta  mid  mathbf  X    alpha    d theta   
  


Examples edit 
Probability of a hypothesis edit 

Contingency table


BowlCookie

  H 
  H 

Total


Plain  E

  
  
  


Choc   E

  
  
  


Total

  
  
  


P H  E                 

Suppose there are two full bowls of cookies  Bowl    has    chocolate chip and    plain cookies  while bowl    has    of each  Our friend Fred picks a bowl at random  and then picks a cookie at random  We may assume there is no reason to believe Fred treats one bowl differently from another  likewise for the cookies  The cookie turns out to be a plain one  How probable is it that Fred picked it out of bowl    
Intuitively  it seems clear that the answer should be more than a half  since there are more plain cookies in bowl     The precise answer is given by Bayes  theorem  Let 
  
    
      
        
          H
          
             
          
        
      
    
      displaystyle H     
  
 correspond to bowl     and 
  
    
      
        
          H
          
             
          
        
      
    
      displaystyle H     
  
 to bowl    
It is given that the bowls are identical from Fred s point of view  thus 
  
    
      
        P
         
        
          H
          
             
          
        
         
         
        P
         
        
          H
          
             
          
        
         
      
    
      displaystyle P H      P H      
  
  and the two must add up to    so both are equal to     
The event 
  
    
      
        E
      
    
      displaystyle E 
  
 is the observation of a plain cookie  From the contents of the bowls  we know that 
  
    
      
        P
         
        E
          x     
        
          H
          
             
          
        
         
         
          
        
           
        
          
         
            
      
    
      displaystyle P E mid H                 
  
 and 
  
    
      
        P
         
        E
          x     
        
          H
          
             
          
        
         
         
          
        
           
        
          
         
            
      
    
      displaystyle P E mid H                 
  
 Bayes  formula then yields

  
    
      
        
          
            
              
                P
                 
                
                  H
                  
                     
                  
                
                  x     
                E
                 
              
              
                
                 
                
                  
                    
                      P
                       
                      E
                        x     
                      
                        H
                        
                           
                        
                      
                       
                      
                      P
                       
                      
                        H
                        
                           
                        
                      
                       
                    
                    
                      P
                       
                      E
                        x     
                      
                        H
                        
                           
                        
                      
                       
                      
                      P
                       
                      
                        H
                        
                           
                        
                      
                       
                      
                       
                      
                      P
                       
                      E
                        x     
                      
                        H
                        
                           
                        
                      
                       
                      
                      P
                       
                      
                        H
                        
                           
                        
                      
                       
                    
                  
                
              
            
            
              
            
            
              
                  xa  
              
              
                
                 
                
                  
                    
                          
                        xd  
                         
                    
                    
                          
                        xd  
                         
                       
                         
                        xd  
                         
                    
                  
                
              
            
            
              
            
            
              
                  xa  
              
              
                
                 
                   
              
            
          
        
      
    
      displaystyle   begin aligned P H     mid E  amp    frac  P E mid H       P H       P E mid H       P H          P E mid H       P H              amp    frac       times           times         times             amp      end aligned   
  

Before we observed the cookie  the probability we assigned for Fred having chosen bowl    was the prior probability  
  
    
      
        P
         
        
          H
          
             
          
        
         
      
    
      displaystyle P H      
  
  which was      After observing the cookie  we must revise the probability to 
  
    
      
        P
         
        
          H
          
             
          
        
          x     
        E
         
      
    
      displaystyle P H     mid E  
  
  which is     

Making a prediction edit 
Example results for archaeology example  This simulation was generated using c      
An archaeologist is working at a site thought to be from the medieval period  between the   th century to the   th century  However  it is uncertain exactly when in this period the site was inhabited  Fragments of pottery are found  some of which are glazed and some of which are decorated  It is expected that if the site were inhabited during the early medieval period  then    of the pottery would be glazed and     of its area decorated  whereas if it had been inhabited in the late medieval period then     would be glazed and    of its area decorated  How confident can the archaeologist be in the date of inhabitation as fragments are unearthed 
The degree of belief in the continuous variable 
  
    
      
        C
      
    
      displaystyle C 
  
  century  is to be calculated  with the discrete set of events 
  
    
      
         
        G
        D
         
        G
        
          
            
              D
                xaf 
            
          
        
         
        
          
            
              G
                xaf 
            
          
        
        D
         
        
          
            
              G
                xaf 
            
          
        
        
          
            
              D
                xaf 
            
          
        
         
      
    
      displaystyle   GD G  bar  D     bar  G  D   bar  G    bar  D     
  
 as evidence  Assuming linear variation of glaze and decoration with time  and that these variables are independent 

  
    
      
        P
         
        E
         
        G
        D
          x     
        C
         
        c
         
         
         
            
         
        
          
            
                  
                x     
                  
            
            
                
                x     
                
            
          
        
         
        c
          x     
          
         
         
         
           
          x     
        
          
            
                 
                x     
                  
            
            
                
                x     
                
            
          
        
         
        c
          x     
          
         
         
      
    
      displaystyle P E GD mid C c          frac                     c            frac                    c      
  


  
    
      
        P
         
        E
         
        G
        
          
            
              D
                xaf 
            
          
        
          x     
        C
         
        c
         
         
         
            
         
        
          
            
                  
                x     
                  
            
            
                
                x     
                
            
          
        
         
        c
          x     
          
         
         
         
           
         
        
          
            
                 
                x     
                  
            
            
                
                x     
                
            
          
        
         
        c
          x     
          
         
         
      
    
      displaystyle P E G  bar  D   mid C c          frac                     c            frac                    c      
  


  
    
      
        P
         
        E
         
        
          
            
              G
                xaf 
            
          
        
        D
          x     
        C
         
        c
         
         
         
         
         
          x     
            
         
          x     
        
          
            
                  
                x     
                  
            
            
                
                x     
                
            
          
        
         
        c
          x     
          
         
         
         
           
          x     
        
          
            
                 
                x     
                  
            
            
                
                x     
                
            
          
        
         
        c
          x     
          
         
         
      
    
      displaystyle P E   bar  G  D mid C c              frac                     c            frac                    c      
  


  
    
      
        P
         
        E
         
        
          
            
              G
                xaf 
            
          
        
        
          
            
              D
                xaf 
            
          
        
          x     
        C
         
        c
         
         
         
         
         
          x     
            
         
          x     
        
          
            
                  
                x     
                  
            
            
                
                x     
                
            
          
        
         
        c
          x     
          
         
         
         
           
         
        
          
            
                 
                x     
                  
            
            
                
                x     
                
            
          
        
         
        c
          x     
          
         
         
      
    
      displaystyle P E   bar  G    bar  D   mid C c              frac                     c            frac                    c      
  

Assume a uniform prior of 
  
    
      
        
          f
          
            C
          
        
         
        c
         
         
           
      
    
      textstyle f  C  c      
  
  and that trials are independent and identically distributed  When a new fragment of type 
  
    
      
        e
      
    
      displaystyle e 
  
 is discovered  Bayes  theorem is applied to update the degree of belief for each 
  
    
      
        c
      
    
      displaystyle c 
  
 

  
    
      
        
          f
          
            C
          
        
         
        c
          x     
        E
         
        e
         
         
        
          
            
              P
               
              E
               
              e
                x     
              C
               
              c
               
            
            
              P
               
              E
               
              e
               
            
          
        
        
          f
          
            C
          
        
         
        c
         
         
        
          
            
              P
               
              E
               
              e
                x     
              C
               
              c
               
            
            
              
                  x   b 
                
                    
                
                
                    
                
              
              
                P
                 
                E
                 
                e
                  x     
                C
                 
                c
                 
                
                  f
                  
                    C
                  
                
                 
                c
                 
                d
                c
              
            
          
        
        
          f
          
            C
          
        
         
        c
         
      
    
      displaystyle f  C  c mid E e    frac  P E e mid C c   P E e   f  C  c    frac  P E e mid C c    int            P E e mid C c f  C  c dc   f  C  c  
  

A computer simulation of the changing belief as    fragments are unearthed is shown on the graph  In the simulation  the site was inhabited around       or 
  
    
      
        c
         
            
      
    
      displaystyle c      
  
  By calculating the area under the relevant portion of the graph for    trials  the archaeologist can say that there is practically no chance the site was inhabited in the   th and   th centuries  about    chance that it was inhabited during the   th century      chance during the   th century and     during the   th century  The Bernstein von Mises theorem asserts here the asymptotic convergence to the  true  distribution because the probability space corresponding to the discrete set of events 
  
    
      
         
        G
        D
         
        G
        
          
            
              D
                xaf 
            
          
        
         
        
          
            
              G
                xaf 
            
          
        
        D
         
        
          
            
              G
                xaf 
            
          
        
        
          
            
              D
                xaf 
            
          
        
         
      
    
      displaystyle   GD G  bar  D     bar  G  D   bar  G    bar  D     
  
 is finite  see above section on asymptotic behaviour of the posterior  

In frequentist statistics and decision theory edit 
A decision theoretic justification of the use of Bayesian inference was given by Abraham Wald  who proved that every unique Bayesian procedure is admissible  Conversely  every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures             
Wald characterized admissible procedures as Bayesian procedures  and limits of Bayesian procedures   making the Bayesian formalism a central technique in such areas of frequentist inference as parameter estimation  hypothesis testing  and computing confidence intervals                                      For example 

 Under some conditions  all admissible procedures are either Bayes procedures or limits of Bayes procedures  in various senses   These remarkable results  at least in their original form  are due essentially to Wald  They are useful because the property of being Bayes is easier to analyze than admissibility              
 In decision theory  a quite general method for proving admissibility consists in exhibiting a procedure as a unique Bayes solution              
 In the first chapters of this work  prior distributions with finite support and the corresponding Bayes procedures were used to establish some of the main theorems relating to the comparison of experiments  Bayes procedures with respect to more general prior distributions have played a very important role in the development of statistics  including its asymptotic theory    There are many problems where a glance at posterior distributions  for suitable priors  yields immediately interesting information  Also  this technique can hardly be avoided in sequential analysis              
 A useful fact is that any Bayes decision rule obtained by taking a proper prior over the whole parameter space must be admissible             
 An important area of investigation in the development of admissibility ideas has been that of conventional sampling theory procedures  and many interesting results have been obtained              
Model selection edit 
Main article  Bayesian model selection
See also  Bayesian information criterion
Bayesian methodology also plays a role in model selection where the aim is to select one model from a set of competing models that represents most closely the underlying process that generated the observed data  In Bayesian model comparison  the model with the highest posterior probability given the data is selected  The posterior probability of a model depends on the evidence  or marginal likelihood  which reflects the probability that the data is generated by the model  and on the prior belief of the model  When two competing models are a priori considered to be equiprobable  the ratio of their posterior probabilities corresponds to the Bayes factor  Since Bayesian model comparison is aimed on selecting the model with the highest posterior probability  this methodology is also referred to as the maximum a posteriori  MAP  selection rule              or the MAP probability rule             

Probabilistic programming edit 
Main article  Probabilistic programming
While conceptually simple  Bayesian methods can be mathematically and numerically challenging  Probabilistic programming languages  PPLs  implement functions to easily build Bayesian models together with efficient automatic inference methods  This helps separate the model building from the inference  allowing practitioners to focus on their specific problems and leaving PPLs to handle the computational details for them                                     

Applications edit 
Statistical data analysis edit 
See the separate Wikipedia entry on Bayesian statistics  specifically the statistical modeling section in that page 

Computer applications edit 
Bayesian inference has applications in artificial intelligence and expert systems   Bayesian inference techniques have been a fundamental part of computerized pattern recognition techniques since the late     s              There is also an ever growing connection between Bayesian methods and simulation based Monte Carlo techniques since complex models cannot be processed in closed form by a Bayesian analysis  while a graphical model structure may allow for efficient simulation algorithms like the Gibbs sampling and other Metropolis Hastings algorithm schemes              Recently     when       Bayesian inference has gained popularity among the phylogenetics community for these reasons  a number of applications allow many demographic and evolutionary parameters to be estimated simultaneously 
As applied to statistical classification  Bayesian inference has been used to develop algorithms for identifying e mail spam  Applications which make use of Bayesian inference for spam filtering include CRM     DSPAM  Bogofilter  SpamAssassin  SpamBayes  Mozilla  XEAMS  and others  Spam classification is treated in more detail in the article on the na ve Bayes classifier 
Solomonoff s Inductive inference is the theory of prediction based on observations  for example  predicting the next symbol based upon a given series of symbols  The only assumption is that the environment follows some unknown but computable probability distribution  It is a formal inductive framework that combines two well studied principles of inductive inference  Bayesian statistics and Occam s Razor                  unreliable source       Solomonoff s universal prior probability of any prefix p of a computable sequence x is the sum of the probabilities of all programs  for a universal computer  that compute something starting with p  Given some p and any computable but unknown probability distribution from which x is sampled  the universal prior and Bayes  theorem can be used to predict the yet unseen parts of x in optimal fashion                         

Bioinformatics and healthcare applications edit 
Bayesian inference has been applied in different Bioinformatics applications  including differential gene expression analysis              Bayesian inference is also used in a general cancer risk model  called CIRI  Continuous Individualized Risk Index   where serial measurements are incorporated to update a Bayesian model which is primarily built from prior knowledge                         

In the courtroom edit 
Main article  Jurimetrics        Bayesian analysis of evidence
Bayesian inference can be used by jurors to coherently accumulate the evidence for and against a defendant  and to see whether  in totality  it meets their personal threshold for  beyond a reasonable doubt                                       Bayes  theorem is applied successively to all evidence presented  with the posterior from one stage becoming the prior for the next  The benefit of a Bayesian approach is that it gives the juror an unbiased  rational mechanism for combining evidence  It may be appropriate to explain Bayes  theorem to jurors in odds form  as betting odds are more widely understood than probabilities  Alternatively  a logarithmic approach  replacing multiplication with addition  might be easier for a jury to handle 

Adding up evidence
If the existence of the crime is not in doubt  only the identity of the culprit  it has been suggested that the prior should be uniform over the qualifying population              For example  if       people could have committed the crime  the prior probability of guilt would be        
The use of Bayes  theorem by jurors is controversial  In the United Kingdom  a defence expert witness explained Bayes  theorem to the jury in R v Adams  The jury convicted  but the case went to appeal on the basis that no means of accumulating evidence had been provided for jurors who did not wish to use Bayes  theorem  The Court of Appeal upheld the conviction  but it also gave the opinion that  To introduce Bayes  Theorem  or any similar method  into a criminal trial plunges the jury into inappropriate and unnecessary realms of theory and complexity  deflecting them from their proper task  
Gardner Medwin             argues that the criterion on which a verdict in a criminal trial should be based is not the probability of guilt  but rather the probability of the evidence  given that the defendant is innocent  akin to a frequentist p value   He argues that if the posterior probability of guilt is to be computed by Bayes  theorem  the prior probability of guilt must be known  This will depend on the incidence of the crime  which is an unusual piece of evidence to consider in a criminal trial  Consider the following three propositions 

A   the known facts and testimony could have arisen if the defendant is guilty 
B   the known facts and testimony could have arisen if the defendant is innocent 
C   the defendant is guilty 
Gardner Medwin argues that the jury should believe both A and not B in order to convict  A and not B implies the truth of C  but the reverse is not true  It is possible that B and C are both true  but in this case he argues that a jury should acquit  even though they know that they will be letting some guilty people go free  See also Lindley s paradox 

Bayesian epistemology edit 
Bayesian epistemology is a movement that advocates for Bayesian inference as a means of justifying the rules of inductive logic 
Karl Popper and David Miller have rejected the idea of Bayesian rationalism  i e  using Bayes rule to make epistemological inferences              It is prone to the same vicious circle as any other justificationist epistemology  because it presupposes what it attempts to justify  According to this view  a rational interpretation of Bayesian inference would see it merely as a probabilistic version of falsification  rejecting the belief  commonly held by Bayesians  that high likelihood achieved by a series of Bayesian updates would prove the hypothesis beyond any reasonable doubt  or even with likelihood greater than   

Other edit 
The scientific method is sometimes interpreted as an application of Bayesian inference  In this view  Bayes  rule guides  or should guide  the updating of probabilities about hypotheses conditional on new observations or experiments              The Bayesian inference has also been applied to treat stochastic scheduling problems with incomplete information by Cai et al                     
Bayesian search theory is used to search for lost objects 
Bayesian inference in phylogeny
Bayesian tool for methylation analysis
Bayesian approaches to brain function  investigate the brain as a Bayesian mechanism 
Bayesian inference in ecological studies                        
Bayesian inference is used to estimate parameters in stochastic chemical kinetic models            
Bayesian inference in econophysics for currency or prediction of trend changes in financial quotations                        
Bayesian inference in marketing
Bayesian inference in motor learning
Bayesian inference is used in probabilistic numerics to solve numerical problems
Bayes and Bayesian inference edit 
The problem considered by Bayes in Proposition        of his essay   An Essay Towards Solving a Problem in the Doctrine of Chances   is the posterior distribution for the parameter a  the success rate  of the binomial distribution      citation needed     

History edit 
Main article  History of statistics        Bayesian statistics
The term Bayesian refers to Thomas Bayes              who proved that probabilistic limits could be placed on an unknown event      citation needed        However  it was Pierre Simon Laplace             who introduced  as Principle VI  what is now called Bayes  theorem and used it to address problems in celestial mechanics  medical statistics  reliability  and jurisprudence              Early Bayesian inference  which used uniform priors following Laplace s principle of insufficient reason  was called  inverse probability   because it infers backwards from observations to parameters  or from effects to causes               After the     s   inverse probability  was largely supplanted  by a collection of methods that came to be called frequentist statistics             
In the   th century  the ideas of Laplace were further developed in two different directions  giving rise to objective and subjective currents in Bayesian practice  In the objective or  non informative  current  the statistical analysis depends on only the model assumed  the data analyzed              and the method assigning the prior  which differs from one objective Bayesian practitioner to another  In the subjective or  informative  current  the specification of the prior depends on the belief  that is  propositions on which the analysis is prepared to act   which can summarize information from experts  previous studies  etc 
In the     s  there was a dramatic growth in research and applications of Bayesian methods  mostly attributed to the discovery of Markov chain Monte Carlo methods  which removed many of the computational problems  and an increasing interest in nonstandard  complex applications              Despite growth of Bayesian research  most undergraduate teaching is still based on frequentist statistics              Nonetheless  Bayesian methods are widely accepted and used  such as for example in the field of machine learning             

See also edit 

Bayesian approaches to brain function
Credibility theory
Epistemology
Free energy principle
Inductive probability
Information field theory
Principle of maximum entropy
Probabilistic causation
Probabilistic programming
References edit 
Citations edit 


   Bayesian   Merriam Webster com Dictionary  Merriam Webster 

  Hacking  Ian  December         Slightly More Realistic Personal Probability   Philosophy of Science               doi                 S CID               

   Bayes  Theorem  Stanford Encyclopedia of Philosophy    Plato stanford edu  Retrieved            

  van Fraassen  B         Laws and Symmetry  Oxford University Press  ISBN                    

  Gelman  Andrew  Carlin  John B   Stern  Hal S   Dunson  David B   Vehtari  Aki  Rubin  Donald B          Bayesian Data Analysis  Third Edition  Chapman and Hall CRC  ISBN                        

  de Carvalho  Miguel  Page  Garritt  Barney  Bradley          On the geometry of Bayesian inference   PDF   Bayesian Analysis                     doi            BA      S CID               

  Lee  Se Yoon          Gibbs sampler and coordinate ascent variational inference  A set theoretical review   Communications in Statistics   Theory and Methods                     arXiv             doi                                S CID                

  Kolmogorov  A N                 Foundations of the Theory of Probability  Chelsea Publishing Company 

  Tjur  Tue         Probability based on Radon measures  Internet Archive  Chichester  Eng          New York        Wiley  ISBN                        

  Taraldsen  Gunnar  Tufto  Jarle  Lindqvist  Bo H                 Improper priors and improper posteriors   Scandinavian Journal of Statistics                   doi         sjos        hdl                ISSN                 S CID                

  Robert  Christian P   Casella  George         Monte Carlo Statistical Methods  Springer  ISBN                      OCLC                 

  Freedman  DA          On the asymptotic behavior of Bayes  estimates in the discrete case   The Annals of Mathematical Statistics                     doi         aoms             JSTOR              

  Freedman  DA          On the asymptotic behavior of Bayes estimates in the discrete case II   The Annals of Mathematical Statistics                   doi         aoms             JSTOR              

  Robins  James  Wasserman  Larry          Conditioning  likelihood  and coherence  A review of some foundational concepts   Journal of the American Statistical Association                       doi                                 S CID                

  Sen  Pranab K   Keating  J  P   Mason  R  L          Pitman s measure of closeness  A comparison of statistical estimators  Philadelphia  SIAM 

  Choudhuri  Nidhan  Ghosal  Subhashis  Roy  Anindya                Bayesian Methods for Function Estimation   Handbook of Statistics  Bayesian Thinking  Vol           pp                CiteSeerX                       doi         s                      ISBN                    

   Maximum A Posteriori  MAP  Estimation   www probabilitycourse com  Retrieved            

  Yu  Angela   Introduction to Bayesian Decision Theory   PDF   cogsci ucsd edu   Archived from the original  PDF  on            

  Hitchcock  David   Posterior Predictive Distribution Stat Slide   PDF   stat sc edu 

  a b Bickel  amp  Doksum        p     

  Kiefer  J   Schwartz R           Admissible Bayes Character of T    R    and Other Fully Invariant Tests for Multivariate Normal Problems   Annals of Mathematical Statistics                   doi         aoms            

  Schwartz  R           Invariant Proper Bayes Tests for Exponential Families   Annals of Mathematical Statistics               doi         aoms            

  Hwang  J  T   amp  Casella  George          Minimax Confidence Sets for the Mean of a Multivariate Normal Distribution   PDF   Annals of Statistics                   doi         aos            

  Lehmann  Erich         Testing Statistical Hypotheses  Second      ed     see p      of Chapter      Admissibility   and pp        of Chapter      Complete Classes 

  Le Cam  Lucien         Asymptotic Methods in Statistical Decision Theory  Springer Verlag  ISBN                          From  Chapter    Posterior Distributions and Bayes Solutions   p      

  Cox  D  R   Hinkley  D V          Theoretical Statistics  Chapman and Hall  p            ISBN                        

  Cox  D  R   Hinkley  D V          Theoretical Statistics  Chapman and Hall  p            ISBN                         

  Stoica  P   Selen  Y           A review of information criterion rules   IEEE Signal Processing Magazine                 doi         MSP               S CID               

  Fatermans  J   Van Aert  S   den Dekker  A J           The maximum a posteriori probability rule for atom column detection from HAADF STEM images   Ultramicroscopy              arXiv             doi         j ultramic              PMID                S CID                

  Bessiere  P   Mazer  E   Ahuactzin  J  M    amp  Mekhnacha  K          Bayesian Programming    edition  Chapman and Hall CRC 

  Daniel Roy          Probabilistic Programming   probabilistic programming org  Archived from the original on             Retrieved            

  Ghahramani  Z          Probabilistic machine learning and artificial intelligence   Nature                       Bibcode     Natur         G  doi         nature       PMID                S CID             

  Fienberg  Stephen E                 When did Bayesian inference become  Bayesian     Bayesian Analysis         doi            BA    

  Jim Albert         Bayesian Computation with R  Second edition  New York  Dordrecht  etc   Springer  ISBN                        

  Rathmanner  Samuel  Hutter  Marcus  Ormerod  Thomas C          A Philosophical Treatise of Universal Induction   Entropy                     arXiv            Bibcode     Entrp         R  doi         e          S CID              

  Hutter  Marcus  He  Yang Hui  Ormerod  Thomas C          On Universal Prediction and Bayesian Confirmation   Theoretical Computer Science                     arXiv            Bibcode     arXiv         H  doi         j tcs              S CID              

  G cs  Peter  Vit nyi  Paul M  B     December         Raymond J  Solomonoff             CiteSeerX                      

  Robinson  Mark D  amp  McCarthy  Davis J  amp  Smyth  Gordon K edgeR  a Bioconductor package for differential expression analysis of digital gene expression data  Bioinformatics 

   CIRI   ciri stanford edu  Retrieved            

  Kurtz  David M   Esfahani  Mohammad S   Scherer  Florian  Soo  Joanne  Jin  Michael C   Liu  Chih Long  Newman  Aaron M   D hrsen  Ulrich  H ttmann  Andreas                Dynamic Risk Profiling Using Serial Tumor Biomarkers for Personalized Outcome Prediction   Cell                   e    doi         j cell              ISSN                 PMC               PMID               

  Dawid  A       P  and Mortera       J          Coherent Analysis of Forensic Identification Evidence   Journal of the Royal Statistical Society  Series      B              

  
Foreman  L       A   Smith  A       F       M   and Evett  I       W           Bayesian analysis of deoxyribonucleic acid profiling data in forensic identification applications  with discussion    Journal of the Royal Statistical Society  Series      A               

  Robertson  B  and Vignaux  G       A         Interpreting Evidence  Evaluating Forensic Science in the Courtroom  John Wiley and Sons  Chichester  ISBN                        

  Dawid  A  P         Bayes  Theorem and Weighing Evidence by Juries  Archived            at the Wayback Machine

  Gardner Medwin  A          What Probability Should the Jury Address    Significance         March      

  Miller  David         Critical Rationalism  Chicago  Open Court  ISBN                        

  Howson  amp  Urbach         Jaynes       

  Cai  X Q   Wu  X Y   Zhou  X           Stochastic scheduling subject to breakdown repeat breakdowns with incomplete information   Operations Research                     doi         opre           

  Ogle  Kiona  Tucker  Colin  Cable  Jessica M                 Beyond simple linear mixing models  process based isotope partitioning of ecological processes   Ecological Applications                   Bibcode     EcoAp         O  doi                             ISSN                 PMID               

  Evaristo  Jaivime  McDonnell  Jeffrey J   Scholl  Martha A   Bruijnzeel  L  Adrian  Chun  Kwok P                 Insights into plant water uptake from xylem water isotope measurements in two tropical catchments with contrasting moisture conditions   Hydrological Processes                      Bibcode     HyPr          E  doi         hyp        ISSN                 S CID                

  Gupta  Ankur  Rawlings  James B   April         Comparison of Parameter Estimation Methods in Stochastic Chemical Kinetic Models  Examples in Systems Biology   AIChE Journal                     Bibcode     AIChE         G  doi         aic        ISSN                 PMC               PMID               

  Fornalski  K W           The Tadpole Bayesian Model for Detecting Trend Changes in Financial Quotations   PDF   R amp R Journal of Statistics and Mathematical Sciences                 

  Sch tz  N   Holschneider  M           Detection of trend changes in time series using Bayesian inference   Physical Review E                  arXiv            Bibcode     PhRvE    b    S  doi         PhysRevE            PMID                S CID               

  Stigler  Stephen M           Chapter     The History of Statistics  Harvard University Press  ISBN                    

  a b Fienberg  Stephen E           When did Bayesian Inference Become  Bayesian     Bayesian Analysis               p      doi            ba    

  Bernardo  Jos  Miguel          Reference analysis   Handbook of statistics  Vol           pp             

  Wolpert  R       L           A Conversation with James O  Berger   Statistical Science                   CiteSeerX                      doi                             MR               S CID                

  Bernardo  Jos  M           A Bayesian mathematical statistics primer   PDF   Icots   

  Bishop  C  M          Pattern Recognition and Machine Learning  New York  Springer  ISBN                     


Sources edit 

Aster  Richard  Borchers  Brian  and Thurber  Clifford         Parameter Estimation and Inverse Problems  Second Edition  Elsevier  ISBN                  ISBN                    
Bickel  Peter J   amp  Doksum  Kjell A          Mathematical Statistics  Volume    Basic and Selected Topics  Second  updated printing            ed    Pearson Prentice Hall  ISBN                        
Box  G       E       P  and Tiao  G       C          Bayesian Inference in Statistical Analysis  Wiley  ISBN                   
Edwards  Ward          Conservatism in Human Information Processing   In Kleinmuntz  B   ed    Formal Representation of Human Judgment  Wiley 
Edwards  Ward         Daniel Kahneman  Paul Slovic  Amos Tversky  eds     Judgment under uncertainty  Heuristics and biases   Science                         Bibcode     Sci           T  doi         science                PMID                S CID                 Chapter  Conservatism in Human Information Processing  excerpted 
Jaynes E       T         Probability Theory  The Logic of Science  CUP  ISBN                         Link to Fragmentary Edition of March       
Howson  C   amp  Urbach  P          Scientific Reasoning  the Bayesian Approach   rd      ed    Open Court Publishing Company  ISBN                        
Phillips  L  D   Edwards  Ward  October         Chapter    Conservatism in a Simple Probability Inference Task  Journal of Experimental Psychology                       In Jie W  Weiss  David J  Weiss  eds    A Science of Decision Making The Legacy of Ward Edwards  Oxford University Press  p            ISBN                        

Further reading edit 
For a full report on the history of Bayesian statistics and the debates with frequentists approaches  read Vallverdu  Jordi         Bayesians Versus Frequentists A Philosophical Debate on Statistical Reasoning  New York  Springer  ISBN                        
Clayton  Aubrey  August        Bernoulli s Fallacy  Statistical Illogic and the Crisis of Modern Science  Columbia University Press  ISBN                        
Elementary edit 
The following books are listed in ascending order of probabilistic sophistication 

Stone  JV          Bayes  Rule  A Tutorial Introduction to Bayesian Analysis     Download first  chapter here  Sebtel Press  England 
Dennis V  Lindley         Understanding Uncertainty  Revised Edition   nd      ed    John Wiley  ISBN                        
Colin Howson  amp  Peter Urbach         Scientific Reasoning  The Bayesian Approach   rd      ed    Open Court Publishing Company  ISBN                        
Berry  Donald A          Statistics  A Bayesian Perspective  Duxbury  ISBN                        
Morris H  DeGroot  amp  Mark J  Schervish         Probability and Statistics  third      ed    Addison Wesley  ISBN                        
Bolstad  William M         Introduction to Bayesian Statistics  Second Edition  John Wiley ISBN                   
Winkler  Robert L         Introduction to Bayesian Inference and Decision   nd      ed    Probabilistic  ISBN                         Updated classic textbook  Bayesian theory clearly presented 
Lee  Peter M  Bayesian Statistics  An Introduction  Fourth Edition         John Wiley ISBN                       
Carlin  Bradley P   amp  Louis  Thomas A          Bayesian Methods for Data Analysis  Third Edition  Boca Raton  FL  Chapman and Hall CRC  ISBN                        
Gelman  Andrew  Carlin  John B   Stern  Hal S   Dunson  David B   Vehtari  Aki  Rubin  Donald B          Bayesian Data Analysis  Third Edition  Chapman and Hall CRC  ISBN                        
Intermediate or advanced edit 
Berger  James O         Statistical Decision Theory and Bayesian Analysis  Springer Series in Statistics  Second      ed    Springer Verlag  Bibcode     sdtb book     B  ISBN                        
Bernardo  Jos       M   Smith  Adrian      F       M          Bayesian Theory  Wiley 
DeGroot  Morris H   Optimal Statistical Decisions  Wiley Classics Library         Originally published        by McGraw Hill   ISBN                  X 
Schervish  Mark J          Theory of statistics  Springer Verlag  ISBN                        
Jaynes  E  T          Probability Theory  The Logic of Science 
O Hagan  A  and Forster  J          Kendall s Advanced Theory of Statistics  Volume  B  Bayesian Inference  Arnold  New York  ISBN                    
Robert  Christian P         The Bayesian Choice  From Decision Theoretic Foundations to Computational Implementation  paperback      ed    Springer  ISBN                        
Pearl  Judea          Probabilistic Reasoning in Intelligent Systems  Networks of Plausible Inference  San Mateo  CA  Morgan Kaufmann 
Pierre Bessi re et al           Bayesian Programming   CRC Press  ISBN                   
Francisco J  Samaniego          A Comparison of the Bayesian and Frequentist Approaches to Estimation   Springer  New York  ISBN                       
External links edit 
 Bayesian approach to statistical problems   Encyclopedia of Mathematics  EMS Press             
Bayesian Statistics from Scholarpedia 
Introduction to Bayesian probability from Queen Mary University of London
Mathematical Notes on Bayesian Statistics and Markov Chain Monte Carlo
Bayesian reading list Archived            at the Wayback Machine  categorized and annotated by Tom Griffiths
A  Hajek and S  Hartmann  Bayesian Epistemology  in  J  Dancy et al   eds    A Companion to Epistemology  Oxford  Blackwell              
S  Hartmann and J  Sprenger  Bayesian Epistemology  in  S  Bernecker and D  Pritchard  eds    Routledge Companion to Epistemology  London  Routledge               
Stanford Encyclopedia of Philosophy   Inductive Logic 
Bayesian Confirmation Theory  PDF 
What is Bayesian Learning 
Data  Uncertainty and Inference   Informal introduction with many examples  ebook  PDF  freely available at causaScientia
vteStatistics
Outline
Index
Descriptive statisticsContinuous dataCenter
Mean
Arithmetic
Arithmetic Geometric
Contraharmonic
Cubic
Generalized power
Geometric
Harmonic
Heronian
Heinz
Lehmer
Median
Mode
Dispersion
Average absolute deviation
Coefficient of variation
Interquartile range
Percentile
Range
Standard deviation
Variance
Shape
Central limit theorem
Moments
Kurtosis
L moments
Skewness
Count data
Index of dispersion
Summary tables
Contingency table
Frequency distribution
Grouped data
Dependence
Partial correlation
Pearson product moment correlation
Rank correlation
Kendall s  
Spearman s  
Scatter plot
Graphics
Bar chart
Biplot
Box plot
Control chart
Correlogram
Fan chart
Forest plot
Histogram
Pie chart
Q Q plot
Radar chart
Run chart
Scatter plot
Stem and leaf display
Violin plot
Data collectionStudy design
Effect size
Missing data
Optimal design
Population
Replication
Sample size determination
Statistic
Statistical power
Survey methodology
Sampling
Cluster
Stratified
Opinion poll
Questionnaire
Standard error
Controlled experiments
Blocking
Factorial experiment
Interaction
Random assignment
Randomized controlled trial
Randomized experiment
Scientific control
Adaptive designs
Adaptive clinical trial
Stochastic approximation
Up and down designs
Observational studies
Cohort study
Cross sectional study
Natural experiment
Quasi experiment
Statistical inferenceStatistical theory
Population
Statistic
Probability distribution
Sampling distribution
Order statistic
Empirical distribution
Density estimation
Statistical model
Model specification
Lp space
Parameter
location
scale
shape
Parametric family
Likelihood       monotone 
Location scale family
Exponential family
Completeness
Sufficiency
Statistical functional
Bootstrap
U
V
Optimal decision
loss function
Efficiency
Statistical distance
divergence
Asymptotics
Robustness
Frequentist inferencePoint estimation
Estimating equations
Maximum likelihood
Method of moments
M estimator
Minimum distance
Unbiased estimators
Mean unbiased minimum variance
Rao Blackwellization
Lehmann Scheff  theorem
Median unbiased
Plug in
Interval estimation
Confidence interval
Pivot
Likelihood interval
Prediction interval
Tolerance interval
Resampling
Bootstrap
Jackknife
Testing hypotheses
    amp    tails
Power
Uniformly most powerful test
Permutation test
Randomization test
Multiple comparisons
Parametric tests
Likelihood ratio
Score Lagrange multiplier
Wald
Specific tests
Z test  normal 
Student s t test
F test
Goodness of fit
Chi squared
G test
Kolmogorov Smirnov
Anderson Darling
Lilliefors
Jarque Bera
Normality  Shapiro Wilk 
Likelihood ratio test
Model selection
Cross validation
AIC
BIC
Rank statistics
Sign
Sample median
Signed rank  Wilcoxon 
Hodges Lehmann estimator
Rank sum  Mann Whitney 
Nonparametric anova
  way  Kruskal Wallis 
  way  Friedman 
Ordered alternative  Jonckheere Terpstra 
Van der Waerden test
Bayesian inference
Bayesian probability
prior
posterior
Credible interval
Bayes factor
Bayesian estimator
Maximum posterior estimator
CorrelationRegression analysisCorrelation
Pearson product moment
Partial correlation
Confounding variable
Coefficient of determination
Regression analysis
Errors and residuals
Regression validation
Mixed effects models
Simultaneous equations models
Multivariate adaptive regression splines  MARS 
Linear regression
Simple linear regression
Ordinary least squares
General linear model
Bayesian regression
Non standard predictors
Nonlinear regression
Nonparametric
Semiparametric
Isotonic
Robust
Homoscedasticity and Heteroscedasticity
Generalized linear model
Exponential families
Logistic  Bernoulli             Binomial            Poisson regressions
Partition of variance
Analysis of variance  ANOVA  anova 
Analysis of covariance
Multivariate ANOVA
Degrees of freedom
Categorical            Multivariate            Time series            Survival analysisCategorical
Cohen s kappa
Contingency table
Graphical model
Log linear model
McNemar s test
Cochran Mantel Haenszel statistics
Multivariate
Regression
Manova
Principal components
Canonical correlation
Discriminant analysis
Cluster analysis
Classification
Structural equation model
Factor analysis
Multivariate distributions
Elliptical distributions
Normal
Time seriesGeneral
Decomposition
Trend
Stationarity
Seasonal adjustment
Exponential smoothing
Cointegration
Structural break
Granger causality
Specific tests
Dickey Fuller
Johansen
Q statistic  Ljung Box 
Durbin Watson
Breusch Godfrey
Time domain
Autocorrelation  ACF 
partial  PACF 
Cross correlation  XCF 
ARMA model
ARIMA model  Box Jenkins 
Autoregressive conditional heteroskedasticity  ARCH 
Vector autoregression  VAR 
Frequency domain
Spectral density estimation
Fourier analysis
Least squares spectral analysis
Wavelet
Whittle likelihood
SurvivalSurvival function
Kaplan Meier estimator  product limit 
Proportional hazards models
Accelerated failure time  AFT  model
First hitting time
Hazard function
Nelson Aalen estimator
Test
Log rank test
ApplicationsBiostatistics
Bioinformatics
Clinical trials            studies
Epidemiology
Medical statistics
Engineering statistics
Chemometrics
Methods engineering
Probabilistic design
Process            quality control
Reliability
System identification
Social statistics
Actuarial science
Census
Crime statistics
Demography
Econometrics
Jurimetrics
National accounts
Official statistics
Population statistics
Psychometrics
Spatial statistics
Cartography
Environmental statistics
Geographic information system
Geostatistics
Kriging

Category
 Mathematics     portal
Commons
 WikiProject

Authority control databases  National GermanyUnited StatesCzech RepublicIsrael





Retrieved from  https   en wikipedia org w index php title Bayesian inference amp oldid