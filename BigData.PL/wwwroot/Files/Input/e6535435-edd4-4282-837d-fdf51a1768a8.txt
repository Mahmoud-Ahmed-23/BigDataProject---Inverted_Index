Machine learning technique


Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
In machine learning  reinforcement learning from human feedback  RLHF  is a technique to align an intelligent agent with human preferences  It involves training a reward model to represent preferences  which can then be used to train other models through reinforcement learning 
In classical reinforcement learning  an intelligent agent s goal is to learn a function that guides its behavior  called a policy  This function is iteratively updated to maximize rewards based on the agent s task performance             However  explicitly defining a reward function that accurately approximates human preferences is challenging  Therefore  RLHF seeks to train a  reward model  directly from human feedback             The reward model is first trained in a supervised manner to predict if a response to a given prompt is good  high reward  or bad  low reward  based on ranking data collected from human annotators  This model then serves as a reward function to improve an agent s policy through an optimization algorithm like proximal policy optimization            
           
           
RLHF has applications in various domains in machine learning  including natural language processing tasks such as text summarization and conversational agents  computer vision tasks like text to image models  and the development of video game bots  While RLHF is an effective method of training models to act better in accordance with human preferences  it also faces challenges due to the way the human preference data is collected  Though RLHF does not require massive amounts of data to improve performance  sourcing high quality preference data is still an expensive process  Furthermore  if the data is not carefully collected from a representative sample  the resulting model may exhibit unwanted biases 


Background and motivation edit 
Optimizing a model based on human feedback is desirable when a task is difficult to specify yet easy to judge             For example  one may want to train a model to generate safe text that is both helpful and harmless  such as lacking bias  toxicity  or otherwise harmful content   Asking humans to manually create examples of harmless and harmful text would be difficult and time consuming  However  humans are adept at swiftly assessing and comparing the harmfulness of different AI generated text  Therefore  a more practical objective would be to allow the model to use this type of human feedback to improve its text generation            
Despite the clear benefits of incorporating human feedback in training models  prior efforts including some that leverage reinforcement learning have encountered significant challenges  Most attempts were either narrow and difficult to generalize  breaking down on more complex tasks                                                or they faced difficulties learning from sparse  lacking specific information and relating to large amounts of text at a time  or noisy  inconsistently rewarding similar outputs  reward functions                         
RLHF was not the first successful method of using human feedback for reinforcement learning  but it is one of the most widely used  The foundation for RLHF was introduced as an attempt to create a general algorithm for learning from a practical amount of human feedback                        The algorithm as used today was introduced by OpenAI in a paper on enhancing text continuation or summarization based on human feedback  and it began to gain popularity when the same method was reused in their paper on InstructGPT                                     RLHF has also been shown to improve the robustness of RL agents and their capacity for exploration  which results in an optimization process more adept at handling uncertainty and efficiently exploring its environment in search of the highest reward             

Collecting human feedback edit 
High level overview of reinforcement learning from human feedback
Human feedback is commonly collected by prompting humans to rank instances of the agent s behavior                                      These rankings can then be used to score outputs  for example  using the Elo rating system  which is an algorithm for calculating the relative skill levels of players in a game based only on the outcome of each game             While ranking outputs is the most widely adopted form of feedback  recent research has explored other forms  such as numerical feedback  natural language feedback  and prompting for direct edits to the model s output             
One initial motivation of RLHF was that it requires relatively small amounts of comparison data to be effective             It has been shown that a small amount of data can lead to comparable results to a larger amount  In addition  increasing the amount of data tends to be less effective than proportionally increasing the size of the reward model              Nevertheless  a larger and more diverse amount of data can be crucial for tasks where it is important to avoid bias from a partially representative group of annotators             
When learning from human feedback through pairwise comparison under the Bradley Terry Luce model  or the Plackett Luce model for K wise comparisons over more than two comparisons   the maximum likelihood estimator  MLE  for linear reward functions has been shown to converge if the comparison data is generated under a well specified linear model  This implies that  under certain conditions  if a model is trained to decide which choices people would prefer between pairs  or groups  of choices  it will necessarily improve at predicting future preferences  This improvement is expected as long as the comparisons it learns from are based on a consistent and simple rule                         
Both offline data collection models  where the model is learning by interacting with a static dataset and updating its policy in batches  as well as online data collection models  where the model directly interacts with the dynamic environment and updates its policy immediately  have been mathematically studied proving sample complexity bounds for RLHF under different feedback models                         
In the offline data collection model  when the objective is policy training  a pessimistic MLE that incorporates a lower confidence bound as the reward estimate is most effective  Moreover  when applicable  it has been shown that considering K wise comparisons directly is asymptotically more efficient than converting them into pairwise comparisons for prediction purposes                                     
In the online scenario  when human feedback is collected through pairwise comparisons under the Bradley Terry Luce model and the objective is to minimize the algorithm s regret  the difference in performance compared to an optimal agent   it has been shown that an optimistic MLE that incorporates an upper confidence bound as the reward estimate can be used to design sample efficient algorithms  meaning that they require relatively little training data   A key challenge in RLHF when learning from pairwise  or dueling  comparisons is associated with the non Markovian nature of its optimal policies  Unlike simpler scenarios where the optimal strategy does not require memory of past actions  in RLHF  the best course of action often depends on previous events and decisions  making the strategy inherently memory dependent             

Applications edit 
RLHF has been applied to various domains of natural language processing  NLP   such as conversational agents  text summarization  and natural language understanding                          Ordinary reinforcement learning  in which agents learn from their actions based on a predefined  reward function   is difficult to apply to NLP tasks because the rewards tend to be difficult to define or measure  especially when dealing with complex tasks that involve human values or preferences             RLHF can steer NLP models  in particular language models  to provide answers that align with human preferences with regard to such tasks by capturing their preferences beforehand in the reward model  This results in a model capable of generating more relevant responses and rejecting inappropriate or irrelevant queries                          Some notable examples of RLHF trained language models are OpenAI s ChatGPT  and its predecessor InstructGPT                                       DeepMind s Sparrow                                      Google s Gemini              and Anthropic s Claude             
In computer vision  RLHF has also been used to align text to image models  Studies that successfully used RLHF for this goal have noted that the use of KL regularization in RLHF  which aims to prevent the learned policy from straying too far from the unaligned model  helped to stabilize the training process by reducing overfitting to the reward model  The final image outputs from models trained with KL regularization were noted to be of significantly higher quality than those trained without                          Other methods tried to incorporate the feedback through more direct training based on maximizing the reward without the use of reinforcement learning but conceded that an RLHF based approach would likely perform better due to the online sample generation used in RLHF during updates as well as the aforementioned KL regularization over the prior model  which mitigates overfitting to the reward function             
RLHF was initially applied to other areas  such as the development of video game bots and tasks in simulated robotics  For example  OpenAI and DeepMind trained agents to play Atari games based on human preferences  In classical RL based training of such bots  the reward function is simply correlated to how well the agent is performing in the game  usually using metrics like the in game score  In comparison  in RLHF  a human is periodically presented with two clips of the agent s behavior in the game and must decide which one looks better  This approach can teach agents to perform at a competitive level without ever having access to their score  In fact  it was shown that RLHF can sometimes lead to superior performance over RL with score metrics because the human s preferences can contain more useful information than performance based metrics                         The agents achieved strong performance in many of the environments tested  often surpassing human performance             

Training edit 
In RLHF  two different models are trained  a reward model and a reinforcement learning  RL  policy  The reward model learns to determine what behavior is desirable based on human feedback  while the policy is guided by the reward model to determine the agent s actions  Both models are commonly initialized using a pre trained autoregressive language model  This model is then customarily trained in a supervised manner on a relatively small dataset of pairs of prompts to an assistant and their accompanying responses  written by human annotators 

Reward model edit 
The reward model is usually initialized with a pre trained model  as this initializes it with an understanding of language and focuses training explicitly on learning human preferences  In addition to being used to initialize the reward model and the RL policy  the model is then also used to sample data to be compared by annotators                         
The reward model is then trained by replacing the final layer of the previous model with a randomly initialized regression head  This change shifts the model from its original classification task over its vocabulary to simply outputting a number corresponding to the score of any given prompt and response  This model is trained on the human preference comparison data collected earlier from the supervised model  In particular  it is trained to minimize the following cross entropy loss function 
  
    
      
        
          
            L
          
        
         
          x b  
         
         
          x     
        
          
             
            
              
                 
              
              
                K
                 
              
              
                 
              
            
          
        
        
          E
          
             
            x
             
            
              y
              
                w
              
            
             
            
              y
              
                l
              
            
             
          
        
         
        log
          x     
         
          x c  
         
        
          r
          
              x b  
          
        
         
        x
         
        
          y
          
            w
          
        
         
          x     
        
          r
          
              x b  
          
        
         
        x
         
        
          y
          
            l
          
        
         
         
         
         
         
          x     
        
          
             
            
              
                 
              
              
                K
                 
              
              
                 
              
            
          
        
        
          E
          
             
            x
             
            
              y
              
                w
              
            
             
            
              y
              
                l
              
            
             
          
        
        log
          x     
        
           
          
            
              
                e
                
                  
                    r
                    
                        x b  
                    
                  
                   
                  x
                   
                  
                    y
                    
                      w
                    
                  
                   
                
              
              
                
                  e
                  
                    
                      r
                      
                          x b  
                      
                    
                     
                    x
                     
                    
                      y
                      
                        w
                      
                    
                     
                  
                
                 
                
                  e
                  
                    
                      r
                      
                          x b  
                      
                    
                     
                    x
                     
                    
                      y
                      
                        l
                      
                    
                     
                  
                
              
            
          
           
        
      
    
      displaystyle   mathcal  L    theta      frac     K  choose    E   x y  w  y  l     log  sigma  r   theta   x y  w   r   theta   x y  l         frac     K  choose    E   x y  w  y  l    log  left   frac  e  r   theta   x y  w     e  r   theta   x y  w    e  r   theta   x y  l      right  
  

where 
  
    
      
        K
      
    
      displaystyle K 
  
 is the number of responses the labelers ranked  
  
    
      
        
          r
          
              x b  
          
        
         
        x
         
        y
         
      
    
      displaystyle r   theta   x y  
  
 is the output of the reward model for prompt 
  
    
      
        x
      
    
      displaystyle x 
  
 and completion 
  
    
      
        y
      
    
      displaystyle y 
  
  
  
    
      
        
          y
          
            w
          
        
      
    
      displaystyle y  w  
  
 is the preferred completion over 
  
    
      
        
          y
          
            l
          
        
      
    
      displaystyle y  l  
  
  
  
    
      
          x c  
         
        x
         
      
    
      displaystyle  sigma  x  
  
 denotes the sigmoid function  and 
  
    
      
        E
         
        X
         
      
    
      displaystyle E X  
  
 denotes the expected value              This can be thought of as a form of logistic regression  where the model predicts the probability that a response 
  
    
      
        
          y
          
            w
          
        
      
    
      displaystyle y  w  
  
 is preferred over 
  
    
      
        
          y
          
            l
          
        
      
    
      displaystyle y  l  
  
 
This loss function essentially measures the difference between the reward model s predictions and the decisions made by humans  The goal is to make the model s guesses as close as possible to the humans  preferences by minimizing the difference measured by this equation  In the case of only pairwise comparisons  
  
    
      
        K
         
         
      
    
      displaystyle K   
  
  so the factor of 
  
    
      
         
        
           
        
        
          
            
              
                 
              
              
                K
                 
              
              
                 
              
            
          
        
         
         
      
    
      displaystyle     tbinom  K        
  
              In general  all 
  
    
      
        
          
            
              
                 
              
              
                K
                 
              
              
                 
              
            
          
        
      
    
      displaystyle   tbinom  K      
  
 comparisons from each prompt are used for training as a single batch             
After training  the outputs of the model are normalized such that the reference completions have a mean score of    That is             
  
    
      
        
            x     
          
            y
          
        
        
          r
          
              x b  
          
        
         
        x
         
        y
         
         
         
      
    
      textstyle  sum   y r   theta   x y    
  
  for each query and reference pair 
  
    
      
         
        x
         
        y
         
      
    
      displaystyle  x y  
  
 by calculating the mean reward across the training dataset and setting it as the bias in the reward head 

Policy edit 
Similarly to the reward model  the human feedback policy is also initialized from a pre trained model             
The key is to understand language generation as if it is a game to be learned by RL  In RL  a policy is a function that maps a game state to a game action  In RLHF  the  game  is the game of replying to prompts  A prompt is a game state  and a response is a game action  This is a fairly trivial kind of game  since every game lasts for exactly one step  Nevertheless  it is a game  and so RL algorithms can be applied to it 
The first step in its training is supervised fine tuning  SFT   This step does not require the reward model  Instead  the pre trained model is trained on a dataset 
  
    
      
        
          D
          
            S
            F
            T
          
        
      
    
      displaystyle D  SFT  
  
 that contains prompt response pairs 
  
    
      
         
        x
         
        y
         
      
    
      displaystyle  x y  
  
  Then  during SFT  the model is trained to auto regressively generate the corresponding response 
  
    
      
        y
      
    
      displaystyle y 
  
 when given a random prompt 
  
    
      
        x
      
    
      displaystyle x 
  
  The original paper recommends to SFT for only one epoch  since more than that causes overfitting 
The dataset 
  
    
      
        
          D
          
            S
            F
            T
          
        
      
    
      displaystyle D  SFT  
  
 is usually written by human contractors  who write both the prompts and responses 
The second step uses a policy gradient method to the reward model  It uses a dataset 
  
    
      
        
          D
          
            R
            L
          
        
      
    
      displaystyle D  RL  
  
  which contains prompts  but not responses  Like most policy gradient methods  this algorithm has an outer loop and two inner loops 

Initialize the policy 
  
    
      
        
            x c  
          
              x d  
          
          
            R
            L
          
        
      
    
      displaystyle  pi    phi    RL  
  
 to 
  
    
      
        
            x c  
          
            S
            F
            T
          
        
      
    
      displaystyle  pi   SFT  
  
  the policy output from SFT 
Loop for many steps 
Initialize a new empty dataset 
  
    
      
        
          D
          
            
                x c  
              
                  x d  
              
              
                R
                L
              
            
          
        
      
    
      displaystyle D   pi    phi    RL   
  
 
Loop for many steps
Sample a random prompt 
  
    
      
        x
      
    
      displaystyle x 
  
 from 
  
    
      
        
          D
          
            R
            L
          
        
      
    
      displaystyle D  RL  
  
 
Generate a response 
  
    
      
        y
      
    
      displaystyle y 
  
 from the policy 
  
    
      
        
            x c  
          
              x d  
          
          
            R
            L
          
        
      
    
      displaystyle  pi    phi    RL  
  
 
Calculate the reward signal 
  
    
      
        
          r
          
              x b  
          
        
         
        x
         
        y
         
      
    
      displaystyle r   theta   x y  
  
 from the reward model 
  
    
      
        
          r
          
              x b  
          
        
      
    
      displaystyle r   theta   
  
 
Add the triple 
  
    
      
         
        x
         
        y
         
        
          r
          
              x b  
          
        
         
        x
         
        y
         
         
      
    
      displaystyle  x y r   theta   x y   
  
 to 
  
    
      
        
          D
          
            
                x c  
              
                  x d  
              
              
                R
                L
              
            
          
        
      
    
      displaystyle D   pi    phi    RL   
  
 
Update 
  
    
      
          x d  
      
    
      displaystyle  phi  
  
 by a policy gradient method to increase the objective function
  
    
      
        
          objective
        
         
          x d  
         
         
        
          E
          
             
            x
             
            y
             
              x   c 
            
              D
              
                
                    x c  
                  
                      x d  
                  
                  
                    RL
                  
                
              
            
          
        
        
           
          
            
              r
              
                  x b  
              
            
             
            x
             
            y
             
              x     
              x b  
            log
              x     
            
               
              
                
                  
                    
                        x c  
                      
                          x d  
                      
                      
                        RL
                      
                    
                     
                    y
                    
                       
                    
                    x
                     
                  
                  
                    
                        x c  
                      
                        SFT
                      
                    
                     
                    y
                    
                       
                    
                    x
                     
                  
                
              
               
            
          
           
        
      
    
      displaystyle   text objective    phi   E   x y  sim D   pi    phi     text RL     left r   theta   x y   beta  log  left   frac   pi    phi     text RL   y x    pi    text SFT   y x    right  right  
  

Note that 
  
    
      
         
        x
         
        y
         
          x   c 
        
          D
          
            
                x c  
              
                  x d  
              
              
                RL
              
            
          
        
      
    
      displaystyle  x y  sim D   pi    phi     text RL    
  
 is equivalent to 
  
    
      
        x
          x   c 
        
          D
          
            R
            L
          
        
         
        y
          x   c 
        
            x c  
          
              x d  
          
          
            RL
          
        
         
          x  c  
        
           
        
        x
         
      
    
      displaystyle x sim D  RL  y sim  pi    phi     text RL    cdot  x  
  
  which means  sample a prompt from 
  
    
      
        
          D
          
            R
            L
          
        
      
    
      displaystyle D  RL  
  
  then sample a response from the policy  
The objective function has two parts  The first part is simply the expected reward 
  
    
      
        E
         
        r
         
      
    
      displaystyle E r  
  
  and is standard for any RL algorithm  The second part is a  penalty term  involving the KL divergence  The strength of the penalty term is determined by the hyperparameter 
  
    
      
          x b  
      
    
      displaystyle  beta  
  
 
This KL term works by penalizing the KL divergence  a measure of statistical distance between distributions  between the model being fine tuned and the initial supervised model  By choosing an appropriate 
  
    
      
          x b  
      
    
      displaystyle  beta  
  
  the training can balance learning from new data while retaining useful information from the initial model  increasing generalization by avoiding fitting too closely to the new data  Aside from preventing the new model from producing outputs too dissimilar those of the initial model  a second motivation of including the KL term is to encourage the model to output high entropy text  so as to prevent the model from collapsing to a small number of canned responses             
In simpler terms  the objective function calculates how well the policy s responses are expected to align with human feedback  The policy generates responses to prompts  and each response is evaluated both on how well it matches human preferences  as measured by the reward model  and how similar it is to responses the model would naturally generate  The goal is to balance improving alignment with human preferences while ensuring the model s responses remain diverse and not too far removed from what it has learned during its initial training  This helps the model not only to provide answers that people find useful or agreeable but also to maintain a broad understanding and avoid overly narrow or repetitive responses 

Proximal policy optimization edit 
Main article  Policy gradient method        Proximal Policy Optimization
The policy function is usually trained by proximal policy optimization  PPO  algorithm  That is  the parameter 
  
    
      
          x d  
      
    
      displaystyle  phi  
  
 is trained by gradient ascent on the clipped surrogate function                         
Classically  the PPO algorithm employs generalized advantage estimation  which means that there is an extra value estimator 
  
    
      
        
          V
          
            
                x be 
              
                t
              
            
          
        
         
        x
         
      
    
      displaystyle V   xi   t   x  
  
  that updates concurrently with the policy 
  
    
      
        
            x c  
          
            
                x d  
              
                t
              
            
          
          
            R
            L
          
        
      
    
      displaystyle  pi    phi   t    RL  
  
 during PPO training  
  
    
      
        
            x c  
          
            
                x d  
              
                t
              
            
          
          
            R
            L
          
        
         
        
          V
          
            
                x be 
              
                t
              
            
          
        
         
        
            x c  
          
            
                x d  
              
                t
                 
                 
              
            
          
          
            R
            L
          
        
         
        
          V
          
            
                x be 
              
                t
                 
                 
              
            
          
        
         
          x     
      
    
      displaystyle  pi    phi   t    RL  V   xi   t    pi    phi   t      RL  V   xi   t      dots  
  
  The value estimator is used only during training  and not outside of training 
The PPO uses gradient descent on the following clipped surrogate advantage 
  
    
      
        
          L
          
            PPO
          
        
         
          x d  
         
          
        
          E
          
            x
              x   c 
            
              D
              
                RL
              
            
             
            y
              x   c 
            
                x c  
              
                
                    x d  
                  
                    t
                  
                
              
            
             
            y
            
               
            
            x
             
          
        
        
           
          
            min
            
               
              
                
                  
                    
                      
                          x c  
                        
                            x d  
                        
                        
                          R
                          L
                        
                      
                       
                      y
                      
                         
                      
                      x
                       
                    
                    
                      
                          x c  
                        
                          
                              x d  
                            
                              t
                            
                          
                        
                        
                          R
                          L
                        
                      
                       
                      y
                      
                         
                      
                      x
                       
                    
                  
                
                A
                 
                x
                 
                y
                 
                 
                
                  c
                  l
                  i
                  p
                
                
                   
                  
                    
                      
                        
                          
                              x c  
                            
                                x d  
                            
                            
                              R
                              L
                            
                          
                           
                          y
                          
                             
                          
                          x
                           
                        
                        
                          
                              x c  
                            
                              
                                  x d  
                                
                                  t
                                
                              
                            
                            
                              R
                              L
                            
                          
                           
                          y
                          
                             
                          
                          x
                           
                        
                      
                    
                     
                     
                      x     
                      x f  
                     
                     
                     
                      x f  
                  
                   
                
                A
                 
                x
                 
                y
                 
              
               
            
          
           
        
      
    
      displaystyle L   text PPO    phi    E  x sim D   text RL   y sim  pi    phi   t   y x   left  min  left   frac   pi    phi    RL  y x    pi    phi   t    RL  y x   A x y   mathrm  clip   left   frac   pi    phi    RL  y x    pi    phi   t    RL  y x       epsilon     epsilon  right A x y  right  right  
  

where the advantage term 
  
    
      
        A
         
        x
         
        y
         
      
    
      displaystyle A x y  
  
 is defined as 
  
    
      
        
          r
          
              x b  
          
        
         
        x
         
        y
         
          x     
        
          V
          
            
                x be 
              
                t
              
            
          
        
         
        x
         
      
    
      displaystyle r   theta   x y  V   xi   t   x  
  
  That is  the advantage is computed as the difference between the reward  the expected return  and the value estimation  the expected return from the policy   This is used to train the policy by gradient ascent on it  usually using a standard momentum gradient optimizer  like the Adam optimizer 
The original paper initialized the value estimator from the trained reward model              Since PPO is an actor critic algorithm  the value estimator is updated concurrently with the policy  via minimizing the squared TD error  which in this case equals the squared advantage term 
  
    
      
        
          L
          
            TD
          
        
         
          x be 
         
         
        
          
            E
          
          
             
            x
             
            y
             
              x   c 
            D
            
              
                  x c  
                
                  
                      x d  
                    
                      t
                    
                  
                
                
                  RL
                
              
            
          
        
        
           
          
            
               
              
                
                  r
                  
                      x b  
                  
                
                 
                x
                 
                y
                 
                  x     
                  x b  
                log
                  x     
                
                   
                  
                    
                      
                        
                            x c  
                          
                            
                                x d  
                              
                                t
                              
                            
                          
                          
                            RL
                          
                        
                         
                        y
                        
                           
                        
                        x
                         
                      
                      
                        
                            x c  
                          
                            SFT
                          
                        
                         
                        y
                        
                           
                        
                        x
                         
                      
                    
                  
                   
                
                  x     
                
                  V
                  
                      x be 
                  
                
                 
                x
                 
              
               
            
            
               
            
          
           
        
      
    
      displaystyle L   text TD    xi    mathbb  E     x y  sim D  pi    phi   t     text RL     left  left r   theta   x y   beta  log  left   frac   pi    phi   t     text RL   y x    pi    text SFT   y x    right  V   xi   x  right      right  
  
which is minimized by gradient descent on it  Other methods than squared TD error might be used  See the actor critic algorithm page for details 

Mixing pretraining gradients edit 
A third term is commonly added to the objective function to prevent the model from catastrophic forgetting  For example  if the model is only trained in customer service  then it might forget general knowledge in geography  To prevent this  the RLHF process incorporates the original language modeling objective  That is  some random texts 
  
    
      
        x
      
    
      displaystyle x 
  
 are sampled from the original pretraining dataset 
  
    
      
        
          D
          
            pretrain
          
        
      
    
      displaystyle D   text pretrain   
  
  and the model is trained to maximize the log likelihood of the text 
  
    
      
        log
          x     
         
        
            x c  
          
              x d  
          
          
            R
            L
          
        
         
        x
         
         
      
    
      displaystyle  log  pi    phi    RL  x   
  
  The final objective function is written as 

  
    
      
        L
         
          x d  
         
         
        
          E
          
             
            x
             
            y
             
              x   c 
            
              D
              
                
                    x c  
                  
                      x d  
                  
                  
                    RL
                  
                
              
            
          
        
        
           
          
            
              r
              
                  x b  
              
            
             
            x
             
            y
             
              x     
              x b  
            log
              x     
            
               
              
                
                  
                    
                        x c  
                      
                          x d  
                      
                      
                        RL
                      
                    
                     
                    y
                    
                       
                    
                    x
                     
                  
                  
                    
                        x c  
                      
                        SFT
                      
                    
                     
                    y
                    
                       
                    
                    x
                     
                  
                
              
               
            
          
           
        
         
          x b  
        
          E
          
            x
              x   c 
            
              D
              
                pretrain
              
            
          
        
         
        log
          x     
         
        
            x c  
          
              x d  
          
          
            RL
          
        
         
        x
         
         
         
      
    
      displaystyle L  phi   E   x y  sim D   pi    phi     text RL     left r   theta   x y   beta  log  left   frac   pi    phi     text RL   y x    pi    text SFT   y x    right  right   gamma E  x sim D   text pretrain     log  pi    phi     text RL   x    
  

where 
  
    
      
          x b  
      
    
      displaystyle  gamma  
  
 controls the strength of this pretraining term              This combined objective function is called PPO ptx  where  ptx  means  Mixing Pretraining Gradients              It was first used in the InstructGPT paper             
In total  this objective function defines the method for adjusting the RL policy  blending the aim of aligning with human feedback and maintaining the model s original language understanding 
So  writing out fully explicitly  the PPO ptx objective function is 
  
    
      
        
          L
          
            PPO ptx
          
        
         
          x d  
         
          
        
          E
          
             
            x
             
            y
             
              x   c 
            
              D
              
                
                    x c  
                  
                    
                        x d  
                      
                        t
                      
                    
                  
                  
                    RL
                  
                
              
            
          
        
        
           
          
            min
            
               
              
                
                  
                    
                      
                          x c  
                        
                            x d  
                        
                        
                          R
                          L
                        
                      
                       
                      y
                      
                         
                      
                      x
                       
                    
                    
                      
                          x c  
                        
                          
                              x d  
                            
                              t
                            
                          
                        
                        
                          R
                          L
                        
                      
                       
                      y
                      
                         
                      
                      x
                       
                    
                  
                
                A
                 
                x
                 
                y
                 
                 
                
                  c
                  l
                  i
                  p
                
                
                   
                  
                    
                      
                        
                          
                              x c  
                            
                                x d  
                            
                            
                              R
                              L
                            
                          
                           
                          y
                          
                             
                          
                          x
                           
                        
                        
                          
                              x c  
                            
                              
                                  x d  
                                
                                  t
                                
                              
                            
                            
                              R
                              L
                            
                          
                           
                          y
                          
                             
                          
                          x
                           
                        
                      
                    
                     
                     
                      x     
                      x f  
                     
                     
                     
                      x f  
                  
                   
                
                A
                 
                x
                 
                y
                 
              
               
            
              x     
              x b  
            log
              x     
            
               
              
                
                  
                    
                        x c  
                      
                          x d  
                      
                      
                        RL
                      
                    
                     
                    y
                    
                       
                    
                    x
                     
                  
                  
                    
                        x c  
                      
                        SFT
                      
                    
                     
                    y
                    
                       
                    
                    x
                     
                  
                
              
               
            
          
           
        
         
          x b  
        
          E
          
            x
              x   c 
            
              D
              
                pretrain
              
            
          
        
         
        log
          x     
         
        
            x c  
          
              x d  
          
          
            RL
          
        
         
        x
         
         
         
      
    
      displaystyle L   text PPO ptx    phi    E   x y  sim D   pi    phi   t     text RL     left  min  left   frac   pi    phi    RL  y x    pi    phi   t    RL  y x   A x y   mathrm  clip   left   frac   pi    phi    RL  y x    pi    phi   t    RL  y x       epsilon     epsilon  right A x y  right   beta  log  left   frac   pi    phi     text RL   y x    pi    text SFT   y x    right  right   gamma E  x sim D   text pretrain     log  pi    phi     text RL   x    
  
which is optimized by gradient ascent on it 

Limitations edit 
RLHF suffers from challenges with collecting human feedback  learning a reward model  and optimizing the policy              Compared to data collection for techniques like unsupervised or self supervised learning  collecting data for RLHF is less scalable and more expensive  Its quality and consistency may vary depending on the task  interface  and the preferences and biases of individual humans                         
The effectiveness of RLHF depends on the quality of human feedback  For instance  the model may become biased  favoring certain groups over others  if the feedback lacks impartiality  is inconsistent  or is incorrect                         There is a risk of overfitting  where the model memorizes specific feedback examples instead of learning to generalize  For instance  feedback predominantly from a specific demographic might lead the model to learn peculiarities or noise  along with the intended alignment  Excessive alignment to the specific feedback it received  that is  to the bias therein  can lead to the model performing sub optimally in new contexts or when used by different groups              A single reward function cannot always represent the opinions of diverse groups of people  Even with a representative sample  conflicting views and preferences may result in the reward model favoring the majority s opinion  potentially disadvantaging underrepresented groups             
In some cases  as is possible in regular reinforcement learning  there may be a risk of the model learning to manipulate the feedback process or game the system to achieve higher rewards rather than genuinely improving its performance              In the case of RLHF  a model may learn to exploit the fact that it is rewarded for what is evaluated positively and not necessarily for what is actually good  which can lead to it learning to persuade and manipulate  For example  models might learn that apparent confidence  even if inaccurate  garners higher rewards  Such behavior  if unchecked  is not just incentivized but can cause significant deployment issues due to the model s potential to mislead  Studies have found that humans are not skilled at identifying mistakes in LLM outputs in complex tasks  therefore  models learning to generate confident sounding yet incorrect text can lead to significant issues when deployed             

Alternatives edit 
Reinforcement learning from AI feedback edit 
Similarly to RLHF  reinforcement learning from AI feedback  RLAIF  relies on training a preference model  except that the feedback is automatically generated              This is notably used in Anthropic s constitutional AI  where the AI feedback is based on the conformance to the principles of a constitution             

Direct alignment algorithms edit 
Direct alignment algorithms  DAA  have been proposed as a new class of algorithms                         that seek to directly optimize large language models  LLMs  on human feedback data in a supervised manner instead of the traditional policy gradient methods 
These algorithms aim to align models with human intent more transparently by removing the intermediate step of training a separate reward model  Instead of first predicting human preferences and then optimizing against those predictions  direct alignment methods train models end to end on human labeled or curated outputs  This reduces potential misalignment risks introduced by proxy objectives or reward hacking 
By directly optimizing for the behavior preferred by humans  these approaches often enable tighter alignment with human values  improved interpretability  and simpler training pipelines compared to RLHF 

Direct preference optimization edit 
Direct preference optimization  DPO  is a technique to learn human preferences  Like RLHF  it has been applied to align pre trained large language models using human generated preference data  Unlike RLHF  however  which first trains a separate intermediate model to understand what good outcomes look like and then teaches the main model how to achieve those outcomes  DPO simplifies the process by directly adjusting the main model according to people s preferences  It uses a change of variables to define the  preference loss  directly as a function of the policy and uses this loss to fine tune the model  helping it understand and prioritize human preferences without needing a separate step  Essentially  this approach directly shapes the model s decisions based on positive or negative human feedback 
Recall  the pipeline of RLHF is as follows 

We begin by gathering human preference dataset 
  
    
      
        D
      
    
      displaystyle D 
  
 
We then fit a reward model 
  
    
      
        
          r
          
              x     
          
        
      
    
      displaystyle r     
  
 to data  by maximum likelihood estimation using the Plackett Luce model
  
    
      
        
          r
          
              x     
          
        
         
        arg
          x     
        
          max
          
            r
          
        
        
          
            E
          
          
             
            x
             
            
              y
              
                 
              
            
             
              x     
             
            
              y
              
                N
              
            
             
              x   c 
            D
          
        
        
           
          
            ln
              x     
            
                x   f 
              
                k
                 
                 
              
              
                N
              
            
            
              
                
                  e
                  
                    r
                     
                    x
                     
                    
                      y
                      
                        k
                      
                    
                     
                  
                
                
                  
                      x     
                    
                      i
                       
                      k
                    
                    
                      N
                    
                  
                  
                    e
                    
                      r
                       
                      x
                       
                      
                        y
                        
                          i
                        
                      
                       
                    
                  
                
              
            
          
           
        
      
    
      displaystyle r      arg  max   r  mathbb  E     x y      dots  y  N   sim D  left  ln  prod   k     N   frac  e  r x y  k      sum   i k   N e  r x y  i      right  
  

We finally train an optimal policy 
  
    
      
        
            x c  
          
              x     
          
        
      
    
      displaystyle  pi      
  
 that maximizes the objective function 
  
    
      
        
            x c  
          
              x     
          
        
         
        arg
          x     
        
          max
          
            
                x c  
              
                RL
              
            
          
        
        
          
            E
          
          
             
            x
             
            y
             
              x   c 
            
              D
              
                
                    x c  
                  
                    RL
                  
                
              
            
          
        
        
           
          
            
              r
              
                  x     
              
            
             
            x
             
            y
             
              x     
              x b  
            log
              x     
            
               
              
                
                  
                    
                        x c  
                      
                        RL
                      
                    
                     
                    y
                    
                       
                    
                    x
                     
                  
                  
                    
                        x c  
                      
                        SFT
                      
                    
                     
                    y
                    
                       
                    
                    x
                     
                  
                
              
               
            
          
           
        
      
    
      displaystyle  pi       arg  max    pi    text RL    mathbb  E     x y  sim D   pi    text RL     left r     x y   beta  log  left   frac   pi    text RL   y x    pi    text SFT   y x    right  right  
  

However  instead of doing the intermediate step of the reward model  DPO directly optimizes for the final policy 
First  solve directly for the optimal policy  which can be done by Lagrange multipliers  as usual in statistical mechanics 
  
    
      
        
            x c  
          
              x     
          
        
         
        y
        
           
        
        x
         
         
        
          
            
              
                  x c  
                
                  SFT
                
              
               
              y
              
                 
              
              x
               
              exp
                x     
               
              
                r
                
                    x     
                
              
               
              x
               
              y
               
              
                 
              
                x b  
               
            
            
              Z
               
              x
               
            
          
        
         
      
    
      displaystyle  pi      y x    frac   pi    text SFT   y x  exp r     x y   beta    Z x     
  

where 
  
    
      
        Z
         
        x
         
      
    
      displaystyle Z x  
  
 is the partition function  This is unfortunately not tractable  since it requires summing over all possible responses  
  
    
      
        Z
         
        x
         
         
        
            x     
          
            y
          
        
        
            x c  
          
            SFT
          
        
         
        y
        
           
        
        x
         
        exp
          x     
         
        
          r
          
              x     
          
        
         
        x
         
        y
         
        
           
        
          x b  
         
         
        
          
            E
          
          
            y
              x   c 
            
                x c  
              
                SFT
              
            
             
              x  c  
            
               
            
            x
             
          
        
         
        exp
          x     
         
        
          r
          
              x     
          
        
         
        x
         
        y
         
        
           
        
          x b  
         
         
      
    
      displaystyle Z x   sum   y  pi    text SFT   y x  exp r     x y   beta    mathbb  E    y sim  pi    text SFT    cdot  x    exp r     x y   beta    
  

Next  invert this relationship to express the reward implicitly in terms of the optimal policy 
  
    
      
        
          r
          
              x     
          
        
         
        x
         
        y
         
         
          x b  
        log
          x     
        
          
            
              
                  x c  
                
                    x     
                
              
               
              y
              
                 
              
              x
               
            
            
              
                  x c  
                
                  SFT
                
              
               
              y
              
                 
              
              x
               
            
          
        
         
          x b  
        log
          x     
        Z
         
        x
         
         
      
    
      displaystyle r     x y   beta  log   frac   pi      y x    pi    text SFT   y x     beta  log Z x   
  

Finally  plug it back to the maximum likelihood estimator  we obtain                        Appendix A       
  
    
      
        
            x c  
          
              x     
          
        
         
        arg
          x     
        
          max
          
              x c  
          
        
        
          
            E
          
          
             
            x
             
            
              y
              
                 
              
            
             
              x     
             
            
              y
              
                N
              
            
             
              x   c 
            D
          
        
        
           
          
            ln
              x     
            
                x   f 
              
                k
                 
                 
              
              
                N
              
            
            
              
                
                  e
                  
                      x b  
                    log
                      x     
                    
                      
                        
                            x c  
                           
                          
                            y
                            
                              k
                            
                          
                          
                             
                          
                          x
                           
                        
                        
                          
                              x c  
                            
                              SFT
                            
                          
                           
                          
                            y
                            
                              k
                            
                          
                          
                             
                          
                          x
                           
                        
                      
                    
                  
                
                
                  
                      x     
                    
                      i
                       
                      k
                    
                    
                      N
                    
                  
                  
                    e
                    
                        x b  
                      log
                        x     
                      
                        
                          
                              x c  
                             
                            
                              y
                              
                                i
                              
                            
                            
                               
                            
                            x
                             
                          
                          
                            
                                x c  
                              
                                SFT
                              
                            
                             
                            
                              y
                              
                                i
                              
                            
                            
                               
                            
                            x
                             
                          
                        
                      
                    
                  
                
              
            
          
           
        
      
    
      displaystyle  pi       arg  max    pi   mathbb  E     x y      dots  y  N   sim D  left  ln  prod   k     N   frac  e   beta  log   frac   pi  y  k  x    pi    text SFT   y  k  x       sum   i k   N e   beta  log   frac   pi  y  i  x    pi    text SFT   y  i  x       right  
  

Usually  DPO is used for modeling human preference in pairwise comparisons  so that 
  
    
      
        N
         
         
      
    
      displaystyle N   
  
  In that case  we have
  
    
      
        
            x c  
          
              x     
          
        
         
        arg
          x     
        
          max
          
              x c  
          
        
        
          
            E
          
          
             
            x
             
            
              y
              
                w
              
            
             
            
              y
              
                l
              
            
             
              x   c 
            D
          
        
        
           
          
            log
              x     
              x c  
            
               
              
                  x b  
                log
                  x     
                
                  
                    
                        x c  
                       
                      
                        y
                        
                          w
                        
                      
                      
                         
                      
                      x
                       
                    
                    
                      
                          x c  
                        
                          SFT
                        
                      
                       
                      
                        y
                        
                          w
                        
                      
                      
                         
                      
                      x
                       
                    
                  
                
                  x     
                  x b  
                log
                  x     
                
                  
                    
                        x c  
                       
                      
                        y
                        
                          l
                        
                      
                      
                         
                      
                      x
                       
                    
                    
                      
                          x c  
                        
                          SFT
                        
                      
                       
                      
                        y
                        
                          l
                        
                      
                      
                         
                      
                      x
                       
                    
                  
                
              
               
            
          
           
        
      
    
      displaystyle  pi       arg  max    pi   mathbb  E     x y  w  y  l   sim D  left  log  sigma  left  beta  log   frac   pi  y  w  x    pi    text SFT   y  w  x     beta  log   frac   pi  y  l  x    pi    text SFT   y  l  x    right  right  
  

DPO eliminates the need for a separate reward model or reinforcement learning loop  treating alignment as a supervised learning problem over preference data  This is simpler to implement and train than RLHF and has been shown to produce comparable and sometimes superior results              Nevertheless  RLHF has also been shown to beat DPO on some datasets  for example  on benchmarks that attempt to measure truthfulness  Therefore  the choice of method may vary depending on the features of the human preference data and the nature of the task             

Identity preference optimization edit 
Identity preference optimization  IPO              is a modification to the original DPO objective that introduces a regularization term to reduce the chance of overfitting  It remains robust to overtraining by assuming noise in the preference data 
Foremost  IPO first applies a non linear mapping over the probability distribution of preferences 
  
    
      
          x a  
         
        q
         
         
        log
          x     
         
        q
        
           
        
         
         
          x     
        q
         
         
      
    
      displaystyle  Psi  q   log q    q   
  
 instead of the Bradley Terry assumption to soften the probability of preferences and smooth the labels  Here  
  
    
      
          x a  
         
        q
         
      
    
      displaystyle  Psi  q  
  
 denotes the 
  
    
      
          x a  
      
    
      displaystyle  Psi  
  
 preference objective separate from the policy objective  This helps avoid the overfitting issue of the assumption that pairwise preferences can be substituted for point wise rewards  which weakens the KL regularization by heavily skewing the preference distribution 
As with DPO  IPO is also formulated as an offline learning objective learned over a human preference dataset 
  
    
      
        D
      
    
      displaystyle D 
  
  In particular  the IPO introduces a new objective by applying a mapping 
  
    
      
          x a  
      
    
      displaystyle  Psi  
  
 over the preference probability distribution  Practically  
  
    
      
          x a  
      
    
      displaystyle  Psi  
  
 is taken as the identity mapping  which results in IPO  Hence  IPO also directly optimizes for the final policy from the preference dataset and bypasses the reward modeling stage by the following objective 

  
    
      
        
          max
          
            
                x c  
              
                  x b  
              
            
          
        
        
          E
        
         
          x a  
         
        
          p
          
              x     
          
        
         
        
          y
          
            w
          
        
          x   b 
        
          y
          
            l
          
        
        
           
        
        x
         
         
         
          x     
          x b  
        
          D
          
            K
            L
          
        
         
        
            x c  
          
              x b  
          
        
        
           
        
        
           
        
        
            x c  
          
            ref
          
        
         
      
    
      displaystyle  max    pi    theta    mathbb  E    Psi  p     y  w  succ y  l  x     beta D  KL   pi    theta     pi    text ref    
  

where 
  
    
      
        
          p
          
              x     
          
        
         
        
          y
          
            w
          
        
          x   b 
        
          y
          
            l
          
        
        
           
        
        x
         
      
    
      displaystyle p     y  w  succ y  l  x  
  
 is preference distribution of the chosen responses 
  
    
      
        
          y
          
            w
          
        
      
    
      displaystyle y  w  
  
 over the rejected responses 
  
    
      
        
          y
          
            l
          
        
      
    
      displaystyle y  l  
  
  However  since 
  
    
      
        
          p
          
              x     
          
        
      
    
      displaystyle p     
  
 is not observed directly  we sample from a Bernoulli distribution from the offline preference dataset as 
  
    
      
        
          p
          
              x     
          
        
         
        y
          x   b 
        
          y
            x     
        
        
           
        
        x
         
         
        
          
            E
          
          
            h
          
        
         
        I
         
        h
        
            xa  prefers  xa  
        
        y
        
            xa  to  xa  
        
        
          y
            x     
        
        
            xa  given  xa  
        
        x
         
         
      
    
      displaystyle p     y succ y  x   mathbb  E    h  I  h  text  prefers   y  text  to   y   text  given   x    
  

To solve this objective  IPO minimizes the quadratic loss function 

  
    
      
        
          
            
              
                
                  Minimize  xa  
                
              
              
                
                  
                    E
                  
                  
                     
                    x
                     
                    
                      y
                      
                        w
                      
                    
                     
                    
                      y
                      
                        l
                      
                    
                     
                      x   c 
                    D
                  
                
                 
                 
                
                  h
                  
                      x c  
                  
                
                 
                x
                 
                
                  y
                  
                    w
                  
                
                 
                
                  y
                  
                    l
                  
                
                 
                  x     
                I
                 
                
                  y
                  
                    w
                  
                
                 
                
                  y
                  
                    l
                  
                
                 
                 
                
                   
                  
                     
                  
                
              
            
            
              
              
                
                 
                
                  
                    E
                  
                  
                    x
                     
                    
                      y
                      
                        w
                      
                    
                     
                    
                      y
                      
                        l
                      
                    
                     
                      x   c 
                    D
                  
                
                 
                I
                
                  h
                  
                      x c  
                  
                
                 
                x
                 
                
                  y
                  
                    w
                  
                
                 
                
                  y
                  
                    l
                  
                
                 
                  x     
                 
                 
                  x     
                I
                 
                
                  h
                  
                      x c  
                  
                
                 
                x
                 
                
                  y
                  
                    l
                  
                
                 
                
                  y
                  
                    w
                  
                
                 
                  x     
                
                  
                     
                     
                  
                
                
                    x b  
                  
                      x     
                     
                  
                
                
                   
                  
                     
                  
                
              
            
            
              
              
                
                 
                
                  
                    E
                  
                  
                    x
                     
                    
                      y
                      
                        w
                      
                    
                     
                    
                      y
                      
                        l
                      
                    
                      x   c 
                    D
                  
                
                 
                
                  h
                  
                      x c  
                  
                
                 
                x
                 
                
                  y
                  
                    w
                  
                
                 
                
                  y
                  
                    l
                  
                
                 
                  x     
                
                  
                     
                     
                  
                
                
                    x b  
                  
                      x     
                     
                  
                
                
                   
                  
                     
                  
                
              
            
          
        
      
    
      displaystyle   begin aligned   text Minimize    amp  mathbb  E     x y  w  y  l   sim D   h   pi   x y  w  y  l   I y  w  y  l           amp   mathbb  E    x y  w  y  l   sim D  Ih   pi   x y  w  y  l      I h   pi   x y  l  y  w     frac         beta              amp   mathbb  E    x y  w  y  l  sim D  h   pi   x y  w  y  l     frac         beta            end aligned   
  

where 
  
    
      
        
          h
          
              x c  
          
        
         
        x
         
        
          y
          
            w
          
        
         
        
          y
          
            l
          
        
         
         
        log
          x     
        
           
          
            
              
                
                    x c  
                  
                      x b  
                  
                
                 
                
                  y
                  
                    w
                  
                
                
                   
                
                x
                 
              
              
                
                    x c  
                  
                    ref
                  
                
                 
                
                  y
                  
                    w
                  
                
                
                   
                
                x
                 
                 
              
            
          
           
        
          x     
        log
          x     
        
           
          
            
              
                
                    x c  
                  
                      x b  
                  
                
                 
                
                  y
                  
                    l
                  
                
                
                   
                
                x
                 
              
              
                
                    x c  
                  
                    ref
                  
                
                 
                
                  y
                  
                    l
                  
                
                
                   
                
                x
                 
              
            
          
           
        
      
    
      displaystyle h   pi   x y  w  y  l    log  left   frac   pi    theta   y  w  x    pi    text ref   y  w  x     right   log  left   frac   pi    theta   y  l  x    pi    text ref   y  l  x    right  
  
 and 
  
    
      
        I
         
        
          y
          
            w
          
        
         
        
          y
          
            l
          
        
         
      
    
      displaystyle I y  w  y  l   
  
 is a function drawn from the Bernoulli distribution from the preference dataset  Here        
  
    
      
        I
         
        y
         
        
          y
            x     
        
         
      
    
      displaystyle I y y   
  
 is   if 
  
    
      
        y
      
    
      displaystyle y 
  
 is preferred to 
  
    
      
        
          y
            x     
        
      
    
      displaystyle y  
  
 which happens with probability 
  
    
      
        
          p
          
              x     
          
        
         
        y
          x   b 
        
          y
            x     
        
         
      
    
      displaystyle p     y succ y   
  
  and   otherwise  As such  the simplification of the expression directly follows from exploiting the symmetry of 
  
    
      
        y
      
    
      displaystyle y 
  
 and 
  
    
      
        
          y
            x     
        
      
    
      displaystyle y  
  
 from the Bernoulli such that for each datapoint 
  
    
      
         
        
          y
          
            w
          
        
         
        
          y
          
            l
          
        
        
           
          
            i
          
        
          x   c 
        D
      
    
      displaystyle  y  w  y  l    i  sim D 
  
  In particular this symmetry can be represented as 
  
    
      
         
        y
         
        
          y
            x     
        
         
        I
         
        y
         
        
          y
            x     
        
         
         
         
         
        
          y
          
            w
             
            i
          
        
         
        
          y
          
            l
             
            i
          
        
         
         
         
      
    
      displaystyle  y y  I y y     y  w i  y  l i     
  
 and  
  
    
      
         
        y
         
        
          y
            x     
        
         
        I
         
        y
         
        
          y
            x     
        
         
         
         
         
        
          y
          
            l
             
            i
          
        
         
        
          y
          
            w
             
            i
          
        
         
         
         
      
    
      displaystyle  y y  I y y     y  l i  y  w i     
  
 with 
  
    
      
        
          
            E
          
          
            y
          
        
         
        
          p
          
            y
          
        
         
         
        
          
             
             
          
        
      
    
      displaystyle  mathbb  E    y  p  y     frac         
  
and 
  
    
      
        
          E
        
         
        I
         
        y
         
        
          y
            x     
        
         
         
         
        
          p
          
            y
          
        
      
    
      displaystyle  mathbb  E   I y y    p  y  
  
 
In summary  IPO can control the gap between the log likelihood ratios of the policy model and the reference by always regularizing the solution towards the reference model  It allows learning directly from preferences without a reward modelling stage and without relying on the Bradley Terry modelisation assumption that assumes that pairwise preferences can be substituted with pointwise rewards              Thus  it avoids overfitting to the preference dataset especially when preferences are near deterministic and the KL term fails 

Kahneman Tversky optimization edit 
Kahneman Tversky optimization  KTO              is another direct alignment algorithm drawing from prospect theory to model uncertainty in human decisions that may not maximize the expected value 
In general  KTO seeks to optimize a class of new loss functions proposed as  human aware losses   HALO  formulated under prospect theory to model  human values  of a query  response pair 
  
    
      
         
        x
         
        y
         
      
    
      textstyle  x y  
  
 as 
  
    
      
        v
         
        
          r
          
              x b  
          
        
         
        x
         
        y
         
          x     
        
          E
          
            Q
          
        
         
        
          r
          
              x b  
          
        
         
        x
         
        y
        
           
            x     
        
         
         
      
    
      displaystyle v r   theta   x y  E  Q  r   theta   x y     
  
  A function is defined as a human aware loss for the value described by the general HALO objective 

  
    
      
        f
         
        
            x c  
          
              x b  
          
        
         
        
            x c  
          
            ref
          
        
         
         
        
          
            E
          
          
            x
             
            y
              x   c 
            D
          
        
         
        
          a
          
            x
             
            y
          
        
        v
        
          
             
          
        
        
          r
          
              x b  
          
        
         
        x
         
        y
         
        
          x     
        
        
          
            
              
                
                  E
                  
                    
                      y
                        x     
                    
                      x   c 
                    Q
                  
                
                 
                
                
                  r
                  
                      x b  
                  
                
                 
                x
                 
                
                  y
                    x     
                
                 
                
                 
              
                x  df 
            
          
          
            reference point
          
        
        
          
             
          
        
         
         
        
          C
          
            D
          
        
      
    
      displaystyle f  pi    theta    pi    text ref     mathbb  E    x y sim D  a  x y v  Bigl   r   theta   x y       underbrace  E  y  sim Q    r   theta   x y          text reference point    Bigr     C  D  
  

where 
  
    
      
        D
      
    
      displaystyle D 
  
 is the preference data  
  
    
      
        
          C
          
            D
          
        
      
    
      displaystyle C  D  
  
 is some constant relevant to the dataset  and 
  
    
      
        Q
      
    
      displaystyle Q 
  
 is some distribution representing the baseline or  reference   Each training example is attached a label 
  
    
      
        
          a
          
            x
             
            y
          
        
          x     
         
         
         
         
          x     
         
         
      
    
      displaystyle a  x y  in           
  
 that tells us if the example is desirable  we want to push up its reward  and    if it s undesirable  in order to push down its reward   Unlike previous definitions of the reward  KTO defines 
  
    
      
        
          r
          
              x b  
          
        
         
        x
         
        y
         
      
    
      displaystyle r   theta   x y  
  
 as the  implied reward  taken by the log likelihood ratio between the policy model and the reference model 
  
    
      
        log
          x     
        
           
          
            
              
                
                    x c  
                  
                      x b  
                  
                
                 
                y
                
                   
                
                x
                 
              
              
                
                    x c  
                  
                    ref
                  
                
                 
                y
                
                   
                
                x
                 
              
            
          
           
        
      
    
      displaystyle  log  left   frac   pi    theta   y x    pi    text ref   y x    right  
  
  Here  the value function 
  
    
      
        v
      
    
      displaystyle v 
  
 is a non linear  typically concave  function that mimics human loss aversion and risk aversion  As opposed to previous preference optimization algorithms  the motivation of KTO lies in maximizing the utility of model outputs from a human perspective rather than maximizing the likelihood of a  better  label  chosen vs  rejected responses   Hence  it constructs a more relaxed generalization to preference distributions by requiring only a binary feedback signal 
  
    
      
        
          a
          
            x
             
            y
          
        
      
    
      displaystyle a  x y  
  
 instead of explicit preference pairs  For each example 
  
    
      
         
        x
         
        y
         
      
    
      displaystyle  x y  
  
 in the dataset 
  
    
      
        D
      
    
      displaystyle D 
  
  KTO explicitly optimizes the HALO objective as 

  
    
      
        
            x c  
          
              x b  
          
          
              x     
          
        
        
         
        
        arg
          x     
        
          max
          
            
                x c  
              
                  x b  
              
            
          
        
        
        
        
          
            E
          
          
             
            x
             
            y
             
            
              x   c 
            
            D
          
        
        
          
             
          
        
        
            x b  
          
            y
          
        
        
          x     
        
        v
         
        x
         
        y
         
        
          
             
          
        
      
    
      displaystyle  pi    theta            arg  max    pi    theta        mathbb  E     x y    sim   D   Bigl    gamma   y      v x y   Bigr    
  
  where 
  
    
      
        
            x b  
          
            y
          
        
      
    
      displaystyle  gamma   y  
  
 is a class specific constant  e g   
  
    
      
        
            x b  
          
            y
          
        
         
        
            x bb 
          
            D
          
        
        
            xa  or  xa  
        
        
            x bb 
          
            U
          
        
      
    
      displaystyle  gamma   y   lambda   D   text  or    lambda   U  
  
  controlling how strongly the model should push up good outputs vs  push down bad ones  The value function 
  
    
      
        v
         
        x
         
        y
         
      
    
      displaystyle v x y  
  
 is defined piecewise depending on whether 
  
    
      
        y
      
    
      displaystyle y 
  
 is desirable  
  
    
      
        
            x bb 
          
            D
          
        
      
    
      displaystyle  lambda   D  
  
  or undesirable  
  
    
      
        
            x bb 
          
            U
          
        
      
    
      displaystyle  lambda   U  
  
  

  
    
      
        v
         
        x
         
        y
         
        
         
        
        
          
             
            
              
                
                  
                      x bb 
                    
                      D
                    
                  
                  
                    x c  
                  
                  
                    
                       
                    
                  
                  
                    x b  
                  
                  
                    
                       
                    
                  
                  
                    r
                    
                        x b  
                    
                  
                   
                  x
                   
                  y
                   
                  
                    x     
                  
                  
                    z
                    
                       
                    
                  
                  
                    
                       
                    
                  
                  
                    
                       
                    
                  
                   
                
                
                  
                  
                    if  xa  
                  
                  y
                    x   c 
                  
                    y
                    
                      
                        d
                        e
                        s
                        i
                        r
                        a
                        b
                        l
                        e
                      
                        x     
                      x
                    
                  
                   
                
              
              
                
                  
                      x bb 
                    
                      U
                    
                  
                  
                    x c  
                  
                  
                    
                       
                    
                  
                  
                    x b  
                  
                  
                    
                       
                    
                  
                  
                    z
                    
                       
                    
                  
                  
                    x     
                  
                  
                    r
                    
                        x b  
                    
                  
                   
                  x
                   
                  y
                   
                  
                    
                       
                    
                  
                  
                    
                       
                    
                  
                   
                
                
                  
                  
                    if  xa  
                  
                  y
                    x   c 
                  
                    y
                    
                      
                        u
                        n
                        d
                        e
                        s
                        i
                        r
                        a
                        b
                        l
                        e
                      
                        x     
                      x
                    
                  
                
              
            
            
          
        
      
    
      displaystyle v x y        begin cases  lambda   D    sigma     bigl      beta     bigl   r   theta   x y      z      bigr     bigr     amp  quad   text if   y sim y   mathrm  desirable   mid x      pt  lambda   U    sigma     bigl      beta     bigl   z         r   theta   x y   bigr     bigr     amp  quad   text if   y sim y   mathrm  undesirable   mid x  end cases   
  

and 
  
    
      
        
          z
          
             
          
        
         
        
          K
          L
        
        
        
          
             
          
        
        
        
            x c  
          
              x b  
          
        
         
        
          y
            x     
        
          x     
        x
         
        
        
          
              x     
          
        
        
        
            x c  
          
            
              r
              e
              f
            
          
        
         
        
          y
            x     
        
          x     
        x
         
        
          
             
          
        
      
    
      textstyle z      mathrm  KL      Bigl      pi    theta   y  mid x     big  Vert     pi    mathrm  ref    y  mid x   Bigr    
  
is a baseline given by the Kullback Leibler divergence  Here  
  
    
      
          x b  
      
    
      displaystyle  beta  
  
 controls how  risk averse  the value function is  larger 
  
    
      
          x b  
      
    
      displaystyle  beta  
  
   faster saturation in the logistic function 
  
    
      
          x c  
      
    
      displaystyle  sigma  
  
   Intuitively  desirable outputs push the model to increase 
  
    
      
        
          r
          
              x b  
          
        
      
    
      displaystyle r   theta   
  
 so that 
  
    
      
        
          r
          
              x b  
          
        
          x     
        
          z
          
             
          
        
      
    
      displaystyle r   theta   z     
  
 becomes more positive  Undesirable ones push it in the opposite direction  so the reward is less than the reference  Since many real world feedback pipelines yield  like dislike  data more easily than pairwise comparisons  KTO is designed to be data cheap and to reflect  loss aversion  more directly by using a straightforward notion of  good vs  bad  at the example level 

See also edit 
Human in the loop
Reward based selection
References edit 


  Russell  Stuart J   Norvig  Peter         Artificial intelligence  a modern approach  Third  Global      ed    Boston Columbus Indianapolis New York San Francisco Upper Saddle River Amsterdam Cape Town Dubai London Madrid Milan Munich Paris Montreal Toronto Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo  Pearson  pp                ISBN                        

  a b Ziegler  Daniel M   Stiennon  Nisan  Wu  Jeffrey  Brown  Tom B   Radford  Alec  Amodei  Dario  Christiano  Paul  Irving  Geoffrey          Fine Tuning Language Models from Human Preferences   arXiv             cs CL  

  a b c d Lambert  Nathan  Castricato  Louis  von Werra  Leandro  Havrilla  Alex   Illustrating Reinforcement Learning from Human Feedback  RLHF    huggingface co  Retrieved   March      

  Schulman  John  Wolski  Filip  Dhariwal  Prafulla  Radford  Alec  Klimov  Oleg          Proximal Policy Optimization Algorithms   arXiv             cs LG  

  Tuan  Yi Lin  Zhang  Jinzhi  Li  Yujia  Lee  Hung yi          Proximal Policy Optimization and its Dynamic Version for Sequence Generation   arXiv             cs CL  

  a b c d e Amodei  Dario  Christiano  Paul  Ray  Alex     June         Learning from human preferences   openai com  Retrieved   March      

  a b Zheng  Rui  Dou  Shihan  Gao  Songyang  Hua  Yuan  Shen  Wei  Wang  Binghai  Liu  Yan  Jin  Senjie  Liu  Qin  Zhou  Yuhao  Xiong  Limao  Chen  Lu  Xi  Zhiheng  Xu  Nuo  Lai  Wenbin  Zhu  Minghao  Chang  Cheng  Yin  Zhangyue  Weng  Rongxiang  Cheng  Wensen  Huang  Haoran  Sun  Tianxiang  Yan  Hang  Gui  Tao  Zhang  Qi  Qiu  Xipeng  Huang  Xuanjing          Secrets of RLHF in Large Language Models Part I  PPO   arXiv             cs CL  

  Knox  W  Bradley  Stone  Peter  Breazeal  Cynthia          Training a Robot via Human Feedback  A Case Study   Social Robotics  Lecture Notes in Computer Science  Vol             Springer International Publishing  pp                doi                               ISBN                         Retrieved    February      

  Akrour  Riad  Schoenauer  Marc  Sebag  Mich le          APRIL  Active Preference Learning Based Reinforcement Learning   Machine Learning and Knowledge Discovery in Databases  Lecture Notes in Computer Science  Vol             Springer  pp                arXiv            doi                              ISBN                         Retrieved    February      

  Wilson  Aaron  Fern  Alan  Tadepalli  Prasad          A Bayesian Approach for Policy Learning from Trajectory Preference Queries   Advances in Neural Information Processing Systems      Curran Associates  Inc  Retrieved    February      

  Schoenauer  Marc  Akrour  Riad  Sebag  Michele  Souplet  Jean Christophe     June         Programming by Feedback   Proceedings of the   st International Conference on Machine Learning  PMLR             Retrieved    February      

  Warnell  Garrett  Waytowich  Nicholas  Lawhern  Vernon  Stone  Peter     April         Deep TAMER  Interactive Agent Shaping in High Dimensional State Spaces   Proceedings of the AAAI Conference on Artificial Intelligence          arXiv             doi         aaai v  i         S CID              

  MacGlashan  James  Ho  Mark K   Loftin  Robert  Peng  Bei  Wang  Guan  Roberts  David L   Taylor  Matthew E   Littman  Michael L     August         Interactive learning from policy dependent human feedback   Proceedings of the   th International Conference on Machine Learning   Volume     JMLR org             arXiv            

  a b c d e f g h i j Nisan Stiennon  Long Ouyang  Jeffrey Wu  Daniel Ziegler  Ryan Lowe  Chelsea Voss  Alec Radford  Dario Amodei  Paul F  Christiano          Learning to summarize with human feedback   Advances in Neural Information Processing Systems     

  a b c d e f g h i j k l Ouyang  Long  Wu  Jeffrey  Jiang  Xu  Almeida  Diogo  Wainwright  Carroll  Mishkin  Pamela  Zhang  Chong  Agarwal  Sandhini  Slama  Katarina  Gray  Alex  Schulman  John  Hilton  Jacob  Kelton  Fraser  Miller  Luke  Simens  Maddie  Askell  Amanda  Welinder  Peter  Christiano  Paul  Leike  Jan  Lowe  Ryan     October        Training language models to follow instructions with human feedback  Thirty Sixth Conference on Neural Information Processing Systems  NeurIPS       arXiv            

  Bai  Yuntao  Jones  Andy  Ndousse  Kamal  Askell  Amanda  Chen  Anna  DasSarma  Nova  Drain  Dawn  Fort  Stanislav  Ganguli  Deep  Henighan  Tom  Joseph  Nicholas  Kadavath  Saurav  Kernion  Jackson  Conerly  Tom  El Showk  Sheer  Elhage  Nelson  Hatfield Dodds  Zac  Hernandez  Danny  Hume  Tristan  Johnston  Scott  Kravec  Shauna  Lovitt  Liane  Nanda  Neel  Olsson  Catherine  Amodei  Dario  Brown  Tom  Clark  Jack  McCandlish  Sam  Olah  Chris  Mann  Ben  Kaplan  Jared          Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback   arXiv             cs CL  

  a b Edwards  Benj    December         OpenAI invites everyone to test ChatGPT  a new AI powered chatbot with amusing results   Ars Technica  Retrieved   March      

  Abhishek  Gupta    February         Getting stakeholder engagement right in responsible AI   VentureBeat  Retrieved   March      

  Fernandes  Patrick  Madaan  Aman  Liu  Emmy  Farinhas  Ant nio  Pedro Henrique Martins  Bertsch  Amanda  de Souza  Jos  G  C   Zhou  Shuyan  Wu  Tongshuang  Neubig  Graham  Martins  Andr  F  T           Bridging the Gap  A Survey on Integrating  Human  Feedback for Natural Language Generation   arXiv             cs CL  

  a b Xie  Tengyang  Jiang  Nan  Wang  Huan  Xiong  Caiming  Bai  Yu          Policy Finetuning  Bridging Sample Efficient Offline and Online Reinforcement Learning   Advances in Neural Information Processing Systems      Curran Associates  Inc                arXiv             Retrieved    March      

  a b Pacchiano  Aldo  Saha  Aadirupa  Lee  Jonathan                Dueling RL  Reinforcement Learning with Trajectory Preferences   Proceedings of the   th International Conference on Artificial Intelligence and Statistics  PMLR             arXiv            

  a b Zhu  Banghua  Jordan  Michael  Jiao  Jiantao                Principled Reinforcement Learning with Human Feedback from Pairwise or K wise Comparisons   Proceedings of the   th International Conference on Machine Learning  PMLR               arXiv            

  Li  Zihao  Yang  Zhuoran  Wang  Mengdi     June         Reinforcement learning with Human Feedback  Learning Dynamic Choices via Pessimism   ILHF Workshop ICML       arXiv             Retrieved    March      

  Ouyang  Long  Wu  Jeff  Jiang  Xu  Almeida  Diogo  Wainwright  Carroll L   Mishkin  Pamela  Zhang  Chong  Agarwal  Sandhini  Slama  Katarina  Ray  Alex  Schulman  John  Hilton  Jacob  Kelton  Fraser  Miller  Luke  Simens  Maddie  Askell  Amanda  Welinder  Peter  Christiano  Paul  Leike  Jan  Lowe  Ryan          Training language models to follow instructions with human feedback   arXiv             cs CL  

  Wiggers  Kyle     February         Can AI really be protected from text based attacks    TechCrunch  Retrieved   March      

  Heikkil   Melissa     February         How OpenAI is trying to make ChatGPT safer and less biased   MIT Technology Review  Retrieved   March      

  Douglas Heaven  Will     November         ChatGPT is OpenAI s latest fix for GPT    It s slick but still spews nonsense   MIT Technology Review  Retrieved   March      

  Glaese  Amelia  McAleese  Nat  Tr bacz  Maja  Aslanides  John  Firoiu  Vlad  Ewalds  Timo  Rauh  Maribeth  Weidinger  Laura  Chadwick  Martin  Thacker  Phoebe  Campbell Gillingham  Lucy  Uesato  Jonathan  Huang  Po Sen  Comanescu  Ramona  Yang  Fan  See  Abigail  Dathathri  Sumanth  Greig  Rory  Chen  Charlie  Fritz  Doug  Elias  Jaume Sanchez  Green  Richard  Mokr   So a  Fernando  Nicholas  Wu  Boxi  Foley  Rachel  Young  Susannah  Gabriel  Iason  Isaac  William  Mellor  John  Hassabis  Demis  Kavukcuoglu  Koray  Hendricks  Lisa Anne  Irving  Geoffrey          Improving alignment of dialogue agents via targeted human judgements   arXiv             cs LG  

  Goldman  Sharon     September         Why DeepMind isn t deploying its new AI chatbot   and what it means for responsible AI   VentureBeat  Retrieved   March      

  The Sparrow team     September         Building safer dialogue agents   www deepmind com  Retrieved   March      

  Pinchai  Sundar  Hassabis  Demis    December         Introducing Gemini  our largest and most capable AI model   Google  Retrieved    February      

  Henshall  Will     July         What to Know About Claude    Anthropic s Rival to ChatGPT   TIME  Retrieved   March      

  Fan  Ying  Watkins  Olivia  Du  Yuqing  Liu  Hao  Ryu  Moonkyung  Boutilier  Craig  Abbeel  Pieter  Ghavamzadeh  Mohammad  Lee  Kangwook  Lee  Kimin    November         DPOK  Reinforcement Learning for Fine tuning Text to Image Diffusion Models   NeurIPS       arXiv             Retrieved   March      

  Xu  Jiazheng  Liu  Xiao  Wu  Yuchen  Tong  Yuxuan  Li  Qinkai  Ding  Ming  Tang  Jie  Dong  Yuxiao     December         ImageReward  Learning and Evaluating Human Preferences for Text to Image Generation   Advances in Neural Information Processing Systems                   arXiv             Retrieved   March      

  Lee  Kimin  Liu  Hao  Ryu  Moonkyung  Watkins  Olivia  Du  Yuqing  Boutilier  Craig  Abbeel  Pieter  Ghavamzadeh  Mohammad  Gu  Shixiang Shane          Aligning Text to Image Models using Human Feedback   arXiv             cs LG  

  Leike  Jan  Martic  Miljan  Legg  Shane     June         Learning through human feedback   www deepmind com  Retrieved   March      

  Christiano  Paul F  Leike  Jan  Brown  Tom  Martic  Miljan  Legg  Shane  Amodei  Dario          Deep Reinforcement Learning from Human Preferences   Advances in Neural Information Processing Systems      Curran Associates  Inc  arXiv             Retrieved   March      

  a b c Casper  Stephen  Davies  Xander  Shi  Claudia  Gilbert  Thomas Krendl  Scheurer  J r my  Rando  Javier  Freedman  Rachel  Korbak  Tomasz  Lindner  David  Freire  Pedro  Wang  Tony Tong  Marks  Samuel  Segerie  Charbel Raphael  Carroll  Micah  Peng  Andi  Christoffersen  Phillip  Damani  Mehul  Slocum  Stewart  Anwar  Usman  Siththaranjan  Anand  Nadeau  Max  Michaud  Eric J   Pfau  Jacob  Krasheninnikov  Dmitrii  Chen  Xin  Langosco  Lauro  Hase  Peter  Biyik  Erdem  Dragan  Anca  Krueger  David  Sadigh  Dorsa  Hadfield Menell  Dylan     September         Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback   Transactions on Machine Learning Research  arXiv            

  Christiano  Paul     January         Thoughts on the impact of RLHF research   Retrieved   March      

  Belenguer  Lorenzo          AI bias  exploring discriminatory algorithmic decision making models and the application of possible machine centric solutions adapted from the pharmaceutical industry   AI and Ethics         AI Ethics           doi         s                   PMC               PMID               

  Zhang  Chiyuan  Bengio  Samy  Hardt  Moritz  Recht  Benjamin  Vinyals  Oriol    November         Understanding deep learning requires rethinking generalization   International Conference on Learning Representations 

  Clark  Jack  Amodei  Dario     December         Faulty reward functions in the wild   OpenAI 

  Lee  Harrison  Phatale  Samrat  Mansoor  Hassan  Lu  Kellie Ren  Mesnard  Thomas  Ferret  Johan  Bishop  Colton  Hall  Ethan  Carbune  Victor  Rastogi  Abhinav                RLAIF  Scaling Reinforcement Learning from Human Feedback with AI Feedback   ICLR 

  Edwards  Benj                AI gains  values  with Anthropic s new Constitutional AI chatbot approach   Ars Technica  Retrieved            

  Rafailov  Rafael  Chittepu  Yaswanth  Park  Ryan  Sikchi  Harshit  Hejna  Joey  Knox  Bradley  Finn  Chelsea  Niekum  Scott          Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms   arXiv             cs LG  

  Shi  Zhengyan  Land  Sander  Locatelli  Acyr  Geist  Matthieu  Bartolo  Max          Understanding Likelihood Over optimisation in Direct Alignment Algorithms   arXiv             cs CL  

  a b Rafailov  Rafael  Sharma  Archit  Mitchell  Eric  Ermon  Stefano  Manning  Christopher D   Finn  Chelsea          Direct Preference Optimization  Your Language Model is Secretly a Reward Model   arXiv             cs LG  

  Wang  Zhilin  Dong  Yi  Zeng  Jiaqi  Adams  Virginia  Sreedhar  Makesh Narsimhan  Egert  Daniel  Delalleau  Olivier  Scowcroft  Jane Polak  Kant  Neel  Swope  Aidan  Kuchaiev  Oleksii          HelpSteer  Multi attribute Helpfulness Dataset for SteerLM   arXiv             cs CL  

  a b Mohammad Gheshlaghi Azar  Rowland  Mark  Piot  Bilal  Guo  Daniel  Calandriello  Daniele  Valko  Michal  Munos  R mi          A General Theoretical Paradigm to Understand Learning from Human Preferences   arXiv             cs AI  

  Ethayarajh  Kawin  Xu  Winnie  Muennighoff  Niklas  Jurafsky  Dan  Kiela  Douwe          KTO  Model Alignment as Prospect Theoretic Optimization   arXiv             cs LG  


Further reading edit 
 Learning RLHF  PPO  with codes  Huggingface TRL    Yiyang Feng   yiyangfeng me  Retrieved            
 The N Implementation Details of RLHF with PPO   huggingface co              Retrieved            
 Proximal Policy Optimization   Spinning Up documentation   spinningup openai com  Retrieved            
Huang  Shengyi  Noukhovitch  Michael  Hosseini  Arian  Rasul  Kashif  Wang  Weixun  Tunstall  Lewis               The N  Implementation Details of RLHF with PPO  A Case Study on TL DR Summarization  arXiv           
vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects






Retrieved from  https   en wikipedia org w index php title Reinforcement learning from human feedback amp oldid