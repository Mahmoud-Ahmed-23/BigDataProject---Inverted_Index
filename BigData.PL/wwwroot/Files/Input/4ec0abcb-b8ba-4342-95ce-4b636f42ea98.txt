Algorithm for modelling sequential data
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
A standard Transformer architecture  showing on the left an encoder  and on the right a decoder  Note  it uses the pre LN convention  which is different from the post LN convention used in the original      Transformer 
The transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi head attention mechanism  which was proposed in the      paper  Attention Is All You Need              Text is converted to numerical representations called tokens  and each token is converted into a vector via lookup from a word embedding table             At each layer  each token is then contextualized within the scope of the context window with other  unmasked  tokens via a parallel multi head attention mechanism  allowing the signal for key tokens to be amplified and less important tokens to be diminished 
Transformers have the advantage of having no recurrent units  therefore requiring less training time than earlier recurrent neural architectures  RNNs  such as long short term memory  LSTM              Later variations have been widely adopted for training large language models  LLM  on large  language  datasets            

Transformers were first developed as an improvement over previous architectures for machine translation                        but have found many applications since  They are used in large scale natural language processing  computer vision  vision transformers   reinforcement learning                        audio             multimodal learning  robotics             and even playing chess              It has also led to the development of pre trained systems  such as generative pre trained transformers  GPTs              and BERT              bidirectional encoder representations from transformers  
History edit 
See also  Timeline of machine learning
Predecessors edit 
For many years  sequence modelling and generation was done by using plain recurrent neural networks  RNNs   A well cited early example was the Elman network         In theory  the information from one token can propagate arbitrarily far down the sequence  but in practice the vanishing gradient problem leaves the model s state at the end of a long sentence without precise  extractable information about preceding tokens 
A key breakthrough was LSTM             note        a RNN which used various innovations to overcome the vanishing gradient problem  allowing efficient learning of long sequence modelling  One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons  so called multiplicative units              Neural networks using multiplicative units were later called sigma pi networks             or higher order networks              LSTM became the standard architecture for long sequence modelling until the      publication of Transformers 
However  LSTM still used sequential processing  like most other RNNs      note        Specifically  RNNs operate one token at a time from first to last  they cannot operate in parallel over all tokens in a sequence 
Modern Transformers overcome this problem  but unlike RNNs  they require computation time that is quadratic in the size of the context window  The linearly scaling fast weight controller        learns to compute a weight matrix for further processing depending on the input              One of its two networks has  fast weights  or  dynamic links                                              A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries              This was later shown to be equivalent to the unnormalized linear Transformer                         

Attention with seq seq edit 
Main article  Seq seq        History
The idea of encoder decoder sequence transduction had been developed in the early     s  see previous papers                           The papers most commonly cited as the originators that produced seq seq are two concurrently published papers from                              
A    M parameter model for machine translation uses two long short term memories  LSTM               Its architecture consists of two parts  The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector  The decoder is another LSTM that converts the vector into a sequence of tokens  Similarly  another    M parameter model used gated recurrent units  GRU  instead of LSTM              Later research showed that GRUs are neither better nor worse than LSTMs for seq seq                         
These early seq seq models had no attention mechanism  and the state vector is accessible only after the last word of the source text was processed  Although in theory such a vector retains the information about the whole original sentence  in practice the information is poorly preserved  This is because the input is processed sequentially by one recurrent network into a fixed size output vector  which is then processed by another recurrent network into an output  If the input is long  then the output vector would not be able to contain all relevant information  degrading the output  As evidence  reversing the input sentence improved seq seq translation             
The RNNsearch model introduced an attention mechanism to seq seq for machine translation to solve the bottleneck problem  of the fixed size output vector   allowing the model to process long distance dependencies more easily  The name is because it  emulates searching through a source sentence during decoding a translation             
The relative performances were compared between global  that of RNNsearch  and local  sliding window  attention model architectures for machine translation  finding that mixed attention had higher quality than global attention  while local attention reduced translation time             
In       Google Translate was revamped to Google Neural Machine Translation  which replaced the previous model based on statistical machine translation  The new model was a seq seq model where the encoder and the decoder were both   layers of bidirectional LSTM              It took nine months to develop  and it outperformed the statistical approach  which took ten years to develop             

Parallelizing attention edit 
Main article  Attention  machine learning         History
Seq seq models with attention  including self attention  still suffered from the same issue with recurrent networks  which is that they are hard to parallelize  which prevented them from being accelerated on GPUs  In       decomposable attention applied a self attention mechanism to feedforward networks  which are easy to parallelize  and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs              One of its authors  Jakob Uszkoreit  suspected that attention without recurrence is sufficient for language translation  thus the title  attention is all you need               That hypothesis was against conventional wisdom at the time  and even his father Hans Uszkoreit  a well known computational linguist  was skeptical              In the same year  self attention  called intra attention or intra sentence attention  was proposed for LSTMs             
In       the original     M sized  encoder decoder transformer model was proposed in the  Attention is all you need  paper  At the time  the focus of the research was on improving seq seq for machine translation  by removing its recurrence to process all tokens in parallel  but preserving its dot product attention mechanism to keep its text processing performance             This led to the introduction of a multi head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence  Its parallelizability was an important factor to its widespread use in large neural networks             


AI boom era edit 
Already in spring       even before the  Attention is all you need  preprint was published  one of the co authors applied the  decoder only  variation of the architecture to generate fictitious Wikipedia articles              Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom 
In language modelling  ELMo        was a bi directional LSTM that produces contextualized word embeddings  improving upon the line of research from bag of words and word vec  It was followed by BERT         an encoder only Transformer model              In      October  Google started using BERT to process search queries              In       Google Translate replaced the previous RNN encoder RNN decoder model by a Transformer encoder RNN decoder model             
Starting in       the OpenAI GPT series of decoder only Transformers became state of the art in natural language generation  In       a chatbot based on GPT    ChatGPT  became unexpectedly             popular  triggering a boom around large language models                         
Since       Transformers have been applied in modalities beyond text  including the vision transformer              speech recognition              robotics             and multimodal              The vision transformer  in turn  stimulated new developments in convolutional neural networks              Image and video generators like DALL E         Stable Diffusion                       and Sora         use Transformers to analyse input data  like text prompts  by breaking it down into  tokens  and then calculating the relevance between each token using self attention  which helps the model understand the context and relationships within the data 

Training edit 
Methods for stabilizing training edit 
The plain transformer architecture had difficulty converging  In the original paper            the authors recommended using learning rate warmup  That is  the learning rate should linearly scale up from   to maximal value for the first part of the training  usually recommended to be    of the total number of training steps   before decaying again 
A      paper found that using layer normalization before  instead of after  multiheaded attention and feedforward layers stabilizes training  not requiring learning rate warmup             

Pretrain finetune edit 
Transformers typically are first pretrained by self supervised learning on a large generic dataset  followed by supervised fine tuning on a small task specific dataset  The pretrain dataset is typically an unlabeled large corpus  such as The Pile  Tasks for pretraining and fine tuning commonly include 

language modeling            
next sentence prediction            
question answering           
reading comprehension
sentiment analysis           
paraphrasing           
The T  transformer report             documents a large number of natural language pretraining tasks  Some examples are 

restoring or repairing incomplete or corrupted text  For example  the input   Thank you                me to your party                week   might generate the output   Thank you for inviting me to your party last week  
translation between natural languages  machine translation 
judging the pragmatic acceptability of natural language  For example  the following sentence might be judged  not acceptable               because even though it is syntactically well formed  it is improbable in ordinary human usage  The course is jumping well 
Note that while each of these tasks is trivial or obvious for human native speakers of the language  or languages   they have typically proved challenging for previous generations of machine learning architecture 

Tasks edit 
See also  Large language model        Evaluation
In general  there are   classes of language modelling tasks   masked                autoregressive               and  prefixLM               These classes are independent of a specific modeling architecture such as Transformer  but they are often discussed in the context of Transformer 
In a masked task              one or more of the tokens is masked out  and the model would produce a probability distribution predicting what the masked out tokens are based on the context  The loss function for the task is typically sum of log perplexities for the masked out tokens  
  
    
      
        
          Loss
        
         
          x     
        
            x     
          
            t
              x     
            
              masked tokens
            
          
        
        ln
          x     
         
        
          probability of  xa  
        
        t
        
            xa  conditional on its context
        
         
      
    
      displaystyle   text Loss     sum   t in   text masked tokens    ln   text probability of   t  text  conditional on its context    
  
and the model is trained to minimize this loss function  The BERT series of models are trained for masked token prediction and another task 
In an autoregressive task              the entire sequence is masked at first  and the model produces a probability distribution for the first token  Then the first token is revealed and the model predicts the second token  and so on  The loss function for the task is still typically the same  The GPT series of models are trained by autoregressive tasks 
In a prefixLM task              the sequence is divided into two parts  The first part is presented as context  and the model predicts the first token of the second part  Then that would be revealed  and the model predicts the second token  and so on  The loss function for the task is still typically the same  The T  series of models are trained by prefixLM tasks 
Note that  masked  as in  masked language modelling  is not  masked  as in  masked attention   and  prefixLM   prefix language modeling  is not  prefixLM   prefix language model  

Architecture edit 
All transformers have the same primary components 

Tokenizers  which convert text into tokens 
Embedding layer  which converts tokens and positions of the tokens into vector representations 
Transformer layers  which carry out repeated transformations on the vector representations  extracting more and more linguistic information  These consist of alternating attention and feedforward layers  There are two major types of transformer layers  encoder layers and decoder layers  with further variants 
Un embedding layer  which converts the final vector representations back to a probability distribution over the tokens 
The following description follows exactly the Transformer as described in the original paper  There are variants  described in the following section 
By convention  we write all vectors as row vectors  This  for example  means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right  as 
  
    
      
        x
        W
      
    
      displaystyle xW 
  
 

Tokenization edit 
Main article  Lexical analysis
As the Transformer architecture natively processes numerical data  not text  there must be a translation between text and tokens  A token is an integer that represents a character  or a short segment of characters  On the input side  the input text is parsed into a token sequence  Similarly  on the output side  the output tokens are parsed back to text  The module doing the conversion between texts and token sequences is a tokenizer 
The set of all tokens is the vocabulary of the tokenizer  and its size is the vocabulary size 
  
    
      
        
          n
          
            vocabulary
          
        
      
    
      displaystyle n   text vocabulary   
  
  When faced with tokens outside the vocabulary  typically a special token is used  written as   UNK   for  unknown  
Some commonly used tokenizers are byte pair encoding  WordPiece  and SentencePiece 

Embedding edit 
Further information  Word embedding
Each token is converted into an embedding vector via a lookup table  Equivalently stated  it multiplies a one hot representation of the token by an embedding matrix 
  
    
      
        M
      
    
      displaystyle M 
  
  For example  if the input token is 
  
    
      
         
      
    
      displaystyle   
  
  then the one hot representation is 
  
    
      
         
         
         
         
         
         
         
         
         
         
         
         
         
          x     
         
      
    
      displaystyle               dots   
  
  and its embedding vector is
  
    
      
        
          E
          m
          b
          e
          d
        
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
          x     
         
        M
      
    
      displaystyle  mathrm  Embed                    dots  M 
  
The token embedding vectors are added to their respective positional encoding vectors  see below   producing the sequence of input vectors 
The number of dimensions in an embedding vector is called hidden size or embedding size and written as 
  
    
      
        
          d
          
            emb
          
        
      
    
      displaystyle d   text emb   
  
              This size is written as 
  
    
      
        
          d
          
            model
          
        
      
    
      displaystyle d   text model   
  
 in the original Transformer paper            

Un embedding edit 
An un embedding layer is almost the reverse of an embedding layer  Whereas an embedding layer converts a token into a vector  an un embedding layer converts a vector into a probability distribution over tokens 
The un embedding layer is a linear softmax layer 
  
    
      
        
          U
          n
          E
          m
          b
          e
          d
        
         
        x
         
         
        
          s
          o
          f
          t
          m
          a
          x
        
         
        x
        W
         
        b
         
      
    
      displaystyle  mathrm  UnEmbed   x   mathrm  softmax   xW b  
  
The matrix has shape 
  
    
      
         
        
          d
          
            emb
          
        
         
        
          n
          
            vocabulary
          
        
         
      
    
      displaystyle  d   text emb   n   text vocabulary    
  
  The embedding matrix 
  
    
      
        M
      
    
      displaystyle M 
  
 and the un embedding matrix 
  
    
      
        W
      
    
      displaystyle W 
  
 are sometimes required to be transposes of each other  a practice called weight tying             

Positional encoding edit 
A diagram of a sinusoidal positional encoding with parameters 
  
    
      
        N
         
             
         
        d
         
           
      
    
      displaystyle N       d     
  

A positional encoding is a fixed size vector representation of the relative positions of tokens within a sequence  it provides the transformer model with information about where the words are in the input sequence  This shall induce a bias towards the order of the input sequence  so that  for example  the input sequence  man bites dog  is processed differently from  dog bites man  
The positional encoding is defined as a function of type 
  
    
      
        f
         
        
          R
        
          x     
        
          
            R
          
          
            d
          
        
         
        d
          x     
        
          Z
        
         
        d
         gt 
         
      
    
      displaystyle f  mathbb  R   to  mathbb  R    d  d in  mathbb  Z   d gt   
  
  where 
  
    
      
        d
      
    
      displaystyle d 
  
 is a positive even integer  The full positional encoding defined in the original paper            is 
  
    
      
         
        f
         
        t
        
           
          
             
            k
          
        
         
        f
         
        t
        
           
          
             
            k
             
             
          
        
         
         
         
        sin
          x     
         
          x b  
         
         
        cos
          x     
         
          x b  
         
         
        
          x     
        k
          x     
         
         
         
         
         
          x     
         
        d
        
           
        
         
          x     
         
         
      
    
      displaystyle  f t    k  f t    k       sin  theta    cos  theta    quad  forall k in        ldots  d       
  
where 
  
    
      
          x b  
         
        
          
            t
            
              r
              
                k
              
            
          
        
         
        r
         
        
          N
          
             
            
               
            
            d
          
        
      
    
      displaystyle  theta    frac  t  r  k    r N    d  
  
 
Here  
  
    
      
        N
      
    
      displaystyle N 
  
 is a free parameter that should be significantly larger than the biggest 
  
    
      
        k
      
    
      displaystyle k 
  
 that would be input into the positional encoding function  The original paper uses 
  
    
      
        N
         
             
      
    
      displaystyle N       
  
 
The function is in a simpler form when written as a complex function of type 
  
    
      
        f
         
        
          R
        
          x     
        
          
            C
          
          
            d
            
               
            
             
          
        
      
    
      displaystyle f  mathbb  R   to  mathbb  C    d    
  

  
    
      
        f
         
        t
         
         
        
          
             
            
              e
              
                i
                t
                
                   
                
                
                  r
                  
                    k
                  
                
              
            
             
          
          
            k
             
             
             
             
             
              x     
             
            
              
                d
                 
              
            
              x     
             
          
        
      
    
      displaystyle f t   left e  it r  k   right   k      ldots    frac  d         
  
where 
  
    
      
        r
         
        
          N
          
             
            
               
            
            d
          
        
      
    
      displaystyle r N    d  
  
 
The main reason for using this positional encoding function is that using it  shifts are linear transformations 
  
    
      
        f
         
        t
         
          x    
        t
         
         
        
          d
          i
          a
          g
        
         
        f
         
          x    
        t
         
         
        f
         
        t
         
      
    
      displaystyle f t  Delta t   mathrm  diag   f  Delta t  f t  
  
where 
  
    
      
          x    
        t
          x     
        
          R
        
      
    
      displaystyle  Delta t in  mathbb  R   
  
 is the distance one wishes to shift  This allows the transformer to take any encoded position  and find the encoding of the position n steps ahead or n steps behind  by a matrix multiplication 
By taking a linear sum  any convolution can also be implemented as linear transformations 
  
    
      
        
            x     
          
            j
          
        
        
          c
          
            j
          
        
        f
         
        t
         
          x    
        
          t
          
            j
          
        
         
         
        
           
          
            
                x     
              
                j
              
            
            
              c
              
                j
              
            
            
            
              d
              i
              a
              g
            
             
            f
             
              x    
            
              t
              
                j
              
            
             
             
          
           
        
        f
         
        t
         
      
    
      displaystyle  sum   j c  j f t  Delta t  j    left  sum   j c  j    mathrm  diag   f  Delta t  j    right f t  
  
for any constants 
  
    
      
        
          c
          
            j
          
        
      
    
      displaystyle c  j  
  
  This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors  This sum of encoded positions  when fed into the attention mechanism  would create attention weights on its neighbors  much like what happens in a convolutional neural network language model  In the author s words   we hypothesized it would allow the model to easily learn to attend by relative position  
In typical implementations  all operations are done over the real numbers  not the complex numbers  but since complex multiplication can be implemented as real   by   matrix multiplication  this is a mere notational difference 

Encoder decoder  overview  edit 
One encoder decoder block
A Transformer is composed of stacked encoder layers and decoder layers 
Like earlier seq seq models  the original transformer model used an encoder decoder architecture  The encoder consists of encoding layers that process all the input tokens together one layer after another  while the decoder consists of decoding layers that iteratively process the encoder s output and the decoder s output tokens so far 
The purpose of each encoder layer is to create contextualized representations of the tokens  where each representation corresponds to a token that  mixes  information from other input tokens via self attention mechanism  Each decoder layer contains two attention sublayers      cross attention for incorporating the output of encoder  contextualized input token representations   and     self attention for  mixing  information among the input tokens to the decoder  i e  the tokens generated so far during inference time                          
Both the encoder and decoder layers have a feed forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps              These feed forward layers contain most of the parameters in a Transformer model 

Feedforward network edit 
The feedforward network module  It is a two layered network that maps 
  
    
      
        
          d
          
            emb
          
        
      
    
      displaystyle d   text emb   
  
 dimensional vectors into 
  
    
      
        
          d
          
            emb
          
        
      
    
      displaystyle d   text emb   
  
 dimensional vectors 
The feedforward network  FFN  modules in a Transformer are   layered multilayer perceptrons 
  
    
      
        
          F
          F
          N
        
         
        x
         
         
          x d  
         
        x
        
          W
          
             
             
             
          
        
         
        
          b
          
             
             
             
          
        
         
        
          W
          
             
             
             
          
        
         
        
          b
          
             
             
             
          
        
      
    
      displaystyle  mathrm  FFN   x   phi  xW       b       W       b       
  
where 
  
    
      
        
          W
          
             
             
             
          
        
      
    
      displaystyle W       
  
 and 
  
    
      
        
          W
          
             
             
             
          
        
      
    
      displaystyle W       
  
 are weight matrices and 
  
    
      
        
          b
          
             
             
             
          
        
      
    
      displaystyle b       
  
 and  
  
    
      
        
          b
          
             
             
             
          
        
      
    
      displaystyle b       
  
 are bias vectors  and 
  
    
      
          x d  
      
    
      displaystyle  phi  
  
 is its activation function  The original Transformer used ReLU activation 
The number of neurons in the middle layer is called intermediate size  GPT               filter size  BERT               or feedforward size  BERT               It is typically larger than the embedding size  For example  in both GPT   series and BERT series  the intermediate size of a model is   times its embedding size  
  
    
      
        
          d
          
            ffn
          
        
         
         
        
          d
          
            emb
          
        
      
    
      displaystyle d   text ffn    d   text emb   
  
 

Scaled dot product attention edit 
Main article  Dot product attention
Attention head edit 
Scaled dot product attention  block diagram
Exact dimension counts within an attention head module
The attention mechanism used in the Transformer architecture are scaled dot product attention units  For each unit  the transformer model learns three weight matrices  the query weights 
  
    
      
        
          W
          
            Q
          
        
      
    
      displaystyle W  Q  
  
  the key weights 
  
    
      
        
          W
          
            K
          
        
      
    
      displaystyle W  K  
  
  and the value weights 
  
    
      
        
          W
          
            V
          
        
      
    
      displaystyle W  V  
  
 
The module takes three sequences  a query sequence  a key sequence  and a value sequence  The query sequence is a sequence of length 
  
    
      
        
            x     
          
            seq  query
          
        
      
    
      displaystyle  ell    text seq  query   
  
  and each entry is a vector of dimension 
  
    
      
        
          d
          
            emb  query
          
        
      
    
      displaystyle d   text emb  query   
  
  Similarly for the key and value sequences 
For each vector 
  
    
      
        
          x
          
            i
             
            
              query
            
          
        
      
    
      displaystyle x  i   text query    
  
 in the query sequence  it is multiplied by a matrix 
  
    
      
        
          W
          
            Q
          
        
      
    
      displaystyle W  Q  
  
 to produce a query vector 
  
    
      
        
          q
          
            i
          
        
         
        
          x
          
            i
             
            
              query
            
          
        
        
          W
          
            Q
          
        
      
    
      displaystyle q  i  x  i   text query   W  Q  
  
  The matrix of all query vectors is the query matrix 
  
    
      
        Q
         
        
          X
          
            query
          
        
        
          W
          
            Q
          
        
      
    
      displaystyle Q X   text query  W  Q  
  
Similarly  we construct the key matrix 
  
    
      
        K
         
        
          X
          
            key
          
        
        
          W
          
            K
          
        
      
    
      displaystyle K X   text key  W  K  
  
 and the value matrix 
  
    
      
        V
         
        
          X
          
            value
          
        
        
          W
          
            V
          
        
      
    
      displaystyle V X   text value  W  V  
  
 
It is usually the case that all 
  
    
      
        
          W
          
            Q
          
        
         
        
          W
          
            K
          
        
         
        
          W
          
            V
          
        
      
    
      displaystyle W  Q  W  K  W  V  
  
 are square matrices  meaning 
  
    
      
        
          d
          
            emb  query
          
        
         
        
          d
          
            query
          
        
      
    
      displaystyle d   text emb  query   d   text query   
  
  etc 
Attention weights are calculated using the query and key vectors  the attention weight 
  
    
      
        
          a
          
            i
            j
          
        
      
    
      displaystyle a  ij  
  
 from token 
  
    
      
        i
      
    
      displaystyle i 
  
 to token 
  
    
      
        j
      
    
      displaystyle j 
  
 is the dot product between 
  
    
      
        
          q
          
            i
          
        
      
    
      displaystyle q  i  
  
 and 
  
    
      
        
          k
          
            j
          
        
      
    
      displaystyle k  j  
  
  The attention weights are divided by the square root of the dimension of the key vectors  
  
    
      
        
          
            
              d
              
                k
              
            
          
        
      
    
      displaystyle   sqrt  d  k    
  
  which stabilizes gradients during training  and passed through a softmax which normalizes the weights  The fact that 
  
    
      
        
          W
          
            Q
          
        
      
    
      displaystyle W  Q  
  
 and 
  
    
      
        
          W
          
            K
          
        
      
    
      displaystyle W  K  
  
 are different matrices allows attention to be non symmetric  if token 
  
    
      
        i
      
    
      displaystyle i 
  
 attends to token 
  
    
      
        j
      
    
      displaystyle j 
  
  i e  
  
    
      
        
          q
          
            i
          
        
          x  c  
        
          k
          
            j
          
        
      
    
      displaystyle q  i  cdot k  j  
  
 is large   this does not necessarily mean that token 
  
    
      
        j
      
    
      displaystyle j 
  
 will attend to token 
  
    
      
        i
      
    
      displaystyle i 
  
  i e  
  
    
      
        
          q
          
            j
          
        
          x  c  
        
          k
          
            i
          
        
      
    
      displaystyle q  j  cdot k  i  
  
 could be small   The output of the attention unit for token 
  
    
      
        i
      
    
      displaystyle i 
  
 is the weighted sum of the value vectors of all tokens  weighted by 
  
    
      
        
          a
          
            i
            j
          
        
      
    
      displaystyle a  ij  
  
  the attention from token 
  
    
      
        i
      
    
      displaystyle i 
  
 to each token 
The attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function  which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations  The matrices 
  
    
      
        Q
      
    
      displaystyle Q 
  
  
  
    
      
        K
      
    
      displaystyle K 
  
 and 
  
    
      
        V
      
    
      displaystyle V 
  
 are defined as the matrices where the 
  
    
      
        i
      
    
      displaystyle i 
  
th rows are vectors 
  
    
      
        
          q
          
            i
          
        
      
    
      displaystyle q  i  
  
  
  
    
      
        
          k
          
            i
          
        
      
    
      displaystyle k  i  
  
  and 
  
    
      
        
          v
          
            i
          
        
      
    
      displaystyle v  i  
  
 respectively  Then we can represent the attention as
  
    
      
        
          
            
              
                
                  Attention
                
                 
                Q
                 
                K
                 
                V
                 
                 
                
                  softmax
                
                
                   
                  
                    
                      
                        Q
                        
                          K
                          
                            
                              T
                            
                          
                        
                      
                      
                        
                          d
                          
                            k
                          
                        
                      
                    
                  
                   
                
                V
              
            
          
        
      
    
      displaystyle   begin aligned   text Attention   Q K V    text softmax   left   frac  QK   mathrm  T      sqrt  d  k     right V end aligned   
  

where the softmax is applied over each of the rows of the matrix 
The number of dimensions in a query vector is query size 
  
    
      
        
          d
          
            query
          
        
      
    
      displaystyle d   text query   
  
 and similarly for the key size 
  
    
      
        
          d
          
            key
          
        
      
    
      displaystyle d   text key   
  
 and value size 
  
    
      
        
          d
          
            value
          
        
      
    
      displaystyle d   text value   
  
  The output dimension of an attention head is its head dimension 
  
    
      
        
          d
          
            head
          
        
      
    
      displaystyle d   text head   
  
  The attention mechanism requires the following three equalities to hold 
  
    
      
        
            x     
          
            seq  key
          
        
         
        
            x     
          
            seq  value
          
        
         
        
        
          d
          
            query
          
        
         
        
          d
          
            key
          
        
         
        
        
          d
          
            value
          
        
         
        
          d
          
            head
          
        
      
    
      displaystyle  ell    text seq  key    ell    text seq  value     d   text query   d   text key     d   text value   d   text head   
  
but is otherwise unconstrained 
If the attention head is used in a self attention fashion  then 
  
    
      
        
          X
          
            query
          
        
         
        
          X
          
            key
          
        
         
        
          X
          
            value
          
        
      
    
      displaystyle X   text query   X   text key   X   text value   
  
  If the attention head is used in a cross attention fashion  then usually 
  
    
      
        
          X
          
            query
          
        
          x     
        
          X
          
            key
          
        
         
        
          X
          
            value
          
        
      
    
      displaystyle X   text query   neq X   text key   X   text value   
  
  It is theoretically possible for all three to be different  but that is rarely the case in practice 

Multiheaded attention edit 
Multiheaded attention  block diagram
Exact dimension counts within a multiheaded attention module
One set of 
  
    
      
        
           
          
            
              W
              
                Q
              
            
             
            
              W
              
                K
              
            
             
            
              W
              
                V
              
            
          
           
        
      
    
      displaystyle  left W  Q  W  K  W  V  right  
  
 matrices is called an attention head  and each layer in a transformer model has multiple attention heads  While each attention head attends to the tokens that are relevant to each token  multiple attention heads allow the model to do this for different definitions of  relevance   Specifically  the query and key projection matrices  
  
    
      
        
          W
          
            Q
          
        
      
    
      displaystyle W  Q  
  
 and 
  
    
      
        
          W
          
            K
          
        
      
    
      displaystyle W  K  
  
   which are involved in the attention score computation  defines the  relevance   Meanwhile  the value projection matrix 
  
    
      
        
          W
          
            V
          
        
      
    
      displaystyle W  V  
  
  in combination with the part of the output projection matrix 
  
    
      
        
          W
          
            O
          
        
      
    
      displaystyle W  O  
  
  determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits  In addition  the scope of attention  or the range of token relationships captured by each attention head  can expand as tokens pass through successive layers  This allows the model to capture more complex and long range dependencies in deeper layers  Many transformer attention heads encode relevance relations that are meaningful to humans  For example  some attention heads can attend mostly to the next word  while others mainly attend from verbs to their direct objects              The computations for each attention head can be performed in parallel  which allows for fast processing  The outputs for the attention layer are concatenated to pass into the feed forward neural network layers 
Concretely  let the multiple attention heads be indexed by 
  
    
      
        i
      
    
      displaystyle i 
  
  then we have
  
    
      
        
          MultiheadedAttention
        
         
        Q
         
        K
         
        V
         
         
        
          
            Concat
          
          
            i
              x     
             
            
              n
              
                heads
              
            
             
          
        
         
        
          Attention
        
         
        Q
        
          W
          
            i
          
          
            Q
          
        
         
        K
        
          W
          
            i
          
          
            K
          
        
         
        V
        
          W
          
            i
          
          
            V
          
        
         
         
        
          W
          
            O
          
        
      
    
      displaystyle   text MultiheadedAttention   Q K V    text Concat    i in  n   text heads       text Attention   QW  i   Q  KW  i   K  VW  i   V   W  O  
  
 where the matrix 
  
    
      
        X
      
    
      displaystyle X 
  
 is the concatenation of word embeddings  and the matrices 
  
    
      
        
          W
          
            i
          
          
            Q
          
        
         
        
          W
          
            i
          
          
            K
          
        
         
        
          W
          
            i
          
          
            V
          
        
      
    
      displaystyle W  i   Q  W  i   K  W  i   V  
  
 are  projection matrices  owned by individual attention head 
  
    
      
        i
      
    
      displaystyle i 
  
  and 
  
    
      
        
          W
          
            O
          
        
      
    
      displaystyle W  O  
  
 is a final projection matrix owned by the whole multi headed attention head 
It is theoretically possible for each attention head to have a different head dimension 
  
    
      
        
          d
          
            head
          
        
      
    
      displaystyle d   text head   
  
  but that is rarely the case in practice 
As an example  in the smallest GPT   model  there are only self attention mechanisms  It has the following dimensions 
  
    
      
        
          d
          
            emb
          
        
         
           
         
        
          n
          
            head
          
        
         
          
         
        
          d
          
            head
          
        
         
          
      
    
      displaystyle d   text emb       n   text head      d   text head      
  
Since 
  
    
      
          
          xd  
          
         
           
      
    
      displaystyle    times        
  
  its output projection matrix 
  
    
      
        
          W
          
            O
          
        
          x     
        
          
            R
          
          
             
              
              xd  
              
             
              xd  
               
          
        
      
    
      displaystyle W  O  in  mathbb  R        times     times      
  
 is a square matrix 

Masked attention edit 
The Transformer architecture is constructed to calculate output tokens iteratively  Assuming 
  
    
      
        t
         
         
      
    
      displaystyle t   
  
 refers to the calculation of the first output token 
  
    
      
        i
         
         
      
    
      displaystyle i   
  
  for step 
  
    
      
        t
         gt 
         
      
    
      displaystyle t gt   
  
  the output token 
  
    
      
        i
         
         
      
    
      displaystyle i   
  
 shall remain constant  This ensures properties of the model similar to autoregressive models             Therefore  at every time step 
  
    
      
        t
      
    
      displaystyle t 
  
  the calculation for all outputs 
  
    
      
        i
      
    
      displaystyle i 
  
 should not have access to tokens at position 
  
    
      
        j
      
    
      displaystyle j 
  
 for 
  
    
      
        j
         gt  
        i
      
    
      displaystyle j gt  i 
  
  as it naturally is the case for time step 
  
    
      
        t
         
        i
      
    
      displaystyle t i 
  
  when tokens 
  
    
      
        j
         gt 
        t
      
    
      displaystyle j gt t 
  
 are not yet calculated   This behavior may be accomplished before the softmax stage by adding a mask matrix 
  
    
      
        M
      
    
      displaystyle M 
  
 that is 
  
    
      
          x     
          x   e 
      
    
      displaystyle   infty  
  
 at entries where the attention link must be cut  and 
  
    
      
         
      
    
      displaystyle   
  
 at other places 
  
    
      
        
          
            
              
                
                  MaskedAttention
                
                 
                Q
                 
                K
                 
                V
                 
                 
                
                  softmax
                
                
                   
                  
                    M
                     
                    
                      
                        
                          Q
                          
                            K
                            
                              
                                T
                              
                            
                          
                        
                        
                          
                            d
                            
                              k
                            
                          
                        
                      
                    
                  
                   
                
                V
              
            
          
        
      
    
      displaystyle   begin aligned   text MaskedAttention   Q K V    text softmax   left M   frac  QK   mathrm  T      sqrt  d  k     right V end aligned   
  
 The following matrix is commonly used in decoder self attention modules  called  causal masking  
  
    
      
        
          M
          
            causal
          
        
         
        
          
             
            
              
                
                   
                
                
                    x     
                    x   e 
                
                
                    x     
                    x   e 
                
                
                    x     
                
                
                    x     
                    x   e 
                
              
              
                
                   
                
                
                   
                
                
                    x     
                    x   e 
                
                
                    x     
                
                
                    x     
                    x   e 
                
              
              
                
                   
                
                
                   
                
                
                   
                
                
                    x     
                
                
                    x     
                    x   e 
                
              
              
                
                    x  ee 
                
                
                    x  ee 
                
                
                    x  ee 
                
                
                    x  f  
                
                
                    x  ee 
                
              
              
                
                   
                
                
                   
                
                
                   
                
                
                    x     
                
                
                   
                
              
            
             
          
        
      
    
      displaystyle M   text causal     begin bmatrix   amp   infty  amp   infty  amp  dots  amp   infty     amp   amp   infty  amp  dots  amp   infty     amp   amp   amp  dots  amp   infty    vdots  amp  vdots  amp  vdots  amp  ddots  amp  vdots     amp   amp   amp  dots  amp   end bmatrix   
  

In words  it means that each token can pay attention to itself  and every token before it  but not any after it  A non masked attention module can be thought of as a masked attention module where the mask has all entries zero  As an example of an uncommon use of mask matrix  the XLNet considers all masks of the form 
  
    
      
        P
        
          M
          
            causal
          
        
        
          P
          
              x     
             
          
        
      
    
      displaystyle PM   text causal  P      
  
  where 
  
    
      
        P
      
    
      displaystyle P 
  
 is a random permutation matrix             

Encoder edit 
One encoder layer
An encoder consists of an embedding layer  followed by multiple encoder layers 
Each encoder layer consists of two major components  a self attention mechanism and a feed forward layer  It takes an input as a sequence of input vectors  applies the self attention mechanism  to produce an intermediate sequence of vectors  then applies the feed forward layer for each vector individually  Schematically  we have 
  
    
      
        
          
            
              
                
                  given input vectors  xa  
                
              
              
                
                  h
                  
                     
                  
                
                 
                
                  h
                  
                     
                  
                
                 
                  x     
              
            
            
              
                
                  combine them into a matrix  xa  
                
                H
              
              
                
                 
                
                  
                     
                    
                      
                        
                          
                            h
                            
                               
                            
                          
                        
                      
                      
                        
                          
                            h
                            
                               
                            
                          
                        
                      
                      
                        
                            x  ee 
                        
                      
                    
                     
                  
                
              
            
            
              
                
                  EncoderLayer
                
                 
                H
                 
              
              
                
                 
                
                  
                     
                    
                      
                        
                          
                            FFN
                          
                           
                          
                            MultiheadedAttention
                          
                           
                          H
                           
                          H
                           
                          H
                          
                             
                            
                               
                            
                          
                           
                        
                      
                      
                        
                          
                            FFN
                          
                           
                          
                            MultiheadedAttention
                          
                           
                          H
                           
                          H
                           
                          H
                          
                             
                            
                               
                            
                          
                           
                        
                      
                      
                        
                            x  ee 
                        
                      
                    
                     
                  
                
              
            
          
        
      
    
      displaystyle   begin aligned   text given input vectors    amp h     h      dots     text combine them into a matrix   H amp    begin bmatrix h      h       vdots  end bmatrix      text EncoderLayer   H  amp    begin bmatrix   text FFN     text MultiheadedAttention   H H H          text FFN     text MultiheadedAttention   H H H         vdots  end bmatrix     end aligned   
  

where 
  
    
      
        
          FFN
        
      
    
      displaystyle   text FFN   
  
 stands for  feed forward network   We can more succinctly write it as
  
    
      
        
          EncoderLayer
        
         
        H
         
         
        
          FFN
        
         
        
          MultiheadedAttention
        
         
        H
         
        H
         
        H
         
         
      
    
      displaystyle   text EncoderLayer   H    text FFN     text MultiheadedAttention   H H H   
  
with the implicit convention that the 
  
    
      
        
          FFN
        
      
    
      displaystyle   text FFN   
  
 is applied to each row of the matrix individually 
The encoder layers are stacked  The first encoder layer takes the sequence of input vectors from the embedding layer  producing a sequence of vectors  This sequence of vectors is processed by the second encoder  and so on  The output from the final encoder layer is then used by the decoder 
As the encoder processes the entire input all at once  every token can attend to every other token  all to all attention   so there is no need for causal masking 

Decoder edit 
One decoder layer
A decoder consists of an embedding layer  followed by multiple decoder layers  followed by an un embedding layer 
Each decoder consists of three major components  a causally masked self attention mechanism  a cross attention mechanism  and a feed forward neural network  The decoder functions in a similar fashion to the encoder  but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders  This mechanism can also be called the encoder decoder attention                        
Like the first encoder  the first decoder takes positional information and embeddings of the output sequence as its input  rather than encodings  The transformer must not use the current or future output to predict an output  so the output sequence must be partially masked to prevent this reverse information flow             This allows for autoregressive text generation  For decoding  all to all attention is inappropriate  because a token cannot attend to tokens not yet generated  Thus  the self attention module in the decoder is causally masked 
In contrast  the cross attention mechanism attends to the output vectors of the encoder  which is computed before the decoder starts decoding  Consequently  there is no need for masking in the cross attention mechanism 
Schematically  we have 
  
    
      
        
          
            
              
                
                  H
                    x     
                
              
              
                
                 
                
                  MaskedMultiheadedAttention
                
                 
                H
                 
                H
                 
                H
                 
              
            
            
              
                
                  DecoderLayer
                
                 
                H
                 
              
              
                
                 
                
                  FFN
                
                 
                
                  MultiheadedAttention
                
                 
                
                  H
                    x     
                
                 
                
                  H
                  
                    E
                  
                
                 
                
                  H
                  
                    E
                  
                
                 
                 
              
            
          
        
      
    
      displaystyle   begin aligned H  amp    text MaskedMultiheadedAttention   H H H     text DecoderLayer   H  amp    text FFN     text MultiheadedAttention   H  H  E  H  E    end aligned   
  
where 
  
    
      
        
          H
          
            E
          
        
      
    
      displaystyle H  E  
  
 is the matrix with rows being the output vectors from the encoder 
The last decoder is followed by a final un embedding layer  to produce the output probabilities over the vocabulary  Then  one of the tokens is sampled according to the probability  and the decoder can be run again to produce the next token  etc  autoregressively generating output text 

Adapted architectures edit 
Many large language models  since they do not need to predict a whole new sequence from an input sequence  only use the encoder or decoder of the original transformer architecture  Early GPT models are decoder only models trained to predict the next token in a sequence              BERT  another language model  only makes use of an encoder  and is trained to predict a randomly masked token in a sequence             

Full transformer architecture edit 
Sublayers edit 
 a  One encoder layer and one decoder layer   b  Two encoder layers and two decoder layers  The sublayers are labelled as well Each encoder layer contains   sublayers  the self attention and the feedforward network  Each decoder layer contains   sublayers  the causally masked self attention  the cross attention  and the feedforward network 
Transformer encoder with norm first and norm last
Transformer decoder with norm first and norm last
Block diagram for the full Transformer architectureSchematic object hierarchy for the full Transformer architecture  in object oriented programming styleThe final points of detail are the residual connections and layer normalization  LayerNorm  or LN   which while conceptually unnecessary  are necessary for numerical stability and convergence 
The residual connection  which is introduced to avoid vanishing gradient issues and stabilize the training process  can be expressed as follows  y   F x    x  The expression indicates that an output y is the sum of the transformation of input x  F x   and the input itself  x   Adding the input x can preserve the input information and avoid issues when the gradient of F x  is close to zero 
Similarly to how the feedforward network modules are applied individually to each vector  the LayerNorm is also applied individually to each vector 
There are two common conventions in use  the post LN and the pre LN convention  In the post LN convention  the output of each sublayer is 
  
    
      
        
          L
          a
          y
          e
          r
          N
          o
          r
          m
        
         
        x
         
        
          S
          u
          b
          l
          a
          y
          e
          r
        
         
        x
         
         
      
    
      displaystyle  mathrm  LayerNorm   x  mathrm  Sublayer   x   
  
where 
  
    
      
        
          S
          u
          b
          l
          a
          y
          e
          r
        
         
        x
         
      
    
      displaystyle  mathrm  Sublayer   x  
  
 is the function implemented by the sublayer itself 
In the pre LN convention  the output of each sublayer is
  
    
      
        x
         
        
          S
          u
          b
          l
          a
          y
          e
          r
        
         
        
          L
          a
          y
          e
          r
          N
          o
          r
          m
        
         
        x
         
         
      
    
      displaystyle x  mathrm  Sublayer    mathrm  LayerNorm   x   
  
The original      Transformer used the post LN convention  It was difficult to train and required careful hyperparameter tuning and a  warm up  in learning rate  where it starts small and gradually increases  The pre LN convention  proposed several times in                   was found to be easier to train  requiring no warm up  leading to faster convergence             

Pseudocode edit 
The following is the pseudocode for a standard pre LN encoder decoder Transformer  adapted from            

input  Encoder input t e
       Decoder input t d
output  Array of probability distributions  with shape  decoder vocabulary size x length decoder output sequence  

   encoder   
z e   encoder tokenizer t e 

for each t in   length z e  do
    z e t    encoder embedding z e t     encoder positional embedding t 

for each l in   length encoder layers  do
    layer   encoder layers l 

       first sublayer   
    z e copy   copy z e 
    for each t in   length z e  do
        z e t    layer layer norm z e t  
    z e   layer multiheaded attention z e  z e  z e 
    for each t in   length z e  do
        z e t    z e t    z e copy t 

       second sublayer   
    z e copy   copy z e 
    for each t in   length z e  do
        z e t    layer layer norm z e t  
    z e   layer feedforward z e 
    for each t in   length z e  do
        z e t    z e t    z e copy t 

for each t in   length z e  do
    z e t    encoder final layer norm z e t  

   decoder   
z d   decoder tokenizer t d 

for each t in   length z d  do
    z d t    decoder embedding z d t     decoder positional embedding t 

for each l in   length decoder layers  do
        layer   decoder layers l 

           first sublayer   
        z d copy   copy z d 
        for each t in   length z d  do
            z d t    layer layer norm z d t  
        z d   layer masked multiheaded attention z d  z d  z d 
        for each t in   length z d  do
            z d t    z d t    z d copy t 

           second sublayer   
        z d copy   copy z d 
        for each t in   length z d  do
            z d t    layer layer norm z d t  
        z d   layer multiheaded attention z d  z e  z e  
        for each i in   length z d  do
            z d t    z d t    z d copy t 

           third sublayer   
        z d copy   copy z d 
        for each t in   length z d  do
            z d t    layer layer norm z d t  
        z d   layer feedforward z d 
        for each t in   length z d  do
            z d t    z d t    z d copy t 

z d   decoder final layer norm z d 

output distributions     
for each t in   length z d  do
    output distributions append decoder unembed z d t   

return output distributions

Terminology edit 
The Transformer architecture  being modular  allows variations  Several common variations are described here             
An  encoder only  Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text  This is usually used for text embedding and representation learning for downstream applications  BERT is encoder only  They are less often used currently  as they were found to be not significantly better than training an encoder decoder Transformer  then taking just the encoder             
A  decoder only  Transformer is not literally decoder only  since without an encoder  the cross attention mechanism has nothing to attend to  Thus  the decoder layers in a decoder only Transformer is composed of just two sublayers  the causally masked self attention  and the feedforward network  This is usually used for text generation and instruction following  The models in the GPT series and Chinchilla series are decoder only 
An  encoder decoder  Transformer is generally the same as the original Transformer  with   sublayers per encoder layer and   sublayers per decoder layer  etc  They might have minor architectural improvements  such as alternative activation functions  changing the location of normalization  etc  This is also usually used for text generation and instruction following  The models in the T  series are encoder decoder             
A  prefixLM   prefix language model  is a decoder only architecture  but with prefix masking  which is different from causal masking  Specifically  it has mask of the form                        Figure         
  
    
      
        
          M
          
            prefixLM
          
        
         
        
          
             
            
              
                
                  
                     
                  
                
                
                    x     
                    x   e 
                
              
              
                
                  
                     
                  
                
                
                  
                    M
                    
                      causal
                    
                  
                
              
            
             
          
        
      
    
      displaystyle M   text prefixLM     begin bmatrix  mathbf      amp   infty    mathbf      amp M   text causal   end bmatrix   
  
where the first columns correspond to the  prefix   and the subsequent columns correspond to the autoregressively generated text based on the prefix  They resemble encoder decoder models  but has less  sparsity   Such models are rarely used  though they are cited as theoretical possibilities and benchmarked comparisons             
There are also mixed seq seq models  For example  in       Google Translate replaced the previous RNN encoder RNN decoder model by a Transformer encoder RNN decoder model  on the argument that an RNN decoder runs much faster than Transformer decoder when run autoregressively             

Subsequent work edit 
Alternative activation functions edit 
The original transformer uses ReLU activation function  Other activation functions were developed  The Llama series and PaLM used SwiGLU              both GPT   and BERT             used GELU             
Alternative activation functions are often used in combination with Gated Linear Units in the feedforward module             

Alternative normalizations edit 
The normalization used in the Transformer can be different from LayerNorm  One example is RMSNorm             which is used in the Llama series  Other examples include CapsuleNorm             ScaleNorm              or FixNorm             

Alternative positional encodings edit 
Transformers may use other positional encoding methods than sinusoidal             
The original Transformer paper reported using a learned positional encoding              but finding it not superior to the sinusoidal one             Later              found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module 

RoPE edit 
RoPE  rotary positional embedding               is best explained by considering a list of   dimensional vectors 
  
    
      
         
         
        
          x
          
             
          
          
             
             
             
          
        
         
        
          x
          
             
          
          
             
             
             
          
        
         
         
         
        
          x
          
             
          
          
             
             
             
          
        
         
        
          x
          
             
          
          
             
             
             
          
        
         
         
         
        
          x
          
             
          
          
             
             
             
          
        
         
        
          x
          
             
          
          
             
             
             
          
        
         
         
         
         
         
         
      
    
      displaystyle   x           x             x           x             x           x                 
  
  Now pick some angle 
  
    
      
          x b  
      
    
      displaystyle  theta  
  
  Then RoPE encoding is
  
    
      
        
          RoPE
        
        
          
             
          
        
        
          x
          
            m
          
          
             
             
             
          
        
         
        
          x
          
            m
          
          
             
             
             
          
        
         
        m
        
          
             
          
        
         
        
          
             
            
              
                
                  cos
                    x     
                  m
                    x b  
                
                
                    x     
                  sin
                    x     
                  m
                    x b  
                
              
              
                
                  sin
                    x     
                  m
                    x b  
                
                
                  cos
                    x     
                  m
                    x b  
                
              
            
             
          
        
        
          
             
            
              
                
                  
                    x
                    
                      m
                    
                    
                       
                       
                       
                    
                  
                
              
              
                
                  
                    x
                    
                      m
                    
                    
                       
                       
                       
                    
                  
                
              
            
             
          
        
         
        
          
             
            
              
                
                  
                    x
                    
                      m
                    
                    
                       
                       
                       
                    
                  
                  cos
                    x     
                  m
                    x b  
                    x     
                  
                    x
                    
                      m
                    
                    
                       
                       
                       
                    
                  
                  sin
                    x     
                  m
                    x b  
                
              
              
                
                  
                    x
                    
                      m
                    
                    
                       
                       
                       
                    
                  
                  cos
                    x     
                  m
                    x b  
                   
                  
                    x
                    
                      m
                    
                    
                       
                       
                       
                    
                  
                  sin
                    x     
                  m
                    x b  
                
              
            
             
          
        
      
    
      displaystyle   text RoPE    big   x  m        x  m        m  big      begin pmatrix  cos m theta  amp   sin m theta    sin m theta  amp  cos m theta  end pmatrix    begin pmatrix x  m         x  m          end pmatrix     begin pmatrix x  m        cos m theta  x  m        sin m theta   x  m        cos m theta  x  m        sin m theta    end pmatrix   
  
Equivalently  if we write the   dimensional vectors as complex numbers 
  
    
      
        
          z
          
            m
          
        
          
        
          x
          
            m
          
          
             
             
             
          
        
         
        i
        
          x
          
            m
          
          
             
             
             
          
        
      
    
      displaystyle z  m   x  m        ix  m        
  
  then RoPE encoding is just multiplication by an angle 
  
    
      
        
          RoPE
        
        
          
             
          
        
        
          z
          
            m
          
        
         
        m
        
          
             
          
        
         
        
          e
          
            i
            m
              x b  
          
        
        
          z
          
            m
          
        
      
    
      displaystyle   text RoPE    big   z  m  m  big    e  im theta  z  m  
  
For a list of 
  
    
      
         
        n
      
    
      displaystyle  n 
  
 dimensional vectors  a RoPE encoder is defined by a sequence of angles 
  
    
      
        
            x b  
          
             
             
             
          
        
         
         
         
         
         
        
            x b  
          
             
            n
             
          
        
      
    
      displaystyle  theta             theta    n   
  
  Then the RoPE encoding is applied to each pair of coordinates 
The benefit of RoPE is that the dot product between two vectors depends on their relative location only 
  
    
      
        
          RoPE
        
        
          
             
          
        
        x
         
        m
        
          
            
               
            
          
          
            T
          
        
        
          RoPE
        
        
          
             
          
        
        y
         
        n
        
          
             
          
        
         
        
          RoPE
        
        
          
             
          
        
        x
         
        m
         
        k
        
          
            
               
            
          
          
            T
          
        
        
          RoPE
        
        
          
             
          
        
        y
         
        n
         
        k
        
          
             
          
        
      
    
      displaystyle   text RoPE    big   x m  big     T   text RoPE    big   y n  big      text RoPE    big   x m k  big     T   text RoPE    big   y n k  big    
  

for any integer 
  
    
      
        k
      
    
      displaystyle k 
  
 

ALiBi edit 
ALiBi  Attention with Linear Biases              is not a replacement for the positional encoder on the original transformer  Instead  it is an additional positional encoder that is directly plugged into the attention mechanism  Specifically  the ALiBi attention mechanism is
  
    
      
        
          
            
              
                
                  Attention
                
                 
                Q
                 
                K
                 
                V
                 
                 
                
                  softmax
                
                
                   
                  
                    
                      
                        
                          Q
                          
                            K
                            
                              
                                T
                              
                            
                          
                        
                        
                          
                            d
                            
                              k
                            
                          
                        
                      
                    
                     
                    s
                    B
                  
                   
                
                V
              
            
          
        
      
    
      displaystyle   begin aligned   text Attention   Q K V    text softmax   left   frac  QK   mathrm  T      sqrt  d  k     sB right V end aligned   
  
Here  
  
    
      
        s
      
    
      displaystyle s 
  
 is a real number   scalar    and 
  
    
      
        B
      
    
      displaystyle B 
  
 is the linear bias matrix defined by
  
    
      
        B
         
        
          
             
            
              
                
                   
                
                
                   
                
                
                   
                
                
                   
                
                
                    x  ef 
                
              
              
                
                    x     
                   
                
                
                   
                
                
                   
                
                
                   
                
                
                    x  ef 
                
              
              
                
                    x     
                   
                
                
                    x     
                   
                
                
                   
                
                
                   
                
                
                    x  ef 
                
              
              
                
                    x     
                   
                
                
                    x     
                   
                
                
                    x     
                   
                
                
                   
                
                
                    x  ef 
                
              
              
                
                    x  ee 
                
                
                    x  ee 
                
                
                    x  ee 
                
                
                    x  ee 
                
                
                    x  f  
                
              
            
             
          
        
      
    
      displaystyle B   begin pmatrix   amp   amp   amp   amp  cdots      amp   amp   amp   amp  cdots      amp    amp   amp   amp  cdots      amp    amp    amp   amp  cdots    vdots  amp  vdots  amp  vdots  amp  vdots  amp  ddots    end pmatrix   
  
in other words  
  
    
      
        
          B
          
            i
             
            j
          
        
         
        j
          x     
        i
      
    
      displaystyle B  i j  j i 
  
  The idea being that the linear bias matrix is a softened mask  Just as 
  
    
      
         
      
    
      displaystyle   
  
 represent full attention paid  and 
  
    
      
          x     
          x   e 
      
    
      displaystyle   infty  
  
 represents no attention paid  the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction 
ALiBi allows pretraining on short context windows  then fine tuning on longer context windows  Since it is directly plugged into the attention mechanism  it can be combined with any positional encoder that is plugged into the  bottom  of the entire network  which is where the sinusoidal encoder on the original transformer  as well as RoPE and many others  are located  

Relative Position Encodings edit 
Relative Position Encodings             is similar to ALiBi  but more generic 
  
    
      
        
          
            
              
                
                  Attention
                
                 
                Q
                 
                K
                 
                V
                 
                 
                
                  softmax
                
                
                   
                  
                    
                      
                        
                          Q
                          
                            K
                            
                              
                                T
                              
                            
                          
                        
                        
                          
                            d
                            
                              k
                            
                          
                        
                      
                    
                     
                    B
                  
                   
                
                V
              
            
          
        
      
    
      displaystyle   begin aligned   text Attention   Q K V    text softmax   left   frac  QK   mathrm  T      sqrt  d  k     B right V end aligned   
  
where 
  
    
      
        B
      
    
      displaystyle B 
  
 is a Toeplitz matrix  that is  
  
    
      
        
          B
          
            i
             
            j
          
        
         
        
          B
          
            
              i
                x     
            
             
            
              j
                x     
            
          
        
      
    
      displaystyle B  i j  B  i  j   
  
 whenever 
  
    
      
        i
          x     
        j
         
        
          i
            x     
        
          x     
        
          j
            x     
        
      
    
      displaystyle i j i  j  
  
  This is contrasted with the original sinusoidal positional encoding  which is an  absolute positional encoding              

Efficient implementation edit 
The transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch  Transformers is a library produced by Hugging Face that supplies transformer based architectures and pretrained models             

KV caching edit 
When an autoregressive transformer is used for inference  such as generating text  the query vector is different at each step  but the already computed key and value vectors are always the same  The KV caching method saves the computed key and value vectors at each attention block  so that they are not recomputed at each new token  PagedAttention applies memory paging to KV caching                                     
If a transformer is used with a baked in prompt  such as   You are a customer support agent       then the key and value vectors can be computed for the prompt  and saved on disk  The saving in compute is significant when the model is used for many short interactions  such as in online chatbots 

FlashAttention edit 
FlashAttention             is an algorithm that implements the transformer attention mechanism efficiently on a GPU  It is a communication avoiding algorithm that performs matrix multiplications in blocks  such that each block fits within the cache of a GPU  and by careful management of the blocks it minimizes data copying between GPU caches  as data movement is slow   See the page on softmax for details 
An improved version  FlashAttention                                        was developed to cater to the rising demand for language models capable of handling longer context lengths  It offers enhancements in work partitioning and parallelism  enabling it to achieve up to     TFLOPs s on A    GPUs  FP   BF     a  x speed increase over the original FlashAttention 
Key advancements in FlashAttention   include the reduction of non matmul FLOPs  improved parallelism over the sequence length dimension  better work partitioning between GPU warps  and added support for head dimensions up to     and multi query attention  MQA  and grouped query attention  GQA              
Benchmarks revealed FlashAttention   to be up to  x faster than FlashAttention and up to  x faster than a standard attention implementation in PyTorch  Future developments include optimization for new hardware like H    GPUs and new data types like FP  

Multi Query Attention edit 


Comparison between several different forms of attention mechanism and the amount of KV caching necessary for each
Multi Query Attention changes the multiheaded attention mechanism              Whereas normally 

  
    
      
        
          MultiheadedAttention
        
         
        Q
         
        K
         
        V
         
         
        
          
            Concat
          
          
            i
              x     
             
            
              n
              
                heads
              
            
             
          
        
        
           
          
            
              Attention
            
             
            X
            
              W
              
                i
              
              
                Q
              
            
             
            X
            
              W
              
                i
              
              
                K
              
            
             
            X
            
              W
              
                i
              
              
                V
              
            
             
          
           
        
        
          W
          
            O
          
        
      
    
      displaystyle   text MultiheadedAttention   Q K V    text Concat    i in  n   text heads     left   text Attention   XW  i   Q  XW  i   K  XW  i   V   right W  O  
  
with Multi Query Attention  there is just one 
  
    
      
        
          W
          
            K
          
        
         
        
          W
          
            V
          
        
      
    
      displaystyle W  K  W  V  
  
  thus 

  
    
      
        
          MultiQueryAttention
        
         
        Q
         
        K
         
        V
         
         
        
          
            Concat
          
          
            i
              x     
             
            
              n
              
                heads
              
            
             
          
        
        
           
          
            
              Attention
            
             
            X
            
              W
              
                i
              
              
                Q
              
            
             
            X
            
              W
              
                K
              
            
             
            X
            
              W
              
                V
              
            
             
          
           
        
        
          W
          
            O
          
        
      
    
      displaystyle   text MultiQueryAttention   Q K V    text Concat    i in  n   text heads     left   text Attention   XW  i   Q  XW  K  XW  V   right W  O  
  

This has a neutral effect on model quality and training speed  but increases inference speed 
More generally  grouped query attention  GQA  partitions attention heads into groups  each of which shares the key value pair  MQA is GQA with one group  while standard multiheaded attention is GQA with the maximal number of groups             

The architecture of V   showing both MLA and a variant of mixture of experts                        Figure         

Multihead Latent Attention  MLA  is a low rank approximation to standard MHA  Specifically  each hidden vector  before entering the attention mechanism  is first projected to two low dimensional spaces   latent space    one for query and one for key value  KV vector   This design minimizes the KV cache  as only the low dimensional KV vector needs to be cached             

Speculative decoding edit 
Speculative decoding                         is a method to accelerate token decoding  Similarly to speculative execution in CPUs  future tokens are computed quickly  then verified  If the quickly computed tokens are incorrect  they are discarded and computed slowly 
The key factor in speculative decoding is that a Transformer decoder can verify faster than it can decode  in the following sense 
Suppose we have two transformer models like GPT   and GPT   small  both with a context window size of      To generate an entire context window autoregressively with greedy decoding with GPT    it must be run for     times  each time generating a token 
  
    
      
        
          x
          
             
          
        
         
        
          x
          
             
          
        
         
         
         
         
         
        
          x
          
               
          
        
      
    
      displaystyle x     x         x       
  
  taking time 
  
    
      
           
        
          T
          
            GPT  
          
        
      
    
      displaystyle    T   text GPT     
  
  However  if we had some educated guess for the values of these tokens  we could verify all of them in parallel  in one run of the model  by checking that each 
  
    
      
        
          x
          
            t
          
        
      
    
      displaystyle x  t  
  
 is indeed the token with the largest log likelihood in the 
  
    
      
        t
      
    
      displaystyle t 
  
 th output 
In speculative decoding  a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model  For example  suppose we use GPT   small to generate four speculative tokens  
  
    
      
        
          
            
              
                x
                  x e 
              
            
          
          
             
          
        
         
        
          
            
              
                x
                  x e 
              
            
          
          
             
          
        
         
        
          
            
              
                x
                  x e 
              
            
          
          
             
          
        
         
        
          
            
              
                x
                  x e 
              
            
          
          
             
          
        
      
    
      displaystyle   tilde  x         tilde  x         tilde  x         tilde  x       
  
  This only takes 
  
    
      
         
        
          T
          
            GPT   small
          
        
      
    
      displaystyle  T   text GPT   small   
  
  These tokens are then run through the larger GPT   in one go  Suppose that 
  
    
      
        
          
            
              
                x
                  x e 
              
            
          
          
             
          
        
      
    
      displaystyle   tilde  x       
  
 and 
  
    
      
        
          
            
              
                x
                  x e 
              
            
          
          
             
          
        
      
    
      displaystyle   tilde  x       
  
 are verified by GPT   as what it would have picked  then those are kept  but 
  
    
      
        
          
            
              
                x
                  x e 
              
            
          
          
             
          
        
      
    
      displaystyle   tilde  x       
  
 is not  so 
  
    
      
        
          
            
              
                x
                  x e 
              
            
          
          
             
          
        
         
        
          
            
              
                x
                  x e 
              
            
          
          
             
          
        
      
    
      displaystyle   tilde  x         tilde  x       
  
 are discarded  and GPT   is run on those  This would take 
  
    
      
         
        
          T
          
            GPT   small
          
        
         
         
        
          T
          
            GPT  
          
        
      
    
      displaystyle  T   text GPT   small    T   text GPT     
  
  which might be shorter than 
  
    
      
         
        
          T
          
            GPT  
          
        
      
    
      displaystyle  T   text GPT     
  
 
For non greedy decoding  similar ideas apply  except the speculative tokens are accepted or rejected stochastically  in a way that guarantees the final output distribution is the same as if speculative decoding was not used                         

Multi token prediction
In Multi Token Prediction  a single forward pass creates a final embedding vector  which then is un embedded into a token probability  However  that vector can then be further processed by another Transformer block to predict the next token  and so on for arbitrarily many steps into the future  This trades off accuracy for speed  since each new token costs just one more Transformer block  rather than the entire stack                         

Sub quadratic transformers edit 
Training transformer based architectures can be expensive  especially for long inputs              Many methods have been developed to attempt to address the issue  In the image domain  Swin Transformer is an efficient architecture that performs attention inside shifting windows              In the audio domain  SepTr decouples the attention in time and frequency domains              Long Range Arena                    is a standard benchmark for comparing the behavior of transformer architectures over long inputs 

Alternative attention graphs edit 
The standard attention graph is either all to all or causal  both of which scales as 
  
    
      
        O
         
        
          N
          
             
          
        
         
      
    
      displaystyle O N      
  
 where 
  
    
      
        N
      
    
      displaystyle N 
  
 is the number of tokens in a sequence 
Reformer                                reduces the computational load from 
  
    
      
        O
         
        
          N
          
             
          
        
         
      
    
      displaystyle O N      
  
 to 
  
    
      
        O
         
        N
        ln
          x     
        N
         
      
    
      displaystyle O N ln N  
  
 by using locality sensitive hashing and reversible layers             
Sparse attention             uses attention graphs that grows slower than 
  
    
      
        O
         
        
          N
          
             
          
        
         
      
    
      displaystyle O N      
  
  For example  BigBird                    uses random small world networks which grows as 
  
    
      
        O
         
        N
         
      
    
      displaystyle O N  
  
 
Ordinary transformers require a memory size that is quadratic in the size of the context window  Attention free transformers             reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value 

Random Feature Attention edit 
Random Feature Attention                     uses Fourier random features 
  
    
      
          x c  
         
        x
         
         
        
          
             
            
              D
            
          
        
         
        cos
          x     
          x  e  
        
          w
          
             
          
        
         
        x
          x  e  
         
        sin
          x     
          x  e  
        
          w
          
             
          
        
         
        x
          x  e  
         
          x  ef 
        cos
          x     
          x  e  
        
          w
          
            D
          
        
         
        x
          x  e  
         
        sin
          x     
          x  e  
        
          w
          
            D
          
        
         
        x
          x  e  
        
           
          
            T
          
        
      
    
      displaystyle  varphi  x    frac      sqrt  D     cos  langle w     x rangle   sin  langle w     x rangle   cdots  cos  langle w  D  x rangle   sin  langle w  D  x rangle    T  
  
where 
  
    
      
        
          w
          
             
          
        
         
         
         
         
         
        
          w
          
            D
          
        
      
    
      displaystyle w         w  D  
  
 are independent samples from the normal distribution 
  
    
      
        N
         
         
         
        
            x c  
          
             
          
        
        I
         
      
    
      displaystyle N    sigma     I  
  
  This choice of parameters satisfy 
  
    
      
        
          E
        
         
          x  e  
          x c  
         
        x
         
         
          x c  
         
        y
         
          x  e  
         
         
        
          e
          
              x     
            
              
                
                    x     
                  x
                    x     
                  y
                  
                      x     
                    
                       
                    
                  
                
                
                   
                  
                      x c  
                    
                       
                    
                  
                
              
            
          
        
      
    
      displaystyle  mathbb  E    langle  varphi  x   varphi  y  rangle   e     frac    x y          sigma         
  
  or 
  
    
      
        
          e
          
              x  e  
            x
             
            y
              x  e  
            
               
            
            
                x c  
              
                 
              
            
          
        
         
        
          E
        
         
          x  e  
        
          e
          
              x     
            x
            
                x     
              
                 
              
            
            
               
            
             
            
                x c  
              
                 
              
            
          
        
          x c  
         
        x
         
         
        
          e
          
              x     
            y
            
                x     
              
                 
              
            
            
               
            
             
            
                x c  
              
                 
              
            
          
        
          x c  
         
        y
         
          x  e  
         
          x     
          x  e  
        
          e
          
              x     
            x
            
                x     
              
                 
              
            
            
               
            
             
            
                x c  
              
                 
              
            
          
        
          x c  
         
        x
         
         
        
          e
          
              x     
            y
            
                x     
              
                 
              
            
            
               
            
             
            
                x c  
              
                 
              
            
          
        
          x c  
         
        y
         
          x  e  
      
    
      displaystyle e   langle x y rangle   sigma        mathbb  E    langle e    x         sigma       varphi  x  e    y         sigma       varphi  y  rangle   approx  langle e    x         sigma       varphi  x  e    y         sigma       varphi  y  rangle  
  
Consequently  the one headed attention  with one query  can be written as 
  
    
      
        
          Attention
        
         
        q
         
        K
         
        V
         
         
        
          softmax
        
        
           
          
            
              
                q
                
                  K
                  
                    
                      T
                    
                  
                
              
              
                
                  d
                  
                    k
                  
                
              
            
          
           
        
        V
          x     
        
          
            
                x c  
               
              q
              
                 
                
                  T
                
              
              
                  x     
                
                  i
                
              
              
                e
                
                    x     
                  
                    k
                    
                      i
                    
                  
                  
                      x     
                    
                       
                    
                  
                  
                     
                  
                   
                  
                      x c  
                    
                       
                    
                  
                
              
                x c  
               
              
                k
                
                  i
                
              
               
              
                v
                
                  i
                
                
                  T
                
              
            
            
                x c  
               
              q
              
                 
                
                  T
                
              
              
                  x     
                
                  i
                
              
              
                e
                
                    x     
                  
                    k
                    
                      i
                    
                  
                  
                      x     
                    
                       
                    
                  
                  
                     
                  
                   
                  
                      x c  
                    
                       
                    
                  
                
              
                x c  
               
              
                k
                
                  i
                
              
               
            
          
        
      
    
      displaystyle   text Attention   q K V    text softmax   left   frac  qK   mathrm  T      sqrt  d  k     right V approx   frac   varphi  q   T  sum   i e    k  i          sigma       varphi  k  i  v  i   T    varphi  q   T  sum   i e    k  i          sigma       varphi  k  i     
  
where 
  
    
      
          x c  
         
        
          d
          
            K
          
          
             
            
               
            
             
          
        
      
    
      displaystyle  sigma  d  K        
  
  Similarly for multiple queries  and for multiheaded attention 
This approximation can be computed in linear time  as we can compute the matrix 
  
    
      
          x c  
         
        
          k
          
            i
          
        
         
        
          v
          
            i
          
          
            T
          
        
      
    
      displaystyle  varphi  k  i  v  i   T  
  
 first  then multiply it with the query  In essence  we have managed to obtain a more precise version of 
  
    
      
        
          Attention
        
         
        Q
         
        K
         
        V
         
         
        
          softmax
        
        
           
          
            
              
                Q
                
                  K
                  
                    
                      T
                    
                  
                
              
              
                
                  d
                  
                    k
                  
                
              
            
          
           
        
        V
          x     
        Q
         
        
          K
          
            T
          
        
        V
        
           
        
        
          
            
              d
              
                k
              
            
          
        
         
      
    
      displaystyle   text Attention   Q K V    text softmax   left   frac  QK   mathrm  T      sqrt  d  k     right V approx Q K  T V   sqrt  d  k     
  
Performer                     uses the same Random Feature Attention  but 
  
    
      
        
          w
          
             
          
        
         
         
         
         
         
        
          w
          
            D
          
        
      
    
      displaystyle w         w  D  
  
 are first independently sampled from the normal distribution 
  
    
      
        N
         
         
         
        
            x c  
          
             
          
        
        I
         
      
    
      displaystyle N    sigma     I  
  
  then they are Gram Schmidt processed 

Multimodality edit 
Transformers can also be used adapted for modalities  input or output  beyond just text  usually by finding a way to  tokenize  the modality 
Multimodal models can either be trained from scratch  or by finetuning  A      study found that Transformers pretrained only on natural language can be finetuned on only       of parameters and become competitive with LSTMs on a variety of logical and visual tasks  demonstrating transfer learning               The LLaVA was a vision language model composed of a language model  Vicuna   B               and a vision model  ViT L      connected by a linear layer  Only the linear layer is finetuned              
Vision transformers             adapt the transformer to computer vision by breaking down input images as a series of patches  turning them into vectors  and treating them like tokens in a standard transformer 
Conformer             and later Whisper              follow the same pattern for speech recognition  first turning the speech signal into a spectrogram  which is then treated like an image  i e  broken down into a series of patches  turned into vectors and treated like tokens in a standard transformer 
Perceivers                           are a variant of Transformers designed for multimodality 
For image generation  notable architectures are DALL E           Parti                      Phenaki                      and Muse                      Unlike later models  DALL E is not a diffusion model  Instead  it uses a decoder only Transformer that autoregressively generates a text  followed by the token representation of an image  which is then converted by a variational autoencoder to an image               Parti is an encoder decoder Transformer  where the encoder processes a text prompt  and the decoder generates a token representation of an image               Muse is an encoder only Transformer that is trained to predict masked image tokens from unmasked image tokens  During generation  all input tokens are masked  and the highest confidence predictions are included for the next iteration  until all tokens are predicted               Phenaki is a text to video model  It is a bidirectional masked transformer conditioned on pre computed text tokens  The generated tokens are then decoded to a video              

Applications edit 
The transformer has had great success in natural language processing  NLP   Many large language models such as GPT    GPT    GPT    Gemini  AlbertAGPT  Claude  BERT  Grok  XLNet  RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP related subtasks and their related real world applications  including 

machine translation
time series prediction
document summarization
document generation
named entity recognition  NER              
writing computer code based on requirements expressed in natural language 
speech to text
Beyond traditional NLP  the transformer architecture has had success in other applications  such as 

biological sequence analysis
video understanding
protein folding  such as AlphaFold 
evaluating chess board positions  Using static evaluation alone  that is  with no Minimax search  transformer achieved an Elo of       putting it at grandmaster level             
See also edit 
seq seq        Family of machine learning approaches
Perceiver        Variant of Transformer designed for multimodal data
Vision transformer        Machine learning model for vision processing
Large language model        Type of machine learning model
BERT  language model         Series of language models developed by Google AI
Generative pre trained transformer        Type of large language model
T   language model         Series of large language models developed by Google AI
Notes edit 


  Gated recurrent units        further reduced its complexity 

  Some architectures  such as RWKV or state space models  avoid the issue 


References edit 


  a b c d e f g h i j k l Vaswani  Ashish  Shazeer  Noam  Parmar  Niki  Uszkoreit  Jakob  Jones  Llion  Gomez  Aidan N  Kaiser   ukasz  Polosukhin  Illia          Attention is All you Need   PDF   Advances in Neural Information Processing Systems      Curran Associates  Inc 

  Hochreiter  Sepp  Schmidhuber  J rgen    November         Long Short Term Memory   Neural Computation                    doi         neco                ISSN                 PMID               S CID              

  a b  Better Language Models and Their Implications   OpenAI              Archived from the original on             Retrieved            

  a b Bahdanau  Cho  Kyunghyun  Bengio  Yoshua  September            Neural Machine Translation by Jointly Learning to Align and Translate   arXiv            cs CL  

  Luong  Minh Thang  Pham  Hieu  Manning  Christopher D   August             Effective Approaches to Attention based Neural Machine Translation   arXiv             cs CL  

  a b Chen  Lili  Lu  Kevin  Rajeswaran  Aravind  Lee  Kimin  Grover  Aditya  Laskin  Michael  Abbeel  Pieter  Srinivas  Aravind  Mordatch  Igor               Decision Transformer  Reinforcement Learning via Sequence Modeling  arXiv           

  Parisotto  Emilio  Song  Francis  Rae  Jack  Pascanu  Razvan  Gulcehre  Caglar  Jayakumar  Siddhant  Jaderberg  Max  Kaufman  Rapha l Lopez  Clark  Aidan  Noury  Seb  Botvinick  Matthew  Heess  Nicolas  Hadsell  Raia                Stabilizing Transformers for Reinforcement Learning   Proceedings of the   th International Conference on Machine Learning  PMLR            

  Radford  Alec  Jong Wook Kim  Xu  Tao  Brockman  Greg  McLeavey  Christine  Sutskever  Ilya          Robust Speech Recognition via Large Scale Weak Supervision   arXiv             eess AS  

  Monastirsky  Maxim  Azulay  Osher  Sintov  Avishai  February         Learning to Throw With a Handful of Samples Using Decision Transformers   IEEE Robotics and Automation Letters                  doi         LRA               ISSN                

  a b Ruoss  Anian  Del tang  Gr goire  Medapati  Sourabh  Grau Moya  Jordi  Wenliang  Li  Catt  Elliot  Reid  John  Genewein  Tim                Grandmaster Level Chess Without Search   arXiv           v   cs LG  

  a b Wolf  Thomas  Debut  Lysandre  Sanh  Victor  Chaumond  Julien  Delangue  Clement  Moi  Anthony  Cistac  Pierric  Rault  Tim  Louf  Remi  Funtowicz  Morgan  Davison  Joe  Shleifer  Sam  von Platen  Patrick  Ma  Clara  Jernite  Yacine  Plu  Julien  Xu  Canwen  Le Scao  Teven  Gugger  Sylvain  Drame  Mariama  Lhoest  Quentin  Rush  Alexander          Transformers  State of the Art Natural Language Processing   Proceedings of the      Conference on Empirical Methods in Natural Language Processing  System Demonstrations  pp              doi          v       emnlp demos    S CID                

  a b c  Open Sourcing BERT  State of the Art Pre training for Natural Language Processing   Google AI Blog    November       Archived from the original on             Retrieved            

  Feldman  J  A   Ballard  D  H                 Connectionist models and their properties   Cognitive Science                  doi         S                      ISSN                

  Rumelhart  David E   McClelland  James L   Hinton  Geoffrey E                Parallel Distributed Processing  Volume    Explorations in the Microstructure of Cognition  Foundations  Chapter    PDF   Cambridge  Mass  Bradford Books  ISBN                        

  Giles  C  Lee  Maxwell  Tom                Learning  invariance  and generalization in high order neural networks   Applied Optics                      doi         AO            ISSN                 PMID               

  a b Schmidhuber  J rgen          Learning to control fast weight memories  an alternative to recurrent nets   PDF   Neural Computation                  doi         neco               S CID               

  Christoph von der Malsburg  The correlation theory of brain function  Internal Report       MPI Biophysical Chemistry        http   cogprints org        vdM correlation pdf See Reprint in Models of Neural Networks II  chapter    pages         Springer  Berlin       

  Jerome A  Feldman   Dynamic connections in neural networks   Biological Cybernetics  vol      no     pp         Dec       

  Hinton  Geoffrey E   Plaut  David C           Using Fast Weights to Deblur Old Memories   Proceedings of the Annual Meeting of the Cognitive Science Society    

  Katharopoulos  Angelos  Vyas  Apoorv  Pappas  Nikolaos  Fleuret  Fran ois          Transformers are RNNs  Fast autoregressive Transformers with linear attention   ICML       PMLR  pp                 

  Schlag  Imanol  Irie  Kazuki  Schmidhuber  J rgen          Linear Transformers Are Secretly Fast Weight Programmers   ICML       Springer  pp                 

  a b c Cho  Kyunghyun  van Merri nboer  Bart  Gulcehre  Caglar  Bahdanau  Dzmitry  Bougares  Fethi  Schwenk  Holger  Bengio  Yoshua  October         Learning Phrase Representations using RNN Encoder Decoder for Statistical Machine Translation   In Moschitti  Alessandro  Pang  Bo  Daelemans  Walter  eds    Proceedings of the      Conference on Empirical Methods in Natural Language Processing  EMNLP   Doha  Qatar  Association for Computational Linguistics  pp                  arXiv            doi         v  D        

  a b c Sutskever  Ilya  Vinyals  Oriol  Le  Quoc Viet     Dec         Sequence to sequence learning with neural networks   arXiv            cs CL    first version posted to arXiv on    Sep      

  Chung  Junyoung  Gulcehre  Caglar  Cho  KyungHyun  Bengio  Yoshua          Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling   arXiv            cs NE  

  Gruber  N   Jockisch  A           Are GRU cells more specific and LSTM cells more sensitive in motive classification of text    Frontiers in Artificial Intelligence         doi         frai             PMC               PMID                S CID               

  Sutskever  Ilya  Vinyals  Oriol  Le  Quoc V          Sequence to Sequence Learning with Neural Networks   Advances in Neural Information Processing Systems      Curran Associates  Inc  arXiv           

  Luong  Minh Thang  Pham  Hieu  Manning  Christopher D           Effective Approaches to Attention based Neural Machine Translation   arXiv             cs CL  

  Wu  Yonghui  et      al                 Google s Neural Machine Translation System  Bridging the Gap between Human and Machine Translation   arXiv             cs CL  

  Lewis Kraus  Gideon                The Great A I  Awakening   The New York Times  ISSN                 Archived from the original on    May       Retrieved            

  Parikh  Ankur P   T ckstr m  Oscar  Das  Dipanjan  Uszkoreit  Jakob                A Decomposable Attention Model for Natural Language Inference   arXiv             cs CL  

  a b Levy  Steven     Google Employees Invented Modern AI  Here s the Inside Story   Wired  ISSN                 Archived from the original on    Mar       Retrieved            

  Cheng  Jianpeng  Dong  Li  Lapata  Mirella  November         Long Short Term Memory Networks for Machine Reading   In Su  Jian  Duh  Kevin  Carreras  Xavier  eds    Proceedings of the      Conference on Empirical Methods in Natural Language Processing  Austin  Texas  Association for Computational Linguistics  pp                doi          v  D        

  Peng  Bo  Alcaide  Eric  Anthony  Quentin  Albalak  Alon  Arcadinho  Samuel  Biderman  Stella  Cao  Huanqi  Cheng  Xin  Chung  Michael               RWKV  Reinventing RNNs for the Transformer Era  arXiv           

  Marche  Stephen                Was Linguistic A I  Created by Accident    The New Yorker  ISSN              X  Retrieved            

  a b c d e f Devlin  Jacob  Chang  Ming Wei  Lee  Kenton  Toutanova  Kristina     October         BERT  Pre training of Deep Bidirectional Transformers for Language Understanding   arXiv           v   cs CL  

   Google  BERT now used on almost every English query   Search Engine Land              Retrieved            

   Recent Advances in Google Translate   research google  Retrieved            

   The inside story of how ChatGPT was built from the people who made it   MIT Technology Review  Retrieved            

   Improving language understanding with unsupervised learning   openai com  June           Archived from the original on             Retrieved            

  finetune transformer lm  OpenAI  June           retrieved           

  a b Dosovitskiy  Alexey  Beyer  Lucas  Kolesnikov  Alexander  Weissenborn  Dirk  Zhai  Xiaohua  Unterthiner  Thomas  Dehghani  Mostafa  Minderer  Matthias  Heigold  Georg  Gelly  Sylvain  Uszkoreit  Jakob                An Image is Worth   x   Words  Transformers for Image Recognition at Scale   arXiv             cs CV  

  a b Gulati  Anmol  Qin  James  Chiu  Chung Cheng  Parmar  Niki  Zhang  Yu  Yu  Jiahui  Han  Wei  Wang  Shibo  Zhang  Zhengdong  Wu  Yonghui  Pang  Ruoming          Conformer  Convolution augmented Transformer for Speech Recognition   arXiv             eess AS  

  Choromanski  Krzysztof  Likhosherstov  Valerii  Dohan  David  Song  Xingyou  Gane  Andreea  Sarlos  Tamas  Hawkins  Peter  Davis  Jared  Mohiuddin  Afroz               Rethinking Attention with Performers  arXiv           

  Liu  Zhuang  Mao  Hanzi  Wu  Chao Yuan  Feichtenhofer  Christoph  Darrell  Trevor  Xie  Saining         A ConvNet for the     s  Conference on Computer Vision and Pattern Recognition  pp                   

  Esser  Patrick  Kulal  Sumith  Blattmann  Andreas  Entezari  Rahim  M ller  Jonas  Saini  Harry  Levi  Yam  Lorenz  Dominik  Sauer  Axel               Scaling Rectified Flow Transformers for High Resolution Image Synthesis  arXiv           

  a b Xiong  Ruibin  Yang  Yunchang  He  Di  Zheng  Kai  Zheng  Shuxin  Xing  Chen  Zhang  Huishuai  Lan  Yanyan  Wang  Liwei  Liu  Tie Yan                On Layer Normalization in the Transformer Architecture   arXiv             cs LG  

  Raffel  Colin  Shazeer  Noam  Roberts  Adam  Lee  Katherine  Narang  Sharan  Matena  Michael  Zhou  Yanqi  Li  Wei  Liu  Peter J                 Exploring the limits of transfer learning with a unified text to text transformer   The Journal of Machine Learning Research                             arXiv             ISSN                

  Raffel  Colin  Shazeer  Noam  Roberts  Adam  Lee  Katherine  Narang  Sharan  Matena  Michael  Zhou  Yanqi  Li  Wei  Liu  Peter J           Exploring the Limits of Transfer Learning with a Unified Text to Text Transformer   arXiv             cs LG  

  a b  Masked language modeling   huggingface co  Retrieved            

  a b  Causal language modeling   huggingface co  Retrieved            

  a b c d Tay  Yi  Dehghani  Mostafa  Tran  Vinh Q   Garcia  Xavier  Wei  Jason  Wang  Xuezhi  Chung  Hyung Won  Shakeri  Siamak  Bahri  Dara               UL   Unifying Language Learning Paradigms  arXiv           

  Press  Ofir  Wolf  Lior               Using the Output Embedding to Improve Language Models  arXiv           

  Lintz  Nathan                Sequence Modeling with Neural Networks  Part     Attention Models   Indico  Archived from the original on             Retrieved            

  a b c Alammar  Jay   The Illustrated Transformer   jalammar github io  Archived from the original on             Retrieved            

  Team  Keras   Keras documentation  GPT Backbone model   keras io  Retrieved            

  Clark  Kevin  Khandelwal  Urvashi  Levy  Omer  Manning  Christopher D   August         What Does BERT Look at  An Analysis of BERT s Attention   Proceedings of the      ACL Workshop BlackboxNLP  Analyzing and Interpreting Neural Networks for NLP  Florence  Italy  Association for Computational Linguistics           arXiv             doi          v  W         Archived from the original on             Retrieved            

  Yang  Zhilin  Dai  Zihang  Yang  Yiming  Carbonell  Jaime  Salakhutdinov  Russ R  Le  Quoc V          XLNet  Generalized Autoregressive Pretraining for Language Understanding   Advances in Neural Information Processing Systems      Curran Associates  Inc  arXiv            

  Radford  Alec  Narasimhan  Karthik  Salimans  Tim  Sutskever  Ilya     June         Improving Language Understanding by Generative Pre Training   PDF   OpenAI  p           Archived  PDF  from the original on    January       Retrieved    January      

  Wang  Qiang  Li  Bei  Xiao  Tong  Zhu  Jingbo  Li  Changliang  Wong  Derek F   Chao  Lidia S                Learning Deep Transformer Models for Machine Translation  arXiv           

  Phuong  Mary  Hutter  Marcus               Formal Algorithms for Transformers  arXiv           

  a b c Raffel  Colin  Shazeer  Noam  Roberts  Adam  Lee  Katherine  Narang  Sharan  Matena  Michael  Zhou  Yanqi  Li  Wei  Liu  Peter J           Exploring the Limits of Transfer Learning with a Unified Text to Text Transformer   Journal of Machine Learning Research                  arXiv             ISSN                

   Recent Advances in Google Translate   Google Research  June          Archived from the original on   Jul       Retrieved            

  a b Shazeer  Noam                GLU Variants Improve Transformer   arXiv             cs LG  

  Hendrycks  Dan  Gimpel  Kevin                Gaussian Error Linear Units  GELUs    arXiv           v   cs LG  

  Zhang  Biao  Sennrich  Rico          Root Mean Square Layer Normalization   Advances in Neural Information Processing Systems      Curran Associates  Inc  arXiv            

  Tembine  Hamidou  Manzoor Ahmed Khan  and Issa Bamia         Mean Field Type Transformers  Mathematics     no            https   doi org         math        

  a b Nguyen  Toan Q   Salazar  Julian               Niehues  Jan  Cattoni  Rolando  St ker  Sebastian  Negri  Matteo  Turchi  Marco  Ha  Thanh Le  Salesky  Elizabeth  Sanabria  Ramon  Barrault  Loic  eds     Transformers without Tears  Improving the Normalization of Self Attention   Proceedings of the   th International Conference on Spoken Language Translation  Hong Kong  Association for Computational Linguistics  arXiv             doi         zenodo         

  Dufter  Philipp  Schmitt  Martin  Sch tze  Hinrich                Position Information in Transformers  An Overview   Computational Linguistics                   arXiv             doi         coli a        ISSN                 S CID                

  Gehring  Jonas  Auli  Michael  Grangier  David  Yarats  Denis  Dauphin  Yann N                 Convolutional Sequence to Sequence Learning   Proceedings of the   th International Conference on Machine Learning  PMLR            

  Haviv  Adi  Ram  Ori  Press  Ofir  Izsak  Peter  Levy  Omer               Transformer Language Models without Positional Encodings Still Learn Positional Information  arXiv           

  Su  Jianlin  Lu  Yu  Pan  Shengfeng  Murtadha  Ahmed  Wen  Bo  Liu  Yunfeng                RoFormer  Enhanced Transformer with Rotary Position Embedding   arXiv             cs CL  

  Press  Ofir  Smith  Noah A   Lewis  Mike                Train Short  Test Long  Attention with Linear Biases Enables Input Length Extrapolation   arXiv             cs CL  

  Shaw  Peter  Uszkoreit  Jakob  Vaswani  Ashish          Self Attention with Relative Position Representations   arXiv             cs CL  

  Ke  Guolin  He  Di  Liu  Tie Yan               Rethinking Positional Encoding in Language Pre training  arXiv           

  Kwon  Woosuk  Li  Zhuohan  Zhuang  Siyuan  Sheng  Ying  Zheng  Lianmin  Yu  Cody Hao  Gonzalez  Joseph  Zhang  Hao  Stoica  Ion                Efficient Memory Management for Large Language Model Serving with PagedAttention   Proceedings of the   th Symposium on Operating Systems Principles  SOSP      New York  NY  USA  Association for Computing Machinery  pp                arXiv             doi                          ISBN                        

  vllm project vllm  vLLM              retrieved           

  Contribution   Woosuk Kwon   Zhuohan Li   Siyuan Zhuang  Ying Sheng  Lianmin Zheng  Cody Yu  Joey Gonzalez  Hao Zhang  and Ion Stoica    Equal                vLLM  Easy  Fast  and Cheap LLM Serving with PagedAttention   vLLM Blog  Retrieved              cite web     CS  maint  multiple names  authors list  link 

  Dao  Tri  Fu  Dan  Ermon  Stefano  Rudra  Atri  R   Christopher                FlashAttention  Fast and Memory Efficient Exact Attention with IO Awareness   Advances in Neural Information Processing Systems                   arXiv            

   Stanford CRFM   crfm stanford edu  Retrieved            

   FlashAttention    Faster Attention with Better Parallelism and Work Partitioning   Princeton NLP              Retrieved            

   Introducing Together AI Chief Scientist Tri Dao  as he releases FlashAttention   to speed up model training and inference   TOGETHER  Retrieved            

  Ainslie  Joshua  Lee Thorp  James  de Jong  Michiel  Zemlyanskiy  Yury  Lebr n  Federico  Sanghai  Sumit                GQA  Training Generalized Multi Query Transformer Models from Multi Head Checkpoints   arXiv             cs CL  

  Chowdhery  Aakanksha  Narang  Sharan  Devlin  Jacob  Bosma  Maarten  Mishra  Gaurav  Roberts  Adam  Barham  Paul  Chung  Hyung Won  Sutton  Charles  Gehrmann  Sebastian  Schuh  Parker  Shi  Kensen  Tsvyashchenko  Sasha  Maynez  Joshua  Rao  Abhishek                PaLM  Scaling Language Modeling with Pathways   arXiv             cs CL  

  Ainslie  Joshua  Lee Thorp  James  de Jong  Michiel  Zemlyanskiy  Yury  Lebr n  Federico  Sanghai  Sumit               GQA  Training Generalized Multi Query Transformer Models from Multi Head Checkpoints  arXiv           

  a b DeepSeek AI  Liu  Aixin  Feng  Bei  Wang  Bin  Wang  Bingxuan  Liu  Bo  Zhao  Chenggang  Dengr  Chengqi  Ruan  Chong     June        DeepSeek V   A Strong  Economical  and Efficient Mixture of Experts Language Model  arXiv            

  a b Leviathan  Yaniv  Kalman  Matan  Matias  Yossi               Fast Inference from Transformers via Speculative Decoding  arXiv           

  Fu  Yao                Towards    x Speedup  Full Stack Transformer Inference Optimization  

  Chen  Charlie  Borgeaud  Sebastian  Irving  Geoffrey  Lespiau  Jean Baptiste  Sifre  Laurent  Jumper  John               Accelerating Large Language Model Decoding with Speculative Sampling  arXiv           

  Gloeckle  Fabian  Badr Youbi Idrissi  Rozi re  Baptiste  Lopez Paz  David  Synnaeve  Gabriel          Better  amp  Faster Large Language Models via Multi token Prediction   arXiv             cs CL  

  DeepSeek AI  et      al           DeepSeek V  Technical Report   arXiv             cs CL  

  a b Kitaev  Nikita  Kaiser   ukasz  Levskaya  Anselm          Reformer  The Efficient Transformer   arXiv             cs LG  

  Liu  Ze  Lin  Yutong  Cao  Yue  Hu  Han  Wei  Yixuan  Zhang  Zheng  Lin  Stephen  Guo  Baining          Swin Transformer  Hierarchical Vision Transformer using Shifted Windows        IEEE CVF International Conference on Computer Vision  ICCV   IEEE  pp                   arXiv             doi         ICCV                  ISBN                        

  Ristea  Nicolaea Catalin  Ionescu  Radu Tudor  Khan  Fahad Shahbaz                SepTr  Separable Transformer for Audio Spectrogram Processing   Interspeech  ISCA             arXiv             doi          Interspeech          

  Tay  Yi  Dehghani  Mostafa  Abnar  Samira  Shen  Yikang  Bahri  Dara  Pham  Philip  Rao  Jinfeng  Yang  Liu  Ruder  Sebastian  Metzler  Donald                Long Range Arena  A Benchmark for Efficient Transformers   arXiv             cs LG  

   Reformer  The Efficient Transformer   Google AI Blog     January       Archived from the original on             Retrieved            

  Gomez  Aidan N  Ren  Mengye  Urtasun  Raquel  Grosse  Roger B          The Reversible Residual Network  Backpropagation Without Storing Activations   Advances in Neural Information Processing Systems      Curran Associates  Inc  arXiv            

  Child  Rewon  Gray  Scott  Radford  Alec  Sutskever  Ilya               Generating Long Sequences with Sparse Transformers  arXiv           

   Constructing Transformers For Longer Sequences with Sparse Attention Methods   Google AI Blog     March       Archived from the original on             Retrieved            

  Zhai  Shuangfei  Talbott  Walter  Srivastava  Nitish  Huang  Chen  Goh  Hanlin  Zhang  Ruixiang  Susskind  Josh                An Attention Free Transformer   arXiv             cs LG  

  Peng  Hao  Pappas  Nikolaos  Yogatama  Dani  Schwartz  Roy  Smith  Noah A   Kong  Lingpeng                Random Feature Attention   arXiv             cs CL  

  Choromanski  Krzysztof  Likhosherstov  Valerii  Dohan  David  Song  Xingyou  Gane  Andreea  Sarlos  Tamas  Hawkins  Peter  Davis  Jared  Belanger  David  Colwell  Lucy  Weller  Adrian                Masked Language Modeling for Proteins via Linearly Scalable Long Context Transformers   arXiv             cs LG  

  Lu  Kevin  Grover  Aditya  Abbeel  Pieter  Mordatch  Igor                Frozen Pretrained Transformers as Universal Computation Engines   Proceedings of the AAAI Conference on Artificial Intelligence                     doi         aaai v  i         ISSN                

   Vicuna  An Open Source Chatbot Impressing GPT   with      ChatGPT Quality   LMSYS Org   lmsys org  Retrieved            

  Liu  Haotian  Li  Chunyuan  Wu  Qingyang  Lee  Yong Jae                Visual Instruction Tuning   Advances in Neural Information Processing Systems                  

  Radford  Alec  Kim  Jong Wook  Xu  Tao  Brockman  Greg  McLeavey  Christine  Sutskever  Ilya          Robust Speech Recognition via Large Scale Weak Supervision   arXiv             eess AS  

  Jaegle  Andrew  Gimeno  Felix  Brock  Andrew  Zisserman  Andrew  Vinyals  Oriol  Carreira  Joao                Perceiver  General Perception with Iterative Attention   arXiv             cs CV  

  Jaegle  Andrew  Borgeaud  Sebastian  Alayrac  Jean Baptiste  Doersch  Carl  Ionescu  Catalin  Ding  David  Koppula  Skanda  Zoran  Daniel  Brock  Andrew  Shelhamer  Evan  H naff  Olivier                Perceiver IO  A General Architecture for Structured Inputs  amp  Outputs   arXiv             cs LG  

   Parti  Pathways Autoregressive Text to Image Model   sites research google  Retrieved            

  a b Villegas  Ruben  Babaeizadeh  Mohammad  Kindermans  Pieter Jan  Moraldo  Hernan  Zhang  Han  Saffar  Mohammad Taghi  Castro  Santiago  Kunze  Julius  Erhan  Dumitru                Phenaki  Variable Length Video Generation from Open Domain Textual Descriptions     cite journal    Cite journal requires       journal   help 

  a b Chang  Huiwen  Zhang  Han  Barber  Jarred  Maschinot  A  J   Lezama  Jose  Jiang  Lu  Yang  Ming Hsuan  Murphy  Kevin  Freeman  William T                 Muse  Text To Image Generation via Masked Generative Transformers   arXiv             cs CV  

  Ramesh  Aditya  Pavlov  Mikhail  Goh  Gabriel  Gray  Scott  Voss  Chelsea  Radford  Alec  Chen  Mark  Sutskever  Ilya               Zero Shot Text to Image Generation  arXiv           

  Yu  Jiahui  Xu  Yuanzhong  Koh  Jing Yu  Luong  Thang  Baid  Gunjan  Wang  Zirui  Vasudevan  Vijay  Ku  Alexander  Yang  Yinfei               Scaling Autoregressive Models for Content Rich Text to Image Generation  arXiv           

  Kariampuzha  William  Alyea  Gioconda  Qu  Sue  Sanjak  Jaleal  Math   Ewy  Sid  Eric  Chatelaine  Haley  Yadaw  Arjun  Xu  Yanji  Zhu  Qian          Precision information extraction for rare disease epidemiology at scale   Journal of Translational Medicine               doi         s                y  PMC               PMID               


Further reading edit 

Alexander Rush  The Annotated transformer Archived            at the Wayback Machine  Harvard NLP group    April     
Phuong  Mary  Hutter  Marcus          Formal Algorithms for Transformers   arXiv             cs LG  
Ferrando  Javier  Sarti  Gabriele  Bisazza  Arianna  Costa juss   Marta R                 A Primer on the Inner Workings of Transformer based Language Models   arXiv             cs CL  

vteGoogle AI
Google
Google Brain
Google DeepMind
Computer programsAlphaGoVersions
AlphaGo       
Master       
AlphaGo Zero       
AlphaZero       
MuZero       
Competitions
Fan Hui       
Lee Sedol       
Ke Jie       
In popular culture
AlphaGo       
The MANIAC       
Other
AlphaFold       
AlphaStar       
AlphaDev       
AlphaGeometry       
Machine learningNeural networks
Inception       
WaveNet       
MobileNet       
Transformer       
EfficientNet       
Gato       
Other
Quantum Artificial Intelligence Lab
TensorFlow
Tensor Processing Unit
Generative AIChatbots
Assistant       
Sparrow       
Gemini       
Language models
BERT       
XLNet       
T        
LaMDA       
Chinchilla       
PaLM       
Gemini       
VideoPoet       
Other
DreamBooth       
NotebookLM       
Vids       
See also
 Attention Is All You Need 
Future of Go Summit
Generative pre trained transformer
Google Labs
Google Pixel
Google Workspace
Robot Constitution

 Category
 Commons

vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects






Retrieved from  https   en wikipedia org w index php title Transformer  deep learning architecture  amp oldid