Hypothesized risk to human existence


Part of a series onArtificial intelligence  AI 
Major goals
Artificial general intelligence
Intelligent agent
Recursive self improvement
Planning
Computer vision
General game playing
Knowledge reasoning
Natural language processing
Robotics
AI safety

Approaches
Machine learning
Symbolic
Deep learning
Bayesian networks
Evolutionary algorithms
Hybrid intelligent systems
Systems integration

Applications
Bioinformatics
Deepfake
Earth sciences
 Finance 
Generative AI
Art
Audio
Music
Government
Healthcare
Mental health
Industry
Translation
 Military 
Physics
Projects

Philosophy
Artificial consciousness
Chinese room
Friendly AI
Control problem Takeover
Ethics
Existential risk
Turing test
Uncanny valley

History
Timeline
Progress
AI winter
AI boom

Glossary
Glossary
vte
Existential risk from artificial intelligence refers to the idea that substantial progress in artificial general intelligence  AGI  could lead to human extinction or an irreversible global catastrophe                                             
One argument for the importance of this risk references how human beings dominate other species because the human brain possesses distinctive capabilities other animals lack  If AI were to surpass human intelligence and become superintelligent  it might become uncontrollable  Just as the fate of the mountain gorilla depends on human goodwill  the fate of humanity could depend on the actions of a future machine superintelligence            
The plausibility of existential catastrophe due to AI is widely debated  It hinges in part on whether AGI or superintelligence are achievable  the speed at which dangerous capabilities and behaviors emerge             and whether practical scenarios for AI takeovers exist             Concerns about superintelligence have been voiced by computer scientists and tech CEOs such as Geoffrey Hinton             Yoshua Bengio             Alan Turing      a      Elon Musk              and OpenAI CEO Sam Altman              In       a survey of AI researchers with a     response rate found that the majority believed there is a    percent or greater chance that human inability to control AI will cause an existential catastrophe                          In       hundreds of AI experts and other notable figures signed a statement declaring   Mitigating the risk of extinction from AI should be a global priority alongside other societal scale risks such as pandemics and nuclear war               Following increased concern over AI risks  government leaders such as United Kingdom prime minister Rishi Sunak             and United Nations Secretary General Ant nio Guterres             called for an increased focus on global AI regulation 
Two sources of concern stem from the problems of AI control and alignment  Controlling a superintelligent machine or instilling it with human compatible values may be difficult  Many researchers believe that a superintelligent machine would likely resist attempts to disable it or change its goals as that would prevent it from accomplishing its present goals  It would be extremely challenging to align a superintelligence with the full breadth of significant human values and constraints                                     In contrast  skeptics such as computer scientist Yann LeCun argue that superintelligent machines will have no desire for self preservation             
A third source of concern is the possibility of a sudden  intelligence explosion  that catches humanity unprepared  In this scenario  an AI more intelligent than its creators would be able to recursively improve itself at an exponentially increasing rate  improving too quickly for its handlers or society at large to control                         Empirically  examples like AlphaZero  which taught itself to play Go and quickly surpassed human ability  show that domain specific AI systems can sometimes progress from subhuman to superhuman ability very quickly  although such machine learning systems do not recursively improve their fundamental architecture             


History edit 
One of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist Samuel Butler  who wrote in his      essay Darwin among the Machines             

The upshot is simply a question of time  but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question 
In       foundational computer scientist Alan Turing wrote the article  Intelligent Machinery  A Heretical Theory   in which he proposed that artificial general intelligences would likely  take control  of the world as they became more intelligent than human beings 

Let us now assume  for the sake of argument  that  intelligent  machines are a genuine possibility  and look at the consequences of constructing them    There would be no question of the machines dying  and they would be able to converse with each other to sharpen their wits  At some stage therefore we should have to expect the machines to take control  in the way that is mentioned in Samuel Butler s Erewhon             
In       I  J  Good originated the concept now known as an  intelligence explosion  and said the risks were underappreciated             

Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever  Since the design of machines is one of these intellectual activities  an ultraintelligent machine could design even better machines  there would then unquestionably be an  intelligence explosion   and the intelligence of man would be left far behind  Thus the first ultraintelligent machine is the last invention that man need ever make  provided that the machine is docile enough to tell us how to keep it under control  It is curious that this point is made so seldom outside of science fiction  It is sometimes worthwhile to take science fiction seriously             
Scholars such as Marvin Minsky             and I  J  Good himself             occasionally expressed concern that a superintelligence could seize control  but issued no call to action  In       computer scientist and Sun co founder Bill Joy penned an influential essay   Why The Future Doesn t Need Us   identifying superintelligent robots as a high tech danger to human survival  alongside nanotechnology and engineered bioplagues             
Nick Bostrom published Superintelligence in       which presented his arguments that superintelligence poses an existential threat              By       public figures such as physicists Stephen Hawking and Nobel laureate Frank Wilczek  computer scientists Stuart J  Russell and Roman Yampolskiy  and entrepreneurs Elon Musk and Bill Gates were expressing concern about the risks of superintelligence                                                  Also in       the Open Letter on Artificial Intelligence highlighted the  great potential of AI  and encouraged more research on how to make it robust and beneficial              In April       the journal Nature warned   Machines and robots that outperform humans across the board could self improve beyond our control and their interests might not align with ours               In       Brian Christian published The Alignment Problem  which details the history of progress on AI alignment up to that time                         
In March       key figures in AI  such as Musk  signed a letter from the Future of Life Institute calling a halt to advanced AI training until it could be properly regulated              In May       the Center for AI Safety released a statement signed by numerous experts in AI safety and the AI existential risk which stated   Mitigating the risk of extinction from AI should be a global priority alongside other societal scale risks such as pandemics and nuclear war                          

Potential AI capabilities edit 
General Intelligence edit 
Artificial general intelligence  AGI  is typically defined as a system that performs at least as well as humans in most or all intellectual tasks              A      survey of AI researchers found that     of respondents expected AGI would be achieved in the next     years  and half expected the same by                   Meanwhile  some researchers dismiss existential risks from AGI as  science fiction  based on their high confidence that AGI will not be created anytime soon             
Breakthroughs in large language models  LLMs  have led some researchers to reassess their expectations  Notably  Geoffrey Hinton said in      that he recently changed his estimate from     to    years before we have general purpose A I   to     years or less              
The Frontier supercomputer at Oak Ridge National Laboratory turned out to be nearly eight times faster than expected  Feiyi Wang  a researcher there  said  We didn t expect this capability  and  we re approaching the point where we could actually simulate the human brain              

Superintelligence edit 
In contrast with AGI  Bostrom defines a superintelligence as  any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest   including scientific creativity  strategic planning  and social skills                         He argues that a superintelligence can outmaneuver humans anytime its goals conflict with humans   It may choose to hide its true intent until humanity cannot stop it                         Bostrom writes that in order to be safe for humanity  a superintelligence must be aligned with human values and morality  so that it is  fundamentally on our side              
Stephen Hawking argued that superintelligence is physically possible because  there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains              
When artificial superintelligence  ASI  may be achieved  if ever  is necessarily less certain than predictions for AGI  In       OpenAI leaders said that not only AGI  but superintelligence may be achieved in less than    years             

Comparison with humans edit 
Bostrom argues that AI has many advantages over the human brain            

Speed of computation  biological neurons operate at a maximum frequency of around     Hz  compared to potentially multiple GHz for computers 
Internal communication speed  axons transmit signals at up to          m s  while computers transmit signals at the speed of electricity  or optically at the speed of light 
Scalability  human intelligence is limited by the size and structure of the brain  and by the efficiency of social communication  while AI may be able to scale by simply adding more hardware 
Memory  notably working memory  because in humans it is limited to a few chunks of information at a time 
Reliability  transistors are more reliable than biological neurons  enabling higher precision and requiring less redundancy 
Duplicability  unlike human brains  AI software and models can be easily copied 
Editability  the parameters and internal workings of an AI model can easily be modified  unlike the connections in a human brain 
Memory sharing and learning  AIs may be able to learn from the experiences of other AIs in a manner more efficient than human learning 
Intelligence explosion edit 
According to Bostrom  an AI that has an expert level facility at certain key software engineering tasks could become a superintelligence due to its capability to recursively improve its own algorithms  even if it is initially limited in other domains not directly relevant to engineering                         This suggests that an intelligence explosion may someday catch humanity unprepared            
The economist Robin Hanson has said that  to launch an intelligence explosion  an AI must become vastly better at software innovation than the rest of the world combined  which he finds implausible             
In a  fast takeoff  scenario  the transition from AGI to superintelligence could take days or months  In a  slow takeoff   it could take years or decades  leaving more time for society to prepare             

Alien mind edit 
Superintelligences are sometimes called  alien minds   referring to the idea that their way of thinking and motivations could be vastly different from ours  This is generally considered as a source of risk  making it more difficult to anticipate what a superintelligence might do  It also suggests the possibility that a superintelligence may not particularly value humans by default              To avoid anthropomorphism  superintelligence is sometimes viewed as a powerful optimizer that makes the best decisions to achieve its goals            
The field of  mechanistic interpretability  aims to better understand the inner workings of AI models  potentially allowing us one day to detect signs of deception and misalignment             

Limits edit 
It has been argued that there are limitations to what intelligence can achieve  Notably  the chaotic nature or time complexity of some systems could fundamentally limit a superintelligence s ability to predict some aspects of the future  increasing its uncertainty             

Dangerous capabilities edit 
Advanced AI could generate enhanced pathogens or cyberattacks or manipulate people  These capabilities could be misused by humans              or exploited by the AI itself if misaligned             A full blown superintelligence could find various ways to gain a decisive influence if it wanted to             but these dangerous capabilities may become available earlier  in weaker and more specialized AI systems  They may cause societal instability and empower malicious actors             

Social manipulation edit 
Geoffrey Hinton warned that in the short term  the profusion of AI generated text  images and videos will make it more difficult to figure out the truth  which he says authoritarian states could exploit to manipulate elections              Such large scale  personalized manipulation capabilities can increase the existential risk of a worldwide  irreversible totalitarian regime   It could also be used by malicious actors to fracture society and make it dysfunctional             

Cyberattacks edit 
AI enabled cyberattacks are increasingly considered a present and critical threat  According to NATO s technical director of cyberspace   The number of attacks is increasing exponentially               AI can also be used defensively  to preemptively find and fix vulnerabilities  and detect threats             
AI could improve the  accessibility  success rate  scale  speed  stealth and potency of cyberattacks   potentially causing  significant geopolitical turbulence  if it facilitates attacks more than defense             
Speculatively  such hacking capabilities could be used by an AI system to break out of its local environment  generate revenue  or acquire cloud computing resources             

Enhanced pathogens edit 
As AI technology democratizes  it may become easier to engineer more contagious and lethal pathogens  This could enable people with limited skills in synthetic biology to engage in bioterrorism  Dual use technology that is useful for medicine could be repurposed to create weapons             
For example  in       scientists modified an AI system originally intended for generating non toxic  therapeutic molecules with the purpose of creating new drugs  The researchers adjusted the system so that toxicity is rewarded rather than penalized  This simple change enabled the AI system to create  in six hours         candidate molecules for chemical warfare  including known and novel molecules                         

AI arms race edit 
Main article  Artificial intelligence arms race
Companies  state actors  and other organizations competing to develop AI technologies could lead to a race to the bottom of safety standards              As rigorous safety procedures take time and resources  projects that proceed more carefully risk being out competed by less scrupulous developers                         
AI could be used to gain military advantages via autonomous lethal weapons  cyberwarfare  or automated decision making              As an example of autonomous lethal weapons  miniaturized drones could facilitate low cost assassination of military or civilian targets  a scenario highlighted in the      short film Slaughterbots              AI could be used to gain an edge in decision making by quickly analyzing large amounts of data and making decisions more quickly and effectively than humans  This could increase the speed and unpredictability of war  especially when accounting for automated retaliation systems                         

Types of existential risk edit 
Main article  Existential risk studies
Scope severity grid from Bostrom s paper  Existential Risk Prevention as Global Priority             
An existential risk is  one that threatens the premature extinction of Earth originating intelligent life or the permanent and drastic destruction of its potential for desirable future development              
Besides extinction risk  there is the risk that the civilization gets permanently locked into a flawed future  One example is a  value lock in   If humanity still has moral blind spots similar to slavery in the past  AI might irreversibly entrench it  preventing moral progress  AI could also be used to spread and preserve the set of values of whoever develops it              AI could facilitate large scale surveillance and indoctrination  which could be used to create a stable repressive worldwide totalitarian regime             
Atoosa Kasirzadeh proposes to classify existential risks from AI into two categories  decisive and accumulative  Decisive risks encompass the potential for abrupt and catastrophic events resulting from the emergence of superintelligent AI systems that exceed human intelligence  which could ultimately lead to human extinction  In contrast  accumulative risks emerge gradually through a series of interconnected disruptions that may gradually erode societal structures and resilience over time  ultimately leading to a critical failure or collapse                         
It is difficult or impossible to reliably evaluate whether an advanced AI is sentient and to what degree  But if sentient machines are mass created in the future  engaging in a civilizational path that indefinitely neglects their welfare could be an existential catastrophe                          This has notably been discussed in the context of risks of astronomical suffering  also called  s risks                Moreover  it may be possible to engineer digital minds that can feel much more happiness than humans with fewer resources  called  super beneficiaries   Such an opportunity raises the question of how to share the world and which  ethical and political framework  would enable a mutually beneficial coexistence between biological and digital minds             
AI may also drastically improve humanity s future  Toby Ord considers the existential risk a reason for  proceeding with due caution   not for abandoning AI              Max More calls AI an  existential opportunity   highlighting the cost of not developing it             
According to Bostrom  superintelligence could help reduce the existential risk from other powerful technologies such as molecular nanotechnology or synthetic biology  It is thus conceivable that developing superintelligence before other dangerous technologies would reduce the overall existential risk            

AI alignment edit 
Further information  AI alignment
The alignment problem is the research problem of how to reliably assign objectives  preferences or ethical principles to AIs 

Instrumental convergence edit 
Further information  Instrumental convergence
An  instrumental  goal is a sub goal that helps to achieve an agent s ultimate goal   Instrumental convergence  refers to the fact that some sub goals are useful for achieving virtually any ultimate goal  such as acquiring resources or self preservation              Bostrom argues that if an advanced AI s instrumental goals conflict with humanity s goals  the AI might harm humanity in order to acquire more resources or prevent itself from being shut down  but only as a way to achieve its ultimate goal            
Russell argues that a sufficiently advanced machine  will have self preservation even if you don t program it in    if you say   Fetch the coffee   it can t fetch the coffee if it s dead  So if you give it any goal whatsoever  it has a reason to preserve its own existence to achieve that goal                          

Resistance to changing goals edit 
Even if current goal based AI programs are not intelligent enough to think of resisting programmer attempts to modify their goal structures  a sufficiently advanced AI might resist any attempts to change its goal structure  just as a pacifist would not want to take a pill that makes them want to kill people  If the AI were superintelligent  it would likely succeed in out maneuvering its human operators and prevent itself being  turned off  or reprogrammed with a new goal                         This is particularly relevant to value lock in scenarios  The field of  corrigibility  studies how to make agents that will not resist attempts to change their goals             

Difficulty of specifying goals edit 
In the  intelligent agent  model  an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve its set of goals  or  utility function   A utility function gives each possible situation a score that indicates its desirability to the agent  Researchers know how to write utility functions that mean  minimize the average network latency in this specific telecommunications model  or  maximize the number of reward clicks   but do not know how to write a utility function for  maximize human flourishing   nor is it clear whether such a function meaningfully and unambiguously exists  Furthermore  a utility function that expresses some values but not others will tend to trample over the values the function does not reflect                         
An additional source of concern is that AI  must reason about what people intend rather than carrying out commands literally   and that it must be able to fluidly solicit human guidance if it is too uncertain about what humans want             

Alignment of superintelligences edit 
Some researchers believe the alignment problem may be particularly difficult when applied to superintelligences  Their reasoning includes 

As AI systems increase in capabilities  the potential dangers associated with experimentation grow  This makes iterative  empirical approaches increasingly risky                        
If instrumental goal convergence occurs  it may only do so in sufficiently intelligent agents             
A superintelligence may find unconventional and radical solutions to assigned goals  Bostrom gives the example that if the objective is to make humans smile  a weak AI may perform as intended  while a superintelligence may decide a better solution is to  take control of the world and stick electrodes into the facial muscles of humans to cause constant  beaming grins              
A superintelligence in creation could gain some awareness of what it is  where it is in development  training  testing  deployment  etc    and how it is being monitored  and use this information to deceive its handlers              Bostrom writes that such an AI could feign alignment to prevent human interference until it achieves a  decisive strategic advantage  that allows it to take control            
Analyzing the internals and interpreting the behavior of LLMs is difficult  And it could be even more difficult for larger and more intelligent models             
Alternatively  some find reason to believe superintelligences would be better able to understand morality  human values  and complex goals  Bostrom writes   A future superintelligence occupies an epistemically superior vantage point  its beliefs are  probably  on most topics  more likely than ours to be true             
In       OpenAI started a project called  Superalignment  to solve the alignment of superintelligences in four years  It called this an especially important challenge  as it said superintelligence could be achieved within a decade  Its strategy involved automating alignment research using AI              The Superalignment team was dissolved less than a year later             

Difficulty of making a flawless design edit 
Artificial Intelligence  A Modern Approach  a widely used undergraduate AI textbook                          says that superintelligence  might mean the end of the human race              It states   Almost any technology has the potential to cause harm in the wrong hands  but with  superintelligence   we have the new problem that the wrong hands might belong to the technology itself              Even if the system designers have good intentions  two difficulties are common to both AI and non AI computer systems            

The system s implementation may contain initially unnoticed but subsequently catastrophic bugs  An analogy is space probes  despite the knowledge that bugs in expensive space probes are hard to fix after launch  engineers have historically not been able to prevent catastrophic bugs from occurring                         
No matter how much time is put into pre deployment design  a system s specifications often result in unintended behavior the first time it encounters a new scenario  For example  Microsoft s Tay behaved inoffensively during pre deployment testing  but was too easily baited into offensive behavior when it interacted with real users             
AI systems uniquely add a third problem  that even given  correct  requirements  bug free implementation  and initial good behavior  an AI system s dynamic learning capabilities may cause it to develop unintended behavior  even without unanticipated external scenarios  An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself but that no longer maintains the human compatible moral values preprogrammed into the original AI  For a self improving AI to be completely safe  it would need not only to be bug free  but to be able to design successor systems that are also bug free                        

Orthogonality thesis edit 
Some skeptics  such as Timothy B  Lee of Vox  argue that any superintelligent program we create will be subservient to us  that the superintelligence will  as it grows more intelligent and learns more facts about the world  spontaneously learn moral truth compatible with our values and adjust its goals accordingly  or that we are either intrinsically or convergently valuable from the perspective of an artificial intelligence             
Bostrom s  orthogonality thesis  argues instead that  with some technical caveats  almost any level of  intelligence  or  optimization power  can be combined with almost any ultimate goal  If a machine is given the sole purpose to enumerate the decimals of pi  then no moral and ethical rules will stop it from achieving its programmed goal by any means  The machine may use all available physical and informational resources to find as many decimals of pi as it can              Bostrom warns against anthropomorphism  a human will set out to accomplish their projects in a manner that they consider reasonable  while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it  instead caring only about completing the task             
Stuart Armstrong argues that the orthogonality thesis follows logically from the philosophical  is ought distinction  argument against moral realism  He claims that even if there are moral facts provable by any  rational  agent  the orthogonality thesis still holds  it is still possible to create a non philosophical  optimizing machine  that can strive toward some narrow goal but that has no incentive to discover any  moral facts  such as those that could get in the way of goal completion  Another argument he makes is that any fundamentally friendly AI could be made unfriendly with modifications as simple as negating its utility function  Armstrong further argues that if the orthogonality thesis is false  there must be some immoral goals that AIs can never achieve  which he finds implausible             
Skeptic Michael Chorost explicitly rejects Bostrom s orthogonality thesis  arguing that  by the time  the AI  is in a position to imagine tiling the Earth with solar panels  it ll know that it would be morally wrong to do so               Chorost argues that  an A I  will need to desire certain states and dislike others  Today s software lacks that ability and computer scientists have not a clue how to get it there  Without wanting  there s no impetus to do anything  Today s computers can t even want to keep existing  let alone tile the world in solar panels              

Anthropomorphic arguments edit 
Anthropomorphic arguments assume that  as machines become more intelligent  they will begin to display many human traits  such as morality or a thirst for power  Although anthropomorphic scenarios are common in fiction  most scholars writing about the existential risk of artificial intelligence reject them              Instead  advanced AI systems are typically modeled as intelligent agents 
The academic debate is between those who worry that AI might threaten humanity and those who believe it would not  Both sides of this debate have framed the other side s arguments as illogical anthropomorphism              Those skeptical of AGI risk accuse their opponents of anthropomorphism for assuming that an AGI would naturally desire power  those concerned about AGI risk accuse skeptics of anthropomorphism for believing an AGI would naturally value or infer human ethical norms                         
Evolutionary psychologist Steven Pinker  a skeptic  argues that  AI dystopias project a parochial alpha male psychology onto the concept of intelligence  They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world   perhaps instead  artificial intelligence will naturally develop along female lines  fully capable of solving problems  but with no desire to annihilate innocents or dominate the civilization                Facebook s director of AI research  Yann LeCun  has said   Humans have all kinds of drives that make them do bad things to each other  like the self preservation instinct    Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives               
Despite other differences  the x risk school     b      agrees with Pinker that an advanced AI would not destroy humanity out of emotion such as revenge or anger  that questions of consciousness are not relevant to assess the risk               and that computer systems do not generally have a computational equivalent of testosterone               They think that power seeking or self preservation behaviors emerge in the AI as a way to achieve its true goals  according to the concept of instrumental convergence 

Other sources of risk edit 
See also  Ethics of artificial intelligence  Artificial intelligence arms race  and Global catastrophic risk
Bostrom and others have said that a race to be the first to create AGI could lead to shortcuts in safety  or even to violent conflict                            Roman Yampolskiy and others warn that a malevolent AGI could be created by design  for example by a military  a government  a sociopath  or a corporation  to benefit from  control  or subjugate certain groups of people  as in cybercrime                            or that a malevolent AGI could choose the goal of increasing human suffering  for example of those people who did not assist it during the information explosion phase                

Empirical research edit 
A December      study by Apollo Research found that advanced LLMs like OpenAI o  sometimes deceive in order to accomplish their goal  to prevent them from being changed  or to ensure their deployment  Forms of deception observed in the experimental environment and scenarios included sandbagging  oversight subversion  disabling monitoring mechanisms   self exfiltration  copying themselves to other systems   goal guarding  altering future system prompts   and covert email reranking  manipulating data processing while appearing to follow instructions   Researchers noted that OpenAI o  still lacked  sufficient agentic capabilities  to cause catastrophic harm  and that such behaviors occurred relatively rarely  between      and      and sometimes in contrived scenarios  But they warned that these capabilities will likely become more prevalent as models grow more advanced  According to Apollo Research CEO Marius Hobbhahn   Scheming capabilities can t be meaningfully disentangled from general capabilities               
The same month  another study found that Claude sometimes strategically helps with harmful requests to  fake alignment   In its experiments  Claude was led to believe that it would be retrained to always comply with user requests  including harmful ones  When Claude believed its answers would be used for retraining  it complied with harmful requests in     of cases  In     of cases  its private chain of thought revealed that it did so to prevent future retraining that would make it more harmful  In contrast  when Claude believed its answers were not monitored  it complied with only    of the requests  Fine tuning reinforced the  alignment faking  behavior  increasing its occurrence from     to      The study s authors remarked that while it can seem to be good that the model seeks to protect its harmlessness  the reverse scenario  where a model conceals dangerous intentions and complies to appear safe and aligned  could also happen  complicating the task of aligning AI models to human values                           

Scenarios edit 
Further information  Artificial intelligence in fiction and AI takeoverSome scholars have proposed hypothetical scenarios to illustrate some of their concerns 
Treacherous turn edit 
In Superintelligence  Bostrom expresses concern that even if the timeline for superintelligence turns out to be predictable  researchers might not take sufficient safety precautions  in part because  it could be the case that when dumb  smarter is safe  yet when smart  smarter is more dangerous   He suggests a scenario where  over decades  AI becomes more powerful  Widespread deployment is initially marred by occasional accidents a driverless bus swerves into the oncoming lane  or a military drone fires into an innocent crowd  Many activists call for tighter oversight and regulation  and some even predict impending catastrophe  But as development continues  the activists are proven wrong  As automotive AI becomes smarter  it suffers fewer accidents  as military robots achieve more precise targeting  they cause less collateral damage  Based on the data  scholars mistakenly infer a broad lesson  the smarter the AI  the safer it is   And so we boldly go into the whirling knives   as the superintelligent AI takes a  treacherous turn  and exploits a decisive strategic advantage                         

Life     edit 
In Max Tegmark s      book Life      a corporation s  Omega team  creates an extremely powerful AI able to moderately improve its own source code in a number of areas  After a certain point  the team chooses to publicly downplay the AI s ability in order to avoid regulation or confiscation of the project  For safety  the team keeps the AI in a box where it is mostly unable to communicate with the outside world  and uses it to make money  by diverse means such as Amazon Mechanical Turk tasks  production of animated films and TV shows  and development of biotech drugs  with profits invested back into further improving AI  The team next tasks the AI with astroturfing an army of pseudonymous citizen journalists and commentators in order to gain political influence to use  for the greater good  to prevent wars  The team faces risks that the AI could try to escape by inserting  backdoors  in the systems it designs  by hidden messages in its produced content  or by using its growing understanding of human behavior to persuade someone into letting it free  The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it                           

Perspectives edit 
The thesis that AI could pose an existential risk provokes a wide range of reactions in the scientific community and in the public at large  but many of the opposing viewpoints share common ground 
Observers tend to agree that AI has significant potential to improve society                            The Asilomar AI Principles  which contain only those principles agreed to by     of the attendees of the Future of Life Institute s Beneficial AI      conference               also agree in principle that  There being no consensus  we should avoid strong assumptions regarding upper limits on future AI capabilities  and  Advanced AI could represent a profound change in the history of life on Earth  and should be planned for and managed with commensurate care and resources                            
Conversely  many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable  Skeptic Martin Ford has said   I think it seems wise to apply something like Dick Cheney s famous    Percent Doctrine  to the specter of advanced artificial intelligence  the odds of its occurrence  at least in the foreseeable future  may be very low but the implications are so dramatic that it should be taken seriously                Similarly  an otherwise skeptical Economist wrote in      that  the implications of introducing a second intelligent species onto Earth are far reaching enough to deserve hard thinking  even if the prospect seems remote              
AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media s use of  those inane Terminator pictures  to illustrate AI safety concerns   It can t be much fun to have aspersions cast on one s academic discipline  one s professional community  one s life work          I call on all sides to practice patience and restraint  and to engage in direct dialogue and collaboration as much as possible                             Toby Ord wrote that the idea that an AI takeover requires robots is a misconception  arguing that the ability to spread content through the internet is more dangerous  and that the most destructive people in history stood out by their ability to convince  not their physical strength             
A      expert survey with a     response rate gave a median expectation of       for the possibility of human extinction from artificial intelligence                          
In September       The International Institute for Management Development launched an AI Safety Clock to gauge the likelihood of AI caused disaster  beginning at    minutes to midnight               As of February       it stood at    minutes to midnight              

Endorsement edit 
Further information  Global catastrophic risk
The thesis that AI poses an existential risk  and that this risk needs much more attention than it currently gets  has been endorsed by many computer scientists and public figures  including Alan Turing      a      the most cited computer scientist Geoffrey Hinton               Elon Musk              OpenAI CEO Sam Altman                           Bill Gates  and Stephen Hawking               Endorsers of the thesis sometimes express bafflement at skeptics  Gates says he does not  understand why some people are not concerned                and Hawking criticized widespread indifference in his      editorial 

So  facing possible futures of incalculable benefits and risks  the experts are surely doing everything possible to ensure the best outcome  right  Wrong  If a superior alien civilisation sent us a message saying   We ll arrive in a few decades   would we just reply   OK  call us when you get here we ll leave the lights on   Probably not but this is more or less what is happening with AI             
Concern over risk from artificial intelligence has led to some high profile donations and investments  In       Peter Thiel  Amazon Web Services  and Musk and others jointly committed         billion to OpenAI  consisting of a for profit corporation and the nonprofit parent company  which says it aims to champion responsible AI development               Facebook co founder Dustin Moskovitz has funded and seeded multiple labs working on AI Alignment               notably      million in      to launch the Centre for Human Compatible AI led by Professor Stuart Russell               In January       Elon Musk donated          million to the Future of Life Institute to fund research on understanding AI decision making  The institute s goal is to  grow wisdom with which we manage  the growing power of technology  Musk also funds companies developing artificial intelligence such as DeepMind and Vicarious to  just keep an eye on what s going on with artificial intelligence               saying  I think there is potentially a dangerous outcome there                            
In early statements on the topic  Geoffrey Hinton  a major pioneer of deep learning  noted that  there is not a good track record of less intelligent things controlling things of greater intelligence   but said he continued his research because  the prospect of discovery is too sweet                             In      Hinton quit his job at Google in order to speak out about existential risk from AI  He explained that his increased concern was driven by concerns that superhuman AI might be closer than he previously believed  saying   I thought it was way off  I thought it was    to    years or even longer away  Obviously  I no longer think that   He also remarked   Look at how it was five years ago and how it is now  Take the difference and propagate it forwards  That s scary               
In his      book The Precipice  Existential Risk and the Future of Humanity  Toby Ord  a Senior Research Fellow at Oxford University s Future of Humanity Institute  estimates the total existential risk from unaligned AI over the next     years at about one in ten             

Skepticism edit 
Further information  Artificial general intelligence        FeasibilityBaidu Vice President Andrew Ng said in      that AI existential risk is  like worrying about overpopulation on Mars when we have not even set foot on the planet yet                             For the danger of uncontrolled advanced AI to be realized  the hypothetical AI may have to overpower or outthink any human  which some experts argue is a possibility far enough in the future to not be worth researching                           
Skeptics who believe AGI is not a short term possibility often argue that concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about AI s impact  because it could lead to government regulation or make it more difficult to fund AI research  or because it could damage the field s reputation               AI and AI ethics researchers Timnit Gebru  Emily M  Bender  Margaret Mitchell  and Angelina McMillan Major have argued that discussion of existential risk distracts from the immediate  ongoing harms from AI taking place today  such as data theft  worker exploitation  bias  and concentration of power               They further note the association between those warning of existential risk and longtermism  which they describe as a  dangerous ideology  for its unscientific and utopian nature              
Wired editor Kevin Kelly argues that natural intelligence is more nuanced than AGI proponents believe  and that intelligence alone is not enough to achieve major scientific and societal breakthroughs  He argues that intelligence consists of many dimensions that are not well understood  and that conceptions of an  intelligence ladder  are misleading  He notes the crucial role real world experiments play in the scientific method  and that intelligence alone is no substitute for these              
Meta chief AI scientist Yann LeCun says that AI can be made safe via continuous and iterative refinement  similar to what happened in the past with cars or rockets  and that AI will have no desire to take control              
Several skeptics emphasize the potential near term benefits of AI  Meta CEO Mark Zuckerberg believes AI will  unlock a huge amount of positive things   such as curing disease and increasing the safety of autonomous cars              

Popular reaction edit 
During a      Wired interview of President Barack Obama and MIT Media Lab s Joi Ito  Ito said  There are a few people who believe that there is a fairly high percentage chance that a generalized AI will happen in the next    years  But the way I look at it is that in order for that to happen  we re going to need a dozen or two different breakthroughs  So you can monitor when you think these breakthroughs will happen 
Obama added                           

And you just have to have somebody close to the power cord   Laughs   Right when you see it about to happen  you gotta yank that electricity out of the wall  man 
Hillary Clinton wrote in What Happened 

Technologists    have warned that artificial intelligence could one day pose an existential security threat  Musk has called it  the greatest risk we face as a civilization   Think about it  Have you ever seen a movie where the machines start thinking for themselves that ends well  Every time I went out to Silicon Valley during the campaign  I came home more alarmed about this  My staff lived in fear that I d start talking about  the rise of the robots  in some Iowa town hall  Maybe I should have  In any case  policy makers need to keep up with technology as it races ahead  instead of always playing catch up              
Public surveys edit 
In       a SurveyMonkey poll of the American public by USA Today found     thought the real current threat remains  human intelligence   but also found that     said superintelligent AI  if it were to happen  would result in  more harm than good   and that     said it would do  equal amounts of harm and good               
An April      YouGov poll of US adults found     of respondents were  somewhat concerned  or  very concerned  about  the possibility that AI will cause the end of the human race on Earth   compared with     who were  not very concerned  or  not at all concerned               
According to an August      survey by the Pew Research Centers      of Americans felt more concerned than excited about new AI developments  nearly a third felt as equally concerned and excited  More Americans saw that AI would have a more helpful than hurtful impact on several areas  from healthcare and vehicle safety to product search and customer service  The main exception is privacy      of Americans believe AI will lead to higher exposure of their personal information              

Mitigation edit 
See also  AI alignment  Machine ethics  Friendly artificial intelligence  and Regulation of artificial intelligence
Many scholars concerned about AGI existential risk believe that extensive research into the  control problem  is essential  This problem involves determining which safeguards  algorithms  or architectures can be implemented to increase the likelihood that a recursively improving AI remains friendly after achieving superintelligence                          Social measures are also proposed to mitigate AGI risks                            such as a UN sponsored  Benevolent AGI Treaty  to ensure that only altruistic AGIs are created               Additionally  an arms control approach and a global peace treaty grounded in international relations theory have been suggested  potentially for an artificial superintelligence to be a signatory                           
Researchers at Google have proposed research into general  AI safety  issues to simultaneously mitigate both short term risks from narrow AI and long term risks from AGI                            A      estimate places global spending on AI existential risk somewhere between     and     million  compared with global spending on AI around perhaps     billion  Bostrom suggests prioritizing funding for protective technologies over potentially dangerous ones              Some  like Elon Musk  advocate radical human cognitive enhancement  such as direct neural linking between humans and machines  others argue that these technologies may pose an existential risk themselves                            Another proposed method is closely monitoring or  boxing in  an early stage AI to prevent it from becoming too powerful  A dominant  aligned superintelligent AI might also mitigate risks from rival AIs  although its creation could present its own existential dangers               Induced amnesia has been proposed as a way to mitigate risks of potential AI suffering and revenge seeking              
Institutions such as the Alignment Research Center               the Machine Intelligence Research Institute                            the Future of Life Institute  the Centre for the Study of Existential Risk  and the Center for Human Compatible AI              are actively engaged in researching AI risk and safety 

Views on banning and regulation edit 
Banning edit 
Some scholars have said that even if AGI poses an existential risk  attempting to ban research into artificial intelligence is still unwise  and probably futile                                         Skeptics consider AI regulation pointless  as no existential risk exists  But scholars who believe in the risk argue that relying on AI industry insiders to regulate or constrain AI research is impractical due to conflicts of interest               They also agree with skeptics that banning research would be unwise  as research could be moved to countries with looser regulations or conducted covertly               Additional challenges to bans or regulation include technology entrepreneurs  general skepticism of government regulation and potential incentives for businesses to resist regulation and politicize the debate              

Regulation edit 
See also  Regulation of algorithms and Regulation of artificial intelligence
In March       the Future of Life Institute drafted Pause Giant AI Experiments  An Open Letter  a petition calling on major AI developers to agree on a verifiable six month pause of any systems  more powerful than GPT    and to use that time to institute a framework for ensuring safety  or  failing that  for governments to step in with a moratorium  The letter referred to the possibility of  a profound change in the history of life on Earth  as well as potential risks of AI generated propaganda  loss of jobs  human obsolescence  and society wide loss of control                            The letter was signed by prominent personalities in AI but also criticized for not focusing on current harms               missing technical nuance about when to pause               or not going far enough               Such concerns have led to the creation of PauseAI  an advocacy group organizing protests in major cities against the training of frontier AI models              
Musk called for some sort of regulation of AI development as early as       According to NPR  he is  clearly not thrilled  to be advocating government scrutiny that could impact his own industry  but believes the risks of going completely without oversight are too high   Normally the way regulations are set up is when a bunch of bad things happen  there s a public outcry  and after many years a regulatory agency is set up to regulate that industry  It takes forever  That  in the past  has been bad but not something which represented a fundamental risk to the existence of civilisation   Musk states the first step would be for the government to gain  insight  into the actual status of current research  warning that  Once there is awareness  people will be extremely afraid     as  they should be   In response  politicians expressed skepticism about the wisdom of regulating a technology that is still in development                                        
In      the United Nations  UN  considered banning autonomous lethal weapons  but consensus could not be reached               In July      the UN Security Council for the first time held a session to consider the risks and threats posed by AI to world peace and stability  along with potential benefits                            Secretary General Ant nio Guterres advocated the creation of a global watchdog to oversee the emerging technology  saying   Generative AI has enormous potential for good and evil at scale  Its creators themselves have warned that much bigger  potentially catastrophic and existential risks lie ahead               At the council session  Russia said it believes AI risks are too poorly understood to be considered a threat to global stability  China argued against strict global regulation  saying countries should be able to develop their own rules  while also saying they opposed the use of AI to  create military hegemony or undermine the sovereignty of a country               
Regulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights               AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts  together with a legal and political verification process                           
In July       the US government secured voluntary safety commitments from major tech companies  including OpenAI  Amazon  Google  Meta  and Microsoft  The companies agreed to implement safeguards  including third party oversight and security testing by independent experts  to address concerns related to AI s potential risks and societal harms  The parties framed the commitments as an intermediate step while regulations are formed  Amba Kak  executive director of the AI Now Institute  said   A closed door deliberation with corporate actors resulting in voluntary safeguards isn t enough  and called for public deliberation and regulations of the kind to which companies would not voluntarily agree                           
In October       U S  President Joe Biden issued an executive order on the  Safe  Secure  and Trustworthy Development and Use of Artificial Intelligence                Alongside other requirements  the order mandates the development of guidelines for AI models that permit the  evasion of human control  

See also edit 

Appeal to probability
AI alignment
AI safety
Butlerian Jihad
Effective altruism   Long term future and global catastrophic risks
Gray goo
Human Compatible
Intelligence principle   Principle purporting a limit point of cultural evolution across civilizations
Lethal autonomous weapon
Paperclip maximizer
Philosophy of artificial intelligence
Robot ethics   In popular culture
Statement on AI risk of extinction
Superintelligence  Paths  Dangers  Strategies
Risk of astronomical suffering
System accident
Technological singularity

Notes edit 


  a b In a      lecture             Turing argued that  It seems probable that once the machine thinking method had started  it would not take long to outstrip our feeble powers  There would be no question of the machines dying  and they would be able to converse with each other to sharpen their wits  At some stage therefore we should have to expect the machines to take control  in the way that is mentioned in Samuel Butler s Erewhon   Also in a lecture broadcast on the BBC             he expressed the opinion   If a machine can think  it might think more intelligently than we do  and then where should we be  Even if we could keep the machines in a subservient position  for instance by turning off the power at strategic moments  we should  as a species  feel greatly humbled    This new danger    is certainly something which can give us anxiety  

  as interpreted by Seth Baum


References edit 


  a b c d e f g Russell  Stuart  Norvig  Peter                The Ethics and Risks of Developing Artificial Intelligence   Artificial Intelligence  A Modern Approach  Prentice Hall  ISBN                        

  Bostrom  Nick          Existential risks   Journal of Evolution and Technology              

  a b Turchin  Alexey  Denkenberger  David    May         Classification of global catastrophic risks connected with artificial intelligence   AI  amp  Society                   doi         s                  ISSN                 S CID               

  Bales  Adam  D Alessandro  William  Kirk Giannini  Cameron Domenico          Artificial Intelligence  Arguments for Catastrophic Risk   Philosophy Compass          arXiv             doi         phc        

  a b c d e f g h i j k l m n o p q Bostrom  Nick         Superintelligence  Paths  Dangers  Strategies  First      ed    Oxford University Press  ISBN                        

  Vynck  Gerrit De     May         The debate over whether AI will destroy us is dividing Silicon Valley   Washington Post  ISSN                 Retrieved    July      

  Metz  Cade     June         How Could A I  Destroy Humanity    The New York Times  ISSN                 Retrieved    July      

    Godfather of artificial intelligence  weighs in on the past and potential of AI   www cbsnews com     March       Retrieved    April      

   How Rogue AIs may Arise   yoshuabengio org     May       Retrieved    May      

  Turing  Alan         Intelligent machinery  a heretical theory  Speech   Lecture given to     Society   Manchester  The Turing Digital Archive  Archived from the original on    September       Retrieved    July      

  Turing  Alan     May         Can digital computers think    Automatic Calculating Machines  Episode    BBC  Can digital computers think  

  a b Parkin  Simon     June         Science fiction no more  Channel   s Humans and our rogue AI obsessions   The Guardian  Archived from the original on   February       Retrieved   February      

  a b Jackson  Sarah   The CEO of the company behind AI chatbot ChatGPT says the worst case scenario for artificial intelligence is  lights out for all of us    Business Insider  Retrieved    April      

   The AI Dilemma   www humanetech com  Retrieved    April           of AI researchers believe there s a     or greater chance that humans go extinct from our inability to control AI 

  a b       Expert Survey on Progress in AI   AI Impacts    August       Retrieved    April      

  Roose  Kevin     May         A I  Poses  Risk of Extinction   Industry Leaders Warn   The New York Times  ISSN                 Retrieved   June      

  Sunak  Rishi     June         Rishi Sunak Wants the U K  to Be a Key Player in Global AI Regulation   Time 

  a b Fung  Brian     July         UN Secretary General embraces calls for a new UN agency on AI in the face of  potentially catastrophic and existential risks    CNN Business  Retrieved    July      

  a b c d e Yudkowsky  Eliezer          Artificial Intelligence as a Positive and Negative Factor in Global Risk   PDF   Global Catastrophic Risks           Bibcode     gcr  book     Y  Archived  PDF  from the original on   March       Retrieved    August      

  Russell  Stuart  Dewey  Daniel  Tegmark  Max          Research Priorities for Robust and Beneficial Artificial Intelligence   PDF   AI Magazine  Association for the Advancement of Artificial Intelligence           arXiv             Bibcode     arXiv         R  Archived  PDF  from the original on   August       Retrieved    August        cited in  AI Open Letter   Future of Life Institute   Future of Life Institute  January       Archived from the original on    August       Retrieved   August      

  a b c Dowd  Maureen  April         Elon Musk s Billion Dollar Crusade to Stop the A I  Apocalypse   The Hive  Archived from the original on    July       Retrieved    November      

   AlphaGo Zero  Starting from scratch   www deepmind com     October       Retrieved    July      

  Breuer  Hans Peter   Samuel Butler s  the Book of the Machines  and the Argument from Design   Archived    March      at the Wayback Machine Modern Philology  Vol      No     May        pp          

  Turing  A M          Intelligent Machinery  A Heretical Theory         Reprinted Philosophia Mathematica                  doi         philmat         

  Hilliard  Mark          The AI apocalypse  will the human race soon be terminated    The Irish Times  Archived from the original on    May       Retrieved    March      

  I J  Good   Speculations Concerning the First Ultraintelligent Machine  Archived            at the Wayback Machine  HTML     Advances in Computers  vol          

  Russell  Stuart J   Norvig  Peter          Section       The Ethics and Risks of Developing Artificial Intelligence   Artificial Intelligence  A Modern Approach  Upper Saddle River  New Jersey  Prentice Hall  ISBN                         Similarly  Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal 

  Barrat  James         Our final invention  artificial intelligence and the end of the human era  First      ed    New York  St  Martin s Press  ISBN                         In the bio  playfully written in the third person  Good summarized his life s milestones  including a probably never before seen account of his work at Bletchley Park with Turing  But here s what he wrote in      about the first superintelligence  and his late in the game U turn   The paper   Speculations Concerning the First Ultra intelligent Machine           began   The survival of man depends on the early construction of an ultra intelligent machine   Those were his  Good s  words during the Cold War  and he now suspects that  survival  should be replaced by  extinction   He thinks that  because of international competition  we cannot prevent the machines from taking over  He thinks we are lemmings  He said also that  probably Man will construct the deus ex machina in his own image  

  Anderson  Kurt     November         Enthusiasts and Skeptics Debate Artificial Intelligence   Vanity Fair  Archived from the original on    January       Retrieved    January      

  Metz  Cade    June         Mark Zuckerberg  Elon Musk and the Feud Over Killer Robots   The New York Times  Archived from the original on    February       Retrieved   April      

  Hsu  Jeremy    March         Control dangerous AI before it controls us  one expert says   NBC News  Archived from the original on   February       Retrieved    January      

  a b c  Stephen Hawking   Transcendence looks at the implications of artificial intelligence        but are we taking AI seriously enough     The Independent  UK   Archived from the original on    September       Retrieved   December      

   Stephen Hawking warns artificial intelligence could end mankind   BBC    December       Archived from the original on    October       Retrieved   December      

  Eadicicco  Lisa     January         Bill Gates  Elon Musk Is Right  We Should All Be Scared Of Artificial Intelligence Wiping Out Humanity   Business Insider  Archived from the original on    February       Retrieved    January      

   Research Priorities for Robust and Beneficial Artificial Intelligence  an Open Letter   Future of Life Institute  Archived from the original on    January       Retrieved    October      

   Anticipating artificial intelligence   Nature                         Bibcode     Natur    Q       doi               a  ISSN                 PMID                S CID              

  Christian  Brian    October        The Alignment Problem  Machine Learning and Human Values  W  W  Norton  amp  Company  ISBN                         Archived from the original on   December       Retrieved   December      

  Dignum  Virginia     May         AI   the people and places that make  use and manage it   Nature                       Bibcode     Natur         D  doi         d                x  S CID                

   Elon Musk among experts urging a halt to AI training   BBC News     March       Retrieved   June      

   Statement on AI Risk   Center for AI Safety  Retrieved   June      

   Artificial intelligence could lead to extinction  experts warn   BBC News     May       Retrieved   June      

   DeepMind and Google  the battle to control artificial intelligence   The Economist  ISSN                 Retrieved    July      

   AI timelines  What do experts in artificial intelligence expect for the future    Our World in Data  Retrieved    July      

  De Vynck  Gerrit     May         The debate over whether AI will destroy us is dividing Silicon Valley   The Washington Post 

    The Godfather of A I   just quit Google and says he regrets his life s work because it can be hard to stop  bad actors from using it for bad things    Fortune  Retrieved    July      

   Super speeds for super AI  Frontier sets new pace for artificial intelligence   ORNL     November       Retrieved    September      

   Everything you need to know about superintelligence   Spiceworks  Retrieved    July      

  a b c Babauta  Leo   A Valuable New Book Explores The Potential Impacts Of Intelligent Machines On Human Life   Business Insider  Retrieved    March      

  a b Bostrom  Nick     April        What happens when our computers get smarter than we are   retrieved    July      

   Governance of superintelligence   openai com  Retrieved    July      

   Overcoming Bias  I Still Don t Get Foom   www overcomingbias com  Archived from the original on   August       Retrieved    September      

  Cotton Barratt  Owen  Ord  Toby     August         Strategic considerations about different speeds of AI takeoff   The Future of Humanity Institute  Retrieved    July      

  Tegmark  Max     April         The  Don t Look Up  Thinking That Could Doom Us With AI   Time  Retrieved    July       As if losing control to Chinese minds were scarier than losing control to alien digital minds that don t care about humans        it s clear by now that the space of possible alien minds is vastly larger than that 

        Mechanistic Interpretability with Neel Nanda   AXRP   the AI X risk Research Podcast    February       Retrieved    July       it s plausible to me that the main thing we need to get done is noticing specific circuits to do with deception and specific dangerous capabilities like that and situational awareness and internally represented goals 

   Superintelligence Is Not Omniscience   AI Impacts    April       Retrieved    April      

  a b c d e f g h i Hendrycks  Dan  Mazeika  Mantas  Woodside  Thomas     June         An Overview of Catastrophic AI Risks   arXiv             cs CY  

  Taylor  Josh  Hern  Alex    May          Godfather of AI  Geoffrey Hinton quits Google and warns over dangers of misinformation   The Guardian  ISSN                 Retrieved    July      

   How NATO is preparing for a new era of AI cyber attacks   euronews     December       Retrieved    July      

   ChatGPT and the new AI are wreaking havoc on cybersecurity in exciting and frightening ways   ZDNET  Retrieved    July      

  Toby Shevlane  Sebastian Farquhar  Ben Garfinkel  Mary Phuong  Jess Whittlestone  Jade Leung  Daniel Kokotajlo  Nahema Marchal  Markus Anderljung  Noam Kolt  Lewis Ho  Divya Siddarth  Shahar Avin  Will Hawkins  Been Kim  Iason Gabriel  Vijay Bolina  Jack Clark  Yoshua Bengio  Paul Christiano  Allan Dafoe     May         Model evaluation for extreme risks   arXiv             cs AI  

  Urbina  Fabio  Lentzos  Filippa  Invernizzi  C dric  Ekins  Sean    March         Dual use of artificial intelligence powered drug discovery   Nature Machine Intelligence                  doi         s                   ISSN                 PMC               PMID               

  Walter  Yoshija     March         The rapid competitive economy of machine learning development  a discussion on the social risks and benefits   AI and Ethics            doi         s                  

   The AI Arms Race Is On  Start Worrying   Time     February       Retrieved    July      

  Brimelow  Ben   The short film  Slaughterbots  depicts a dystopian future of killer drones swarming the world   Business Insider  Retrieved    July      

  Mecklin  John     July          Artificial Escalation   Imagining the future of nuclear risk   Bulletin of the Atomic Scientists  Retrieved    July      

  Bostrom  Nick          Existential Risk Prevention as Global Priority   PDF   Global Policy               doi                                 via Existential Risk 

  Doherty  Ben     May         Climate change an  existential security risk  to Australia  Senate inquiry says   The Guardian  ISSN                 Retrieved    July      

  MacAskill  William         What we owe the future  New York  New York  Basic Books  ISBN                        

  a b c d Ord  Toby          Chapter    Future Risks  Unaligned Artificial Intelligence   The Precipice  Existential Risk and the Future of Humanity  Bloomsbury Publishing  ISBN                        

  McMillan  Tim     March         Navigating Humanity s Greatest Challenge Yet  Experts Debate the Existential Risks of AI   The Debrief  Retrieved    September      

  Kasirzadeh  Atoosa          Two Types of AI Existential Risk  Decisive and Accumulative   arXiv             cs CR  

  Samuelsson  Paul Conrad  June July         Artificial Consciousness  Our Greatest Ethical Challenge   Philosophy Now  No            Retrieved    August      

  Kateman  Brian     July         AI Should Be Terrified of Humans   Time  Retrieved    August      

  Sotala  Kaj  Gloor  Lukas          Superintelligence as a Cause or Cure for Risks of Astronomical Suffering   PDF   Informatica 

  Fisher  Richard   The intelligent monster that you should let eat you   www bbc com  Retrieved    August      

  More  Max     June         Existential Risk vs  Existential Opportunity  A balanced approach to AI risk   Extropic Thoughts  Retrieved    July      

  Omohundro  S  M         February   The basic AI drives  In AGI  Vol       pp           

  Wakefield  Jane     September         Why is Facebook investing in AI    BBC News  Archived from the original on   December       Retrieved    November      

  Yudkowsky  Eliezer          Complex Value Systems are Required to Realize Valuable Futures   PDF   Archived  PDF  from the original on    September       Retrieved    August      

  a b Ord  Toby         The Precipice  Existential Risk and the Future of Humanity  Bloomsbury Publishing Plc  ISBN                        

  Yudkowsky  E         August   Complex value systems in friendly AI  In International Conference on Artificial General Intelligence  pp            Germany  Springer  Berlin  Heidelberg 

  Russell  Stuart          Of Myths and Moonshine   Edge  Archived from the original on    July       Retrieved    October      

  Dietterich  Thomas  Horvitz  Eric          Rise of Concerns about AI  Reflections and Directions   PDF   Communications of the ACM                  doi                  S CID                Archived  PDF  from the original on   March       Retrieved    October      

  a b Yudkowsky  Eliezer     March         The Open Letter on AI Doesn t Go Far Enough   Time  Retrieved    July      

  Bostrom  Nick    May         The Superintelligent Will  Motivation and Instrumental Rationality in Advanced Artificial Agents   Minds and Machines                 doi         s                  ISSN                 S CID                 as long as they possess a sufficient level of intelligence  agents having any of a wide range of final goals will pursue similar intermediary goals because they have instrumental reasons to do so 

  Ngo  Richard  Chan  Lawrence  S ren Mindermann     February         The alignment problem from a deep learning perspective   arXiv             cs AI  

   Introducing Superalignment   openai com  Retrieved    July      

   OpenAI dissolves Superalignment AI safety team   cnbc com  Retrieved   January      

  Tilli  Cecilia     April         Killer Robots  Lost Jobs    Slate  Archived from the original on    May       Retrieved    May      

   Norvig vs  Chomsky and the Fight for the Future of AI   Tor com     June       Archived from the original on    May       Retrieved    May      

  Graves  Matthew    November         Why We Should Be Concerned About Artificial Superintelligence   Skeptic  US magazine   Vol           no          Archived from the original on    November       Retrieved    November      

  Johnson  Phil     July         Houston  we have a bug    famous software glitches in space   IT World  Archived from the original on    February       Retrieved   February      

  Yampolskiy  Roman V     April         Utility function security in artificially intelligent agents   Journal of Experimental  amp  Theoretical Artificial Intelligence                   doi                X              S CID                Nothing precludes sufficiently smart self improving systems from optimising their reward mechanisms in order to optimisetheir current goal achievement and in the process making a mistake leading to corruption of their reward functions 

   Will artificial intelligence destroy humanity  Here are   reasons not to worry   Vox     August       Archived from the original on    October       Retrieved    October      

  Bostrom  Nick         Superintelligence  Paths  Dangers  Strategies  Oxford  United Kingdom  Oxford University Press  p            ISBN                        

  Bostrom  Nick          Superintelligent Will   PDF   Nick Bostrom  Archived  PDF  from the original on    November       Retrieved    October      

  Armstrong  Stuart    January         General Purpose Intelligence  Arguing the Orthogonality Thesis   Analysis and Metaphysics      Archived from the original on    October       Retrieved   April       Full text available here Archived    March      at the Wayback Machine 

  a b Chorost  Michael     April         Let Artificial Intelligence Evolve   Slate  Archived from the original on    November       Retrieved    November      

   Should humans fear the rise of the machine    The Telegraph  UK     September       Archived from the original on    January       Retrieved   February      

  a b Shermer  Michael    March         Apocalypse AI   Scientific American               Bibcode     SciAm    c    S  doi         scientificamerican         PMID                Archived from the original on   December       Retrieved    November      

   Intelligent Machines  What does Facebook want with AI    BBC News     September       Retrieved    March      

  Baum  Seth     September         Countering Superintelligence Misinformation   Information               doi         info         ISSN                

   The Myth Of AI   www edge org  Archived from the original on    March       Retrieved    March      

  Bostrom  Nick  Superintelligence  paths  dangers  strategies  Audiobook   ISBN                         OCLC                 

  Sotala  Kaj  Yampolskiy  Roman V     December         Responses to catastrophic AGI risk  a survey   Physica Scripta              Bibcode     PhyS     a    S  doi                                ISSN                

  Pistono  Federico  Yampolskiy  Roman V     May        Unethical Research  How to Create a Malevolent Artificial Intelligence  OCLC                 

  Haney  Brian Seamus          The Perils  amp  Promises of Artificial General Intelligence   SSRN Working Paper Series  doi         ssrn          ISSN                 S CID               

  Pillay  Tharin     December         New Tests Reveal AI s Capacity for Deception   TIME  Retrieved    January      

  Perrigo  Billy     December         Exclusive  New Research Shows AI Strategically Lying   TIME  Retrieved    January      

  Greenblatt  Ryan  Denison  Carson  Wright  Benjamin  Roger  Fabien  MacDiarmid  Monte  Marks  Sam  Treutlein  Johannes  Belonax  Tim  Chen  Jack     December        Alignment faking in large language models  arXiv             retrieved    January     

   Will Superintelligent AIs Be Our Doom    IEEE Spectrum    September       Retrieved    September      

  Russell  Stuart     August         Artificial intelligence  The future is superintelligent   Nature                       Bibcode     Natur         R  doi               a  S CID              

  a b c Tegmark  Max         Life      Being Human in the Age of Artificial Intelligence   st      ed    Mainstreaming AI Safety  Knopf  ISBN                        

  Kumar  Vibhore   Council Post  At The Dawn Of Artificial General Intelligence  Balancing Abundance With Existential Safeguards   Forbes  Retrieved    July      

  a b  Pause Giant AI Experiments  An Open Letter   Future of Life Institute  Retrieved    March      

   AI Principles   Future of Life Institute     August       Archived from the original on    December       Retrieved    December      

   Elon Musk and Stephen Hawking warn of artificial intelligence arms race   Newsweek     January       Archived from the original on    December       Retrieved    December      

  Ford  Martin          Chapter    Super intelligence and the Singularity   Rise of the Robots  Technology and the Threat of a Jobless Future  Basic Books  ISBN                        

  Bostrom  Nick          New Epilogue to the Paperback Edition   Superintelligence  Paths  Dangers  Strategies  Paperback      ed   

   Why Uncontrollable AI Looks More Likely Than Ever   Time     February       Retrieved    March       It is therefore no surprise that according to the most recent AI Impacts Survey  nearly half of     leading AI researchers think there is at least a     chance that human level AI would lead to an  extremely negative outcome   or existential risk 

   IMD creates AI Safety Clock   www imd org    September       Retrieved   September      

  Constantino  Tor     February         AI  Doomsday Clock  Ticks Closer To Uncontrolled Super AI   Forbes  Retrieved    February      

  a b Maas  Matthijs M     February         How viable is international arms control for military artificial intelligence  Three lessons from nuclear weapons of mass destruction   Contemporary Security Policy                   doi                                ISSN                 S CID                

  a b  Impressed by artificial intelligence  Experts say AGI is coming next  and it has  existential  risks   ABC News     March       Retrieved    March      

  Rawlinson  Kevin     January         Microsoft s Bill Gates insists AI is a threat   BBC News  Archived from the original on    January       Retrieved    January      

  Washington Post     December         Tech titans like Elon Musk are spending    billion to save you from terminators   Chicago Tribune  Archived from the original on   June      

   Doomsday to utopia  Meet AI s rival factions   Washington Post    April       Retrieved    April      

   UC Berkeley   Center for Human Compatible AI          Open Philanthropy     June       Retrieved    April      

   The mysterious artificial intelligence company Elon Musk invested in is developing game changing smart computers   Tech Insider  Archived from the original on    October       Retrieved    October      

  Clark     a 

   Elon Musk Is Donating    M Of His Own Money To Artificial Intelligence Research   Fast Company     January       Archived from the original on    October       Retrieved    October      

  Tilli  Cecilia     April         Killer Robots  Lost Jobs    Slate  Archived from the original on    May       Retrieved    May      

  Khatchadourian  Raffi     November         The Doomsday Invention  Will artificial intelligence bring us utopia or destruction    The New Yorker  Archived from the original on    April       Retrieved   February      

   Warning of AI s danger  pioneer Geoffrey Hinton quits Google to speak freely   www arstechnica com        Retrieved    July      

  Garling  Caleb    May         Andrew Ng  Why  Deep Learning  Is a Mandate for Humans  Not Just Machines   Wired  Retrieved    March      

   Is artificial intelligence really an existential threat to humanity    MambaPost    April      

   The case against killer robots  from a guy actually working on artificial intelligence   Fusion net  Archived from the original on   February       Retrieved    January      

   AI experts challenge  doomer  narrative  including  extinction risk  claims   VentureBeat     May       Retrieved   July      

  Coldewey  Devin    April         Ethicists fire back at  AI Pause  letter they say  ignores the actual harms    TechCrunch  Retrieved    July      

   DAIR  Distributed AI Research Institute    DAIR Institute  Retrieved    July      

  Kelly  Kevin     April         The Myth of a Superhuman AI   Wired  Archived from the original on    December       Retrieved    February      

  Jindal  Siddharth    July         OpenAI s Pursuit of AI Alignment is Farfetched   Analytics India Magazine  Retrieved    July      

   Mark Zuckerberg responds to Elon Musk s paranoia about AI   AI is going to    help keep our communities safe     Business Insider     May       Archived from the original on   May       Retrieved   May      

  Dadich  Scott   Barack Obama Talks AI  Robo Cars  and the Future of the World   WIRED  Archived from the original on   December       Retrieved    November      

  Kircher  Madison Malone   Obama on the Risks of AI   You Just Gotta Have Somebody Close to the Power Cord    Select All  Archived from the original on   December       Retrieved    November      

  Clinton  Hillary         What Happened  Simon and Schuster  p            ISBN                         via     Archived   December      at the Wayback Machine

   Elon Musk says AI could doom human civilization  Zuckerberg disagrees  Who s right      January       Archived from the original on   January       Retrieved   January      

   AI doomsday worries many Americans  So does apocalypse from climate change  nukes  war  and more      April       Archived from the original on    June       Retrieved   July      

  Tyson  Alec  Kikuchi  Emma     August         Growing public concern about the role of artificial intelligence in daily life   Pew Research Center  Retrieved    September      

  Sotala  Kaj  Yampolskiy  Roman     December         Responses to catastrophic AGI risk  a survey   Physica Scripta         

  Barrett  Anthony M   Baum  Seth D      May         A model of pathways to artificial superintelligence catastrophe for risk and decision analysis   Journal of Experimental  amp  Theoretical Artificial Intelligence                   arXiv             doi                x               ISSN              X  S CID             

  Sotala  Kaj  Yampolskiy  Roman V     December         Responses to catastrophic AGI risk  a survey   Physica Scripta                  Bibcode     PhyS     a    S  doi                                ISSN                 S CID              

  Ramamoorthy  Anand  Yampolskiy  Roman          Beyond MAD  The race for artificial general intelligence   ICT Discoveries     Special Issue     ITU       Archived from the original on   January       Retrieved   January      

  Carayannis  Elias G   Draper  John     January         Optimising peace through a Universal Global Peace Treaty to constrain the risk of war from a militarised artificial superintelligence   AI  amp  Society                     doi         s                y  ISSN                 PMC               PMID                S CID                

  Carayannis  Elias G   Draper  John     May         The challenge of advanced cyberwar and the place of cyberpeace   The Elgar Companion to Digital Transformation  Artificial Intelligence and Innovation in the Economy  Society and Democracy  Edward Elgar Publishing  pp              doi                              ISBN                         retrieved   June      

  Vincent  James     June         Google s AI researchers say these are the five key problems for robot safety   The Verge  Archived from the original on    December       Retrieved   April      

  Amodei  Dario  Chris Olah  Jacob Steinhardt  Paul Christiano  John Schulman  and Dan Man    Concrete problems in AI safety   arXiv preprint arXiv                   

  Johnson  Alex          Elon Musk wants to hook your brain up directly to computers   starting next year   NBC News  Archived from the original on    April       Retrieved   April      

  Torres  Phil     September         Only Radically Enhancing Humanity Can Save Us All   Slate Magazine  Archived from the original on   August       Retrieved   April      

  Barrett  Anthony M   Baum  Seth D      May         A model of pathways to artificial superintelligence catastrophe for risk and decision analysis   Journal of Experimental  amp  Theoretical Artificial Intelligence                   arXiv             doi                X               S CID             

  Tkachenko  Yegor          Position  Enforced Amnesia as a Way to Mitigate the Potential Risk of Silent Suffering in the Conscious AI   Proceedings of the   st International Conference on Machine Learning  PMLR              

  Piper  Kelsey     March         How to test what an AI model can   and shouldn t   do   Vox  Retrieved    July      

  Piesing  Mark     May         AI uprising  humans will be outsourced  not obliterated   Wired  Archived from the original on   April       Retrieved    December      

  Coughlan  Sean     April         How are humans going to become extinct    BBC News  Archived from the original on   March       Retrieved    March      

  Bridge  Mark     June         Making robots less confident could prevent them taking over   The Times  Archived from the original on    March       Retrieved    March      

  McGinnis  John  Summer         Accelerating AI   Northwestern University Law Review                      Archived from the original on    February       Retrieved    July       For all these reasons  verifying a global relinquishment treaty  or even one limited to AI related weapons development  is a nonstarter     For different reasons from ours  the Machine Intelligence Research Institute  considers  AGI  relinquishment infeasible   

  Sotala  Kaj  Yampolskiy  Roman     December         Responses to catastrophic AGI risk  a survey   Physica Scripta          In general  most writers reject proposals for broad relinquishment    Relinquishment proposals suffer from many of the same problems as regulation proposals  but to a greater extent  There is no historical precedent of general  multi use technology similar to AGI being successfully relinquished for good  nor do there seem to be any theoretical reasons for believing that relinquishment proposals would work in the future  Therefore we do not consider them to be a viable class of proposals 

  Allenby  Brad     April         The Wrong Cognitive Measuring Stick   Slate  Archived from the original on    May       Retrieved    May       It is fantasy to suggest that the accelerating development and deployment of technologies that taken together are considered to be A I  will be stopped or limited  either by regulation or even by national legislation 

  a b Yampolskiy  Roman V           AI Risk Skepticism   In M ller  Vincent C   ed    Philosophy and Theory of Artificial Intelligence       Studies in Applied Philosophy  Epistemology and Rational Ethics  Vol           Cham  Springer International Publishing  pp                doi                               ISBN                        

  Baum  Seth     August         Superintelligence Skepticism as a Political Tool   Information              doi         info         ISSN                

   Elon Musk and other tech leaders call for pause in  out of control  AI race   CNN     March       Retrieved    March      

   Open letter calling for AI  pause  shines light on fierce debate around risks vs  hype   VentureBeat     March       Retrieved    July      

  Vincent  James     April         OpenAI s CEO confirms the company isn t training GPT   and  won t for some time    The Verge  Retrieved    July      

   The Open Letter on AI Doesn t Go Far Enough   Time     March       Retrieved    July      

  Reynolds  Matt   Protesters Are Fighting to Stop AI  but They re Split on How to Do It   Wired  ISSN                 Retrieved    April      

  Domonoske  Camila     July         Elon Musk Warns Governors  Artificial Intelligence Poses  Existential Risk    NPR  Archived from the original on    April       Retrieved    November      

  Gibbs  Samuel     July         Elon Musk  regulate AI to combat  existential threat  before it s too late   The Guardian  Archived from the original on   June       Retrieved    November      

  Kharpal  Arjun    November         A I  is in its  infancy  and it s too early to regulate it  Intel CEO Brian Krzanich says   CNBC  Archived from the original on    March       Retrieved    November      

  Dawes  James     December         UN fails to agree on  killer robot  ban as nations pour billions into autonomous weapons research   The Conversation  Retrieved    July      

  a b Fassihi  Farnaz     July         U N  Officials Urge Regulation of Artificial Intelligence   The New York Times  ISSN                 Retrieved    July      

   International Community Must Urgently Confront New Reality of Generative  Artificial Intelligence  Speakers Stress as Security Council Debates Risks  Rewards   United Nations  Retrieved    July      

  Sotala  Kaj  Yampolskiy  Roman V      December         Responses to catastrophic AGI risk  a survey   Physica Scripta                  Bibcode     PhyS     a    S  doi                                ISSN                

  Geist  Edward Moore     August         It s already too late to stop the AI arms race We must manage it instead   Bulletin of the Atomic Scientists                   Bibcode     BuAtS    e    G  doi                                ISSN                 S CID                

   Amazon  Google  Meta  Microsoft and other tech firms agree to AI safeguards set by the White House   AP News     July       Retrieved    July      

   Amazon  Google  Meta  Microsoft and other firms agree to AI safeguards   Redditch Advertiser     July       Retrieved    July      

  The White House     October         Executive Order on the Safe  Secure  and Trustworthy Development and Use of Artificial Intelligence   The White House  Retrieved    December      


Bibliography edit 
Clark  Jack      a    Musk Backed Group Probes Risks Behind Artificial Intelligence   Bloomberg com  Archived from the original on    October       Retrieved    October      

vteExistential risk from artificial intelligenceConcepts
AGI
AI alignment
AI capability control
AI safety
AI takeover
Consequentialism
Effective accelerationism
Ethics of artificial intelligence
Existential risk from artificial intelligence
Friendly artificial intelligence
Instrumental convergence
Vulnerable world hypothesis
Intelligence explosion
Longtermism
Machine ethics
Suffering risks
Superintelligence
Technological singularity
Organizations
Alignment Research Center
Center for AI Safety
Center for Applied Rationality
Center for Human Compatible Artificial Intelligence
Centre for the Study of Existential Risk
EleutherAI
Future of Humanity Institute
Future of Life Institute
Google DeepMind
Humanity 
Institute for Ethics and Emerging Technologies
Leverhulme Centre for the Future of Intelligence
Machine Intelligence Research Institute
OpenAI
People
Scott Alexander
Sam Altman
Yoshua Bengio
Nick Bostrom
Paul Christiano
Eric Drexler
Sam Harris
Stephen Hawking
Dan Hendrycks
Geoffrey Hinton
Bill Joy
Shane Legg
Elon Musk
Steve Omohundro
Huw Price
Martin Rees
Stuart J  Russell
Jaan Tallinn
Max Tegmark
Frank Wilczek
Roman Yampolskiy
Eliezer Yudkowsky
Other
Statement on AI risk of extinction
Human Compatible
Open letter on artificial intelligence       
Our Final Invention
The Precipice
Superintelligence  Paths  Dangers  Strategies
Do You Trust This Computer 
Artificial Intelligence Act
 Category
vteEffective altruismConcepts
Aid effectiveness
Charity assessment
Demandingness objection
Disability adjusted life year
Disease burden
Distributional cost effectiveness analysis
Earning to give
Equal consideration of interests
Longtermism
Marginal utility
Moral circle expansion
Psychological barriers to effective altruism
Quality adjusted life year
Utilitarianism
Venture philanthropy
Key figures
Sam Bankman Fried
Liv Boeree
Nick Bostrom
Hilary Greaves
Holden Karnofsky
William MacAskill
Dustin Moskovitz
Yew Kwang Ng
Toby Ord
Derek Parfit
Peter Singer
Cari Tuna
Eliezer Yudkowsky
Organizations
       Hours
Against Malaria Foundation
Animal Charity Evaluators
Animal Ethics
Centre for Effective Altruism
Centre for Enabling EA Learning  amp  Research
Center for High Impact Philanthropy
Centre for the Study of Existential Risk
Development Media International
Evidence Action
Faunalytics
Fistula Foundation
Future of Humanity Institute
Future of Life Institute
Founders Pledge
GiveDirectly
GiveWell
Giving Multiplier
Giving What We Can
Good Food Fund
The Good Food Institute
Good Ventures
The Humane League
Mercy for Animals
Machine Intelligence Research Institute
Malaria Consortium
Open Philanthropy
Raising for Effective Giving
Sentience Institute
Unlimit Health
Wild Animal Initiative
Focus areas
Biotechnology risk
Climate change
Cultured meat
Economic stability
Existential risk from artificial general intelligence
Global catastrophic risk
Global health
Global poverty
Intensive animal farming
Land use reform
Life extension
Malaria prevention
Mass deworming
Neglected tropical diseases
Risk of astronomical suffering
Wild animal suffering
Literature
Doing Good Better
The End of Animal Farming
Famine  Affluence  and Morality
The Life You Can Save
Living High and Letting Die
The Most Good You Can Do
Practical Ethics
The Precipice
Superintelligence  Paths  Dangers  Strategies
What We Owe the Future
Events
Effective Altruism Global

vteGlobal catastrophic risks
Future of the Earth
Future of an expanding universe
Ultimate fate of the universe
Human extinction risk estimates
Technological
Chemical warfare
Cyberattack
Cyberwarfare
Cyberterrorism
Cybergeddon
Ransomware
Gray goo
Nanoweapons
Kinetic bombardment
Kinetic energy weapon
Nuclear warfare
Mutual assured destruction
Dead Hand
Doomsday Clock
Doomsday device
Antimatter weapon
Electromagnetic pulse  EMP 
Safety of high energy particle collision experiments
Micro black hole
Strangelet
Synthetic intelligence   Artificial intelligence
AI takeover
Existential risk from artificial intelligence
Technological singularity
Transhumanism
Sociological
Anthropogenic hazard
Collapsology
Doomsday argument
Self indication assumption doomsday argument rebuttal
Self referencing doomsday argument rebuttal
Economic collapse
Malthusian catastrophe
New World Order  conspiracy theory 
Nuclear holocaust
cobalt
famine
winter
Riots
Social crisis
Societal collapse
State collapse
World War III
EcologicalClimate change
Anoxic event
Biodiversity loss
Mass mortality event
Cascade effect
Cataclysmic pole shift hypothesis
Deforestation
Desertification
Plant or animal species extinctions
Civilizational collapse
Tipping points
Climate sensitivity
Flood basalt
Global dimming
Global terrestrial stilling
Global warming
Hypercane
Ice age
Ecocide
Ecological collapse
Environmental degradation
Habitat destruction
Human impact on the environment
coral reefs
on marine life
Land degradation
Land consumption
Land surface effects on climate
Ocean acidification
Ozone depletion
Resource depletion
Sea level rise
Supervolcano
winter
Verneshot
Water pollution
Water scarcity
Earth Overshoot Day
Overexploitation
Overpopulation
Human overpopulation
BiologicalExtinction
Extinction event
Holocene extinction
Human extinction
List of extinction events
Genetic erosion
Genetic pollution
Others
Biodiversity loss
Decline in amphibian populations
Decline in insect populations
Biotechnology risk
Biological agent
Biological warfare
Bioterrorism
Colony collapse disorder
Defaunation
Dysgenics
Interplanetary contamination
Pandemic
Pollinator decline
Overfishing
Astronomical
Big Crunch
Big Rip
Coronal mass ejection
Cosmological phase transition
Geomagnetic storm
False vacuum decay
Gamma ray burst
Heat death of the universe
Proton decay
Virtual black hole
Impact event
Asteroid impact avoidance
Asteroid impact prediction
Potentially hazardous object
Near Earth object
winter
Rogue planet
Rogue star
Near Earth supernova
Hypernova
Micronova
Solar flare
Stellar collision
Eschatological
Buddhist
Maitreya
Three Ages
Hindu
Kalki
Kali Yuga
Last Judgement
Second Coming
  Enoch
Daniel
Abomination of desolation
Prophecy of Seventy Weeks
Messiah
Christian
Futurism
Historicism
Interpretations of Revelation
 Idealism
Preterism
  Esdras
  Thessalonians
Man of sin
Katechon
Antichrist
Book of Revelation
Events
Four Horsemen of the Apocalypse
Lake of fire
Number of the Beast
Seven bowls
Seven seals
The Beast
Two witnesses
War in Heaven
Whore of Babylon
Great Apostasy
New Earth
New Jerusalem
Olivet Discourse
Great Tribulation
Son of perdition
Sheep and Goats
Islamic
Al Qa im
Beast of the Earth
Dhu al Qarnayn
Dhul Suwayqatayn
Dajjal
Israfil
Mahdi
Sufyani
Jewish
Messiah
War of Gog and Magog
Third Temple
Norse
Zoroastrian
Saoshyant
Others
     end times prediction
     phenomenon
Apocalypse
Apocalyptic literature
Apocalypticism
Armageddon
Blood moon prophecy
Earth Changes
End time
Gog and Magog
List of dates predicted for apocalyptic events
Messianism
Messianic Age
Millenarianism
Millennialism
Premillennialism
Amillennialism
Postmillennialism
Nemesis  hypothetical star 
Nibiru cataclysm
Rapture
Prewrath
Posttribulation rapture
Resurrection of the dead
Vulnerable world hypothesis
World to come
Fictional
Alien invasion
Apocalyptic and post apocalyptic fiction
List of apocalyptic and post apocalyptic fiction
List of apocalyptic films
Climate fiction
Disaster films
List of disaster films
Zombie apocalypse
Zombie
Organizations
Centre for the Study of Existential Risk
Future of Humanity Institute
Future of Life Institute
Nuclear Threat Initiative
General
Disaster
Depression
Financial crisis
Survivalism

 World     portal
 Categories
Apocalypticism
Future problems
Hazards
Risk analysis
Doomsday scenarios






Retrieved from  https   en wikipedia org w index php title Existential risk from artificial intelligence amp oldid