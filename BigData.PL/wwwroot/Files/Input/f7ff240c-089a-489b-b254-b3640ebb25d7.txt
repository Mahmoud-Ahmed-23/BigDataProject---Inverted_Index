Class of algorithms for pattern analysis
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
In machine learning  kernel machines are a class of algorithms for pattern analysis  whose best known member is the support vector machine  SVM   These methods involve using linear classifiers to solve nonlinear problems             The general task of pattern analysis is to find and study general types of relations  for example clusters  rankings  principal components  correlations  classifications  in datasets  For many algorithms that solve these tasks  the data in raw representation have to be explicitly transformed into feature vector representations via a user specified feature map  in contrast  kernel methods require only a user specified kernel  i e   a similarity function over all pairs of data points computed using inner products  The feature map in kernel machines is infinite dimensional but only requires a finite dimensional matrix from user input according to the representer theorem  Kernel machines are slow to compute for datasets larger than a couple of thousand examples without parallel processing  
Kernel methods  owe their name to the use of kernel functions  which enable them to operate in a high dimensional  implicit feature space without ever computing the coordinates of the data in that space  but rather by simply computing the inner products between the images of all pairs of data in the feature space   This operation is often computationally cheaper than the explicit computation of the coordinates   This approach is called the  kernel trick              Kernel functions have been introduced for sequence data  graphs  text  images  as well as vectors 
Algorithms capable of operating with kernels include the kernel perceptron  support vector machines  SVM   Gaussian processes  principal components analysis  PCA   canonical correlation analysis  ridge regression  spectral clustering  linear adaptive filters and many others 
Most kernel algorithms are based on convex optimization or eigenproblems and are statistically well founded  Typically  their statistical properties are analyzed using statistical learning theory  for example  using Rademacher complexity  


Motivation and informal explanation edit 
Kernel methods can be thought of as instance based learners  rather than learning some fixed set of parameters corresponding to the features of their inputs  they instead  remember  the 
  
    
      
        i
      
    
      displaystyle i 
  
 th training example 
  
    
      
         
        
          
            x
          
          
            i
          
        
         
        
          y
          
            i
          
        
         
      
    
      displaystyle   mathbf  x    i  y  i   
  
 and learn for it a corresponding weight 
  
    
      
        
          w
          
            i
          
        
      
    
      displaystyle w  i  
  
   Prediction for unlabeled inputs  i e   those not in the training set  is treated by the application of a similarity function 
  
    
      
        k
      
    
      displaystyle k 
  
  called a kernel  between the unlabeled input 
  
    
      
        
          
            x
              x     
          
        
      
    
      displaystyle  mathbf  x    
  
 and each of the training inputs 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
      displaystyle  mathbf  x    i  
  
   For instance  a kernelized binary classifier typically computes a weighted sum of similarities

  
    
      
        
          
            
              y
                x e 
            
          
        
         
        sgn
          x     
        
            x     
          
            i
             
             
          
          
            n
          
        
        
          w
          
            i
          
        
        
          y
          
            i
          
        
        k
         
        
          
            x
          
          
            i
          
        
         
        
          
            x
              x     
          
        
         
         
      
    
      displaystyle   hat  y    operatorname  sgn   sum   i     n w  i y  i k  mathbf  x    i   mathbf  x      
  

where


  
    
      
        
          
            
              y
                x e 
            
          
        
          x     
         
          x     
         
         
         
         
         
      
    
      displaystyle   hat  y   in           
  
 is the kernelized binary classifier s predicted label for the unlabeled input 
  
    
      
        
          
            x
              x     
          
        
      
    
      displaystyle  mathbf  x    
  
 whose hidden true label 
  
    
      
        y
      
    
      displaystyle y 
  
 is of interest 

  
    
      
        k
          x a 
        
          
            X
          
        
          xd  
        
          
            X
          
        
          x     
        
          R
        
      
    
      displaystyle k colon   mathcal  X   times   mathcal  X   to  mathbb  R   
  
 is the kernel function that measures similarity between any pair of inputs 
  
    
      
        
          x
        
         
        
          
            x
              x     
          
        
          x     
        
          
            X
          
        
      
    
      displaystyle  mathbf  x    mathbf  x    in   mathcal  X   
  
 
the sum ranges over the n labeled examples 
  
    
      
         
         
        
          
            x
          
          
            i
          
        
         
        
          y
          
            i
          
        
         
        
           
          
            i
             
             
          
          
            n
          
        
      
    
      displaystyle     mathbf  x    i  y  i      i     n  
  
 in the classifier s training set  with 
  
    
      
        
          y
          
            i
          
        
          x     
         
          x     
         
         
         
         
         
      
    
      displaystyle y  i  in           
  
 
the 
  
    
      
        
          w
          
            i
          
        
          x     
        
          R
        
      
    
      displaystyle w  i  in  mathbb  R   
  
 are the weights for the training examples  as determined by the learning algorithm 
the sign function 
  
    
      
        sgn
      
    
      displaystyle  operatorname  sgn   
  
 determines whether the predicted classification 
  
    
      
        
          
            
              y
                x e 
            
          
        
      
    
      displaystyle   hat  y   
  
 comes out positive or negative 
Kernel classifiers were described as early as the     s  with the invention of the kernel perceptron             They rose to great prominence with the popularity of the support vector machine  SVM  in the     s  when the SVM was found to be competitive with neural networks on tasks such as handwriting recognition 

Mathematics  the kernel trick edit 
SVM with feature map given by 
  
    
      
          x c  
         
         
        a
         
        b
         
         
         
         
        a
         
        b
         
        
          a
          
             
          
        
         
        
          b
          
             
          
        
         
      
    
      displaystyle  varphi   a b    a b a     b      
  
 and thus with the kernel function 
  
    
      
        k
         
        
          x
        
         
        
          y
        
         
         
        
          x
        
          x  c  
        
          y
        
         
        
          
              x     
            
              x
            
              x     
          
          
             
          
        
        
          
              x     
            
              y
            
              x     
          
          
             
          
        
      
    
      displaystyle k  mathbf  x    mathbf  y     mathbf  x   cdot  mathbf  y    left   mathbf  x   right       left   mathbf  y   right       
  
  The training points are mapped to a   dimensional space where a separating hyperplane can be easily found 
The kernel trick avoids the explicit mapping that is needed to get linear learning algorithms to learn a nonlinear function or decision boundary   For all 
  
    
      
        
          x
        
      
    
      displaystyle  mathbf  x   
  
 and 
  
    
      
        
          
            x
              x     
          
        
      
    
      displaystyle  mathbf  x    
  
 in the input space 
  
    
      
        
          
            X
          
        
      
    
      displaystyle   mathcal  X   
  
  certain functions 
  
    
      
        k
         
        
          x
        
         
        
          
            x
              x     
          
        
         
      
    
      displaystyle k  mathbf  x    mathbf  x     
  
 can be expressed as an inner product in another space 
  
    
      
        
          
            V
          
        
      
    
      displaystyle   mathcal  V   
  
  The function 
  
    
      
        k
          x a 
        
          
            X
          
        
          xd  
        
          
            X
          
        
          x     
        
          R
        
      
    
      displaystyle k colon   mathcal  X   times   mathcal  X   to  mathbb  R   
  
 is often referred to as a kernel or a kernel function  The word  kernel  is used in mathematics to denote a weighting function for a weighted sum or integral 
Certain problems in machine learning have more structure than an arbitrary weighting function 
  
    
      
        k
      
    
      displaystyle k 
  
   The computation is made much simpler if the kernel can be written in the form of a  feature map  
  
    
      
          x c  
          x a 
        
          
            X
          
        
          x     
        
          
            V
          
        
      
    
      displaystyle  varphi  colon   mathcal  X   to   mathcal  V   
  
 which satisfies

  
    
      
        k
         
        
          x
        
         
        
          
            x
              x     
          
        
         
         
          x  e  
          x c  
         
        
          x
        
         
         
          x c  
         
        
          
            x
              x     
          
        
         
        
            x  e  
          
            
              V
            
          
        
         
      
    
      displaystyle k  mathbf  x    mathbf  x      langle  varphi   mathbf  x     varphi   mathbf  x     rangle    mathcal  V    
  
The key restriction is that 
  
    
      
          x  e  
          x  c  
         
          x  c  
        
            x  e  
          
            
              V
            
          
        
      
    
      displaystyle  langle  cdot   cdot  rangle    mathcal  V   
  
 must be a proper inner product  On the other hand  an explicit representation for 
  
    
      
          x c  
      
    
      displaystyle  varphi  
  
 is not necessary  as long as 
  
    
      
        
          
            V
          
        
      
    
      displaystyle   mathcal  V   
  
 is an inner product space   The alternative follows from Mercer s theorem  an implicitly defined function 
  
    
      
          x c  
      
    
      displaystyle  varphi  
  
 exists whenever the space 
  
    
      
        
          
            X
          
        
      
    
      displaystyle   mathcal  X   
  
 can be equipped with a suitable measure ensuring the function 
  
    
      
        k
      
    
      displaystyle k 
  
 satisfies Mercer s condition 
Mercer s theorem is similar to a generalization of the result from linear algebra that associates an inner product to any positive definite matrix  In fact  Mercer s condition can be reduced to this simpler case  If we choose as our measure the counting measure 
  
    
      
          x bc 
         
        T
         
         
        
           
        
        T
        
           
        
      
    
      displaystyle  mu  T   T  
  
 for all 
  
    
      
        T
          x     
        X
      
    
      displaystyle T subset X 
  
  which counts the number of points inside the set 
  
    
      
        T
      
    
      displaystyle T 
  
  then the integral in Mercer s theorem reduces to a summation
  
    
      
        
            x     
          
            i
             
             
          
          
            n
          
        
        
            x     
          
            j
             
             
          
          
            n
          
        
        k
         
        
          
            x
          
          
            i
          
        
         
        
          
            x
          
          
            j
          
        
         
        
          c
          
            i
          
        
        
          c
          
            j
          
        
          x     
          
      
    
      displaystyle  sum   i     n  sum   j     n k  mathbf  x    i   mathbf  x    j  c  i c  j  geq    
  
If this summation holds for all finite sequences of points 
  
    
      
         
        
          
            x
          
          
             
          
        
         
          x     
         
        
          
            x
          
          
            n
          
        
         
      
    
      displaystyle   mathbf  x        dotsc   mathbf  x    n   
  
 in 
  
    
      
        
          
            X
          
        
      
    
      displaystyle   mathcal  X   
  
 and all choices of 
  
    
      
        n
      
    
      displaystyle n 
  
 real valued coefficients 
  
    
      
         
        
          c
          
             
          
        
         
          x     
         
        
          c
          
            n
          
        
         
      
    
      displaystyle  c      dots  c  n   
  
  cf  positive definite kernel   then the function 
  
    
      
        k
      
    
      displaystyle k 
  
 satisfies Mercer s condition 
Some algorithms that depend on arbitrary relationships in the native space 
  
    
      
        
          
            X
          
        
      
    
      displaystyle   mathcal  X   
  
 would  in fact  have a linear interpretation in a different setting  the range space of 
  
    
      
          x c  
      
    
      displaystyle  varphi  
  
  The linear interpretation gives us insight about the algorithm  Furthermore  there is often no need to compute 
  
    
      
          x c  
      
    
      displaystyle  varphi  
  
 directly during computation  as is the case with support vector machines  Some cite this running time shortcut as the primary benefit  Researchers also use it to justify the meanings and properties of existing algorithms 
Theoretically  a Gram matrix 
  
    
      
        
          K
        
          x     
        
          
            R
          
          
            n
              xd  
            n
          
        
      
    
      displaystyle  mathbf  K   in  mathbb  R    n times n  
  
 with respect to 
  
    
      
         
        
          
            x
          
          
             
          
        
         
          x     
         
        
          
            x
          
          
            n
          
        
         
      
    
      displaystyle    mathbf  x        dotsc   mathbf  x    n    
  
  sometimes also called a  kernel matrix               where 
  
    
      
        
          K
          
            i
            j
          
        
         
        k
         
        
          
            x
          
          
            i
          
        
         
        
          
            x
          
          
            j
          
        
         
      
    
      displaystyle K  ij  k  mathbf  x    i   mathbf  x    j   
  
  must be positive semi definite  PSD              Empirically  for machine learning heuristics  choices of a function 
  
    
      
        k
      
    
      displaystyle k 
  
 that do not satisfy Mercer s condition may still perform reasonably if 
  
    
      
        k
      
    
      displaystyle k 
  
 at least approximates the intuitive idea of similarity             Regardless of whether 
  
    
      
        k
      
    
      displaystyle k 
  
 is a Mercer kernel  
  
    
      
        k
      
    
      displaystyle k 
  
 may still be referred to as a  kernel  
If the kernel function 
  
    
      
        k
      
    
      displaystyle k 
  
 is also a covariance function as used in Gaussian processes  then the Gram matrix 
  
    
      
        
          K
        
      
    
      displaystyle  mathbf  K   
  
 can also be called a covariance matrix            

Applications edit 
Application areas of kernel methods are diverse and include geostatistics             kriging  inverse distance weighting   D reconstruction  bioinformatics  cheminformatics  information extraction and handwriting recognition 

Popular kernels edit 
Fisher kernel
Graph kernels
Kernel smoother
Polynomial kernel
Radial basis function kernel  RBF 
String kernels
Neural tangent kernel
Neural network Gaussian process  NNGP  kernel
See also edit 
Kernel methods for vector output
Kernel density estimation
Representer theorem
Similarity learning
Cover s theorem
References edit 


   Kernel method   Engati  Retrieved            

  Theodoridis  Sergios         Pattern Recognition  Elsevier B V  p            ISBN                    

  Aizerman  M  A   Braverman  Emmanuel M   Rozonoer  L  I           Theoretical foundations of the potential function method in pattern recognition learning   Automation and Remote Control               Cited in Guyon  Isabelle  Boser  B   Vapnik  Vladimir         Automatic capacity tuning of very large VC dimension classifiers  Advances in neural information processing systems  CiteSeerX                     

  Hofmann  Thomas  Sch lkopf  Bernhard  Smola  Alexander J           Kernel Methods in Machine Learning   The Annals of Statistics          arXiv math          doi                             S CID               

  Mohri  Mehryar  Rostamizadeh  Afshin  Talwalkar  Ameet         Foundations of Machine Learning  US  Massachusetts  MIT Press  ISBN                    

  Sewell  Martin   Support Vector Machines  Mercer s Condition   Support Vector Machines  Archived from the original on             Retrieved            

  Rasmussen  Carl Edward  Williams  Christopher K  I          Gaussian Processes for Machine Learning  MIT Press  ISBN                  X       page      needed     

  Honarkhah  M   Caers  J           Stochastic Simulation of Patterns Using Distance Based Pattern Modeling   Mathematical Geosciences                   Bibcode     MaGeo         H  doi         s                  S CID               


Further reading edit 
Shawe Taylor  J   Cristianini  N          Kernel Methods for Pattern Analysis  Cambridge University Press  ISBN                    
Liu  W   Principe  J   Haykin  S          Kernel Adaptive Filtering  A Comprehensive Introduction  Wiley  ISBN                    
Sch lkopf  B   Smola  A  J   Bach  F          Learning with Kernels        Support Vector Machines  Regularization  Optimization  and Beyond  MIT Press  ISBN                        
External links edit 
Kernel Machines Org community website
onlineprediction net Kernel Methods Article





Retrieved from  https   en wikipedia org w index php title Kernel method amp oldid