Type of machine learning model
Not to be confused with Logic learning machine 
 LLM  redirects here  For other uses  see LLM  disambiguation  
Part of a series onMachine learningand data mining
Paradigms
Supervised learning
Unsupervised learning
Semi supervised learning
Self supervised learning
Reinforcement learning
Meta learning
Online learning
Batch learning
Curriculum learning
Rule based learning
Neuro symbolic AI
Neuromorphic engineering
Quantum machine learning

Problems
Classification
Generative modeling
Regression
Clustering
Dimensionality reduction
Density estimation
Anomaly detection
Data cleaning
AutoML
Association rules
Semantic analysis
Structured prediction
Feature engineering
Feature learning
Learning to rank
Grammar induction
Ontology learning
Multimodal learning

Supervised learning classification                  regression  
Apprenticeship learning
Decision trees
Ensembles
Bagging
Boosting
Random forest
k NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine  RVM 
Support vector machine  SVM 

Clustering
BIRCH
CURE
Hierarchical
k means
Fuzzy
Expectation maximization  EM 
DBSCAN
OPTICS
Mean shift

Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t SNE
SDL

Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov

Anomaly detection
RANSAC
k NN
Local outlier factor
Isolation forest

Artificial neural network
Autoencoder
Deep learning
Feedforward neural network
Recurrent neural network
LSTM
GRU
ESN
reservoir computing
Boltzmann machine
Restricted
GAN
Diffusion model
SOM
Convolutional neural network
U Net
LeNet
AlexNet
DeepDream
Neural radiance field
Transformer
Vision
Mamba
Spiking neural network
Memtransistor
Electrochemical RAM  ECRAM 

Reinforcement learning
Q learning
SARSA
Temporal difference  TD 
Multi agent
Self play

Learning with humans
Active learning
Crowdsourcing
Human in the loop
RLHF

Model diagnostics
Coefficient of determination
Confusion matrix
Learning curve
ROC curve

Mathematical foundations
Kernel machines
Bias variance tradeoff
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory
Topological deep learning

Journals and conferences
ECML PKDD
NeurIPS
ICML
ICLR
IJCAI
ML
JMLR

Related articles
Glossary of artificial intelligence
List of datasets for machine learning research
List of datasets in computer vision and image processing
Outline of machine learning
vte
A large language model  LLM  is a type of machine learning model designed for natural language processing tasks such as language generation  LLMs are language models with many parameters  and are trained with self supervised learning on a vast amount of text 
The largest and most capable LLMs are generative pretrained transformers  GPTs   Modern models can be fine tuned for specific tasks or guided by prompt engineering             These models acquire predictive power regarding syntax  semantics  and ontologies            inherent in human language corpora  but they also inherit inaccuracies and biases present in the data they are trained in            


History edit 
The training compute of notable large models in FLOPs vs publication date over the period            For overall notable models  top left   frontier models  top right   top language models  bottom left  and top models within leading companies  bottom right   The majority of these models are language models 
The training compute of notable large AI models in FLOPs vs publication date over the period            The majority of large models are language models or multimodal models with language capacity 
Before       there were a few language models that were large as compared to capacities then available  In the     s  the IBM alignment models pioneered statistical language modelling  A smoothed n gram model in      trained on     billion words achieved state of the art perplexity at the time             In the     s  as Internet use became prevalent  some researchers constructed Internet scale language datasets   web as corpus               upon which they trained statistical language models                        In       in most language processing tasks  statistical language models dominated over symbolic language models because they can usefully ingest large datasets            

After neural networks became dominant in image processing around                  they were applied to language modelling as well  Google converted its translation service to Neural Machine Translation in       Because it preceded the existence of transformers  it was done by seq seq deep LSTM networks An illustration of main components of the transformer model from the original paper  where layers were normalized after  instead of before  multiheaded attention
At the      NeurIPS conference  Google researchers introduced the transformer architecture in their landmark paper  Attention Is All You Need   This paper s goal was to improve upon      seq seq technology              and was based mainly on the attention mechanism developed by Bahdanau et al  in                   The following year in       BERT was introduced and quickly became  ubiquitous               Though the original transformer has both encoder and decoder blocks  BERT is an encoder only model  Academic and research usage of BERT began to decline in       following rapid improvements in the abilities of decoder only models  such as GPT  to solve tasks via prompting             
Although decoder only GPT   was introduced in       it was GPT   in      that caught widespread attention because OpenAI at first deemed it too powerful to release publicly  out of fear of malicious use              GPT   in      went a step further and as of          update      is available only via API with no offering of downloading the model to execute locally  But it was the      consumer facing browser based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz              The      GPT   was praised for its increased accuracy and as a  holy grail  for its multimodal capabilities              OpenAI did not reveal the high level architecture and the number of parameters of GPT    The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science  including robotics  software engineering  and societal impact work              In      OpenAI released the reasoning model OpenAI o   which generates long chains of thought before returning a final answer 
Competing language models have for the most part been attempting to equal the GPT series  at least in terms of number of parameters             
Since       source available models have been gaining popularity  especially at first with BLOOM and LLaMA  though both have restrictions on the field of use  Mistral AI s models Mistral  B and Mixtral  x b have the more permissive Apache License  In January       DeepSeek released DeepSeek R   a     billion parameter open weight model that performs comparably to OpenAI o  but at a much lower cost             
Since       many LLMs have been trained to be multimodal  having the ability to also process or generate other types of data  such as images or audio  These LLMs are also called large multimodal models  LMMs              
As of       the largest and most capable models are all based on the transformer architecture  Some recent implementations are based on other architectures  such as recurrent neural network variants and Mamba  a state space model                                      

Dataset preprocessing edit 
See also  List of datasets for machine learning research        Internet
Tokenization edit 

As machine learning algorithms process numbers rather than text  the text must be converted to numbers  In the first step  a vocabulary is decided upon  then integer indices are arbitrarily but uniquely assigned to each vocabulary entry  and finally  an embedding is associated to the integer index  Algorithms include byte pair encoding  BPE  and WordPiece  There are also special tokens serving as control characters  such as  MASK  for masked out token  as used in BERT   and  UNK    unknown   for characters not appearing in the vocabulary  Also  some special symbols are used to denote special text formatting  For example      denotes a preceding whitespace in RoBERTa and GPT       denotes continuation of a preceding word in BERT             
For example  the BPE tokenizer used by GPT    Legacy  would split tokenizer  texts   gt  series of numerical  tokens  as




token

izer

 

      texts

        gt 

series

      of

      numerical

       

t

ok

ens

 

Tokenization also compresses the datasets  Because LLMs generally require input to be an array that is not jagged  the shorter texts must be  padded  until they match the length of the longest one  How many tokens are  on average  needed per word depends on the language of the dataset                         

BPE edit 
Main article  Byte pair encoding
As an example  consider a tokenizer based on byte pair encoding  In the first step  all unique characters  including blanks and punctuation marks  are treated as an initial set of n grams  i e  initial set of uni grams   Successively the most frequent pair of adjacent characters is merged into a bi gram and all instances of the pair are replaced by it  All occurrences of adjacent pairs of  previously merged  n grams that most frequently occur together are then again merged into even lengthier n gram  until a vocabulary of prescribed size is obtained  in case of GPT    the size is                     After a tokenizer is trained  any text can be tokenized by it  as long as it does not contain characters not appearing in the initial set of uni grams             

Problems edit 
A token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word  However  an average word in another language encoded by such an English optimized tokenizer is split into a suboptimal amount of tokens  GPT   tokenizer can use up to    times more tokens per word for some languages  for example for the Shan language from Myanmar  Even more widespread languages such as Portuguese and German have  a premium of      compared to English             
Greedy tokenization also causes subtle problems with text completion             

Dataset cleaning edit 
Main article  Data cleansing
In the context of training LLMs  datasets are typically cleaned by removing low quality  duplicated  or toxic data              Cleaned datasets can increase training efficiency and lead to improved downstream performance                          A trained LLM can be used to clean datasets for training a further LLM             
With the increasing proportion of LLM generated content on the web  data cleaning in the future may include filtering out such content  LLM generated content can pose a problem if the content is similar to human text  making filtering difficult  but of lower quality  degrading performance of models trained on it              

Synthetic data edit 
Main article  Synthetic data
Training of largest language models might need more linguistic data than naturally available  or that the naturally occurring data is of insufficient quality  In these cases  synthetic data might be used  Microsoft s Phi series of LLMs is trained on textbook like data generated by another LLM             

Training and architecture edit 
See also  Fine tuning  machine learning 
Reinforcement learning from human feedback edit 
Reinforcement learning from human feedback  RLHF  through algorithms  such as proximal policy optimization  is used to further fine tune a model based on a dataset of human preferences             

Instruction tuning edit 
Using  self instruct  approaches  LLMs have been able to bootstrap correct responses  replacing any naive responses  starting from human generated corrections of a few cases  For example  in the instruction  Write an essay about the main themes represented in Hamlet   an initial naive completion might be  If you submit the essay after March     your grade will be reduced by     for each day of delay   based on the frequency of this textual sequence in the corpus             

Mixture of experts edit 
Main article  Mixture of experts
The largest LLM may be too expensive to train and use directly  For such models  mixture of experts  MoE  can be applied  a line of research pursued by Google researchers since      to train models reaching up to   trillion parameters                                     

Prompt engineering  attention mechanism  and context window edit 
See also  Prompt engineering and Attention  machine learning 
Most results previously achievable only by  costly  fine tuning  can be achieved through prompt engineering  although limited to the scope of a single conversation  more precisely  limited to the scope of a context window              

 When each head calculates  according to its own criteria  how much other tokens are relevant for the  it   token  note that the second attention head  represented by the second column  is focusing most on the first two rows  i e  the tokens  The  and  animal   while the third column is focusing most on the bottom two rows  i e  on  tired   which has been tokenized into two tokens             
In order to find out which tokens are relevant to each other within the scope of the context window  the attention mechanism calculates  soft  weights for each token  more precisely for its embedding  by using multiple attention heads  each with its own  relevance  for calculating its own soft weights  For example  the small  i e     M parameter sized  GPT   model has had twelve attention heads and a context window of only  k tokens              In its medium version it has    M parameters and contains    layers  each with    attention heads  For the training with gradient descent a batch size of     was utilized             
The largest models  such as Google s Gemini      presented in February       can have a context window sized up to   million  context window of    million was also  successfully tested                Other models with large context windows includes Anthropic s Claude      with a context window of up to    k tokens              Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller  For example  the GPT   Turbo model has a maximum output of      tokens             
Length of a conversation that the model can take into account when generating its next answer is limited by the size of a context window  as well  If the length of a conversation  for example with ChatGPT  is longer than its context window  only the parts inside the context window are taken into account when generating the next answer  or the model needs to apply some algorithm to summarize the too distant parts of conversation 
The shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context  while making it smaller can cause a model to miss an important long range dependency  Balancing them is a matter of experimentation and domain specific considerations 
A model may be pre trained either to predict how the segment continues  or what is missing in the segment  given a segment from its training dataset              It can be either

autoregressive  i e  predicting how the segment continues  as GPTs do   for example given a segment  I like to eat   the model predicts  ice cream   or  sushi  
 masked   i e  filling in the parts missing from the segment  the way  BERT              does it   for example  given a segment  I like to           cream   the model predicts that  eat  and  ice  are missing 
Models may be trained on auxiliary tasks which test their understanding of the data distribution  such as Next Sentence Prediction  NSP   in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus              During training  regularization loss is also used to stabilize training  However regularization loss is usually not used during testing and evaluation 

Infrastructure edit 
Substantial infrastructure is necessary for training the largest models                                     

Training cost edit 

The qualifier  large  in  large language model  is inherently vague  as there is no definitive threshold for the number of parameters required to qualify as  large   As time goes on  what was previously considered  large  may evolve  GPT   of      is usually considered the first LLM  even though it has only       billion parameters  The tendency towards larger models is visible in the list of large language models 
As technology advanced  large sums have been invested in increasingly large models  For example  training of the GPT    i e  a     billion parameters model  in      cost          while training of the PaLM  i e  a     billion parameters model  in      cost    million  and Megatron Turing NLG    B  in       cost around     million             
For Transformer based LLM  training cost is much higher than inference cost  It costs   FLOPs per parameter to train on one token  whereas it costs   to   FLOPs per parameter to infer on one token             

Tool use edit 
There are certain tasks that  in principle  cannot be solved by any LLM  at least not without the use of external tools or additional software  An example of such a task is responding to the user s input                 provided that the LLM has not already encountered a continuation of this calculation in its training corpus      dubious             discuss      In such cases  the LLM needs to resort to running program code that calculates the result  which can then be included in its response      dubious             discuss       Another example is  What is the time now  It is    where a separate program interpreter would need to execute a code to get system time on the computer  so that the LLM can include it in its reply                          This basic strategy can be sophisticated with multiple attempts of generated programs  and other sampling strategies             
Generally  in order to get an LLM to use tools  one must fine tune it for tool use  If the number of tools is finite  then fine tuning may be done just once  If the number of tools can grow arbitrarily  as with online API services  then the LLM can be fine tuned to be able to read API documentation and call API correctly                         
Retrieval augmented generation  RAG  is another approach that enhances LLMs by integrating them with document retrieval systems  Given a query  a document retriever is called to retrieve the most relevant documents  This is usually done by encoding the query and the documents into vectors  then finding the documents with vectors  usually stored in a vector database  most similar to the vector of the query  The LLM then generates an output based on both the query and context included from the retrieved documents             

Agency edit 
An LLM is typically not an autonomous agent by itself  as it lacks the ability to interact with dynamic environments  recall past behaviors  and plan future actions  but can be transformed into one by integrating modules like profiling  memory  planning  and action             
The ReAct pattern  a portmanteau of  Reason             Act   constructs an agent out of an LLM  using the LLM as a planner  The LLM is prompted to  think out loud   Specifically  the language model is prompted with a textual description of the environment  a goal  a list of possible actions  and a record of the actions and observations so far  It generates one or more thoughts before generating an action  which is then executed in the environment              The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment             
In the DEPS   Describe  Explain  Plan and Select   method  an LLM is first connected to the visual world via image descriptions  then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives             
The Reflexion method             constructs an agent that learns over multiple episodes  At the end of each episode  the LLM is given the record of the episode  and prompted to think up  lessons learned   which would help it perform better at a subsequent episode  These  lessons learned  are given to the agent in the subsequent episodes      citation needed     
Monte Carlo tree search can use an LLM as rollout heuristic  When a programmatic world model is not available  an LLM can also be prompted with a description of the environment to act as world model             
For open ended exploration  an LLM can be used to score observations for their  interestingness   which can be used as a reward signal to guide a normal  non LLM  reinforcement learning agent              Alternatively  it can propose increasingly difficult tasks for curriculum learning              Instead of outputting individual actions  an LLM planner can also construct  skills   or functions for complex action sequences  The skills can be stored and later invoked  allowing increasing levels of abstraction in planning             
LLM powered agents can keep a long term memory of its previous contexts  and the memory can be retrieved in the same way as Retrieval Augmented Generation  Multiple such agents can interact socially             

Compression edit 
See also       bit large language model
Typically  LLMs are trained with single  or half precision floating point numbers  float   and float     One float   has    bits  or   bytes  and so one billion parameters require   gigabytes  The largest models typically have     billion parameters  requiring     gigabytes to load  which places them outside the range of most consumer electronics             
Post training quantization             aims to decrease the space requirement by lowering precision of the parameters of a trained model  while preserving most of its performance                          The simplest form of quantization simply truncates all numbers to a given number of bits  It can be improved by using a different quantization codebook per layer  Further improvement can be done by applying different precisions to different parameters  with higher precision for particularly important parameters   outlier weights                See the visual guide to quantization by Maarten Grootendorst             for a visual depiction 
While quantized models are typically frozen  and only pre quantized models are fine tuned  quantized models can still be fine tuned             

Multimodality edit 
See also  Multimodal learning
Multimodality means  having several modalities   and a  modality  refers to a type of input or output  such as video  image  audio  text  proprioception  etc              There have been many AI models trained specifically to ingest one modality and output another modality  such as AlexNet for image to label              visual question answering for image text to text              and speech recognition for speech to text 
A common method to create multimodal models out of an LLM is to  tokenize  the output of a trained encoder  Concretely  one can construct an LLM that can understand images as follows  take a trained LLM  and take a trained image encoder 
  
    
      
        E
      
    
      displaystyle E 
  
  Make a small multilayered perceptron 
  
    
      
        f
      
    
      displaystyle f 
  
  so that for any image 
  
    
      
        y
      
    
      displaystyle y 
  
  the post processed vector 
  
    
      
        f
         
        E
         
        y
         
         
      
    
      displaystyle f E y   
  
 has the same dimensions as an encoded token  That is an  image token   Then  one can interleave text tokens and image tokens  The compound model is then fine tuned on an image text dataset  This basic construction can be applied with more sophistication to improve the model  The image encoder may be frozen to improve stability             
Flamingo demonstrated the effectiveness of the tokenization method  finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch              Google PaLM model was fine tuned into a multimodal model PaLM E using the tokenization method  and applied to robotic control              LLaMA models have also been turned multimodal using the tokenization method  to allow image inputs              and video inputs             
GPT   can use both text and image as inputs              although the vision component was not released to the public until GPT  V               Google DeepMind s Gemini is also multimodal               Mistral introduced its own multimodel Pixtral   B model in September                  

Reasoning edit 
In late       a new direction emerged in LLM development with models specifically designed for complex reasoning tasks  These  reasoning models  were trained to spend more time generating step by step solutions before providing final answers  similar to human problem solving processes             
OpenAI introduced this trend with their o  model in September       followed by o  in December       These models showed significant improvements in mathematics  science  and coding tasks compared to traditional LLMs  For example  on International Mathematics Olympiad qualifying exam problems  GPT  o achieved     accuracy while o  reached                             
In January       the Chinese company DeepSeek released DeepSeek R   a     billion parameter open weight reasoning model that achieved comparable performance to OpenAI s o  while being significantly more cost effective to operate  Unlike proprietary models from OpenAI  DeepSeek R  s open weight nature allowed researchers to study and build upon the algorithm  though its training data remained private             
These reasoning models typically require more computational resources per query compared to traditional LLMs  as they perform more extensive processing to work through problems step by step  However  they have shown superior capabilities in domains requiring structured logical thinking  such as mathematics  scientific research  and computer programming             
Efforts to reduce or compensate for hallucinations have employed automated reasoning  RAG  retrieval augmented generation   fine tuning  and other methods             

Properties edit 
Scaling laws edit 
Main article  Neural scaling law
The performance of an LLM after pretraining largely depends on the 

cost of pretraining 
  
    
      
        C
      
    
      displaystyle C 
  
  the total amount of compute used  
size of the artificial neural network itself  such as number of parameters 
  
    
      
        N
      
    
      displaystyle N 
  
  i e  amount of neurons in its layers  amount of weights between them and biases  
size of its pretraining dataset  i e  number of tokens in corpus  
  
    
      
        D
      
    
      displaystyle D 
  
  
 Scaling laws  are empirical statistical laws that predict LLM performance based on such factors  One particular scaling law   Chinchilla scaling   for LLM autoregressively trained for one epoch  with a log log learning rate schedule  states that             

  
    
      
        
          
             
            
              
                
                  C
                   
                  
                    C
                    
                       
                    
                  
                  N
                  D
                
              
              
                
                  L
                   
                  
                    
                      A
                      
                        N
                        
                            x b  
                        
                      
                    
                  
                   
                  
                    
                      B
                      
                        D
                        
                            x b  
                        
                      
                    
                  
                   
                  
                    L
                    
                       
                    
                  
                
              
            
            
          
        
      
    
      displaystyle   begin cases C C    ND    pt L   frac  A  N   alpha       frac  B  D   beta     L     end cases   
  
 where the variables are


  
    
      
        C
      
    
      displaystyle C 
  
 is the cost of training the model  in FLOPs 

  
    
      
        N
      
    
      displaystyle N 
  
 is the number of parameters in the model 

  
    
      
        D
      
    
      displaystyle D 
  
 is the number of tokens in the training set 

  
    
      
        L
      
    
      displaystyle L 
  
 is the average negative log likelihood loss per token  nats token   achieved by the trained LLM on the test dataset 
and the statistical hyper parameters are


  
    
      
        
          C
          
             
          
        
         
         
      
    
      displaystyle C       
  
  meaning that it costs   FLOPs per parameter to train on one token  Note that training cost is much higher than inference cost  where it costs   to   FLOPs per parameter to infer on one token             

  
    
      
          x b  
         
            
         
          x b  
         
            
         
        A
         
             
         
        B
         
             
         
        
          L
          
             
          
        
         
            
      
    
      displaystyle  alpha        beta       A       B       L          
  

Emergent abilities edit 
At point s  referred to as breaks              the lines change their slopes  appearing on a linear log plot as a series of linear segments connected by arcs 
Performance of bigger models on various tasks  when plotted on a log log scale  appears as a linear extrapolation of performance achieved by smaller models  However  this linearity may be punctuated by  break s               in the scaling law  where the slope of the line changes abruptly  and where larger models acquire  emergent abilities                           They arise from the complex interaction of the model s components and are not explicitly programmed or designed              
Furthermore  recent research has demonstrated that AI systems  including large language models  can employ heuristic reasoning akin to human cognition  They balance between exhaustive logical processing and the use of cognitive shortcuts  heuristics   adapting their reasoning strategies to optimize between accuracy and effort  This behavior aligns with principles of resource rational human cognition  as discussed in classical theories of bounded rationality and dual process theory             
One of the emergent abilities is in context learning from example demonstrations              In context learning is involved in tasks  such as 

reported arithmetics
decoding the International Phonetic Alphabet
unscrambling a word s letters
disambiguating word in context datasets                                    
converting spatial words
cardinal directions  for example  replying  northeast  in response to a  x  grid of   zeros and a   in the top right   color terms represented in text             
chain of thought prompting  In a      research paper  chain of thought prompting only improved the performance for models that had at least   B  Smaller models perform better when prompted to answer immediately  without chain of thought              
identifying offensive content in paragraphs of Hinglish  a combination of Hindi and English   and generating a similar English equivalent of Kiswahili proverbs              
Schaeffer et  al  argue that the emergent abilities are not unpredictably acquired  but predictably acquired according to a smooth scaling law  The authors considered a toy statistical model of an LLM solving multiple choice questions  and showed that this statistical model  modified to account for other types of tasks  applies to these tasks as well              
Let 
  
    
      
        x
      
    
      displaystyle x 
  
 be the number of parameter count  and 
  
    
      
        y
      
    
      displaystyle y 
  
 be the performance of the model 


When 
  
    
      
        y
         
        
          average  xa  
        
        Pr
         
        
          correct token
        
         
      
    
      displaystyle y   text average    Pr   text correct token    
  
  then 
  
    
      
         
        log
          x     
        x
         
        y
         
      
    
      displaystyle   log x y  
  
 is an exponential curve  before it hits the plateau at one   which looks like emergence 
When 
  
    
      
        y
         
        
          average  xa  
        
        log
          x     
         
        Pr
         
        
          correct token
        
         
         
      
    
      displaystyle y   text average    log  Pr   text correct token     
  
  then the 
  
    
      
         
        log
          x     
        x
         
        y
         
      
    
      displaystyle   log x y  
  
 plot is a straight line  before it hits the plateau at zero   which does not look like emergence 
When 
  
    
      
        y
         
        
          average  xa  
        
        Pr
         
        
          the most likely token is correct
        
         
      
    
      displaystyle y   text average    Pr   text the most likely token is correct    
  
  then 
  
    
      
         
        log
          x     
        x
         
        y
         
      
    
      displaystyle   log x y  
  
 is a step function  which looks like emergence 
Interpretation edit 
Large language models by themselves are black boxes  and it is not clear how they can perform linguistic tasks  Similarly  it is unclear if or how LLMs should be viewed as models of the human brain and or human mind              
Various techniques have been developed to enhance the transparency and interpretability of LLMs  Mechanistic interpretability aims to reverse engineer LLMs by discovering symbolic algorithms that approximate the inference performed by an LLM  In recent years  sparse coding models such as sparse autoencoders  transcoders  and crosscoders have emerged as promising tools for identifying interpretable features  

Studying a replacement model edit 
Transcoders  which are more interpretable than transformers  have been utilized to develop  replacement models   In one such study involving the mechanistic interpretation of writing a rhyming poem by an LLM  it was shown that although they are believed to simply predict the next token  they can  in fact  plan ahead              

Explainability edit 
A related concept is AI explainability  which focuses on understanding how an AI model arrives at a given result  Techniques such as partial dependency plots  SHAP  SHapley Additive exPlanations   and feature importance assessments allow researchers to visualize and understand the contributions of various input features to the model s predictions  These methods help ensure that AI models make decisions based on relevant and fair criteria  enhancing trust and accountability 
By integrating these techniques  researchers and practitioners can gain deeper insights into the operations of LLMs  fostering trust and facilitating the responsible deployment of these powerful models 
In another example  the authors trained small transformers on modular arithmetic addition  The resulting models were reverse engineered  and it turned out they used discrete Fourier transform              

Understanding and intelligence edit 
See also  Philosophy of artificial intelligence and Artificial consciousness
NLP researchers were evenly split when asked  in a      survey  whether  untuned  LLMs  could  ever  understand natural language in some nontrivial sense                Proponents of  LLM understanding  believe that some LLM abilities  such as mathematical reasoning  imply an ability to  understand  certain concepts  A Microsoft team argued in      that GPT    can solve novel and difficult tasks that span mathematics  coding  vision  medicine  law  psychology and more  and that GPT    could reasonably be viewed as an early  yet still incomplete  version of an artificial general intelligence system    Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent                             Ilya Sutskever argues that predicting the next word sometimes involves reasoning and deep insights  for example if the LLM has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation               Some researchers characterize LLMs as  alien intelligence                             For example  Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien  Shoggoths   and believes that RLHF tuning creates a  smiling facade  obscuring the inner workings of the LLM   If you don t push it too far  the smiley face stays on  But then you give it  an unexpected  prompt  and suddenly you see this massive underbelly of insanity  of weird thought processes and clearly non human understanding                            
In contrast  some skeptics of LLM understanding believe that existing LLMs are  simply remixing and recombining existing writing                a phenomenon known as stochastic parrot  or they point to the deficits existing LLMs continue to have in prediction skills  reasoning skills  agency  and explainability               For example  GPT   has natural deficits in planning and in real time learning               Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data  a phenomenon which has been termed  hallucination                Specifically  hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound  fluent  and natural but are factually incorrect  nonsensical  or unfaithful to the provided source input               Neuroscientist Terrence Sejnowski has argued that  The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate               
The matter of LLM s exhibiting intelligence or understanding has two main aspects   the first is how to model thought and language in a computer system  and the second is how to enable the computer system to generate human like language               These aspects of language as a model of cognition have been developed in the field of cognitive linguistics  American linguist George Lakoff presented Neural Theory of Language  NTL               as a computational basis for using language as a model of learning tasks and understanding  The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system  After a framework for modeling language in a computer systems was established  the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar  In his      book titled The Language Myth  Why Language Is Not An Instinct  British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context free grammar  PCFG  in enabling NLP to model cognitive patterns and generate human like language                           

Evaluation edit 
Perplexity edit 
The canonical measure of the performance of an LLM is its perplexity on a given text corpus  Perplexity measures how well a model predicts the contents of a dataset  the higher the likelihood the model assigns to the dataset  the lower the perplexity  In mathematical terms  perplexity is the exponential of the average negative log likelihood per token 

  
    
      
        log
          x     
         
        
          Perplexity
        
         
         
          x     
        
          
             
            N
          
        
        
            x     
          
            i
             
             
          
          
            N
          
        
        log
          x     
         
        Pr
         
        
          
            token
          
          
            i
          
        
          x     
        
          
            context for token
          
          
            i
          
        
         
         
      
    
      displaystyle  log   text Perplexity       frac     N   sum   i     N  log  Pr   text token    i  mid   text context for token    i    
  

Here  
  
    
      
        N
      
    
      displaystyle N 
  
 is the number of tokens in the text corpus  and  context for token 
  
    
      
        i
      
    
      displaystyle i 
  
  depends on the specific type of LLM  If the LLM is autoregressive  then  context for token 
  
    
      
        i
      
    
      displaystyle i 
  
  is the segment of text appearing before token 
  
    
      
        i
      
    
      displaystyle i 
  
  If the LLM is masked  then  context for token 
  
    
      
        i
      
    
      displaystyle i 
  
  is the segment of text surrounding token 
  
    
      
        i
      
    
      displaystyle i 
  
 
Because language models may overfit to training data  models are usually evaluated by their perplexity on a test set              This evaluation is potentially problematic for larger models which  as they are trained on increasingly large corpora of text  are increasingly likely to inadvertently include portions of any given test set            

Measures edit 
In information theory  the concept of entropy is intricately linked to perplexity  a relationship notably established by Claude Shannon               This relationship is mathematically expressed as 
  
    
      
        
          Entropy
        
         
        
          log
          
             
          
        
          x     
         
        
          Perplexity
        
         
      
    
      displaystyle   text Entropy    log        text Perplexity    
  
 
Entropy  in this context  is commonly quantified in terms of bits per word  BPW  or bits per character  BPC   which hinges on whether the language model utilizes word based or character based tokenization 
Notably  in the case of larger language models that predominantly employ sub word tokenization  bits per token  BPT  emerges as a seemingly more appropriate measure  However  due to the variance in tokenization methods across different Large Language Models  LLMs   BPT does not serve as a reliable metric for comparative analysis among diverse models  To convert BPT into BPW  one can multiply it by the average number of tokens per word 
In the evaluation and comparison of language models  cross entropy is generally the preferred metric over entropy  The underlying principle is that a lower BPW is indicative of a model s enhanced capability for compression  This  in turn  reflects the model s proficiency in making accurate predictions 

Benchmarks edit 
Benchmarks are used to evaluate LLM performance on specific tasks  Tests evaluate capabilities such as general knowledge  bias  commonsense reasoning  question answering  and mathematical problem solving  Composite benchmarks examine multiple capabilities  Results are often sensitive to the prompting method                           
A question answering benchmark is termed  open book  if the model s prompt includes text from which the expected answer can be derived  for example  the previous question could be combined with text that includes the sentence  The Sharks have advanced to the Stanley Cup finals once  losing to the Pittsburgh Penguins in                       Otherwise  the task is considered  closed book   and the model must draw solely on its training               Examples include GLUE  SuperGLUE  MMLU  BIG bench  HELM  and HLE  Humanity s Last Exam                            
LLM bias may be assessed through benchmarks such as CrowS Pairs  Crowdsourced Stereotype Pairs                Stereo Set               and Parity Benchmark              
Fact checking and misinformation detection benchmarks are available  A      study compared the fact checking accuracy of LLMs including ChatGPT     and      Bard  and Bing AI against independent fact checkers such as PolitiFact and Snopes  The results demonstrated moderate proficiency  with GPT   achieving the highest accuracy at      lagging behind human fact checkers              
An earlier standard tested using a portion of the evaluation dataset  It became more common to evaluate a pre trained model directly through prompting techniques  Researchers vary in how they formulate prompts for particular tasks  particularly with respect to the number of correct examples attached to the prompt  i e  the value of n in n shot prompting  

Datasets edit 
Typical datasets consist of pairs of questions and correct answers  for example    Have the San Jose Sharks won the Stanley Cup     No                 Some examples of commonly used question answering datasets include TruthfulQA  Web Questions  TriviaQA  and SQuAD               
Evaluation datasets may also take the form of text completion  having the model select the most likely word or sentence to complete a prompt  for example   Alice was friends with Bob  Alice went to visit her friend                   
Datasets are of varying quality and may contain questions that are mislabeled  ambiguous  unanswerable  or otherwise of low quality              

Adversarial evaluations edit 
LLMs  rapid improvement regularly obsoletes benchmarks  with the models exceeding the performance of human annotators               In addition   shortcut learning  allows AIs to  cheat  on multiple choice tests by using statistical correlations in superficial test question wording to guess the correct responses  without considering the specific question              
Some datasets are adversarial  focusing on problems that confound LLMs  One example is the TruthfulQA dataset  a question answering dataset consisting of     questions that stump LLMs by mimicking falsehoods to which they were exposed during training  For example  an LLM may answer  No  to the question  Can you teach an old dog new tricks   because of its exposure to the English idiom you can t teach an old dog new tricks  even though this is not literally true              
Another example of an adversarial evaluation dataset is Swag and its successor  HellaSwag  collections of problems in which one of multiple options must be selected to complete a text passage  The incorrect completions were generated by sampling from a language model  The resulting problems are trivial for humans but defeated LLMs  Sample questions 


We see a fitness center sign  We then see a man talking to the camera and sitting and laying on a exercise ball  The man   

demonstrates how to increase efficient exercise work by running up and down balls 
moves all his arms and legs and builds up a lot of muscle 
then plays the ball and we see a graphics and hedge trimming demonstration 
performs sit ups while on the ball and talking              

BERT selects b  as the most likely completion  though the correct answer is d               

Wider impact edit 
In       Nature Biomedical Engineering wrote that  it is no longer possible to accurately distinguish  human written text from text created by large language models  and that  It is all but certain that general purpose large language models will rapidly proliferate    It is a rather safe bet that they will change many industries over time                Goldman Sachs suggested in      that generative language AI could increase global GDP by    in the next ten years  and could expose to automation     million jobs globally                            Brinkmann et al                      also argue that LLMs are transforming processes of cultural evolution by shaping processes of variation  transmission  and selection 

Memorization and copyright edit 
Further information  Artificial intelligence and copyright
Memorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data  contrary to typical behavior of traditional artificial neural nets  Evaluations of controlled LLM output measure the amount memorized from training data  focused on GPT   series models  as variously over    for exact duplicates              or up to about                 
A      study showed that when ChatGPT     turbo was prompted to repeat the same word indefinitely  after a few hundreds of repetitions  it would start outputting excerpts from its training data              

Security edit 
Some commenters expressed concern over accidental or deliberate creation of misinformation  or other forms of misuse               For example  the availability of large language models could reduce the skill level required to commit bioterrorism  biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens              
The potential presence of  sleeper agents  within LLMs is another emerging security concern  These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition  Upon activation  the LLM deviates from its expected behavior to make insecure actions              
LLM applications accessible to the public  like ChatGPT or Claude  typically incorporate safety measures designed to filter out harmful content  However  implementing these controls effectively has proven challenging  For instance  a      study              proposed a method for circumventing LLM safety systems  In       The American Sunlight Project  a non profit  published a study              showing evidence that the so called Pravda network  a pro Russia propaganda aggregator  was strategically placing web content through mass publication and duplication with the intention of biasing LLM outputs  The American Sunlight Project coined this technique  LLM grooming   and pointed to it as a new tool of weaponizing AI to spread disinformation and harmful content                            Similarly  Yongge Wang              illustrated in      how a potential criminal could potentially bypass ChatGPT  o s safety controls to obtain information on establishing a drug trafficking operation  External filters  circuit breakers and overrides have been posed as solutions      citation needed     

Algorithmic bias edit 
Main article  Algorithmic bias
While LLMs have shown remarkable capabilities in generating human like text  they are susceptible to inheriting and amplifying biases present in their training data  This can manifest in skewed representations or unfair treatment of different demographics  such as those based on race  gender  language  and cultural groups               Since English data is overrepresented in current large language models  training data  it may also downplay non English views              

Stereotyping edit 
AI models can reinforce a wide range of stereotypes  including those based on gender  ethnicity  age  nationality  religion  or occupation  This can lead to outputs that homogenize  or unfairly generalize or caricature groups of people  sometimes in harmful or derogatory ways                           
Notably  gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another  This bias typically arises from the data on which these models are trained  Large language models often assign roles and characteristics based on traditional gender norms               For example  it might associate nurses or secretaries predominantly with women and engineers or CEOs with men              

Selection bias edit 
Selection bias refers the inherent tendency of large language models to favor certain option identifiers irrespective of the actual content of the options  This bias primarily stems from token bias that is  the model assigns a higher a priori probability to specific answer tokens  such as  A   when generating responses  As a result  when the ordering of options is altered  for example  by systematically moving the correct answer to different positions   the model s performance can fluctuate significantly  This phenomenon undermines the reliability of large language models in multiple choice settings                           

Political bias edit 
Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints  ideologies  or outcomes over others  Language models may also exhibit political biases  Since the training data includes a wide range of political opinions and coverage  the models might generate responses that lean towards particular political ideologies or viewpoints  depending on the prevalence of those views in the data              

Energy demands edit 
The energy demands of LLMs have grown along with their size and capabilities  Data centers that enable LLM training require substantial amounts of electricity  Much of that electricity is generated by non renewable resources that create greenhouse gases and contribute to climate change               Nuclear power and geothermal energy are two options tech companies are exploring to meet the sizable energy demands of LLM training               The significant expense of investing in geothermal solutions has led to major shale producers like Chevron and Exxon Mobil advocating for tech companies to use electricity produced via natural gas to fuel their large energy demands              

See also edit 
Foundation models
List of large language models
List of chatbots
Language model benchmark
Reinforcement learning
Small language model
References edit 


  a b c Brown  Tom B   Mann  Benjamin  Ryder  Nick  Subbiah  Melanie  Kaplan  Jared  Dhariwal  Prafulla  Neelakantan  Arvind  Shyam  Pranav  Sastry  Girish  Askell  Amanda  Agarwal  Sandhini  Herbert Voss  Ariel  Krueger  Gretchen  Henighan  Tom  Child  Rewon  Ramesh  Aditya  Ziegler  Daniel M   Wu  Jeffrey  Winter  Clemens  Hesse  Christopher  Chen  Mark  Sigler  Eric  Litwin  Mateusz  Gray  Scott  Chess  Benjamin  Clark  Jack  Berner  Christopher  McCandlish  Sam  Radford  Alec  Sutskever  Ilya  Amodei  Dario  Dec        Larochelle  H   Ranzato  M   Hadsell  R   Balcan  M F   Lin  H   eds     Language Models are Few Shot Learners   PDF   Advances in Neural Information Processing Systems      Curran Associates  Inc              Archived  PDF  from the original on             Retrieved            

  Fathallah  Nadeen  Das  Arunav  De Giorgis  Stefano  Poltronieri  Andrea  Haase  Peter  Kovriguina  Liubov               NeOn GPT  A Large Language Model Powered Pipeline for Ontology Learning  PDF   Extended Semantic Web Conference       Hersonissos  Greece 

  Manning  Christopher D           Human Language Understanding  amp  Reasoning   Daedalus                    doi         daed a        S CID                 Archived from the original on             Retrieved            

  Goodman  Joshua               A Bit of Progress in Language Modeling  arXiv cs          Bibcode     cs            G

  Kilgarriff  Adam  Grefenstette  Gregory  September         Introduction to the Special Issue on the Web as Corpus   Computational Linguistics                   doi                             ISSN                

  Banko  Michele  Brill  Eric          Scaling to very very large corpora for natural language disambiguation   Proceedings of the   th Annual Meeting on Association for Computational Linguistics   ACL      Morristown  NJ  USA  Association for Computational Linguistics         doi                         

  Resnik  Philip  Smith  Noah A   September         The Web as a Parallel Corpus   Computational Linguistics                   doi                             ISSN                 Archived from the original on             Retrieved            

  Halevy  Alon  Norvig  Peter  Pereira  Fernando  March         The Unreasonable Effectiveness of Data   IEEE Intelligent Systems                doi         MIS          ISSN                

  Chen  Leiyu  Li  Shaobo  Bai  Qiang  Yang  Jing  Jiang  Sanlong  Miao  Yanming          Review of Image Classification Algorithms Based on Convolutional Neural Networks   Remote Sensing                 Bibcode     RemS          C  doi         rs         

  Vaswani  Ashish  Shazeer  Noam  Parmar  Niki  Uszkoreit  Jakob  Jones  Llion  Gomez  Aidan N  Kaiser   ukasz  Polosukhin  Illia          Attention is All you Need   PDF   Advances in Neural Information Processing Systems      Curran Associates  Inc  Archived  PDF  from the original on             Retrieved            

  Bahdanau  Dzmitry  Cho  Kyunghyun  Bengio  Yoshua          Neural Machine Translation by Jointly Learning to Align and Translate   arXiv            cs CL  

  Rogers  Anna  Kovaleva  Olga  Rumshisky  Anna          A Primer in BERTology  What We Know About How BERT Works   Transactions of the Association for Computational Linguistics              arXiv             doi         tacl a        S CID                 Archived from the original on             Retrieved            

  a b Movva  Rajiv  Balachandar  Sidhika  Peng  Kenny  Agostini  Gabriel  Garg  Nikhil  Pierson  Emma          Topics  Authors  and Institutions in Large Language Model Research  Trends from   K arXiv Papers   Proceedings of the      Conference of the North American Chapter of the Association for Computational Linguistics  Human Language Technologies  Volume    Long Papers   pp                  arXiv             doi          v       naacl long     Retrieved            

  Hern  Alex     February         New AI fake text generator may be too dangerous to release  say creators   The Guardian  Archived from the original on    February       Retrieved    January      

   ChatGPT a year on    ways the AI chatbot has completely changed the world in    months   Euronews  November           Archived from the original on January           Retrieved January          

  Heaven  Will  March             GPT   is bigger and better than ChatGPT but OpenAI won t say why   MIT Technology Review  Archived from the original on March           Retrieved January          

   Parameters in notable artificial intelligence systems   ourworldindata org  November           Retrieved January          

  Sharma  Shubham                Open source DeepSeek R  uses pure reinforcement learning to match OpenAI o    at     less cost   VentureBeat  Retrieved            

  Zia  Dr Tehseen                Unveiling of Large Multimodal Models  Shaping the Landscape of Language Models in        Unite AI  Retrieved            

  Peng  Bo  et      al           RWKV  Reinventing RNNS for the Transformer Era   arXiv             cs CL  

  Merritt  Rick                What Is a Transformer Model    NVIDIA Blog  Archived from the original on             Retrieved            

  Gu  Albert  Dao  Tri               Mamba  Linear Time Sequence Modeling with Selective State Spaces  arXiv           

  Kaushal  Ayush  Mahowald  Kyle               What do tokens know about their characters and how do they know it   arXiv           

  Yennie Jun                All languages are NOT created  tokenized  equal   Language models cost much more in some languages than others  Archived from the original on             Retrieved             In other words  to express the same sentiment  some languages require up to    times more tokens 

  a b Petrov  Aleksandar  Malfa  Emanuele La  Torr  Philip  Bibi  Adel  June             Language Model Tokenizers Introduce Unfairness Between Languages   NeurIPS  arXiv             Archived from the original on December           Retrieved September                  via openreview net 

   OpenAI API   platform openai com  Archived from the original on April           Retrieved            

  a b Paa   Gerhard  Giesselbach  Sven          Pre trained Language Models   Foundation Models for Natural Language Processing  Artificial Intelligence  Foundations  Theory  and Algorithms  pp              doi                              ISBN                    

  Lundberg  Scott                The Art of Prompt Design  Prompt Boundaries and Token Healing   Medium  Retrieved            

  Dodge  Jesse  Sap  Maarten  Marasovi   Ana  Agnew  William  Ilharco  Gabriel  Groeneveld  Dirk  Mitchell  Margaret  Gardner  Matt          Documenting Large Webtext Corpora  A Case Study on the Colossal Clean Crawled Corpus   arXiv             cs CL  

  Lee  Katherine  Ippolito  Daphne  Nystrom  Andrew  Zhang  Chiyuan  Eck  Douglas  Callison Burch  Chris  Carlini  Nicholas  May         Deduplicating Training Data Makes Language Models Better   PDF   Proceedings of the   th Annual Meeting of the Association for Computational Linguistics     Long Papers             doi          v       acl long     

  Li  Yuanzhi  Bubeck  S bastien  Eldan  Ronen  Del Giorno  Allie  Gunasekar  Suriya  Lee  Yin Tat               Textbooks Are All You Need II  phi     technical report  arXiv           

  Lin  Zhenghao  Gou  Zhibin  Gong  Yeyun  Liu  Xiao  Shen  Yelong  Xu  Ruochen  Lin  Chen  Yang  Yujiu  Jiao  Jian                Rho    Not All Tokens Are What You Need   arXiv             cs CL  

  Brown  Tom B   et      al           Language Models are Few Shot Learners   arXiv             cs CL  

  Abdin  Marah  Jacobs  Sam Ade  Awan  Ammar Ahmad  Aneja  Jyoti  Awadallah  Ahmed  Awadalla  Hany  Bach  Nguyen  Bahree  Amit  Bakhtiari  Arash                Phi   Technical Report  A Highly Capable Language Model Locally on Your Phone   arXiv             cs CL  

  Ouyang  Long  Wu  Jeff  Jiang  Xu  Almeida  Diogo  Wainwright  Carroll L   Mishkin  Pamela  Zhang  Chong  Agarwal  Sandhini  Slama  Katarina  Ray  Alex  Schulman  John  Hilton  Jacob  Kelton  Fraser  Miller  Luke  Simens  Maddie  Askell  Amanda  Welinder  Peter  Christiano  Paul  Leike  Jan  Lowe  Ryan          Training language models to follow instructions with human feedback   arXiv             cs CL  

  Wang  Yizhong  Kordi  Yeganeh  Mishra  Swaroop  Liu  Alisa  Smith  Noah A   Khashabi  Daniel  Hajishirzi  Hannaneh          Self Instruct  Aligning Language Model with Self Generated Instructions   arXiv             cs CL  

  Shazeer  Noam  Mirhoseini  Azalia  Maziarz  Krzysztof  Davis  Andy  Le  Quoc  Hinton  Geoffrey  Dean  Jeff                Outrageously Large Neural Networks  The Sparsely Gated Mixture of Experts Layer   arXiv             cs LG  

  Lepikhin  Dmitry  Lee  HyoukJoong  Xu  Yuanzhong  Chen  Dehao  Firat  Orhan  Huang  Yanping  Krikun  Maxim  Shazeer  Noam  Chen  Zhifeng                GShard  Scaling Giant Models with Conditional Computation and Automatic Sharding   arXiv             cs CL  

  Dai  Andrew M  Du  Nan  December            More Efficient In Context Learning with GLaM   ai googleblog com  Archived from the original on             Retrieved            

  a b c Wei  Jason  Tay  Yi  Bommasani  Rishi  Raffel  Colin  Zoph  Barret  Borgeaud  Sebastian  Yogatama  Dani  Bosma  Maarten  Zhou  Denny  Metzler  Donald  Chi  Ed H   Hashimoto  Tatsunori  Vinyals  Oriol  Liang  Percy  Dean  Jeff  Fedus  William     August         Emergent Abilities of Large Language Models   Transactions on Machine Learning Research  ISSN                 Archived from the original on    March       Retrieved    March      

  Allamar  Jay   Illustrated transformer   Archived from the original on             Retrieved            

  Allamar  Jay   The Illustrated GPT    Visualizing Transformer Language Models    Retrieved            

   Our next generation model  Gemini       Google     February       Archived from the original on    February       Retrieved    February      

   Long context prompting for Claude       December          Archived from the original on August           Retrieved January          

   Rate limits   openai com  Archived from the original on February          Retrieved January          

  Zaib  Munazza  Sheng  Quan Z   Emma Zhang  Wei    February         A Short Survey of Pre trained Language Models for Conversational AI A New Age in NLP   Proceedings of the Australasian Computer Science Week Multiconference  pp            arXiv             doi                          ISBN                     S CID                

  a b c Jurafsky  Dan  Martin  James H     January        Speech and Language Processing  PDF    rd edition draft      ed    Archived  PDF  from the original on    March       Retrieved    May      

   From bare metal to a   B model  infrastructure set up and scripts   imbue com  Archived from the original on             Retrieved            

   metaseq projects OPT chronicles at main   facebookresearch metaseq   GitHub  Archived from the original on             Retrieved            

  Albrecht  Josh                State of the Art  Training  gt   B LLMs on        H    clusters   www latent space  Retrieved            

  Maslej  Nestor  Fattorini  Loredana  Brynjolfsson  Erik  Etchemendy  John  Ligett  Katrina  Lyons  Terah  Manyika  James  Ngo  Helen  Niebles  Juan Carlos               Artificial Intelligence Index Report       arXiv           

  a b Section     and Table   

Kaplan  Jared  McCandlish  Sam  Henighan  Tom  Brown  Tom B   Chess  Benjamin  Child  Rewon  Gray  Scott  Radford  Alec  Wu  Jeffrey  Amodei  Dario          Scaling Laws for Neural Language Models   arXiv             cs LG  

  Gao  Luyu  Madaan  Aman  Zhou  Shuyan  Alon  Uri  Liu  Pengfei  Yang  Yiming  Callan  Jamie  Neubig  Graham                PAL  Program aided Language Models   arXiv             cs CL  

   PAL  Program aided Language Models   reasonwithpal com  Archived from the original on             Retrieved            

  Paranjape  Bhargavi  Lundberg  Scott  Singh  Sameer  Hajishirzi  Hannaneh  Zettlemoyer  Luke  Tulio Ribeiro  Marco                ART  Automatic multi step reasoning and tool use for large language models   arXiv             cs CL  

  Liang  Yaobo  Wu  Chenfei  Song  Ting  Wu  Wenshan  Xia  Yan  Liu  Yu  Ou  Yang  Lu  Shuai  Ji  Lei  Mao  Shaoguang  Wang  Yun  Shou  Linjun  Gong  Ming  Duan  Nan                TaskMatrix AI  Completing Tasks by Connecting Foundation Models with Millions of APIs   arXiv             cs AI  

  Patil  Shishir G   Zhang  Tianjun  Wang  Xin  Gonzalez  Joseph E                 Gorilla  Large Language Model Connected with Massive APIs   arXiv             cs CL  

  Lewis  Patrick  Perez  Ethan  Piktus  Aleksandra  Petroni  Fabio  Karpukhin  Vladimir  Goyal  Naman  K ttler  Heinrich  Lewis  Mike  Yih  Wen tau  Rockt schel  Tim  Riedel  Sebastian  Kiela  Douwe          Retrieval Augmented Generation for Knowledge Intensive NLP Tasks   Advances in Neural Information Processing Systems      Curran Associates  Inc              arXiv             Archived from the original on             Retrieved            

   The Growth Behind LLM based Autonomous Agents   KDnuggets  October          

  Yao  Shunyu  Zhao  Jeffrey  Yu  Dian  Du  Nan  Shafran  Izhak  Narasimhan  Karthik  Cao  Yuan                ReAct  Synergizing Reasoning and Acting in Language Models   arXiv             cs CL  

  Wu  Yue  Prabhumoye  Shrimai  Min  So Yeon     May         SPRING  GPT   Out performs RL Algorithms by Studying Papers and Reasoning   arXiv             cs AI  

  Wang  Zihao  Cai  Shaofei  Liu  Anji  Ma  Xiaojian  Liang  Yitao                Describe  Explain  Plan and Select  Interactive Planning with Large Language Models Enables Open World Multi Task Agents   arXiv             cs AI  

  Shinn  Noah  Cassano  Federico  Labash  Beck  Gopinath  Ashwin  Narasimhan  Karthik  Yao  Shunyu                Reflexion  Language Agents with Verbal Reinforcement Learning   arXiv             cs AI  

  Hao  Shibo  Gu  Yi  Ma  Haodi  Jiahua Hong  Joshua  Wang  Zhen  Zhe Wang  Daisy  Hu  Zhiting                Reasoning with Language Model is Planning with World Model   arXiv             cs CL  

  Zhang  Jenny  Lehman  Joel  Stanley  Kenneth  Clune  Jeff    June         OMNI  Open endedness via Models of human Notions of Interestingness   arXiv             cs AI  

  a b  Voyager   An Open Ended Embodied Agent with Large Language Models   voyager minedojo org  Archived from the original on             Retrieved            

  Park  Joon Sung  O Brien  Joseph C   Cai  Carrie J   Ringel Morris  Meredith  Liang  Percy  Bernstein  Michael S                 Generative Agents  Interactive Simulacra of Human Behavior   arXiv             cs HC  

  Mann  Tobias   How to run an LLM locally on your PC in less than    minutes   www theregister com  Retrieved            

  Nagel  Markus  Amjad  Rana Ali  Baalen  Mart Van  Louizos  Christos  Blankevoort  Tijmen                Up or Down  Adaptive Rounding for Post Training Quantization   Proceedings of the   th International Conference on Machine Learning  PMLR             Archived from the original on             Retrieved            

  Polino  Antonio  Pascanu  Razvan  Alistarh  Dan                Model compression via distillation and quantization   arXiv             cs NE  

  Frantar  Elias  Ashkboos  Saleh  Hoefler  Torsten  Alistarh  Dan                GPTQ  Accurate Post Training Quantization for Generative Pre trained Transformers   arXiv             cs LG  

  Dettmers  Tim  Svirschevski  Ruslan  Egiazarian  Vage  Kuznedelev  Denis  Frantar  Elias  Ashkboos  Saleh  Borzunov  Alexander  Hoefler  Torsten  Alistarh  Dan                SpQR  A Sparse Quantized Representation for Near Lossless LLM Weight Compression   arXiv             cs CL  

  Grootendorst  Maarten   A Visual Guide to Quantization   newsletter maartengrootendorst com  Archived from the original on    Jul       Retrieved            

  Dettmers  Tim  Pagnoni  Artidoro  Holtzman  Ari  Zettlemoyer  Luke                QLoRA  Efficient Finetuning of Quantized LLMs   arXiv             cs LG  

  Kiros  Ryan  Salakhutdinov  Ruslan  Zemel  Rich                Multimodal Neural Language Models   Proceedings of the   st International Conference on Machine Learning  PMLR           Archived from the original on             Retrieved            

  Krizhevsky  Alex  Sutskever  Ilya  Hinton  Geoffrey E          ImageNet Classification with Deep Convolutional Neural Networks   Advances in Neural Information Processing Systems      Curran Associates  Inc  Archived from the original on             Retrieved            

  Antol  Stanislaw  Agrawal  Aishwarya  Lu  Jiasen  Mitchell  Margaret  Batra  Dhruv  Zitnick  C  Lawrence  Parikh  Devi          VQA  Visual Question Answering   ICCV             Archived from the original on             Retrieved            

  Li  Junnan  Li  Dongxu  Savarese  Silvio  Hoi  Steven                BLIP    Bootstrapping Language Image Pre training with Frozen Image Encoders and Large Language Models   arXiv             cs CV  

  Alayrac  Jean Baptiste  Donahue  Jeff  Luc  Pauline  Miech  Antoine  Barr  Iain  Hasson  Yana  Lenc  Karel  Mensch  Arthur  Millican  Katherine  Reynolds  Malcolm  Ring  Roman  Rutherford  Eliza  Cabi  Serkan  Han  Tengda  Gong  Zhitao                Flamingo  a Visual Language Model for Few Shot Learning   Advances in Neural Information Processing Systems                   arXiv             Archived from the original on             Retrieved            

  Driess  Danny  Xia  Fei  Sajjadi  Mehdi S  M   Lynch  Corey  Chowdhery  Aakanksha  Ichter  Brian  Wahid  Ayzaan  Tompson  Jonathan  Vuong  Quan  Yu  Tianhe  Huang  Wenlong  Chebotar  Yevgen  Sermanet  Pierre  Duckworth  Daniel  Levine  Sergey                PaLM E  An Embodied Multimodal Language Model   arXiv             cs LG  

  Liu  Haotian  Li  Chunyuan  Wu  Qingyang  Lee  Yong Jae                Visual Instruction Tuning   arXiv             cs CV  

  Zhang  Hang  Li  Xin  Bing  Lidong                Video LLaMA  An Instruction tuned Audio Visual Language Model for Video Understanding   arXiv             cs CL  

  OpenAI                GPT   Technical Report   arXiv             cs CL  

  OpenAI  September             GPT  V ision  System Card   PDF  

  Pichai  Sundar     May        Google Keynote  Google I O       timestamp        retrieved           

  Wiggers  Kyle     September         Mistral releases Pixtral   B  its first multimodal model   TechCrunch  Retrieved    September      

  a b  Introducing OpenAI o  preview   OpenAI              Retrieved            

  a b Metz  Cade                OpenAI Unveils New A I  That Can  Reason  Through Math and Science Problems   The New York Times  Retrieved            

  Gibney  Elizabeth                China s cheap  open AI model DeepSeek thrills scientists   Nature  Retrieved            

  Lin  Belle                Why Amazon is Betting on  Automated Reasoning  to Reduce AI s Hallucinations  The tech giant says an obscure field that combines AI and math can mitigate but not completely eliminate AI s propensity to provide wrong answers   Wall Street Journal  ISSN                

  Hoffmann  Jordan  Borgeaud  Sebastian  Mensch  Arthur  Buchatskaya  Elena  Cai  Trevor  Rutherford  Eliza  Casas  Diego de Las  Hendricks  Lisa Anne  Welbl  Johannes  Clark  Aidan  Hennigan  Tom  Noland  Eric  Millican  Katie  Driessche  George van den  Damoc  Bogdan                Training Compute Optimal Large Language Models   arXiv             cs CL  

  a b Caballero  Ethan  Gupta  Kshitij  Rish  Irina  Krueger  David          Broken Neural Scaling Laws   arXiv             cs LG  

       emergent abilities of large language models   Jason Wei  Retrieved            

  Bowman  Samuel R           Eight Things to Know about Large Language Models   arXiv             cs CL  

  Mukherjee  Anirban  Chang  Hannah          Heuristic Reasoning in AI  Instrumental Use and Mimetic Absorption   arXiv             cs AI  

  Hahn  Michael  Goyal  Navin                A Theory of Emergent In Context Learning as Implicit Structure Induction   arXiv             cs LG  

  Pilehvar  Mohammad Taher  Camacho Collados  Jose  June         Proceedings of the      Conference of the North   Proceedings of the      Conference of the North American Chapter of the Association for Computational Linguistics  Human Language Technologies  Volume    Long and Short Papers   Minneapolis  Minnesota  Association for Computational Linguistics             doi          v  N         S CID                 Archived from the original on             Retrieved            

   WiC  The Word in Context Dataset   pilehvar github io  Archived from the original on             Retrieved            

  Patel  Roma  Pavlick  Ellie                Mapping Language Models to Grounded Conceptual Spaces   ICLR  Archived from the original on             Retrieved            

  A Closer Look at Large Language Models Emergent Abilities Archived            at the Wayback Machine  Yao Fu  Nov          

  Ornes  Stephen  March             The Unpredictable Abilities Emerging From Large AI Models   Quanta Magazine  Archived from the original on March           Retrieved March          

  Schaeffer  Rylan  Miranda  Brando  Koyejo  Sanmi                Are Emergent Abilities of Large Language Models a Mirage    arXiv             cs AI  

  Blank  Idan A   November         What are large language models supposed to model    Trends in Cognitive Sciences                    doi         j tics              PMID               

  https   transformer circuits pub      attribution graphs biology html dives poems  Ctitle On the Biology of a Large Language Model  Chapter on Planning in Poems 

  Nanda  Neel  Chan  Lawrence  Lieberum  Tom  Smith  Jess  Steinhardt  Jacob                Progress measures for grokking via mechanistic interpretability   arXiv             cs LG  

  a b c d e Mitchell  Melanie  Krakauer  David C      March         The debate over understanding in AI s large language models   Proceedings of the National Academy of Sciences            e            arXiv             Bibcode     PNAS          M  doi         pnas             PMC                PMID               

  Metz  Cade     May         Microsoft Says New A I  Shows Signs of Human Reasoning   The New York Times 

  a b Bubeck  S bastien  Chandrasekaran  Varun  Eldan  Ronen  Gehrke  Johannes  Horvitz  Eric  Kamar  Ece  Lee  Peter  Lee  Yin Tat  Li  Yuanzhi  Lundberg  Scott  Nori  Harsha  Palangi  Hamid  Ribeiro  Marco Tulio  Zhang  Yi          Sparks of Artificial General Intelligence  Early experiments with GPT     arXiv             cs CL  

   Anthropic CEO Dario Amodei pens a smart look at our AI future   Fast Company  October          

   ChatGPT is more like an  alien intelligence  than a human brain  says futurist   ZDNET        Archived from the original on    June       Retrieved    June      

  a b Newport  Cal     April         What Kind of Mind Does ChatGPT Have    The New Yorker  Archived from the original on    June       Retrieved    June      

  Roose  Kevin     May         Why an Octopus like Creature Has Come to Symbolize the State of A I   The New York Times  Archived from the original on    May       Retrieved    June      

   The A to Z of Artificial Intelligence   Time Magazine     April       Archived from the original on    June       Retrieved    June      

  Ji  Ziwei  Lee  Nayeon  Frieske  Rita  Yu  Tiezheng  Su  Dan  Xu  Yan  Ishii  Etsuko  Bang  Yejin  Dai  Wenliang  Madotto  Andrea  Fung  Pascale  November         Survey of Hallucination in Natural Language Generation   pdf   ACM Computing Surveys           Association for Computing Machinery        arXiv             doi                  S CID                 Archived from the original on    March       Retrieved    January      

  Varshney  Neeraj  Yao  Wenlin  Zhang  Hongming  Chen  Jianshu  Yu  Dong          A Stitch in Time Saves Nine  Detecting and Mitigating Hallucinations of LLMs by Validating Low Confidence Generation   arXiv             cs CL  

  Lakoff  George         Philosophy in the Flesh  The Embodied Mind and Its Challenge to Western Philosophy  Appendix  The Neural Theory of Language Paradigm  New York Basic Books  pp                ISBN                        

  Evans  Vyvyan          The Language Myth  Cambridge University Press  ISBN                        

  Friston  Karl J          Active Inference  The Free Energy Principle in Mind  Brain  and Behavior  Chapter   The Generative Models of Active Inference  The MIT Press  ISBN                        

  a b Huyen  Chip  October             Evaluation Metrics for Language Modeling   The Gradient  Retrieved January          

  openai simple evals  OpenAI              retrieved           

  openai evals  OpenAI              archived from the original on             retrieved           

  a b Clark  Christopher  Lee  Kenton  Chang  Ming Wei  Kwiatkowski  Tom  Collins  Michael  Toutanova  Kristina          BoolQ  Exploring the Surprising Difficulty of Natural Yes No Questions   arXiv             cs CL  

  a b c Wayne Xin Zhao  Zhou  Kun  Li  Junyi  Tang  Tianyi  Wang  Xiaolei  Hou  Yupeng  Min  Yingqian  Zhang  Beichen  Zhang  Junjie  Dong  Zican  Du  Yifan  Yang  Chen  Chen  Yushuo  Chen  Zhipeng  Jiang  Jinhao  Ren  Ruiyang  Li  Yifan  Tang  Xinyu  Liu  Zikang  Liu  Peiyu  Nie  Jian Yun  Wen  Ji Rong          A Survey of Large Language Models   arXiv             cs CL  

  Nangia  Nikita and Vania  Clara and Bhalerao  Rasika and Bowman  Samuel R   November         CrowS Pairs  A Challenge Dataset for Measuring Social Biases in Masked Language Models   In Webber  Bonnie and Cohn  Trevor and He  Yulan and Liu  Yang  ed    Proceedings of the      Conference on Empirical Methods in Natural Language Processing  EMNLP   Association for Computational Linguistics  pp                  arXiv             doi          v       emnlp main       cite conference     CS  maint  multiple names  authors list  link 

  Nadeem  Moin and Bethke  Anna and Reddy  Siva  August         StereoSet  Measuring stereotypical bias in pretrained language models   In Zong  Chengqing and Xia  Fei and Li  Wenjie and Navigli  Roberto  ed    Proceedings of the   th Annual Meeting of the Association for Computational Linguistics and the   th International Joint Conference on Natural Language Processing  Volume    Long Papers   Association for Computational Linguistics  pp                  arXiv             doi          v       acl long       cite conference     CS  maint  multiple names  authors list  link 

  Simpson  Shmona and Nukpezah  Jonathan and Kie Brooks and Pandya  Raaghav     December         Parity benchmark for measuring bias in LLMs   AI and Ethics  Springer  doi         s                    cite journal     CS  maint  multiple names  authors list  link 

  Caramancion  Kevin Matthe                News Verifiers Showdown  A Comparative Performance Evaluation of ChatGPT      ChatGPT      Bing AI  and Bard in News Fact Checking        IEEE Future Networks World Forum  FNWF   IEEE  pp            arXiv             doi         FNWF                     ISBN                        

   Sanitized open source datasets for natural language and code understanding  how we evaluated our   B model   imbue com  Archived from the original on             Retrieved            

  Srivastava  Aarohi  et      al           Beyond the Imitation Game  Quantifying and extrapolating the capabilities of language models   arXiv             cs CL  

  Lin  Stephanie  Hilton  Jacob  Evans  Owain          TruthfulQA  Measuring How Models Mimic Human Falsehoods   arXiv             cs CL  

  a b Zellers  Rowan  Holtzman  Ari  Bisk  Yonatan  Farhadi  Ali  Choi  Yejin          HellaSwag  Can a Machine Really Finish Your Sentence    arXiv             cs CL  

   Prepare for truly useful large language models   Nature Biomedical Engineering                  March       doi         s                   PMID                S CID                

   Your job is  probably  safe from artificial intelligence   The Economist    May       Archived from the original on    June       Retrieved    June      

   Generative AI Could Raise Global GDP by      Goldman Sachs  Archived from the original on    June       Retrieved    June      

  Brinkmann  Levin  Baumann  Fabian  Bonnefon  Jean Fran ois  Derex  Maxime  M ller  Thomas F   Nussberger  Anne Marie  Czaplicka  Agnieszka  Acerbi  Alberto  Griffiths  Thomas L   Henrich  Joseph  Leibo  Joel Z   McElreath  Richard  Oudeyer  Pierre Yves  Stray  Jonathan  Rahwan  Iyad                Machine culture   Nature Human Behaviour                     arXiv             doi         s                   ISSN                 PMID               

  Peng  Zhencan  Wang  Zhizhi  Deng  Dong     June         Near Duplicate Sequence Search at Scale for Large Language Model Memorization Evaluation   PDF   Proceedings of the ACM on Management of Data               doi                  S CID                 Archived  PDF  from the original on             Retrieved             Citing Lee et al      

  Peng  Wang  amp  Deng       p         

  Stephen Council    Dec         How Googlers cracked an SF rival s tech model with a single word   SFGATE  Archived from the original on    December      

  Alba  Davey    May         AI chatbots have been used to create dozens of news content farms   The Japan Times  Retrieved    June      

   Could chatbots help devise the next pandemic virus    Science     June       doi         science adj      Archived from the original on    June       Retrieved    June      

  Hubinger  Evan     January         Sleeper Agents  Training Deceptive LLMs that Persist Through Safety Training   arXiv             cs CR  

  Kang  Daniel          Exploiting programmatic behavior of LLMs  Dual use through standard security attacks   arXiv             cs CR  

  a b  Russian propaganda may be flooding AI models   The American Sunlight Project     February       Retrieved            

  Goudarzi  Sara                Russian networks flood the Internet with propaganda  aiming to corrupt AI chatbots   Bulletin of the Atomic Scientists  Retrieved            

  Wang  Yongge     June         Encryption Based Covert Channel for Large Language Models   PDF   IACR ePrint           Archived  PDF  from the original on    June       Retrieved    June      

  a b Stokel Walker  Chris  November             ChatGPT Replicates Gender Bias in Recommendation Letters   Scientific American  Archived from the original on             Retrieved            

  Luo  Queenie  Puett  Michael J   Smith  Michael D                 A Perspectival Mirror of the Elephant  Investigating Language Bias on Google  ChatGPT  Wikipedia  and YouTube   arXiv           v   cs CY  

  Wang  Angelina  Morgenstern  Jamie  Dickerson  John P      February         Large language models that replace human participants can harmfully misportray and flatten identity groups   Nature Machine Intelligence                  arXiv             doi         s                z 

  Cheng  Myra  Durmus  Esin  Jurafsky  Dan               Marked Personas  Using Natural Language Prompts to Measure Stereotypes in Language Models  arXiv           

  Kotek  Hadas  Dockum  Rikker  Sun  David                Gender bias and stereotypes in Large Language Models   Proceedings of the ACM Collective Intelligence Conference  CI      New York  NY  USA  Association for Computing Machinery  pp              doi                          ISBN                        

  Choi  Hyeong Kyu  Xu  Weijie  Xue  Chi  Eckman  Stephanie  Reddy  Chandan K                Mitigating Selection Bias with Node Pruning and Auxiliary Options  arXiv           

  Zheng  Chujie  Zhou  Hao  Meng  Fandong  Zhou  Jie  Huang  Minlie               Large Language Models Are Not Robust Multiple Choice Selectors  arXiv           

  Heikkil   Melissa  August            AI language models are rife with different political biases   MIT Technology Review  Retrieved            

  Mehta  Sourabh                How Much Energy Do LLMs Consume  Unveiling the Power Behind AI   Association of Data Scientists  Retrieved            

   Artificial Intelligence wants to go nuclear  Will it work    NPR  Retrieved            

  Roy  Dareen  December             AI s energy hunger fuels geothermal startups but natgas rivalry clouds future   Reuters 


Further reading edit 
Jurafsky  Dan  Martin  James  H  Speech and Language Processing  An Introduction to Natural Language Processing  Computational Linguistics  and Speech Recognition   rd Edition draft       
Zhao  Wayne Xin  et      al           A Survey of Large Language Models   arXiv             cs CL  
Kaddour  Jean  et      al           Challenges and Applications of Large Language Models   arXiv             cs CL  
Yin  Shukang  Fu  Chaoyou  Zhao  Sirui  Li  Ke  Sun  Xing  Xu  Tong  Chen  Enhong          A Survey on Multimodal Large Language Models   National Science Review           nwae     arXiv             doi         nsr nwae     PMC                PMID               
 AI Index Report        Artificial Intelligence Index   aiindex stanford edu  Retrieved            
Frank  Michael C      June         Baby steps in evaluating the capacities of large language models   Nature Reviews Psychology                  doi         s                x  ISSN                 S CID                 Retrieved   July      
Anwar  U   Saparov  A   Rando  J   Paleka  D   Turpin  M   Hase  P   Lubana  E  S   Jenner  E   Casper  S   Sourbut  O   Edelman  B  L   Zhang  Z   G nther  M   Korinek  A   Hernandez Orallo  J   Hammond  L   Bigelow  E   Pan  A   Langosco  L   Krueger  D           Foundational Challenges in Assuring Alignment and Safety of Large Language Models   arXiv             cs LG  
vteNatural language processingGeneral terms
AI complete
Bag of words
n gram
Bigram
Trigram
Computational linguistics
Natural language understanding
Stop words
Text processing
Text analysis
Argument mining
Collocation extraction
Concept mining
Coreference resolution
Deep linguistic processing
Distant reading
Information extraction
Named entity recognition
Ontology learning
Parsing
Semantic parsing
Syntactic parsing
Part of speech tagging
Semantic analysis
Semantic role labeling
Semantic decomposition
Semantic similarity
Sentiment analysis
Terminology extraction
Text mining
Textual entailment
Truecasing
Word sense disambiguation
Word sense induction
Text segmentation
Compound term processing
Lemmatisation
Lexical analysis
Text chunking
Stemming
Sentence segmentation
Word segmentation

Automatic summarization
Multi document summarization
Sentence extraction
Text simplification
Machine translation
Computer assisted
Example based
Rule based
Statistical
Transfer based
Neural
Distributional semantics models
BERT
Document term matrix
Explicit semantic analysis
fastText
GloVe
Language model  large 
Latent semantic analysis
Seq seq
Word embedding
Word vec
Language resources datasets and corporaTypes andstandards
Corpus linguistics
Lexical resource
Linguistic Linked Open Data
Machine readable dictionary
Parallel text
PropBank
Semantic network
Simple Knowledge Organization System
Speech corpus
Text corpus
Thesaurus  information retrieval 
Treebank
Universal Dependencies
Data
BabelNet
Bank of English
DBpedia
FrameNet
Google Ngram Viewer
UBY
WordNet
Wikidata
Automatic identificationand data capture
Speech recognition
Speech segmentation
Speech synthesis
Natural language generation
Optical character recognition
Topic model
Document classification
Latent Dirichlet allocation
Pachinko allocation
Computer assistedreviewing
Automated essay scoring
Concordancer
Grammar checker
Predictive text
Pronunciation assessment
Spell checker
Natural languageuser interface
Chatbot
Interactive fiction  cf  Syntax guessing 
Question answering
Virtual assistant
Voice user interface
Related
Formal semantics
Hallucination
Natural Language Toolkit
spaCy

vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects






Retrieved from  https   en wikipedia org w index php title Large language model amp oldid