This article has multiple issues  Please help improve it or discuss these issues on the talk page   Learn how and when to remove these messages 

      Some of this article s listed sources may not be reliable  Please help improve this article by looking for better  more reliable sources  Unreliable citations may be challenged and removed    October        Learn how and when to remove this message 
The neutrality of this article is disputed  Relevant discussion may be found on the talk page  Please do not remove this message until conditions to do so are met    March        Learn how and when to remove this message 
    
  Learn how and when to remove this message 
Structuring text as input to generative artificial intelligence
Not to be confused with Command prompt Prompt engineering is the process of structuring or crafting an instruction in order to produce the best possible output from a generative artificial intelligence  AI  model            
A prompt is natural language text describing the task that an AI should perform             A prompt for a text to text language model can be a query  a command  or a longer statement including context  instructions  and conversation history  Prompt engineering may involve phrasing a query  specifying a style  choice of words and grammar             providing relevant context  or describing a character for the AI to mimic            
When communicating with a text to image or a text to audio model  a typical prompt is a description of a desired output such as  a high quality photo of an astronaut riding a horse             or  Lo fi slow BPM electro chill with organic samples              Prompting a text to image model may involve adding  removing  emphasizing  and re ordering words to achieve a desired subject  style             layout  lighting             and aesthetic 


History edit 
In       researchers first proposed that all previously separate tasks in natural language processing  NLP  could be cast as a question answering problem over a context  In addition  they trained a first single  joint  multi task model that would answer any task related question like  What is the sentiment  or  Translate this sentence to German  or  Who is the president             
The AI boom saw an increase in the amount of  prompting technique  to get the model to output the desired outcome and avoid nonsensical output  a process characterized by trial and error             After the release of ChatGPT in       prompt engineering was soon seen as an important business skill  albeit one with an uncertain economic future            
A repository for prompts reported that over       public prompts for around     datasets were available in February                   In       the chain of thought prompting technique was proposed by Google researchers                          In       several text to text and text to image prompt databases were made publicly available                          The Personalized Image Prompt  PIP  dataset  a generated image text dataset that has been categorized by       users  has also been made available publicly in                  

Text to text edit 
Multiple distinct prompt engineering techniques have been published 

Chain of thought edit 
See also  Reflection  artificial intelligence 
According to Google Research  chain of thought  CoT  prompting is a technique that allows large language models  LLMs  to solve a problem as a series of intermediate steps before giving a final answer  In       Google Brain reported that chain of thought prompting improves reasoning ability by inducing the model to answer a multi step problem with steps of reasoning that mimic a train of thought                          Chain of thought techniques were developed to help LLMs handle multi step reasoning tasks  such as arithmetic or commonsense reasoning questions                         
For example  given the question   Q  The cafeteria had    apples  If they used    to make lunch and bought   more  how many apples do they have    Google claims that a CoT prompt might induce the LLM to answer  A  The cafeteria had    apples originally  They used    to make lunch  So they had              They bought   more apples  so they have            The answer is                 When applied to PaLM  a     billion parameter language model  according to Google  CoT prompting significantly aided the model  allowing it to perform comparably with task specific fine tuned models on several tasks  achieving state of the art results at the time on the GSM K mathematical reasoning benchmark              It is possible to fine tune models on CoT reasoning datasets to enhance this capability further and stimulate better interpretability                         
An example of a CoT prompting             

   Q   question 
   A  Let s think step by step 

As originally proposed by Google              each CoT prompt included a few Q amp A examples  This made it a few shot prompting technique  However  according to researchers at Google and the University of Tokyo  simply appending the words  Let s think step by step               has also proven effective  which makes CoT a zero shot prompting technique  OpenAI claims that this prompt allows for better scaling as a user no longer needs to formulate many specific CoT Q amp A examples             

In context learning edit 
In context learning  refers to a model s ability to temporarily learn from prompts  For example  a prompt may include a few examples for a model to learn from  such as asking the model to complete  maison   house  chat   cat  chien     the expected response being dog               an approach called few shot learning             
In context learning is an emergent ability             of large language models  It is an emergent property of model scale  meaning that breaks             in downstream scaling laws occur  leading to its efficacy increasing at a different rate in larger models than in smaller models                          Unlike training and fine tuning  which produce lasting changes  in context learning is temporary              Training models to perform in context learning can be viewed as a form of meta learning  or  learning to learn              

Self consistency decoding edit 
Self consistency decoding             performs several chain of thought rollouts  then selects the most commonly reached conclusion out of all the rollouts  If the rollouts disagree by a lot  a human can be queried for the correct chain of thought             

Tree of thought edit 
Tree of thought prompting generalizes chain of thought by prompting the model to generate one or more  possible next steps   and then running the model on each of the possible next steps by breadth first  beam  or some other method of tree search              The LLM has additional modules that can converse the history of the problem solving process to the LLM  which allows the system to  backtrack steps  the problem solving process 

Prompting to disclose uncertainty edit 
By default  the output of language models may not contain estimates of uncertainty  The model may output text that appears confident  though the underlying token predictions have low likelihood scores  Large language models like GPT   can have accurately calibrated likelihood scores in their token predictions              and so the model output uncertainty can be directly estimated by reading out the token prediction likelihood scores 

Prompting to estimate model sensitivity edit 
Research consistently demonstrates that LLMs are highly sensitive to subtle variations in prompt formatting  structure  and linguistic properties  Some studies have shown up to    accuracy points across formatting changes in few shot settings              Linguistic features significantly influence prompt effectiveness such as morphology  syntax  and lexico semantic changes which meaningfully enhance task performance across a variety of tasks                         Clausal syntax  for example  improves consistency and reduces uncertainty in knowledge retrieval              This sensitivity persists even with larger model sizes  additional few shot examples  or instruction tuning 
To address sensitivity of models and make them more robust  several methods have been proposed  FormatSpread facilitates systematic analysis by evaluating a range of plausible prompt formats  offering a more comprehensive performance interval              Similarly  PromptEval estimates performance distributions across diverse prompts  enabling robust metrics such as performance quantiles and accurate evaluations under constrained budgets             

Automatic prompt generation edit 
Retrieval augmented generation edit 
Main article  Retrieval augmented generation
Retrieval augmented generation  RAG  is a technique that enables generative artificial intelligence  Gen AI  models to retrieve and incorporate new information  It modifies interactions with a large language model  LLM  so that the model responds to user queries with reference to a specified set of documents  using this information to supplement information from its pre existing training data  This allows LLMs to use domain specific and or updated information             
RAG improves large language models  LLMs  by incorporating information retrieval before generating responses  Unlike traditional LLMs that rely on static training data  RAG pulls relevant text from databases  uploaded documents  or web sources  According to Ars Technica   RAG is a way of improving LLM performance  in essence by blending the LLM process with a web search or other document look up process to help LLMs stick to the facts   This method helps reduce AI hallucinations  which have led to real world issues like chatbots inventing policies or lawyers citing nonexistent legal cases  By dynamically retrieving information  RAG enables AI to provide more accurate responses without frequent retraining             

Graph retrieval augmented generation edit 
GraphRAG with a knowledge graph combining access patterns for unstructured  structured  and mixed data
GraphRAG              coined by Microsoft Research  is a technique that extends RAG with the use of a knowledge graph  usually  LLM generated  to allow the model to connect disparate pieces of information  synthesize insights  and holistically understand summarized semantic concepts over large data collections  It was shown to be effective on datasets like the Violent Incident Information from News Articles  VIINA              
Earlier work showed the effectiveness of using a knowledge graph for question answering using text to query generation              These techniques can be combined to search across both unstructured and structured data  providing expanded context  and improved ranking 

Using language models to generate prompts edit 
Large language models  LLM  themselves can be used to compose prompts for large language models              The automatic prompt engineer algorithm uses one LLM to beam search over prompts for another LLM                         

There are two LLMs  One is the target LLM  and another is the prompting LLM 
Prompting LLM is presented with example input output pairs  and asked to generate instructions that could have caused a model following the instructions to generate the outputs  given the inputs 
Each of the generated instructions is used to prompt the target LLM  followed by each of the inputs  The log probabilities of the outputs are computed and added  This is the score of the instruction 
The highest scored instructions are given to the prompting LLM for further variations 
Repeat until some stopping criteria is reached  then output the highest scored instructions 
CoT examples can be generated by LLM themselves  In  auto CoT               a library of questions are converted to vectors by a model such as BERT  The question vectors are clustered  Questions nearest to the centroids of each cluster are selected  An LLM does zero shot CoT on each question  The resulting CoT examples are added to the dataset  When prompted with a new question  CoT examples to the nearest questions can be retrieved and added to the prompt 

Text to image edit 
See also  Artificial intelligence art        Prompt engineering and sharing
Example of prompt engineering for text to image generation  with Fooocus
In       text to image models like DALL E    Stable Diffusion  and Midjourney were released to the public              These models take text prompts as input and use them to generate AI generated images  Text to image models typically do not understand grammar and sentence structure in the same way as large language models              thus may require a different set of prompting techniques 
Text to image models do not natively understand negation  The prompt  a party with no cake  is likely to produce an image including a cake              As an alternative  negative prompts allow a user to indicate  in a separate prompt  which terms should not appear in the resulting image              Techniques such as framing the normal prompt into a sequence to sequence language modeling problem can be used to automatically generate an output for the negative prompt             

Demonstration of the effect of negative prompts on images generated with Stable Diffusion
Top  no negative promptCentre   green trees Bottom   round stones  round rocks 
Prompt formats edit 
A text to image prompt commonly includes a description of the subject of the art  the desired medium  such as digital painting or photography   style  such as hyperrealistic or pop art   lighting  such as rim lighting or crepuscular rays   color  and texture              Word order also affects the output of a text to image prompt  Words closer to the start of a prompt may be emphasized more heavily            
The Midjourney documentation encourages short  descriptive prompts  instead of  Show me a picture of lots of blooming California poppies  make them bright  vibrant orange  and draw them in an illustrated style with colored pencils   an effective prompt might be  Bright orange California poppies drawn with colored pencils              

Artist styles edit 
Some text to image models are capable of imitating the style of particular artists by name  For example  the phrase in the style of Greg Rutkowski has been used in Stable Diffusion and Midjourney prompts to generate images in the distinctive style of Polish digital artist Greg Rutkowski              Famous artists such as Vincent van Gogh and Salvador Dal  have also been used for styling and testing             

Non text prompts edit 
Some approaches augment or replace natural language text prompts with non text input 

Textual inversion and embeddings edit 
For text to image models  textual inversion             performs an optimization process to create a new word embedding based on a set of example images  This embedding vector acts as a  pseudo word  which can be included in a prompt to express the content or style of the examples 

Image prompting edit 
In       Meta s AI research released Segment Anything  a computer vision model that can perform image segmentation by prompting  As an alternative to text prompts  Segment Anything can accept bounding boxes  segmentation masks  and foreground background points             

Using gradient descent to search for prompts edit 
In  prefix tuning                prompt tuning   or  soft prompting               floating point valued vectors are searched directly by gradient descent to maximize the log likelihood on outputs 
Formally  let 
  
    
      
        
          E
        
         
         
        
          
            e
            
               
            
          
        
         
          x     
         
        
          
            e
            
              k
            
          
        
         
      
    
      displaystyle  mathbf  E      mathbf  e        dots   mathbf  e  k      
  
 be a set of soft prompt tokens  tunable embeddings   while 
  
    
      
        
          X
        
         
         
        
          
            x
            
               
            
          
        
         
          x     
         
        
          
            x
            
              m
            
          
        
         
      
    
      displaystyle  mathbf  X      mathbf  x        dots   mathbf  x  m      
  
 and 
  
    
      
        
          Y
        
         
         
        
          
            y
            
               
            
          
        
         
          x     
         
        
          
            y
            
              n
            
          
        
         
      
    
      displaystyle  mathbf  Y      mathbf  y        dots   mathbf  y  n      
  
 be the token embeddings of the input and output respectively  During training  the tunable embeddings  input  and output tokens are concatenated into a single sequence 
  
    
      
        
          concat
        
         
        
          E
        
         
        
          X
        
         
        
          Y
        
         
      
    
      displaystyle   text concat    mathbf  E    mathbf  X    mathbf  Y    
  
  and fed to the LLMs  The losses are computed over the 
  
    
      
        
          Y
        
      
    
      displaystyle  mathbf  Y   
  
 tokens  the gradients are backpropagated to prompt specific parameters  in prefix tuning  they are parameters associated with the prompt tokens at each layer  in prompt tuning  they are merely the soft tokens added to the vocabulary             
More formally  this is prompt tuning  Let an LLM be written as 
  
    
      
        L
        L
        M
         
        X
         
         
        F
         
        E
         
        X
         
         
      
    
      displaystyle LLM X  F E X   
  
  where 
  
    
      
        X
      
    
      displaystyle X 
  
 is a sequence of linguistic tokens  
  
    
      
        E
      
    
      displaystyle E 
  
 is the token to vector function  and 
  
    
      
        F
      
    
      displaystyle F 
  
 is the rest of the model  In prefix tuning  one provides a set of input output pairs 
  
    
      
         
         
        
          X
          
            i
          
        
         
        
          Y
          
            i
          
        
         
        
           
          
            i
          
        
      
    
      displaystyle    X  i  Y  i      i  
  
  and then use gradient descent to search for 
  
    
      
        arg
          x     
        
          max
          
            
              
                Z
                  x e 
              
            
          
        
        
            x     
          
            i
          
        
        log
          x     
        P
        r
         
        
          Y
          
            i
          
        
        
           
        
        
          
            
              Z
                x e 
            
          
        
          x     
        E
         
        
          X
          
            i
          
        
         
         
      
    
      displaystyle  arg  max    tilde  Z   sum   i  log Pr Y  i    tilde  Z   ast E X  i    
  
  In words  
  
    
      
        log
          x     
        P
        r
         
        
          Y
          
            i
          
        
        
           
        
        
          
            
              Z
                x e 
            
          
        
          x     
        E
         
        
          X
          
            i
          
        
         
         
      
    
      displaystyle  log Pr Y  i    tilde  Z   ast E X  i    
  
 is the log likelihood of outputting 
  
    
      
        
          Y
          
            i
          
        
      
    
      displaystyle Y  i  
  
  if the model first encodes the input 
  
    
      
        
          X
          
            i
          
        
      
    
      displaystyle X  i  
  
 into the vector 
  
    
      
        E
         
        
          X
          
            i
          
        
         
      
    
      displaystyle E X  i   
  
  then prepend the vector with the  prefix vector  
  
    
      
        
          
            
              Z
                x e 
            
          
        
      
    
      displaystyle   tilde  Z   
  
  then apply 
  
    
      
        F
      
    
      displaystyle F 
  
 
For prefix tuning  it is similar  but the  prefix vector  
  
    
      
        
          
            
              Z
                x e 
            
          
        
      
    
      displaystyle   tilde  Z   
  
 is pre appended to the hidden states in every layer of the model 
An earlier result             uses the same idea of gradient descent search  but is designed for masked language models like BERT  and searches only over token sequences  rather than numerical vectors  Formally  it searches for 
  
    
      
        arg
          x     
        
          max
          
            
              
                X
                  x e 
              
            
          
        
        
            x     
          
            i
          
        
        log
          x     
        P
        r
         
        
          Y
          
            i
          
        
        
           
        
        
          
            
              X
                x e 
            
          
        
          x     
        
          X
          
            i
          
        
         
      
    
      displaystyle  arg  max    tilde  X   sum   i  log Pr Y  i    tilde  X   ast X  i   
  
 where 
  
    
      
        
          
            
              X
                x e 
            
          
        
      
    
      displaystyle   tilde  X   
  
 is ranges over token sequences of a specified length 

Limitations edit 
While the process of writing and refining a prompt for an LLM or generative AI shares some parallels with an iterative engineering design process  such as through discovering  best principles  to reuse and discovery through reproducible experimentation  the actual learned principles and skills depend heavily on the specific model being learned rather than being generalizable across the entire field of prompt based generative models  Such patterns are also volatile and exhibit significantly different results from seemingly insignificant prompt changes              According to The Wall Street Journal in       the job of prompt engineer was one of the hottest in       but has become obsolete due to models that better intuit user intent and to company trainings             

Prompt injection edit 
Main article  Prompt injection
See also  SQL injection  Cross site scripting  and Social engineering  security 
Prompt injection is a cybersecurity exploit in which adversaries craft inputs that appear legitimate but are designed to cause unintended behavior in machine learning models  particularly large language models  LLMs   This attack takes advantage of the model s inability to distinguish between developer defined prompts and user inputs  allowing adversaries to bypass safeguards and influence model behaviour  While LLMs are designed to follow trusted instructions  they can be manipulated into carrying out unintended responses through carefully crafted inputs                         

References edit 

  a b c Genkina  Dina  March            AI Prompt Engineering is Dead  Long live AI prompt engineering   IEEE Spectrum  Retrieved January          

  Radford  Alec  Wu  Jeffrey  Child  Rewon  Luan  David  Amodei  Dario  Sutskever  Ilya          Language Models are Unsupervised Multitask Learners   PDF   OpenAI  We demonstrate language models can perform down stream tasks in a zero shot setting   without any parameter or architecture modification

  a b Wahle  Jan Philip  Ruas  Terry  Xu  Yang  Gipp  Bela          Paraphrase Types Elicit Prompt Engineering Capabilities   In Al Onaizan  Yaser  Bansal  Mohit  Chen  Yun Nung  eds    Proceedings of the      Conference on Empirical Methods in Natural Language Processing  Miami  Florida  USA  Association for Computational Linguistics  pp                    arXiv             doi          v       emnlp main     

  Heaven  Will Douglas  April            This horse riding astronaut is a milestone on AI s long road towards understanding   MIT Technology Review  Retrieved August          

  Wiggers  Kyle  June             Meta open sources an AI powered music generator   TechCrunch  Retrieved August           Next  I gave a more complicated prompt to attempt to throw MusicGen for a loop   Lo fi slow BPM electro chill with organic samples  

  a b Diab  Mohamad  Herrera  Julian  Chernow  Bob  October             Stable Diffusion Prompt Book   PDF   Retrieved August          Prompt engineering is the process of structuring words that can be interpreted and understood by a text to image model  Think of it as the language you need to speak in order to tell an AI model what to draw 

   How to Write AI Photoshoot Prompts  A Guide for Better Product Photos   claid ai  June           Retrieved June          

  McCann  Bryan  Shirish  Nitish  Xiong  Caiming  Socher  Richard          The Natural Language Decathlon  Multitask Learning as Question Answering   arXiv             cs CL  

  Knoth  Nils  Tolzin  Antonia  Janson  Andreas  Leimeister  Jan Marco  June            AI literacy and its implications for prompt engineering strategies   Computers and Education  Artificial Intelligence             doi         j caeai              ISSN              X 

  Bach  Stephen H   Sanh  Victor  Yong  Zheng Xin  Webson  Albert  Raffel  Colin  Nayak  Nihal V   Sharma  Abheesht  Kim  Taewoon  M Saiful Bari  Fevry  Thibault  Alyafeai  Zaid  Dey  Manan  Santilli  Andrea  Sun  Zhiqing  Ben David  Srulik  Xu  Canwen  Chhablani  Gunjan  Wang  Han  Jason Alan Fries  Al shaibani  Maged S   Sharma  Shanya  Thakker  Urmish  Almubarak  Khalid  Tang  Xiangru  Radev  Dragomir  Mike Tian Jian Jiang  Rush  Alexander M           PromptSource  An Integrated Development Environment and Repository for Natural Language Prompts   arXiv             cs LG  

  a b c d e f Wei  Jason  Wang  Xuezhi  Schuurmans  Dale  Bosma  Maarten  Ichter  Brian  Xia  Fei  Chi  Ed H   Le  Quoc V   Zhou  Denny  October            Chain of Thought Prompting Elicits Reasoning in Large Language Models  Advances in Neural Information Processing Systems  NeurIPS        Vol           arXiv            

  Wei  Jason  Zhou  May             Language Models Perform Reasoning via Chain of Thought   ai googleblog com  Retrieved March          

  Chen  Brian X   June             How to Turn Your Chatbot Into a Life Coach   The New York Times 

  Chen  Brian X   May             Get the Best From ChatGPT With These Golden Prompts   The New York Times  ISSN                 Retrieved August          

  Chen  Zijie  Zhang  Lichao  Weng  Fangsheng  Pan  Lili  Lan  Zhenzhong  June             Tailored Visions  Enhancing Text to Image Generation with Personalized Prompt Rewriting        IEEE CVF Conference on Computer Vision and Pattern Recognition  CVPR   IEEE  pp                  arXiv             doi         cvpr                  ISBN                        

  Sharan Narang and Aakanksha Chowdhery  April            Pathways Language Model  PaLM   Scaling to     Billion Parameters for Breakthrough Performance  

  Dang  Ekta  February            Harnessing the power of GPT   in scientific research   VentureBeat  Retrieved March          

  Montti  Roger  May             Google s Chain of Thought Prompting Can Boost Today s Best Algorithms   Search Engine Journal  Retrieved March          

  Chung  Hyung Won  Hou  Le  Longpre  Shayne  Zoph  Barret  Tay  Yi  Fedus  William  Li  Yunxuan  Wang  Xuezhi  Dehghani  Mostafa  Brahma  Siddhartha  Webson  Albert  Gu  Shixiang Shane  Dai  Zhuyun  Suzgun  Mirac  Chen  Xinyun  Chowdhery  Aakanksha  Castro Ros  Alex  Pellat  Marie  Robinson  Kevin  Valter  Dasha  Narang  Sharan  Mishra  Gaurav  Yu  Adams  Zhao  Vincent  Huang  Yanping  Dai  Andrew  Yu  Hongkun  Petrov  Slav  Chi  Ed H   Dean  Jeff  Devlin  Jacob  Roberts  Adam  Zhou  Denny  Le  Quoc V   Wei  Jason          Scaling Instruction Finetuned Language Models   arXiv             cs LG  

  Wei  Jason  Tay  Yi  November             Better Language Models Without Massive Compute   ai googleblog com  Retrieved March          

  a b Kojima  Takeshi  Shixiang Shane Gu  Reid  Machel  Matsuo  Yutaka  Iwasawa  Yusuke          Large Language Models are Zero Shot Reasoners   arXiv             cs CL  

  Dickson  Ben  August             LLMs have not learned our language   we re trying to learn theirs   VentureBeat  Retrieved March          

  Garg  Shivam  Tsipras  Dimitris  Liang  Percy  Valiant  Gregory          What Can Transformers Learn In Context  A Case Study of Simple Function Classes   arXiv             cs CL  

  Brown  Tom  Mann  Benjamin  Ryder  Nick  Subbiah  Melanie  Kaplan  Jared D   Dhariwal  Prafulla  Neelakantan  Arvind          Language models are few shot learners   Advances in Neural Information Processing Systems                 arXiv            

  Wei  Jason  Tay  Yi  Bommasani  Rishi  Raffel  Colin  Zoph  Barret  Borgeaud  Sebastian  Yogatama  Dani  Bosma  Maarten  Zhou  Denny  Metzler  Donald  Chi  Ed H   Hashimoto  Tatsunori  Vinyals  Oriol  Liang  Percy  Dean  Jeff  Fedus  William  August             Emergent Abilities of Large Language Models   arXiv             cs CL   In prompting  a pre trained language model is given a prompt  e g  a natural language instruction  of a task and completes the response without any further training or gradient updates to its parameters    The ability to perform a task via few shot prompting is emergent when a model has random performance until a certain scale  after which performance increases to well above random

  Caballero  Ethan  Gupta  Kshitij  Rish  Irina  Krueger  David  July            Broken Neural Scaling Laws  arXiv           

  Wei  Jason  Tay  Yi  Bommasani  Rishi  Raffel  Colin  Zoph  Barret  Borgeaud  Sebastian  Yogatama  Dani  Bosma  Maarten  Zhou  Denny  Metzler  Donald  Chi  Ed H   Hashimoto  Tatsunori  Vinyals  Oriol  Liang  Percy  Dean  Jeff  Fedus  William  August             Emergent Abilities of Large Language Models   arXiv             cs CL  

  Musser  George   How AI Knows Things No One Told It   Scientific American  Retrieved May           By the time you type a query into ChatGPT  the network should be fixed  unlike humans  it should not continue to learn  So it came as a surprise that LLMs do  in fact  learn from their users  prompts an ability known as in context learning 

  Garg  Shivam  Tsipras  Dimitris  Liang  Percy  Valiant  Gregory          What Can Transformers Learn In Context  A Case Study of Simple Function Classes   NeurIPS  arXiv             Training a model to perform in context learning can be viewed as an instance of the more general learning to learn or meta learning paradigm

  Wang  Xuezhi  Wei  Jason  Schuurmans  Dale  Le  Quoc  Chi  Ed  Narang  Sharan  Chowdhery  Aakanksha  Zhou  Denny  March            Self Consistency Improves Chain of Thought Reasoning in Language Models   arXiv             cs CL  

  Diao  Shizhe  Wang  Pengcheng  Lin  Yong  Zhang  Tong  February            Active Prompting with Chain of Thought for Large Language Models   arXiv             cs CL  

  Yao  Shunyu  Yu  Dian  Zhao  Jeffrey  Shafran  Izhak  Griffiths  Thomas L   Cao  Yuan  Narasimhan  Karthik  May             Tree of Thoughts  Deliberate Problem Solving with Large Language Models   arXiv             cs CL  

  OpenAI and over     people  March             GPT   Technical Report   arXiv             cs CL    See Figure    

  a b Sclar  Melanie  Choi  Yejin  Tsvetkov  Yulia  Suhr  Alane  July            Quantifying Language Models  Sensitivity to Spurious Features in Prompt Design or  How I learned to start worrying about prompt formatting   arXiv             cs CL  

  Leidinger  Alina  van Rooij  Robert  Shutova  Ekaterina         Bouamor  Houda  Pino  Juan  Bali  Kalika  eds     The language of prompting  What linguistic properties make a prompt successful    Findings of the Association for Computational Linguistics  EMNLP       Singapore  Association for Computational Linguistics             arXiv             doi          v       findings emnlp     

  Linzbach  Stephan  Dimitrov  Dimitar  Kallmeyer  Laura  Evang  Kilian  Jabeen  Hajira  Dietze  Stefan  June         Dissecting Paraphrases  The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models   In Duh  Kevin  Gomez  Helena  Bethard  Steven  eds    Proceedings of the      Conference of the North American Chapter of the Association for Computational Linguistics  Human Language Technologies  Volume    Long Papers   Mexico City  Mexico  Association for Computational Linguistics  pp                  arXiv             doi          v       naacl long     

  Polo  Felipe Maia  Xu  Ronald  Weber  Lucas  Silva  M rian  Bhardwaj  Onkar  Choshen  Leshem  de Oliveira  Allysson Flavio Melo  Sun  Yuekai  Yurochkin  Mikhail  October             Efficient multi prompt evaluation of LLMs   arXiv             cs CL  

   Why Google s AI Overviews gets things wrong   MIT Technology Review  May           Retrieved March         

   Can a technology called RAG keep AI models from making stuff up    Ars Technica  June          Retrieved March         

  Larson  Jonathan  Truitt  Steven  February            GraphRAG  Unlocking LLM discovery on narrative private data  Microsoft

  Edge  Darren  Trinh  Ha  Cheng  Newman  Bradley  Joshua  Chao  Alex  Mody  Apurva  Truitt  Steven  Larson  Jonathan          From Local to Global  A Graph RAG Approach to Query Focused Summarization   arXiv             cs CL  

  Sequeda  Juan  Allemang  Dean  Jacob  Bryon          A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model s Accuracy for Question Answering on Enterprise SQL Databases   arXiv             cs AI  

  Singh  Chandan  Morris  John  Aneja  Jyoti  Rush  Alexander  Gao  Jianfeng  October            Explaining Patterns in Data with Language Models via Interpretable Autoprompting   arXiv             cs LG  

  Zhou  Yongchao  Ioan Muresanu  Andrei  Han  Ziwen  Paster  Keiran  Pitis  Silviu  Chan  Harris  Ba  Jimmy  November            Large Language Models Are Human Level Prompt Engineers   arXiv             cs LG  

  Pryzant  Reid  Iter  Dan  Li  Jerry  Lee  Yin Tat  Zhu  Chenguang  Zeng  Michael          Automatic Prompt Optimization with  Gradient Descent  and Beam Search   Microsoft Azure AI  arXiv            

  Zhang  Zhuosheng  Zhang  Aston  Li  Mu  Smola  Alex  October            Automatic Chain of Thought Prompting in Large Language Models   arXiv             cs CL  

  Monge  Jim Clyde  August             Dall E  VS Stable Diffusion  Same Prompt  Different Results   MLearning ai  Retrieved August          

  a b c  Prompts   docs midjourney com  Retrieved August          

  Max Woolf  November             Stable Diffusion     and the Importance of Negative Prompts for Good Results   Retrieved August          

  Goldblum  R   Pillarisetty  R   Dauphinee  M  J   Talal  N           Acceleration of autoimmunity in NZB NZW F  mice by graft versus host disease   Clinical and Experimental Immunology                   ISSN                 PMC               PMID           

   Stable Diffusion prompt  a definitive guide   May           Retrieved August          

  Heikkil   Melissa  September             This Artist Is Dominating AI Generated Art and He s Not Happy About It   MIT Technology Review  Retrieved August          

  Solomon  Tessa  August             The AI Powered Ask Dal  and Hello Vincent Installations Raise Uncomfortable Questions about Ventriloquizing the Dead   ARTnews com  Retrieved January          

  Gal  Rinon  Alaluf  Yuval  Atzmon  Yuval  Patashnik  Or  Bermano  Amit H   Chechik  Gal  Cohen Or  Daniel          An Image is Worth One Word  Personalizing Text to Image Generation using Textual Inversion   arXiv             cs CV   Using only     images of a user provided concept  like an object or a style  we learn to represent it through new  words  in the embedding space of a frozen text to image model 

  Kirillov  Alexander  Mintun  Eric  Ravi  Nikhila  Mao  Hanzi  Rolland  Chloe  Gustafson  Laura  Xiao  Tete  Whitehead  Spencer  Berg  Alexander C   Lo  Wan Yen  Doll r  Piotr  Girshick  Ross  April            Segment Anything   arXiv             cs CV  

  Li  Xiang Lisa  Liang  Percy          Prefix Tuning  Optimizing Continuous Prompts for Generation   Proceedings of the   th Annual Meeting of the Association for Computational Linguistics and the   th International Joint Conference on Natural Language Processing  Volume    Long Papers   pp                  doi          V       ACL LONG      S CID                 In this paper  we propose prefix tuning  a lightweight alternative to fine tuning    Prefix tuning draws inspiration from prompting

  Lester  Brian  Al Rfou  Rami  Constant  Noah          The Power of Scale for Parameter Efficient Prompt Tuning   Proceedings of the      Conference on Empirical Methods in Natural Language Processing  pp                  arXiv             doi          V       EMNLP MAIN      S CID                 In this work  we explore  prompt tuning   a simple yet effective mechanism for learning  soft prompts    Unlike the discrete text prompts used by GPT    soft prompts are learned through back propagation

  Sun  Simeng  Liu  Yang  Iter  Dan  Zhu  Chenguang  Iyyer  Mohit          How Does In Context Learning Help Prompt Tuning    arXiv             cs CL  

  Shin  Taylor  Razeghi  Yasaman  Logan IV  Robert L   Wallace  Eric  Singh  Sameer  November         AutoPrompt  Eliciting Knowledge from Language Models with Automatically Generated Prompts   Proceedings of the      Conference on Empirical Methods in Natural Language Processing  EMNLP   Online  Association for Computational Linguistics  pp                  doi          v       emnlp main      S CID                

  Meincke  Lennart and Mollick  Ethan R  and Mollick  Lilach and Shapiro  Dan  Prompting Science Report    Prompt Engineering is Complicated and Contingent  March            Available at SSRN  https   papers ssrn com sol  papers cfm abstract id        

  Bousquette  Isabelle  April             The Hottest AI Job of      Is Already Obsolete   Wall Street Journal  ISSN                 Retrieved May         

  Vigliarolo  Brandon  September             GPT    prompt injection  attack causes bot bad manners   The Register  Retrieved February         

   What is a prompt injection attack    IBM  March           Retrieved March         





Scholia has a topic profile for Prompt engineering 

vteGenerative AIConcepts
Autoencoder
Deep learning
Generative adversarial network
Generative pre trained transformer
Large language model
Neural network
Prompt engineering
Retrieval augmented generation
Reinforcement learning from human feedback
Self supervised learning
Transformer
Variational autoencoder
Vision transformer
Word embedding
ModelsText
Claude
DBRX
DeepSeek
ERNIE
Gemini
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Granite
Grok
Llama
Manus
Mistral Large
PanGu  
Qwen
Image
Aurora
DALL E
Firefly
Flux
GPT Image  
Ideogram
Imagen
Midjourney
Stable Diffusion
Speech
   ai
WaveNet
Video
Dream Machine
Gen  
Hailuo AI
Kling
Sora
Veo
VideoPoet
Music
Endel
Suno AI
Udio
Companies
   AI
Alibaba
Anthropic
Baichuan
Baidu
DeepSeek
ElevenLabs
Google DeepMind
Hugging Face
Kuaishou
Meta AI
MiniMax
Mistral AI
Moonshot AI
OpenAI
Runway
Stability AI
Synthesia
xAI
Zhipu AI

 Category
 Commons

vteArtificial intelligence  AI History  timeline Concepts
Parameter
Hyperparameter
Loss functions
Regression
Bias variance tradeoff
Double descent
Overfitting
Clustering
Gradient descent
SGD
Quasi Newton method
Conjugate gradient method
Backpropagation
Attention
Convolution
Normalization
Batchnorm
Activation
Softmax
Sigmoid
Rectifier
Gating
Weight initialization
Regularization
Datasets
Augmentation
Prompt engineering
Reinforcement learning
Q learning
SARSA
Imitation
Policy gradient
Diffusion
Latent diffusion model
Autoregression
Adversary
RAG
Uncanny valley
RLHF
Self supervised learning
Recursive self improvement
Word embedding
Hallucination
Applications
Machine learning
In context learning
Artificial neural network
Deep learning
Language model
Large language model
NMT
Artificial general intelligence  AGI 
ImplementationsAudio visual
AlexNet
WaveNet
Human image synthesis
HWR
OCR
Speech synthesis
   ai
ElevenLabs
Speech recognition
Whisper
Facial recognition
AlphaFold
Text to image models
Aurora
DALL E
Firefly
Flux
Ideogram
Imagen
Midjourney
Stable Diffusion
Text to video models
Dream Machine
Runway Gen
Hailuo AI
Kling
Sora
Veo
Music generation
Suno AI
Udio
Text
Word vec
Seq seq
GloVe
BERT
T 
Llama
Chinchilla AI
PaLM
GPT
 
 
 
J
ChatGPT
 
 o
o 
o 
   
   
o 
Claude
Gemini
chatbot
Grok
LaMDA
BLOOM
Project Debater
IBM Watson
IBM Watsonx
Granite
PanGu  
DeepSeek
Qwen
Decisional
AlphaGo
AlphaZero
OpenAI Five
Self driving car
MuZero
Action selection
AutoGPT
Robot control
People
Alan Turing
Warren Sturgis McCulloch
Walter Pitts
John von Neumann
Claude Shannon
Marvin Minsky
John McCarthy
Nathaniel Rochester
Allen Newell
Cliff Shaw
Herbert A  Simon
Oliver Selfridge
Frank Rosenblatt
Bernard Widrow
Joseph Weizenbaum
Seymour Papert
Seppo Linnainmaa
Paul Werbos
J rgen Schmidhuber
Yann LeCun
Geoffrey Hinton
John Hopfield
Yoshua Bengio
Lotfi A  Zadeh
Stephen Grossberg
Alex Graves
Andrew Ng
Fei Fei Li
Alex Krizhevsky
Ilya Sutskever
Demis Hassabis
David Silver
Ian Goodfellow
Andrej Karpathy
James Goodnight
Architectures
Neural Turing machine
Differentiable neural computer
Transformer
Vision transformer  ViT 
Recurrent neural network  RNN 
Long short term memory  LSTM 
Gated recurrent unit  GRU 
Echo state network
Multilayer perceptron  MLP 
Convolutional neural network  CNN 
Residual neural network  RNN 
Highway network
Mamba
Autoencoder
Variational autoencoder  VAE 
Generative adversarial network  GAN 
Graph neural network  GNN 

 Portals
Technology
 Category
Artificial neural networks
Machine learning
 List
Companies
Projects






Retrieved from  https   en wikipedia org w index php title Prompt engineering amp oldid