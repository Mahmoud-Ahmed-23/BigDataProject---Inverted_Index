Mathematical model for sequential decision making under uncertainty
Markov decision process  MDP   also called a stochastic dynamic program or stochastic control problem  is a model for sequential decision making when outcomes are uncertain            
Originating from operations research in the     s                        MDPs have since gained recognition in a variety of fields  including ecology  economics  healthcare  telecommunications and reinforcement learning             Reinforcement learning utilizes the MDP framework to model the interaction between a learning agent and its environment  In this framework  the interaction is characterized by states  actions  and rewards  The MDP framework is designed to provide a simplified representation of key elements of artificial intelligence challenges  These elements encompass the understanding of cause and effect  the management of uncertainty and nondeterminism  and the pursuit of explicit goals            
The name comes from its connection to Markov chains  a concept developed by the Russian mathematician Andrey Markov  The  Markov  in  Markov decision process  refers to the underlying structure of state transitions that still follow the Markov property  The process is called a  decision process  because it involves making decisions that influence these state transitions  extending the concept of a Markov chain into the realm of decision making under uncertainty 


Definition edit 
Example of a simple MDP with three states  green circles  and two actions  orange circles   with two rewards  orange arrows 
A Markov decision process is a   tuple 
  
    
      
         
        S
         
        A
         
        
          P
          
            a
          
        
         
        
          R
          
            a
          
        
         
      
    
      displaystyle  S A P  a  R  a   
  
  where 


  
    
      
        S
      
    
      displaystyle S 
  
 is a set of states called the state space  The state space may be discrete or continuous  like the set of real numbers 

  
    
      
        A
      
    
      displaystyle A 
  
 is a set of actions called the action space  alternatively  
  
    
      
        
          A
          
            s
          
        
      
    
      displaystyle A  s  
  
 is the set of actions available from state 
  
    
      
        s
      
    
      displaystyle s 
  
   As for state  this set may be discrete or continuous 

  
    
      
        
          P
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
      
    
      displaystyle P  a  s s   
  
 is  on an intuitive level  the probability that action 
  
    
      
        a
      
    
      displaystyle a 
  
 in state 
  
    
      
        s
      
    
      displaystyle s 
  
 at time 
  
    
      
        t
      
    
      displaystyle t 
  
 will lead to state 
  
    
      
        
          s
            x     
        
      
    
      displaystyle s  
  
 at time 
  
    
      
        t
         
         
      
    
      displaystyle t   
  
  In general  this probability transition is defined to satisfy 
  
    
      
        Pr
         
        
          s
          
            t
             
             
          
        
          x     
        
          S
            x     
        
          x     
        
          s
          
            t
          
        
         
        s
         
        
          a
          
            t
          
        
         
        a
         
         
        
            x   b 
          
            
              S
                x     
            
          
        
        
          P
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
        d
        
          s
            x     
        
         
      
    
      displaystyle  Pr s  t    in S  mid s  t  s a  t  a   int   S  P  a  s s  ds   
  
 for every 
  
    
      
        
          S
            x     
        
          x     
        S
      
    
      displaystyle S  subseteq S 
  
 measurable  In case the state space is discrete  the integral is intended with respect to the counting measure  so that the latter simplifies as  
  
    
      
        
          P
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
         
        Pr
         
        
          s
          
            t
             
             
          
        
         
        
          s
            x     
        
          x     
        
          s
          
            t
          
        
         
        s
         
        
          a
          
            t
          
        
         
        a
         
      
    
      displaystyle P  a  s s    Pr s  t    s  mid s  t  s a  t  a  
  
  In case 
  
    
      
        S
          x     
        
          
            R
          
          
            d
          
        
      
    
      displaystyle S subseteq  mathbb  R    d  
  
  the integral is usually intended with respect to the Lebesgue measure 

  
    
      
        
          R
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
      
    
      displaystyle R  a  s s   
  
 is the immediate reward  or expected immediate reward  received after transitioning from state 
  
    
      
        s
      
    
      displaystyle s 
  
 to state 
  
    
      
        
          s
            x     
        
      
    
      displaystyle s  
  
  due to action 
  
    
      
        a
      
    
      displaystyle a 
  
 
A policy function 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
 is a  potentially probabilistic  mapping from state space  
  
    
      
        S
      
    
      displaystyle S 
  
  to action space  
  
    
      
        A
      
    
      displaystyle A 
  
  

Optimization objective edit 
The goal in a Markov decision process is to find a good  policy  for the decision maker  a function 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
 that specifies the action 
  
    
      
          x c  
         
        s
         
      
    
      displaystyle  pi  s  
  
 that the decision maker will choose when in state 
  
    
      
        s
      
    
      displaystyle s 
  
  Once a Markov decision process is combined with a policy in this way  this fixes the action for each state and the resulting combination behaves like a Markov chain  since the action chosen in state 
  
    
      
        s
      
    
      displaystyle s 
  
 is completely determined by 
  
    
      
          x c  
         
        s
         
      
    
      displaystyle  pi  s  
  
  
The objective is to choose a policy 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
 that will maximize some cumulative function of the random rewards  typically the expected discounted sum over a potentially infinite horizon 


  
    
      
        E
        
           
          
            
                x     
              
                t
                 
                 
              
              
                  x   e 
              
            
            
              
                  x b  
                
                  t
                
              
              
                R
                
                  
                    a
                    
                      t
                    
                  
                
              
               
              
                s
                
                  t
                
              
               
              
                s
                
                  t
                   
                   
                
              
               
            
          
           
        
      
    
      displaystyle E left  sum   t      infty    gamma   t R  a  t   s  t  s  t      right  
  
  where we choose 
  
    
      
        
          a
          
            t
          
        
         
          x c  
         
        
          s
          
            t
          
        
         
      
    
      displaystyle a  t   pi  s  t   
  
  i e  actions given by the policy   And the expectation is taken over 
  
    
      
        
          s
          
            t
             
             
          
        
          x   c 
        
          P
          
            
              a
              
                t
              
            
          
        
         
        
          s
          
            t
          
        
         
        
          s
          
            t
             
             
          
        
         
      
    
      displaystyle s  t    sim P  a  t   s  t  s  t     
  

where 
  
    
      
          xa  
          x b  
          xa  
      
    
      displaystyle    gamma    
  
 is the discount factor satisfying 
  
    
      
         
          x     
          xa  
          x b  
          xa  
          x     
          xa  
         
      
    
      displaystyle   leq    gamma    leq     
  
  which is usually close to 
  
    
      
         
      
    
      displaystyle   
  
  for example  
  
    
      
          x b  
         
         
        
           
        
         
         
         
        r
         
      
    
      displaystyle  gamma       r  
  
 for some discount rate 
  
    
      
        r
      
    
      displaystyle r 
  
   A lower discount factor motivates the decision maker to favor taking actions early  rather than postpone them indefinitely 
Another possible  but strictly related  objective that is commonly used is the 
  
    
      
        H
          x     
      
    
      displaystyle H  
  
step return  This time  instead of using a discount factor 
  
    
      
          xa  
          x b  
          xa  
      
    
      displaystyle    gamma    
  
  the agent is interested only in the first 
  
    
      
        H
      
    
      displaystyle H 
  
 steps of the process  with each reward having the same weight 


  
    
      
        E
        
           
          
            
                x     
              
                t
                 
                 
              
              
                H
                  x     
                 
              
            
            
              
                R
                
                  
                    a
                    
                      t
                    
                  
                
              
               
              
                s
                
                  t
                
              
               
              
                s
                
                  t
                   
                   
                
              
               
            
          
           
        
      
    
      displaystyle E left  sum   t     H    R  a  t   s  t  s  t      right  
  
  where we choose 
  
    
      
        
          a
          
            t
          
        
         
          x c  
         
        
          s
          
            t
          
        
         
      
    
      displaystyle a  t   pi  s  t   
  
  i e  actions given by the policy   And the expectation is taken over 
  
    
      
        
          s
          
            t
             
             
          
        
          x   c 
        
          P
          
            
              a
              
                t
              
            
          
        
         
        
          s
          
            t
          
        
         
        
          s
          
            t
             
             
          
        
         
      
    
      displaystyle s  t    sim P  a  t   s  t  s  t     
  

where 
  
    
      
          xa  
        H
          xa  
      
    
      displaystyle   H   
  
 is the time horizon  Compared to the previous objective  the latter one is more used in Learning Theory 
A policy that maximizes the function above is called an optimal policy and is usually denoted 
  
    
      
        
            x c  
          
              x     
          
        
      
    
      displaystyle  pi      
  
  A particular MDP may have multiple distinct optimal policies  Because of the Markov property  it can be shown that the optimal policy is a function of the current state  as assumed above 

Simulator models edit 
In many cases  it is difficult to represent the transition probability distributions  
  
    
      
        
          P
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
      
    
      displaystyle P  a  s s   
  
  explicitly  In such cases  a simulator can be used to model the MDP implicitly by providing samples from the transition distributions  One common form of implicit MDP model is an episodic environment simulator that can be started from an initial state and yields a subsequent state and reward every time it receives an action input  In this manner  trajectories of states  actions  and rewards  often called episodes may be produced 
Another form of simulator is a generative model  a single step simulator that can generate samples of the next state and reward given any state and action              Note that this is a different meaning from the term generative model in the context of statistical classification   In algorithms that are expressed using pseudocode  
  
    
      
        G
      
    
      displaystyle G 
  
 is often used to represent a generative model  For example  the expression 
  
    
      
        
          s
            x     
        
         
        r
          x     
        G
         
        s
         
        a
         
      
    
      displaystyle s  r gets G s a  
  
 might denote the action of sampling from the generative model where 
  
    
      
        s
      
    
      displaystyle s 
  
 and 
  
    
      
        a
      
    
      displaystyle a 
  
 are the current state and action  and 
  
    
      
        
          s
            x     
        
      
    
      displaystyle s  
  
 and 
  
    
      
        r
      
    
      displaystyle r 
  
 are the new state and reward  Compared to an episodic simulator  a generative model has the advantage that it can yield data from any state  not only those encountered in a trajectory 
These model classes form a hierarchy of information content  an explicit model trivially yields a generative model through sampling from the distributions  and repeated application of a generative model yields an episodic simulator  In the opposite direction  it is only possible to learn approximate models through regression  The type of model available for a particular MDP plays a significant role in determining which solution algorithms are appropriate  For example  the dynamic programming algorithms described in the next section require an explicit model  and Monte Carlo tree search requires a generative model  or an episodic simulator that can be copied at any state   whereas most reinforcement learning algorithms require only an episodic simulator 

Example edit 
Pole Balancing example  rendering of the environment from the Open AI gym benchmark 
An example of MDP is the Pole Balancing model  which comes from classic control theory 
In this example  we have


  
    
      
        S
      
    
      displaystyle S 
  
 is the set of ordered tuples 
  
    
      
         
          x b  
         
        
          
            
                x b  
                x d  
            
          
        
         
        x
         
        
          
            
              x
                x d  
            
          
        
         
          x     
        
          
            R
          
          
             
          
        
      
    
      displaystyle   theta    dot   theta    x   dot  x    subset  mathbb  R       
  
 given by pole angle  angular velocity  position of the cart and its speed 

  
    
      
        A
      
    
      displaystyle A 
  
 is 
  
    
      
         
          x     
         
         
         
         
      
    
      displaystyle          
  
  corresponding to applying a force on the left  right  on the cart 

  
    
      
        
          P
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
      
    
      displaystyle P  a  s s   
  
 is the transition of the system  which in this case is going to be deterministic and driven by the laws of mechanics 

  
    
      
        
          R
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
      
    
      displaystyle R  a  s s   
  
 is 
  
    
      
         
      
    
      displaystyle   
  
 if the pole is up after the transition  zero otherwise  Therefore  this function only depend on 
  
    
      
        
          s
            x     
        
      
    
      displaystyle s  
  
 in this specific case 
Algorithms edit 
Solutions for MDPs with finite state and action spaces may be found through a variety of methods such as dynamic programming  The algorithms in this section apply to MDPs with finite state and action spaces and explicitly given transition probabilities and reward functions  but the basic concepts may be extended to handle other problem classes  for example using function approximation  Also  some processes with countably infinite state and action spaces can be exactly reduced to ones with finite state and action spaces            
The standard family of algorithms to calculate optimal policies for finite state and action MDPs requires storage for two arrays indexed by state  value 
  
    
      
        V
      
    
      displaystyle V 
  
  which contains real values  and policy 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
  which contains actions  At the end of the algorithm  
  
    
      
          x c  
      
    
      displaystyle  pi  
  
 will contain the solution and 
  
    
      
        V
         
        s
         
      
    
      displaystyle V s  
  
 will contain the discounted sum of the rewards to be earned  on average  by following that solution from state 
  
    
      
        s
      
    
      displaystyle s 
  
 
The algorithm has two steps      a value update and     a policy update  which are repeated in some order for all the states until no further changes take place   Both recursively update a new estimation of the optimal policy and state value using an older estimation of those values 


  
    
      
        V
         
        s
         
          
        
            x     
          
            
              s
                x     
            
          
        
        
          P
          
              x c  
             
            s
             
          
        
         
        s
         
        
          s
            x     
        
         
        
           
          
            
              R
              
                  x c  
                 
                s
                 
              
            
             
            s
             
            
              s
                x     
            
             
             
              x b  
            V
             
            
              s
                x     
            
             
          
           
        
      
    
      displaystyle V s    sum   s  P   pi  s   s s   left R   pi  s   s s    gamma V s   right  
  


  
    
      
          x c  
         
        s
         
          
        
          argmax
          
            a
          
        
          x     
        
           
          
            
                x     
              
                
                  s
                    x     
                
              
            
            
              P
              
                a
              
            
             
            s
             
            
              s
                x     
            
             
            
               
              
                
                  R
                  
                    a
                  
                
                 
                s
                 
                
                  s
                    x     
                
                 
                 
                  x b  
                V
                 
                
                  s
                    x     
                
                 
              
               
            
          
           
        
      
    
      displaystyle  pi  s    operatorname  argmax    a  left   sum   s  P  a  s s   left R  a  s s    gamma V s   right  right   
  

Their order depends on the variant of the algorithm  one can also do them for all states at once or state by state  and more often to some states than others  As long as no state is permanently excluded from either of the steps  the algorithm will eventually arrive at the correct solution            

Notable variants edit 
Value iteration edit 
In value iteration  Bellman        which is also called backward induction 
the 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
 function is not used  instead  the value of 
  
    
      
          x c  
         
        s
         
      
    
      displaystyle  pi  s  
  
 is calculated within 
  
    
      
        V
         
        s
         
      
    
      displaystyle V s  
  
 whenever it is needed  Substituting the calculation of 
  
    
      
          x c  
         
        s
         
      
    
      displaystyle  pi  s  
  
 into the calculation of 
  
    
      
        V
         
        s
         
      
    
      displaystyle V s  
  
 gives the combined step     further explanation needed      


  
    
      
        
          V
          
            i
             
             
          
        
         
        s
         
          
        
          max
          
            a
          
        
        
           
          
            
                x     
              
                
                  s
                    x     
                
              
            
            
              P
              
                a
              
            
             
            s
             
            
              s
                x     
            
             
            
               
              
                
                  R
                  
                    a
                  
                
                 
                s
                 
                
                  s
                    x     
                
                 
                 
                  x b  
                
                  V
                  
                    i
                  
                
                 
                
                  s
                    x     
                
                 
              
               
            
          
           
        
         
      
    
      displaystyle V  i    s    max   a  left   sum   s  P  a  s s   left R  a  s s    gamma V  i  s   right  right    
  

where 
  
    
      
        i
      
    
      displaystyle i 
  
 is the iteration number  Value iteration starts at 
  
    
      
        i
         
         
      
    
      displaystyle i   
  
 and 
  
    
      
        
          V
          
             
          
        
      
    
      displaystyle V     
  
 as a guess of the value function  It then iterates  repeatedly computing 
  
    
      
        
          V
          
            i
             
             
          
        
      
    
      displaystyle V  i    
  
 for all states 
  
    
      
        s
      
    
      displaystyle s 
  
  until 
  
    
      
        V
      
    
      displaystyle V 
  
 converges with the left hand side equal to the right hand side  which is the  Bellman equation  for this problem     clarification needed        Lloyd Shapley s      paper on stochastic games included as a special case the value iteration method for MDPs             but this was recognized only later on            

Policy iteration edit 
In policy iteration  Howard       harv error  no target  CITEREFHoward      help   step one is performed once  and then step two is performed once  then both are repeated until policy converges  Then step one is again performed once and so on   Policy iteration was invented by Howard to optimize Sears catalogue mailing  which he had been optimizing using value iteration              
Instead of repeating step two to convergence  it may be formulated and solved as a set of linear equations  These equations are merely obtained by making 
  
    
      
        s
         
        
          s
            x     
        
      
    
      displaystyle s s  
  
 in the step two equation      clarification needed      Thus  repeating step two to convergence can be interpreted as solving the linear equations by relaxation 
This variant has the advantage that there is a definite stopping condition  when the array 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
 does not change in the course of applying step   to all states  the algorithm is completed 
Policy iteration is usually slower than value iteration for a large number of possible states 

Modified policy iteration edit 
In modified policy iteration  van Nunen       Puterman  amp  Shin        step one is performed once  and then step two is repeated several times                          Then step one is again performed once and so on 

Prioritized sweeping edit 
In this variant  the steps are preferentially applied to states which are in some way important   whether based on the algorithm  there were large changes in 
  
    
      
        V
      
    
      displaystyle V 
  
 or 
  
    
      
          x c  
      
    
      displaystyle  pi  
  
 around those states recently  or based on use  those states are near the starting state  or otherwise of interest to the person or program using the algorithm  

Computational complexity edit 
Algorithms for finding optimal policies with time complexity polynomial in the size of the problem representation exist for finite MDPs  Thus  decision problems based on MDPs are in computational complexity class P              However  due to the curse of dimensionality  the size of the problem representation is often exponential in the number of state and action variables  limiting exact solution techniques to problems that have a compact representation  In practice  online planning techniques such as Monte Carlo tree search can find useful solutions in larger problems  and  in theory  it is possible to construct online planning algorithms that can find an arbitrarily near optimal policy with no computational complexity dependence on the size of the state space             

Extensions and generalizations edit 
A Markov decision process is a stochastic game with only one player 

Partial observability edit 
Main article  Partially observable Markov decision process
The solution above assumes that the state 
  
    
      
        s
      
    
      displaystyle s 
  
 is known when action is to be taken  otherwise 
  
    
      
          x c  
         
        s
         
      
    
      displaystyle  pi  s  
  
 cannot be calculated  When this assumption is not true  the problem is called a partially observable Markov decision process or POMDP 

Constrained Markov decision processes edit 
Constrained Markov decision processes  CMDPS  are extensions to Markov decision process  MDPs   There are three fundamental differences between MDPs and CMDPs             

There are multiple costs incurred after applying an action instead of one 
CMDPs are solved with linear programs only  and dynamic programming does not work 
The final policy depends on the starting state 
The method of Lagrange multipliers applies to CMDPs 
Many Lagrangian based algorithms have been developed 

Natural policy gradient primal dual method             
There are a number of applications for CMDPs  It has recently been used in motion planning scenarios in robotics             

Continuous time Markov decision process edit 
In discrete time Markov Decision Processes  decisions are made at discrete time intervals  However  for continuous time Markov decision processes  decisions can be made at any time the decision maker chooses  In comparison to discrete time Markov decision processes  continuous time Markov decision processes can better model the decision making process for a system that has continuous dynamics  i e        the system dynamics is defined by ordinary differential equations  ODEs   These kind of applications raise in queueing systems  epidemic processes  and population processes 
Like the discrete time Markov decision processes  in continuous time Markov decision processes the agent aims at finding the optimal policy which could maximize the expected cumulated reward  The only difference with the standard case stays in the fact that  due to the continuous nature of the time variable  the sum is replaced by an integral 


  
    
      
        max
        
          E
          
              x c  
          
        
          x     
        
           
          
            
              
              
                
                    x   b 
                  
                     
                  
                  
                      x   e 
                  
                
                
                    x b  
                  
                    t
                  
                
                r
                 
                s
                 
                t
                 
                 
                  x c  
                 
                s
                 
                t
                 
                 
                 
                
                d
                t
                
              
               
            
            
              s
              
                 
              
            
          
           
        
      
    
      displaystyle  max  operatorname  E     pi   left  left  int        infty   gamma   t r s t   pi  s t     dt   right s     right  
  

where 
  
    
      
         
          x     
          x b  
         lt 
          
      
    
      displaystyle   leq  gamma  lt    
  


Discrete space  Linear programming formulation edit 
If the state space and action space are finite  we could use linear programming to find the optimal policy  which was one of the earliest approaches applied  Here we only consider the ergodic model  which means our continuous time MDP becomes an ergodic continuous time Markov chain under a stationary policy  Under this assumption  although the decision maker can make a decision at any time in the current state  there is no benefit in taking multiple actions  It is better to take an action only at the time when system is transitioning from the current state to another state  Under some conditions              if our optimal value function 
  
    
      
        
          V
          
              x     
          
        
      
    
      displaystyle V     
  
 is independent of state 
  
    
      
        i
      
    
      displaystyle i 
  
  we will have the following inequality 


  
    
      
        g
          x     
        R
         
        i
         
        a
         
         
        
            x     
          
            j
              x     
            S
          
        
        q
         
        j
          x     
        i
         
        a
         
        h
         
        j
         
        
          x     
        i
          x     
        S
        
            xa  and  xa  
        
        a
          x     
        A
         
        i
         
      
    
      displaystyle g geq R i a   sum   j in S q j mid i a h j  quad  forall i in S  text  and   a in A i  
  

If there exists a function 
  
    
      
        h
      
    
      displaystyle h 
  
  then 
  
    
      
        
          
            
              
                V
                  xaf 
              
            
          
          
              x     
          
        
      
    
      displaystyle   bar  V       
  
 will be the smallest 
  
    
      
        g
      
    
      displaystyle g 
  
 satisfying the above equation  In order to find 
  
    
      
        
          
            
              
                V
                  xaf 
              
            
          
          
              x     
          
        
      
    
      displaystyle   bar  V       
  
  we could use the following linear programming model 

Primal linear program P LP 

  
    
      
        
          
            
              
                
                  Minimize
                
                
              
              
                g
              
            
            
              
                
                  s t
                
                
              
              
                g
                  x     
                
                    x     
                  
                    j
                      x     
                    S
                  
                
                q
                 
                j
                  x     
                i
                 
                a
                 
                h
                 
                j
                 
                  x     
                R
                 
                i
                 
                a
                 
                
                
                  x     
                i
                  x     
                S
                 
                
                a
                  x     
                A
                 
                i
                 
              
            
          
        
      
    
      displaystyle   begin aligned   text Minimize   quad  amp g    text s t   quad  amp g  sum   j in S q j mid i a h j  geq R i a      forall i in S   a in A i  end aligned   
  

Dual linear program D LP 

  
    
      
        
          
            
              
                
                  Maximize
                
              
              
                
                
                    x     
                  
                    i
                      x     
                    S
                  
                
                
                    x     
                  
                    a
                      x     
                    A
                     
                    i
                     
                  
                
                R
                 
                i
                 
                a
                 
                y
                 
                i
                 
                a
                 
              
            
            
              
                
                  s t 
                
              
              
                
                
                    x     
                  
                    i
                      x     
                    S
                  
                
                
                    x     
                  
                    a
                      x     
                    A
                     
                    i
                     
                  
                
                q
                 
                j
                  x     
                i
                 
                a
                 
                y
                 
                i
                 
                a
                 
                 
                 
                
                  x     
                j
                  x     
                S
                 
              
            
            
              
              
                
                
                    x     
                  
                    i
                      x     
                    S
                  
                
                
                    x     
                  
                    a
                      x     
                    A
                     
                    i
                     
                  
                
                y
                 
                i
                 
                a
                 
                 
                 
                 
              
            
            
              
              
                y
                 
                i
                 
                a
                 
                  x     
                 
                
                  x     
                a
                  x     
                A
                 
                i
                 
                
                    xa  and  xa  
                
                  x     
                i
                  x     
                S
              
            
          
        
      
    
      displaystyle   begin aligned   text Maximize   amp  sum   i in S  sum   a in A i  R i a y i a     text s t    amp  sum   i in S  sum   a in A i  q j mid i a y i a    quad  forall j in S    amp  sum   i in S  sum   a in A i  y i a       amp y i a  geq   qquad  forall a in A i   text  and    forall i in S end aligned   
  


  
    
      
        y
         
        i
         
        a
         
      
    
      displaystyle y i a  
  
 is a feasible solution to the D LP if 
  
    
      
        y
         
        i
         
        a
         
      
    
      displaystyle y i a  
  
 is nonnative and satisfied the constraints in the D LP problem  A feasible solution 
  
    
      
        
          y
          
              x     
          
        
         
        i
         
        a
         
      
    
      displaystyle y     i a  
  
 to the D LP is said to be an optimal solution if


  
    
      
        
          
            
              
                
                    x     
                  
                    i
                      x     
                    S
                  
                
                
                    x     
                  
                    a
                      x     
                    A
                     
                    i
                     
                  
                
                R
                 
                i
                 
                a
                 
                
                  y
                  
                      x     
                  
                
                 
                i
                 
                a
                 
                  x     
                
                    x     
                  
                    i
                      x     
                    S
                  
                
                
                    x     
                  
                    a
                      x     
                    A
                     
                    i
                     
                  
                
                R
                 
                i
                 
                a
                 
                y
                 
                i
                 
                a
                 
              
            
          
        
      
    
      displaystyle   begin aligned  sum   i in S  sum   a in A i  R i a y     i a  geq  sum   i in S  sum   a in A i  R i a y i a  end aligned   
  

for all feasible solution 
  
    
      
        y
         
        i
         
        a
         
      
    
      displaystyle y i a  
  
 to the D LP  Once we have found the optimal solution 
  
    
      
        
          y
          
              x     
          
        
         
        i
         
        a
         
      
    
      displaystyle y     i a  
  
  we can use it to establish the optimal policies 

Continuous space  Hamilton Jacobi Bellman equation edit 
In continuous time MDP  if the state space and action space are continuous  the optimal criterion could be found by solving Hamilton Jacobi Bellman  HJB  partial differential equation  In order to discuss the HJB equation  we need to reformulate
our problem


  
    
      
        
          
            
              
                V
                 
                s
                 
                 
                 
                 
                 
                 
                 
                

                
              
              
                
                
                  max
                  
                    a
                     
                    t
                     
                     
                      x c  
                     
                    s
                     
                    t
                     
                     
                  
                
                
                    x   b 
                  
                     
                  
                  
                    T
                  
                
                r
                 
                s
                 
                t
                 
                 
                a
                 
                t
                 
                 
                
                d
                t
                 
                D
                 
                s
                 
                T
                 
                 
              
            
            
              
                
                  s t 
                
                
              
              
                
                  
                    
                      d
                      s
                       
                      t
                       
                    
                    
                      d
                      t
                    
                  
                
                 
                f
                 
                t
                 
                s
                 
                t
                 
                 
                a
                 
                t
                 
                 
              
            
          
        
      
    
      displaystyle   begin aligned V s          amp  max   a t   pi  s t    int       T r s t  a t    dt D s T      text s t    quad  amp   frac  ds t   dt   f t s t  a t   end aligned   
  


  
    
      
        D
         
          x  c  
         
      
    
      displaystyle D  cdot   
  
 is the terminal reward function  
  
    
      
        s
         
        t
         
      
    
      displaystyle s t  
  
 is the system state vector  
  
    
      
        a
         
        t
         
      
    
      displaystyle a t  
  
 is the system control vector we try to find  
  
    
      
        f
         
          x  c  
         
      
    
      displaystyle f  cdot   
  
 shows how the state vector changes over time  The Hamilton Jacobi Bellman equation is as follows 


  
    
      
         
         
        
          max
          
            u
          
        
         
        r
         
        t
         
        s
         
        a
         
         
        
          
            
                x     
              V
               
              t
               
              s
               
            
            
                x     
              x
            
          
        
        f
         
        t
         
        s
         
        a
         
         
      
    
      displaystyle    max   u  r t s a    frac   partial V t s    partial x  f t s a   
  

We could solve the equation to find the optimal control 
  
    
      
        a
         
        t
         
      
    
      displaystyle a t  
  
  which could give us the optimal value function 
  
    
      
        
          V
          
              x     
          
        
      
    
      displaystyle V     
  


Reinforcement learning edit 
Main article  Reinforcement learning
Reinforcement learning is an interdisciplinary area of machine learning and optimal control that has  as main objective  finding an approximately optimal policy for MDPs where transition probabilities and rewards are unknown             
Reinforcement learning can solve Markov Decision processes without explicit specification of the transition probabilities which are instead needed to perform policy iteration  In this setting  transition probabilities and rewards must be learned from experience  i e  by letting an agent interact with the MDP for a given number of steps  Both on a theoretical and on a practical level  effort is put in maximizing the sample efficiency  i e  minimimizing the number of samples needed to learn a policy whose performance is 
  
    
      
          x b  
          x     
      
    
      displaystyle  varepsilon   
  
close to the optimal one  due to the stochastic nature of the process  learning the optimal policy with a finite number of samples is  in general  impossible  

Reinforcement Learning for discrete MDPs edit 
For the purpose of this section  it is useful to define a further function  which corresponds to taking the action 
  
    
      
        a
      
    
      displaystyle a 
  
 and then continuing optimally  or according to whatever policy one currently has  


  
    
      
          xa  
        Q
         
        s
         
        a
         
         
        
            x     
          
            
              s
                x     
            
          
        
        
          P
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
         
        
          R
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
         
          x b  
        V
         
        
          s
            x     
        
         
         
         
          xa  
      
    
      displaystyle   Q s a   sum   s  P  a  s s   R  a  s s    gamma V s       
  

While this function is also unknown  experience during learning is based on 
  
    
      
         
        s
         
        a
         
      
    
      displaystyle  s a  
  
 pairs  together with the outcome 
  
    
      
        
          s
            x     
        
      
    
      displaystyle s  
  
  that is   I was in state 
  
    
      
        s
      
    
      displaystyle s 
  
 and I tried doing 
  
    
      
        a
      
    
      displaystyle a 
  
 and 
  
    
      
        
          s
            x     
        
      
    
      displaystyle s  
  
 happened    Thus  one has an array 
  
    
      
        Q
      
    
      displaystyle Q 
  
 and uses experience to update it directly  This is known as Q learning 

Other scopes edit 
Learning automata edit 
Main article  Learning automata
Another application of MDP process in machine learning theory is called learning automata  This is also one type of reinforcement learning if the environment is stochastic  The first detail learning automata paper is surveyed by Narendra and Thathachar         which were originally described explicitly as finite state automata              Similar to reinforcement learning  a learning automata algorithm also has the advantage of solving the problem when probability or rewards are unknown  The difference between learning automata and Q learning is that the former technique omits the memory of Q values  but updates the action probability directly to find the learning result  Learning automata is a learning scheme with a rigorous proof of convergence             
In learning automata theory  a stochastic automaton consists of 

a set x of possible inputs 
a set                 s   of possible internal states 
a set                 r   of possible outputs  or actions  with r   s 
an initial state probability vector p        p           ps      
a computable function A which after each time step t generates p t      from p t   the current input  and the current state  and
a function G        which generates the output at each time step 
The states of such an automaton correspond to the states of a  discrete state discrete parameter Markov process               At each time step t                the automaton reads an input from its environment  updates P t  to P t      by A  randomly chooses a successor state according to the probabilities P t      and outputs the corresponding action  The automaton s environment  in turn  reads the action and sends the next input to the automaton             

Category theoretic interpretation edit 
Other than the rewards  a Markov decision process 
  
    
      
         
        S
         
        A
         
        P
         
      
    
      displaystyle  S A P  
  
 can be understood in terms of Category theory  Namely  let 
  
    
      
        
          
            A
          
        
      
    
      displaystyle   mathcal  A   
  
 denote the free monoid with generating set A  Let Dist denote the Kleisli category of the Giry monad  Then a functor 
  
    
      
        
          
            A
          
        
          x     
        
          D
          i
          s
          t
        
      
    
      displaystyle   mathcal  A   to  mathbf  Dist   
  
 encodes both the set S of states and the probability function P 
In this way  Markov decision processes could be generalized from monoids  categories with one object  to arbitrary categories  One can call the result 
  
    
      
         
        
          
            C
          
        
         
        F
         
        
          
            C
          
        
          x     
        
          D
          i
          s
          t
        
         
      
    
      displaystyle    mathcal  C   F   mathcal  C   to  mathbf  Dist    
  
 a context dependent Markov decision process  because moving from one object to another in 
  
    
      
        
          
            C
          
        
      
    
      displaystyle   mathcal  C   
  
 changes the set of available actions and the set of possible states      citation needed     

Alternative notations edit 
The terminology and notation for MDPs are not entirely settled  There are two main streams   one focuses on maximization problems from contexts like economics  using the terms action  reward  value  and calling the discount factor        or         while the other focuses on minimization problems from engineering and navigation     citation needed       using the terms control  cost  cost to go  and calling the discount factor         In addition  the notation for the transition probability varies 



in this article
alternative
comment


action a
control u



reward R
cost g

g is the negative of R


value V
cost to go J

J is the negative of V


policy       
policy       



discounting factor       
discounting factor       



transition probability 
  
    
      
        
          P
          
            a
          
        
         
        s
         
        
          s
            x     
        
         
      
    
      displaystyle P  a  s s   
  

transition probability 
  
    
      
        
          p
          
            s
            
              s
                x     
            
          
        
         
        a
         
      
    
      displaystyle p  ss   a  
  



In addition  transition probability is sometimes written 
  
    
      
        Pr
         
        s
         
        a
         
        
          s
            x     
        
         
      
    
      displaystyle  Pr s a s   
  
  
  
    
      
        Pr
         
        
          s
            x     
        
          x     
        s
         
        a
         
      
    
      displaystyle  Pr s  mid s a  
  
 or  rarely  
  
    
      
        
          p
          
            
              s
                x     
            
            s
          
        
         
        a
         
         
      
    
      displaystyle p  s s  a   
  



See also edit 

Probabilistic automata
Odds algorithm
Quantum finite automata
Partially observable Markov decision process
Dynamic programming
Bellman equation for applications to economics 
Hamilton Jacobi Bellman equation
Optimal control
Recursive economics
Mabinogion sheep problem
Stochastic games
Q learning
Markov chain

References edit 


  Puterman  Martin L          Markov decision processes  discrete stochastic dynamic programming  Wiley series in probability and mathematical statistics  Applied probability and statistics section  New York  Wiley  ISBN                        

  Schneider  S   Wagner  D  H                 Error detection in redundant systems   Papers presented at the February              western joint computer conference  Techniques for reliability on   IRE AIEE ACM      Western   New York  NY  USA  Association for Computing Machinery  pp                doi                          ISBN                           cite book    ISBN   Date incompatibility  help 

  Bellman  Richard                Dynamic programming and stochastic control processes   Information and Control                  doi         S                      ISSN                

  a b Sutton  Richard S   Barto  Andrew G          Reinforcement learning  an introduction  Adaptive computation and machine learning series   nd      ed    Cambridge  Massachusetts  The MIT Press  ISBN                        

  Kearns  Michael  Mansour  Yishay  Ng  Andrew          A Sparse Sampling Algorithm for Near Optimal Planning in Large Markov Decision Processes   Machine Learning                         doi         A               

  Wrobel  A           On Markovian decision models with a finite skeleton   Zeitschrift f r Operations Research                 doi         bf          S CID              

  Reinforcement Learning  Theory and Python Implementation  Beijing  China Machine Press        p           ISBN                    

  Shapley  Lloyd          Stochastic Games   Proceedings of the National Academy of Sciences of the United States of America                      Bibcode     PNAS          S  doi         pnas             PMC               PMID               

  Kallenberg  Lodewijk          Finite state and action MDPs   In Feinberg  Eugene A   Shwartz  Adam  eds    Handbook of Markov decision processes  methods and applications  Springer  ISBN                        

  Howard        Comments on the Origin and Application of Markov Decision Processes 

  Puterman  M  L   Shin  M  C           Modified Policy Iteration Algorithms for Discounted Markov Decision Problems   Management Science                      doi         mnsc            

  van Nunen  J A  E  E          A set of successive approximation methods for discounted Markovian decision problems   Zeitschrift f r Operations Research                   doi         bf          S CID              

  Papadimitriou  Christos  Tsitsiklis  John          The Complexity of Markov Decision Processes   Mathematics of Operations Research                   doi         moor           hdl              Retrieved November         

  Kearns  Michael  Mansour  Yishay  Ng  Andrew  November         A Sparse Sampling Algorithm for Near Optimal Planning in Large Markov Decision Processes   Machine Learning                     doi         A               

  Altman  Eitan         Constrained Markov decision processes  Vol          CRC Press 

  
Ding  Dongsheng  Zhang  Kaiqing  Jovanovic  Mihailo  Basar  Tamer         Natural policy gradient primal dual method for constrained Markov decision processes  Advances in Neural Information Processing Systems 

  Feyzabadi  S   Carpin  S         Aug         Risk aware path planning using hierarchical constrained Markov Decision Processes   Automation Science and Engineering  CASE   IEEE International Conference  pp                

  Continuous Time Markov Decision Processes  Stochastic Modelling and Applied Probability  Vol                 doi                            ISBN                        

  Shoham  Y   Powers  R   Grenager  T           Multi agent reinforcement learning  a critical survey   PDF   Technical Report  Stanford University        Retrieved            

  Narendra  K  S   Thathachar  M  A  L           Learning Automata   A Survey   IEEE Transactions on Systems  Man  and Cybernetics  SMC                 CiteSeerX                       doi         TSMC               ISSN                

  a b Narendra  Kumpati S   Thathachar  Mandayam A  L          Learning automata  An introduction  Prentice Hall  ISBN                    

  Narendra  amp  Thathachar       p     left 


Sources edit 
Bellman  R          Dynamic Programming  Princeton University Press  ISBN                          citation    ISBN   Date incompatibility  help   Dover paperback edition       
Further reading edit 
Bellman   R  E                 Dynamic Programming  Dover paperback      ed    Princeton  NJ  Princeton University Press  ISBN                        
Bertsekas  D          Dynamic Programming and Optimal Control  Vol          MA  Athena 
Derman  C          Finite state Markovian decision processes  Academic Press 
Feinberg  E A   Shwartz  A   eds          Handbook of Markov Decision Processes  Boston  MA  Kluwer  ISBN                    
Guo  X   Hern ndez Lerma  O          Continuous Time Markov Decision Processes  Stochastic Modelling and Applied Probability  Springer  ISBN                    
Meyn  S  P          Control Techniques for Complex Networks  Cambridge University Press  ISBN                         Archived from the original on    June       Appendix contains abridged  Meyn  amp  Tweedie   Archived from the original on    December      
Puterman   M  L          Markov Decision Processes  Wiley 
Ross  S  M          Introduction to stochastic dynamic programming  PDF   Academic press 
Sutton  R  S   Barto  A  G          Reinforcement Learning  An Introduction  Cambridge  MA  The MIT Press 
Tijms   H C          A First Course in Stochastic Models  Wiley  ISBN                    





Retrieved from  https   en wikipedia org w index php title Markov decision process amp oldid